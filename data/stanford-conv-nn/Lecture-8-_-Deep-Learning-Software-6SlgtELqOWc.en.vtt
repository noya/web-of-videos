WEBVTT
Kind: captions
Language: en

00:00:09.739 --> 00:00:10.898
- Hello?

00:00:10.898 --> 00:00:13.891
Okay, it's after 12, so
I want to get started.

00:00:13.891 --> 00:00:16.151
So today, lecture eight,
we're going to talk about

00:00:16.151 --> 00:00:17.822
deep learning software.

00:00:17.822 --> 00:00:19.593
This is a super exciting
topic because it changes

00:00:19.593 --> 00:00:21.283
a lot every year.

00:00:21.283 --> 00:00:23.156
But also means it's a lot
of work to give this lecture

00:00:23.156 --> 00:00:25.621
'cause it changes a lot every year.

00:00:25.621 --> 00:00:28.302
But as usual, a couple
administrative notes

00:00:28.302 --> 00:00:30.024
before we dive into the material.

00:00:30.024 --> 00:00:32.323
So as a reminder the
project proposals for your

00:00:32.323 --> 00:00:34.563
course projects were due on Tuesday.

00:00:34.563 --> 00:00:37.405
So hopefully you all turned that in,

00:00:37.405 --> 00:00:39.704
and hopefully you all
have a somewhat good idea

00:00:39.704 --> 00:00:41.566
of what kind of projects
you want to work on

00:00:41.566 --> 00:00:42.766
for the class.

00:00:42.766 --> 00:00:45.852
So we're in the process of
assigning TA's to projects

00:00:45.852 --> 00:00:47.979
based on what the project area is

00:00:47.979 --> 00:00:50.217
and the expertise of the TA's.

00:00:50.217 --> 00:00:52.546
So we'll have some more
information about that

00:00:52.546 --> 00:00:54.264
in the next couple days I think.

00:00:54.264 --> 00:00:56.563
We're also in the process
of grading assignment one,

00:00:56.563 --> 00:00:59.724
so stay tuned and we'll get
those grades back to you

00:00:59.724 --> 00:01:00.942
as soon as we can.

00:01:00.942 --> 00:01:03.464
Another reminder is that
assignment two has been out

00:01:03.464 --> 00:01:04.449
for a while.

00:01:04.449 --> 00:01:08.680
That's going to be due next week,
a week from today, Thursday.

00:01:08.680 --> 00:01:10.848
And again, when working on assignment two,

00:01:10.848 --> 00:01:12.846
remember to stop your
Google Cloud instances

00:01:12.846 --> 00:01:16.231
when you're not working to
try to preserve your credits.

00:01:16.231 --> 00:01:18.264
And another bit of
confusion, I just wanted to

00:01:18.264 --> 00:01:21.326
re-emphasize is that for
assignment two you really

00:01:21.326 --> 00:01:24.812
only need to use GPU instances
for the last notebook.

00:01:24.812 --> 00:01:29.021
For all of the several
notebooks it's just in Python

00:01:29.021 --> 00:01:32.250
and Numpy so you don't need
any GPUs for those questions.

00:01:32.250 --> 00:01:34.414
So again, conserve your credits,

00:01:34.414 --> 00:01:36.701
only use GPUs when you need them.

00:01:36.701 --> 00:01:39.973
And the final reminder is
that the midterm is coming up.

00:01:39.973 --> 00:01:41.516
It's kind of hard to
believe we're there already,

00:01:41.516 --> 00:01:45.683
but the midterm will be in
class on Tuesday, five nine.

00:01:45.683 --> 00:01:47.901
So the midterm will be more theoretical.

00:01:47.901 --> 00:01:51.018
It'll be sort of pen and paper
working through different

00:01:51.018 --> 00:01:54.137
kinds of, slightly more
theoretical questions

00:01:54.137 --> 00:01:55.450
to check your understanding
of the material that we've

00:01:55.450 --> 00:01:57.071
covered so far.

00:01:57.071 --> 00:01:59.652
And I think we'll probably
post at least a short sort of

00:01:59.652 --> 00:02:02.506
sample of the types of
questions to expect.

00:02:02.506 --> 00:02:03.695
Question?

00:02:03.695 --> 00:02:05.310
[student's words obscured
due to lack of microphone]

00:02:05.310 --> 00:02:08.233
Oh yeah, question is
whether it's open-book,

00:02:08.233 --> 00:02:10.676
so we're going to say
closed note, closed book.

00:02:10.676 --> 00:02:12.037
So just,

00:02:12.037 --> 00:02:13.757
Yeah, yeah, so that's what
we've done in the past

00:02:13.757 --> 00:02:15.671
is just closed note,
closed book, relatively

00:02:15.671 --> 00:02:17.568
just like want to check
that you understand

00:02:17.568 --> 00:02:21.735
the intuition behind most of
the stuff we've presented.

00:02:23.618 --> 00:02:26.015
So, a quick recap as a reminder
of what we were talking

00:02:26.015 --> 00:02:27.577
about last time.

00:02:27.577 --> 00:02:29.737
Last time we talked about
fancier optimization algorithms

00:02:29.737 --> 00:02:33.162
for deep learning models
including SGD Momentum,

00:02:33.162 --> 00:02:34.975
Nesterov, RMSProp and Adam.

00:02:34.975 --> 00:02:37.257
And we saw that these
relatively small tweaks

00:02:37.257 --> 00:02:42.139
on top of vanilla SGD, are
relatively easy to implement

00:02:42.139 --> 00:02:45.492
but can make your networks
converge a bit faster.

00:02:45.492 --> 00:02:46.955
We also talked about regularization,

00:02:46.955 --> 00:02:48.529
especially dropout.

00:02:48.529 --> 00:02:50.666
So remember dropout, you're
kind of randomly setting

00:02:50.666 --> 00:02:52.586
parts of the network to zero
during the forward pass,

00:02:52.586 --> 00:02:54.959
and then you kind of
marginalize out over that noise

00:02:54.959 --> 00:02:56.975
in the back at test time.

00:02:56.975 --> 00:02:58.575
And we saw that this was
kind of a general pattern

00:02:58.575 --> 00:03:00.676
across many different
types of regularization

00:03:00.676 --> 00:03:02.805
in deep learning, where
you might add some kind

00:03:02.805 --> 00:03:05.132
of noise during training,
but then marginalize out

00:03:05.132 --> 00:03:07.037
that noise at test time
so it's not stochastic

00:03:07.037 --> 00:03:08.415
at test time.

00:03:08.415 --> 00:03:10.156
We also talked about
transfer learning where you

00:03:10.156 --> 00:03:12.533
can maybe download big
networks that were pre-trained

00:03:12.533 --> 00:03:14.354
on some dataset and then
fine tune them for your

00:03:14.354 --> 00:03:15.376
own problem.

00:03:15.376 --> 00:03:17.577
And this is one way that you
can attack a lot of problems

00:03:17.577 --> 00:03:19.647
in deep learning, even
if you don't have a huge

00:03:19.647 --> 00:03:21.314
dataset of your own.

00:03:22.781 --> 00:03:24.239
So today we're going to
shift gears a little bit

00:03:24.239 --> 00:03:25.947
and talk about some of the nuts and bolts

00:03:25.947 --> 00:03:29.615
about writing software and
how the hardware works.

00:03:29.615 --> 00:03:31.956
And a little bit, diving
into a lot of details

00:03:31.956 --> 00:03:33.973
about what the software
looks like that you actually

00:03:33.973 --> 00:03:36.276
use to train these things in practice.

00:03:36.276 --> 00:03:39.203
So we'll talk a little
bit about CPUs and GPUs

00:03:39.203 --> 00:03:41.476
and then we'll talk about
several of the major

00:03:41.476 --> 00:03:43.050
deep learning frameworks
that are out there in use

00:03:43.050 --> 00:03:43.967
these days.

00:03:45.471 --> 00:03:48.174
So first, we've sort of
mentioned this off hand

00:03:48.174 --> 00:03:49.890
a bunch of different times,

00:03:49.890 --> 00:03:52.961
that computers have CPUs,
computers have GPUs.

00:03:52.961 --> 00:03:55.257
Deep learning uses GPUs,
but we weren't really

00:03:55.257 --> 00:03:57.455
too explicit up to this
point about what exactly

00:03:57.455 --> 00:03:59.726
these things are and
why one might be better

00:03:59.726 --> 00:04:02.655
than another for different tasks.

00:04:02.655 --> 00:04:04.655
So, who's built a computer before?

00:04:04.655 --> 00:04:06.472
Just kind of show of hands.

00:04:06.472 --> 00:04:09.039
So, maybe about a third
of you, half of you,

00:04:09.039 --> 00:04:10.965
somewhere around that ballpark.

00:04:10.965 --> 00:04:14.119
So this is a shot of my computer at home

00:04:14.119 --> 00:04:15.174
that I built.

00:04:15.174 --> 00:04:17.839
And you can see that there's
a lot of stuff going on

00:04:17.839 --> 00:04:20.095
inside the computer,
maybe, hopefully you know

00:04:20.095 --> 00:04:22.261
what most of these parts are.

00:04:22.261 --> 00:04:25.594
And the CPU is the
Central Processing Unit.

00:04:25.594 --> 00:04:28.419
That's this little chip
hidden under this cooling fan

00:04:28.419 --> 00:04:31.391
right here near the top of the case.

00:04:31.391 --> 00:04:34.217
And the CPU is actually
relatively small piece.

00:04:34.217 --> 00:04:36.617
It's a relatively small
thing inside the case.

00:04:36.617 --> 00:04:39.555
It's not taking up a lot of space.

00:04:39.555 --> 00:04:42.204
And the GPUs are these
two big monster things

00:04:42.204 --> 00:04:44.845
that are taking up a
gigantic amount of space

00:04:44.845 --> 00:04:46.221
in the case.

00:04:46.221 --> 00:04:47.411
They have their own cooling,

00:04:47.411 --> 00:04:48.760
they're taking a lot of power.

00:04:48.760 --> 00:04:50.296
They're quite large.

00:04:50.296 --> 00:04:53.539
So, just in terms of how
much power they're using,

00:04:53.539 --> 00:04:55.906
in terms of how big they
are, the GPUs are kind of

00:04:55.906 --> 00:04:57.455
physically imposing and
taking up a lot of space

00:04:57.455 --> 00:04:59.139
in the case.

00:04:59.139 --> 00:05:00.927
So the question is what are these things

00:05:00.927 --> 00:05:04.516
and why are they so
important for deep learning?

00:05:04.516 --> 00:05:07.402
Well, the GPU is called a graphics card,

00:05:07.402 --> 00:05:08.937
or Graphics Processing Unit.

00:05:08.937 --> 00:05:12.430
And these were really developed,
originally for rendering

00:05:12.430 --> 00:05:14.457
computer graphics, and
especially around games

00:05:14.457 --> 00:05:16.166
and that sort of thing.

00:05:16.166 --> 00:05:20.225
So another show of hands,
who plays video games at home

00:05:20.225 --> 00:05:23.247
sometimes, from time to
time on their computer?

00:05:23.247 --> 00:05:25.693
Yeah, so again, maybe
about half, good fraction.

00:05:25.693 --> 00:05:28.159
So for those of you who've
played video games before

00:05:28.159 --> 00:05:29.717
and who've built your own computers,

00:05:29.717 --> 00:05:32.196
you probably have your own
opinions on this debate.

00:05:32.196 --> 00:05:34.095
[laughs]

00:05:34.095 --> 00:05:37.666
So this is one of those big
debates in computer science.

00:05:37.666 --> 00:05:40.349
You know, there's like Intel versus AMD,

00:05:40.349 --> 00:05:42.620
NVIDIA versus AMD for graphics cards.

00:05:42.620 --> 00:05:45.394
It's up there with Vim
versus Emacs for text editor.

00:05:45.394 --> 00:05:47.660
And pretty much any gamer
has their own opinions

00:05:47.660 --> 00:05:50.768
on which of these two sides they prefer

00:05:50.768 --> 00:05:51.945
for their own cards.

00:05:51.945 --> 00:05:54.895
And in deep learning we
kind of have mostly picked

00:05:54.895 --> 00:05:59.116
one side of this fight, and that's NVIDIA.

00:05:59.116 --> 00:06:00.816
So if you guys have AMD cards,

00:06:00.816 --> 00:06:03.026
you might be in a little
bit more trouble if you want

00:06:03.026 --> 00:06:05.117
to use those for deep learning.

00:06:05.117 --> 00:06:07.729
And really, NVIDIA's been
pushing a lot for deep learning

00:06:07.729 --> 00:06:08.812
in the last several years.

00:06:08.812 --> 00:06:11.997
It's been kind of a large focus
of some of their strategy.

00:06:11.997 --> 00:06:14.017
And they put in a lot
effort into engineering

00:06:14.017 --> 00:06:17.647
sort of good solutions
to make their hardware

00:06:17.647 --> 00:06:19.354
better suited for deep learning.

00:06:19.354 --> 00:06:23.855
So most people in deep learning
when we talk about GPUs,

00:06:23.855 --> 00:06:27.718
we're pretty much exclusively
talking about NVIDIA GPUs.

00:06:27.718 --> 00:06:29.607
Maybe in the future this'll
change a little bit,

00:06:29.607 --> 00:06:31.465
and there might be new players coming up,

00:06:31.465 --> 00:06:35.268
but at least for now
NVIDIA is pretty dominant.

00:06:35.268 --> 00:06:37.225
So to give you an idea of
like what is the difference

00:06:37.225 --> 00:06:40.163
between a CPU and a GPU,
I've kind of made a little

00:06:40.163 --> 00:06:41.705
spread sheet here.

00:06:41.705 --> 00:06:44.759
On the top we have two of
the kind of top end Intel

00:06:44.759 --> 00:06:47.364
consumer CPUs, and on
the bottom we have two of

00:06:47.364 --> 00:06:52.079
NVIDIA's sort of current
top end consumer GPUs.

00:06:52.079 --> 00:06:55.975
And there's a couple general
trends to notice here.

00:06:55.975 --> 00:06:58.445
Both GPUs and CPUs are
kind of a general purpose

00:06:58.445 --> 00:07:01.359
computing machine where
they can execute programs

00:07:01.359 --> 00:07:03.284
and do sort of arbitrary instructions,

00:07:03.284 --> 00:07:05.987
but they're qualitatively
pretty different.

00:07:05.987 --> 00:07:09.298
So CPUs tend to have just a few cores,

00:07:09.298 --> 00:07:12.700
for consumer desktop CPUs these days,

00:07:12.700 --> 00:07:14.941
they might have something like four or six

00:07:14.941 --> 00:07:16.714
or maybe up to 10 cores.

00:07:16.714 --> 00:07:20.393
With hyperthreading technology
that means they can run,

00:07:20.393 --> 00:07:22.726
the hardware can physically
run, like maybe eight

00:07:22.726 --> 00:07:24.893
or up to 20 threads concurrently.

00:07:24.893 --> 00:07:29.700
So the CPU can maybe do 20
things in parallel at once.

00:07:29.700 --> 00:07:31.691
So that's just not a gigantic number,

00:07:31.691 --> 00:07:34.527
but those threads for a
CPU are pretty powerful.

00:07:34.527 --> 00:07:36.084
They can actually do a lot of things,

00:07:36.084 --> 00:07:37.223
they're very fast.

00:07:37.223 --> 00:07:39.066
Every CPU instruction can
actually do quite a lot

00:07:39.066 --> 00:07:39.899
of stuff.

00:07:39.899 --> 00:07:43.011
And they can all work
pretty independently.

00:07:43.011 --> 00:07:45.303
For GPUs it's a little bit different.

00:07:45.303 --> 00:07:48.672
So for GPUs we see that
these sort of common top end

00:07:48.672 --> 00:07:51.909
consumer GPUs have thousands of cores.

00:07:51.909 --> 00:07:55.340
So the NVIDIA Titan XP
which is the current

00:07:55.340 --> 00:07:59.007
top of the line consumer
GPU has 3840 cores.

00:08:00.818 --> 00:08:02.223
So that's a crazy number.

00:08:02.223 --> 00:08:04.615
That's like way more than
the 10 cores that you'll get

00:08:04.615 --> 00:08:06.357
for a similarly priced CPU.

00:08:06.357 --> 00:08:09.574
The downside of a GPU is
that each of those cores,

00:08:09.574 --> 00:08:12.207
one, it runs at a much slower clock speed.

00:08:12.207 --> 00:08:14.439
And two they really
can't do quite as much.

00:08:14.439 --> 00:08:17.441
You can't really compare
CPU cores and GPU cores

00:08:17.441 --> 00:08:19.680
apples to apples.

00:08:19.680 --> 00:08:22.510
The GPU cores can't really
operate very independently.

00:08:22.510 --> 00:08:24.460
They all kind of need to work together

00:08:24.460 --> 00:08:26.589
and sort of paralyze one
task across many cores

00:08:26.589 --> 00:08:29.297
rather than each core
totally doing its own thing.

00:08:29.297 --> 00:08:32.406
So you can't really compare
these numbers directly.

00:08:32.406 --> 00:08:34.709
But it should give you the sense that due

00:08:34.709 --> 00:08:37.019
to the large number of
cores GPUs can sort of,

00:08:37.019 --> 00:08:39.044
are really good for
parallel things where you

00:08:39.044 --> 00:08:41.370
need to do a lot of things
all at the same time,

00:08:41.370 --> 00:08:44.742
but those things are all
pretty much the same flavor.

00:08:44.742 --> 00:08:48.014
Another thing to point
out between CPUs and GPUs

00:08:48.014 --> 00:08:49.387
is this idea of memory.

00:08:49.387 --> 00:08:52.887
Right, so CPUs have some cache on the CPU,

00:08:53.770 --> 00:08:56.151
but that's relatively
small and the majority

00:08:56.151 --> 00:08:58.523
of the memory for your
CPU is pulling from your

00:08:58.523 --> 00:09:00.969
system memory, the RAM,
which will maybe be like

00:09:00.969 --> 00:09:04.538
eight, 12, 16, 32 gigabytes
of RAM on a typical

00:09:04.538 --> 00:09:06.589
consumer desktop these days.

00:09:06.589 --> 00:09:09.479
Whereas GPUs actually
have their own RAM built

00:09:09.479 --> 00:09:10.646
into the chip.

00:09:12.055 --> 00:09:13.627
There's a pretty large
bottleneck communicating

00:09:13.627 --> 00:09:16.434
between the RAM in your
system and the GPU,

00:09:16.434 --> 00:09:18.508
so the GPUs typically have their own

00:09:18.508 --> 00:09:22.675
relatively large block of
memory within the card itself.

00:09:23.955 --> 00:09:27.172
And for the Titan XP, which
again is maybe the current

00:09:27.172 --> 00:09:29.092
top of the line consumer card,

00:09:29.092 --> 00:09:33.481
this thing has 12 gigabytes
of memory local to the GPU.

00:09:33.481 --> 00:09:35.462
GPUs also have their own caching system

00:09:35.462 --> 00:09:37.755
where there are sort of
multiple hierarchies of caching

00:09:37.755 --> 00:09:40.067
between the 12 gigabytes of GPU memory

00:09:40.067 --> 00:09:41.790
and the actual GPU cores.

00:09:41.790 --> 00:09:44.575
And that's somewhat similar
to the caching hierarchy

00:09:44.575 --> 00:09:46.908
that you might see in a CPU.

00:09:47.985 --> 00:09:50.652
So, CPUs are kind of good for
general purpose processing.

00:09:50.652 --> 00:09:52.583
They can do a lot of different things.

00:09:52.583 --> 00:09:54.572
And GPUs are maybe more
specialized for these highly

00:09:54.572 --> 00:09:57.089
paralyzable algorithms.

00:09:57.089 --> 00:09:59.167
So the prototypical algorithm
of something that works

00:09:59.167 --> 00:10:01.969
really really well and
is like perfectly suited

00:10:01.969 --> 00:10:04.106
to a GPU is matrix multiplication.

00:10:04.106 --> 00:10:06.733
So remember in matrix
multiplication on the left

00:10:06.733 --> 00:10:09.687
we've got like a matrix
composed of a bunch of rows.

00:10:09.687 --> 00:10:12.482
We multiply that on the right
by another matrix composed

00:10:12.482 --> 00:10:14.348
of a bunch of columns
and then this produces

00:10:14.348 --> 00:10:17.498
another, a final matrix
where each element in the

00:10:17.498 --> 00:10:20.718
output matrix is a dot product
between one of the rows

00:10:20.718 --> 00:10:22.780
and one of the columns of
the two input matrices.

00:10:22.780 --> 00:10:25.009
And these dot products
are all independent.

00:10:25.009 --> 00:10:27.548
Like you could imagine,
for this output matrix

00:10:27.548 --> 00:10:29.202
you could split it up completely

00:10:29.202 --> 00:10:31.110
and have each of those different elements

00:10:31.110 --> 00:10:33.653
of the output matrix all
being computed in parallel

00:10:33.653 --> 00:10:35.646
and they all sort of are
running the same computation

00:10:35.646 --> 00:10:38.289
which is taking a dot
product of these two vectors.

00:10:38.289 --> 00:10:40.754
But exactly where they're
reading that data from

00:10:40.754 --> 00:10:44.177
is from different places
in the two input matrices.

00:10:44.177 --> 00:10:46.092
So you could imagine that
for a GPU you can just

00:10:46.092 --> 00:10:48.797
like blast this out and
have all of this elements

00:10:48.797 --> 00:10:50.390
of the output matrix
all computed in parallel

00:10:50.390 --> 00:10:53.909
and that could make this thing
computer super super fast

00:10:53.909 --> 00:10:55.166
on GPU.

00:10:55.166 --> 00:10:57.985
So that's kind of the
prototypical type of problem

00:10:57.985 --> 00:11:00.431
that like where a GPU
is really well suited,

00:11:00.431 --> 00:11:02.293
where a CPU might have
to go in and step through

00:11:02.293 --> 00:11:04.023
sequentially and compute
each of these elements

00:11:04.023 --> 00:11:04.940
one by one.

00:11:06.337 --> 00:11:09.473
That picture is a little
bit of a caricature because

00:11:09.473 --> 00:11:11.648
CPUs these days have multiple cores,

00:11:11.648 --> 00:11:13.829
they can do vectorized
instructions as well,

00:11:13.829 --> 00:11:16.652
but still, for these like
massively parallel problems

00:11:16.652 --> 00:11:19.568
GPUs tend to have much better throughput.

00:11:19.568 --> 00:11:21.350
Especially when these matrices
get really really big.

00:11:21.350 --> 00:11:24.265
And by the way, convolution
is kind of the same

00:11:24.265 --> 00:11:25.404
kind of story.

00:11:25.404 --> 00:11:27.827
Where you know in convolution
we have this input tensor,

00:11:27.827 --> 00:11:30.128
we have this weight tensor
and then every point in the

00:11:30.128 --> 00:11:33.026
output tensor after a
convolution is again some inner

00:11:33.026 --> 00:11:35.081
product between some part of the weights

00:11:35.081 --> 00:11:36.359
and some part of the input.

00:11:36.359 --> 00:11:38.326
And you can imagine that a
GPU could really paralyze

00:11:38.326 --> 00:11:41.693
this computation, split it
all up across the many cores

00:11:41.693 --> 00:11:43.354
and compute it very quickly.

00:11:43.354 --> 00:11:45.746
So that's kind of the
general flavor of the types

00:11:45.746 --> 00:11:48.677
of problems where GPUs give
you a huge speed advantage

00:11:48.677 --> 00:11:49.510
over CPUs.

00:11:51.695 --> 00:11:54.023
So you can actually write
programs that run directly

00:11:54.023 --> 00:11:55.498
on GPUs.

00:11:55.498 --> 00:11:58.136
So NVIDIA has this CUDA
abstraction that lets you write

00:11:58.136 --> 00:12:00.378
code that kind of looks like C,

00:12:00.378 --> 00:12:03.614
but executes directly on the GPUs.

00:12:03.614 --> 00:12:05.484
But CUDA code is really really tricky.

00:12:05.484 --> 00:12:08.023
It's actually really tough
to write CUDA code that's

00:12:08.023 --> 00:12:10.056
performant and actually
squeezes all the juice out

00:12:10.056 --> 00:12:12.002
of these GPUs.

00:12:12.002 --> 00:12:13.842
You have to be very careful
managing the memory hierarchy

00:12:13.842 --> 00:12:16.140
and making sure you
don't have cache misses

00:12:16.140 --> 00:12:19.163
and branch mispredictions
and all that sort of stuff.

00:12:19.163 --> 00:12:21.373
So it's actually really really
hard to write performant

00:12:21.373 --> 00:12:22.930
CUDA code on your own.

00:12:22.930 --> 00:12:25.885
So as a result NVIDIA has
released a lot of libraries

00:12:25.885 --> 00:12:29.152
that implement common
computational primitives

00:12:29.152 --> 00:12:32.537
that are very very highly
optimized for GPUs.

00:12:32.537 --> 00:12:35.938
So for example NVIDIA has a
cuBLAS library that implements

00:12:35.938 --> 00:12:38.152
different kinds of matrix multiplications

00:12:38.152 --> 00:12:40.610
and different matrix operations
that are super optimized,

00:12:40.610 --> 00:12:43.517
run really well on GPU,
get very close to sort of

00:12:43.517 --> 00:12:46.438
theoretical peak hardware utilization.

00:12:46.438 --> 00:12:48.817
Similarly they have a cuDNN
library which implements

00:12:48.817 --> 00:12:51.964
things like convolution,
forward and backward passes,

00:12:51.964 --> 00:12:54.499
batch normalization, recurrent networks,

00:12:54.499 --> 00:12:56.116
all these kinds of
computational primitives

00:12:56.116 --> 00:12:57.454
that we need in deep learning.

00:12:57.454 --> 00:13:00.053
NVIDIA has gone in there and
released their own binaries

00:13:00.053 --> 00:13:02.060
that compute these
primitives very efficiently

00:13:02.060 --> 00:13:03.842
on NVIDIA hardware.

00:13:03.842 --> 00:13:07.777
So in practice, you tend not
to end up writing your own

00:13:07.777 --> 00:13:09.624
CUDA code for deep learning.

00:13:09.624 --> 00:13:12.642
You typically are just
mostly calling into existing

00:13:12.642 --> 00:13:14.173
code that other people have written.

00:13:14.173 --> 00:13:16.493
Much of which is the stuff
which has been heavily

00:13:16.493 --> 00:13:19.573
optimized by NVIDIA already.

00:13:19.573 --> 00:13:22.457
There's another sort of
language called OpenCL

00:13:22.457 --> 00:13:23.693
which is a bit more general.

00:13:23.693 --> 00:13:25.850
Runs on more than just NVIDIA GPUs,

00:13:25.850 --> 00:13:29.185
can run on AMD hardware, can run on CPUs,

00:13:29.185 --> 00:13:33.524
but OpenCL, nobody's really
spent a really large amount

00:13:33.524 --> 00:13:36.742
of effort and energy trying
to get optimized deep learning

00:13:36.742 --> 00:13:39.934
primitives for OpenCL, so
it tends to be a lot less

00:13:39.934 --> 00:13:43.938
performant the super
optimized versions in CUDA.

00:13:43.938 --> 00:13:46.262
So maybe in the future we
might see a bit of a more open

00:13:46.262 --> 00:13:49.008
standard and we might see
this across many different

00:13:49.008 --> 00:13:51.839
more types of platforms,
but at least for now,

00:13:51.839 --> 00:13:55.488
NVIDIA's kind of the main game
in town for deep learning.

00:13:55.488 --> 00:13:58.159
So you can check, there's a
lot of different resources

00:13:58.159 --> 00:14:00.853
for learning about how you can
do GPU programming yourself.

00:14:00.853 --> 00:14:01.686
It's kind of fun.

00:14:01.686 --> 00:14:03.919
It's sort of a different
paradigm of writing code

00:14:03.919 --> 00:14:05.900
because it's this massively
parallel architecture,

00:14:05.900 --> 00:14:08.023
but that's a bit beyond
the scope of this course.

00:14:08.023 --> 00:14:10.424
And again, you don't really
need to write your own

00:14:10.424 --> 00:14:12.263
CUDA code much in practice
for deep learning.

00:14:12.263 --> 00:14:14.872
And in fact, I've never
written my own CUDA code

00:14:14.872 --> 00:14:16.600
for any research project, so,

00:14:16.600 --> 00:14:18.856
but it is kind of useful
to know like how it works

00:14:18.856 --> 00:14:20.552
and what are the basic
ideas even if you're not

00:14:20.552 --> 00:14:22.219
writing it yourself.

00:14:23.488 --> 00:14:26.060
So if you want to look at
kind of CPU GPU performance

00:14:26.060 --> 00:14:29.168
in practice, I did some
benchmarks last summer

00:14:29.168 --> 00:14:31.501
comparing a decent Intel CPU

00:14:34.183 --> 00:14:36.766
against a bunch of different
GPUs that were sort

00:14:36.766 --> 00:14:38.747
of near top of the line at that time.

00:14:38.747 --> 00:14:41.098
And these were my own
benchmarks that you can find

00:14:41.098 --> 00:14:44.787
more details on GitHub,
but my findings were that

00:14:44.787 --> 00:14:48.954
for things like VGG 16 and
19, ResNets, various ResNets,

00:14:49.830 --> 00:14:53.186
then you typically see
something like a 65 to 75 times

00:14:53.186 --> 00:14:57.114
speed up when running the
exact same computation

00:14:57.114 --> 00:15:00.984
on a top of the line GPU, in
this case a Pascal Titan X,

00:15:00.984 --> 00:15:04.183
versus a top of the line,
well, not quite top of the line

00:15:04.183 --> 00:15:08.604
CPU, which in this case
was an Intel E5 processor.

00:15:08.604 --> 00:15:12.388
Although, I'd like to make
one sort of caveat here

00:15:12.388 --> 00:15:14.194
is that you always need
to be super careful

00:15:14.194 --> 00:15:15.550
whenever you're reading
any kind of benchmarks

00:15:15.550 --> 00:15:18.040
about deep learning, because
it's super easy to be

00:15:18.040 --> 00:15:20.103
unfair between different things.

00:15:20.103 --> 00:15:22.103
And you kind of need to know
a lot of the details about

00:15:22.103 --> 00:15:24.374
what exactly is being
benchmarked in order to know

00:15:24.374 --> 00:15:26.339
whether or not the comparison is fair.

00:15:26.339 --> 00:15:29.068
So in this case I'll come
right out and tell you

00:15:29.068 --> 00:15:31.473
that probably this comparison
is a little bit unfair

00:15:31.473 --> 00:15:35.855
to CPU because I didn't
spend a lot of effort

00:15:35.855 --> 00:15:37.638
trying to squeeze the maximal performance

00:15:37.638 --> 00:15:38.721
out of CPUs.

00:15:38.721 --> 00:15:41.065
I probably could have tuned
the blast libraries better

00:15:41.065 --> 00:15:42.483
for the CPU performance.

00:15:42.483 --> 00:15:43.707
And I probably could
have gotten these numbers

00:15:43.707 --> 00:15:44.540
a bit better.

00:15:44.540 --> 00:15:46.540
This was sort of out
of the box performance

00:15:46.540 --> 00:15:49.180
between just installing
Torch, running it on a CPU,

00:15:49.180 --> 00:15:51.964
just installing Torch running it on a GPU.

00:15:51.964 --> 00:15:53.884
So this is kind of out
of the box performance,

00:15:53.884 --> 00:15:56.277
but it's not really like
peak, possible, theoretical

00:15:56.277 --> 00:15:57.872
throughput on the CPU.

00:15:57.872 --> 00:16:00.263
But that being said, I
think there are still pretty

00:16:00.263 --> 00:16:02.422
substantial speed ups to be had here.

00:16:02.422 --> 00:16:05.660
Another kind of interesting
outcome from this benchmarking

00:16:05.660 --> 00:16:09.602
was comparing these
optimized cuDNN libraries

00:16:09.602 --> 00:16:12.478
from NVIDIA for convolution
and whatnot versus

00:16:12.478 --> 00:16:15.543
sort of more naive CUDA
that had been hand written

00:16:15.543 --> 00:16:17.623
out in the open source community.

00:16:17.623 --> 00:16:19.815
And you can see that if you
compare the same networks

00:16:19.815 --> 00:16:22.278
on the same hardware with
the same deep learning

00:16:22.278 --> 00:16:24.653
framework and the only
difference is swapping out

00:16:24.653 --> 00:16:27.340
these cuDNN versus sort of
hand written, less optimized

00:16:27.340 --> 00:16:30.312
CUDA you can see something
like nearly a three X speed up

00:16:30.312 --> 00:16:33.714
across the board when you
switch from the relatively

00:16:33.714 --> 00:16:35.777
simple CUDA to these like
super optimized cuDNN

00:16:35.777 --> 00:16:37.442
implementations.

00:16:37.442 --> 00:16:40.044
So in general, whenever
you're writing code on GPU,

00:16:40.044 --> 00:16:43.143
you should probably almost
always like just make sure

00:16:43.143 --> 00:16:45.202
you're using cuDNN because
you're leaving probably

00:16:45.202 --> 00:16:47.703
a three X performance boost
on the table if you're

00:16:47.703 --> 00:16:51.602
not calling into cuDNN for your stuff.

00:16:51.602 --> 00:16:53.362
So another problem that
comes up in practice,

00:16:53.362 --> 00:16:55.383
when you're training these things is that

00:16:55.383 --> 00:16:57.564
you know, your model is
maybe sitting on the GPU,

00:16:57.564 --> 00:16:59.922
the weights of the model
are in that 12 gigabytes

00:16:59.922 --> 00:17:02.882
of local storage on the
GPU, but your big dataset

00:17:02.882 --> 00:17:05.202
is sitting over on the
right on a hard drive

00:17:05.202 --> 00:17:07.244
or an SSD or something like that.

00:17:07.244 --> 00:17:10.204
So if you're not careful
you can actually bottleneck

00:17:10.204 --> 00:17:12.122
your training by just
trying to read the data

00:17:12.122 --> 00:17:13.205
off the disk.

00:17:14.322 --> 00:17:16.114
'Cause the GPU is super
fast, it can compute

00:17:16.114 --> 00:17:18.642
forward and backward quite
fast, but if you're reading

00:17:18.642 --> 00:17:20.974
sequentially off a spinning
disk, you can actually

00:17:20.974 --> 00:17:23.003
bottleneck your training quite,

00:17:23.003 --> 00:17:25.700
and that can be really
bad and slow you down.

00:17:25.700 --> 00:17:27.180
So some solutions here
are that like you know

00:17:27.180 --> 00:17:29.848
if your dataset's really
small, sometimes you might just

00:17:29.848 --> 00:17:31.459
read the whole dataset into RAM.

00:17:31.459 --> 00:17:33.484
Or even if your dataset isn't so small,

00:17:33.484 --> 00:17:35.005
but you have a giant
server with a ton of RAM,

00:17:35.005 --> 00:17:36.479
you might do that anyway.

00:17:36.479 --> 00:17:38.663
You can also make sure
you're using an SSD instead

00:17:38.663 --> 00:17:42.917
of a hard drive, that can help
a lot with read throughput.

00:17:42.917 --> 00:17:45.495
Another common strategy
is to use multiple threads

00:17:45.495 --> 00:17:49.162
on the CPU that are
pre-fetching data off RAM

00:17:49.162 --> 00:17:52.152
or off disk, buffering it
in memory, in RAM so that

00:17:52.152 --> 00:17:54.705
then you can continue
feeding that buffer data down

00:17:54.705 --> 00:17:57.724
to the GPU with good performance.

00:17:57.724 --> 00:17:59.107
This is a little bit painful to set up,

00:17:59.107 --> 00:18:01.826
but again like, these
GPU's are so fast that

00:18:01.826 --> 00:18:03.682
if you're not really
careful with trying to feed

00:18:03.682 --> 00:18:05.543
them data as quickly as possible,

00:18:05.543 --> 00:18:07.164
just reading the data
can sometimes bottleneck

00:18:07.164 --> 00:18:08.804
the whole training process.

00:18:08.804 --> 00:18:11.657
So that's something to be aware of.

00:18:11.657 --> 00:18:13.528
So that's kind of the
brief introduction to like

00:18:13.528 --> 00:18:15.900
sort of GPU CPU hardware
in practice when it comes

00:18:15.900 --> 00:18:17.432
to deep learning.

00:18:17.432 --> 00:18:19.215
And then I wanted to
switch gears a little bit

00:18:19.215 --> 00:18:21.616
and talk about the
software side of things.

00:18:21.616 --> 00:18:23.923
The various deep learning
frameworks that people are using

00:18:23.923 --> 00:18:25.006
in practice.

00:18:25.006 --> 00:18:26.190
But I guess before I move on,

00:18:26.190 --> 00:18:28.819
is there any sort of
questions about CPU GPU?

00:18:28.819 --> 00:18:30.519
Yeah, question?

00:18:30.519 --> 00:18:34.686
[student's words obscured
due to lack of microphone]

00:18:40.961 --> 00:18:42.689
Yeah, so the question
is what can you sort of,

00:18:42.689 --> 00:18:44.289
what can you do mechanically
when you're coding

00:18:44.289 --> 00:18:45.854
to avoid these problems?

00:18:45.854 --> 00:18:47.872
Probably the biggest thing
you can do in software

00:18:47.872 --> 00:18:50.833
is set up sort of pre-fetching on the CPU.

00:18:50.833 --> 00:18:53.097
Like you couldn't like,
sort of a naive thing

00:18:53.097 --> 00:18:55.054
would be you have this
sequential process where you

00:18:55.054 --> 00:18:57.441
first read data off
disk, wait for the data,

00:18:57.441 --> 00:18:58.791
wait for the minibatch to be read,

00:18:58.791 --> 00:19:00.697
then feed the minibatch to the GPU,

00:19:00.697 --> 00:19:02.458
then go forward and backward on the GPU,

00:19:02.458 --> 00:19:04.442
then read another minibatch
and sort of do this all

00:19:04.442 --> 00:19:05.442
in sequence.

00:19:06.714 --> 00:19:08.217
And if you actually have multiple,

00:19:08.217 --> 00:19:10.533
like instead you might have
CPU threads running in the

00:19:10.533 --> 00:19:13.277
background that are
fetching data off the disk

00:19:13.277 --> 00:19:15.469
such that while the,

00:19:15.469 --> 00:19:17.076
you can sort of interleave
all of these things.

00:19:17.076 --> 00:19:18.794
Like the GPU is computing,

00:19:18.794 --> 00:19:21.506
the CPU background threads
are feeding data off disk

00:19:21.506 --> 00:19:23.499
and your main thread is kind
of waiting for these things to,

00:19:23.499 --> 00:19:25.792
just doing a bit of synchronization
between these things

00:19:25.792 --> 00:19:28.534
so they're all happening in parallel.

00:19:28.534 --> 00:19:30.937
And thankfully if you're using
some of these deep learning

00:19:30.937 --> 00:19:32.851
frameworks that we're about to talk about,

00:19:32.851 --> 00:19:34.709
then some of this work has
already been done for you

00:19:34.709 --> 00:19:38.016
'cause it's a little bit painful.

00:19:38.016 --> 00:19:40.110
So the landscape of
deep learning frameworks

00:19:40.110 --> 00:19:41.738
is super fast moving.

00:19:41.738 --> 00:19:44.789
So last year when I gave
this lecture I talked mostly

00:19:44.789 --> 00:19:47.915
about Caffe, Torch, Theano and TensorFlow.

00:19:47.915 --> 00:19:51.449
And when I last gave this talk,
again more than a year ago,

00:19:51.449 --> 00:19:53.753
TensorFlow was relatively new.

00:19:53.753 --> 00:19:57.814
It had not seen super widespread
adoption yet at that time.

00:19:57.814 --> 00:20:00.232
But now I think in the
last year TensorFlow

00:20:00.232 --> 00:20:01.903
has gotten much more popular.

00:20:01.903 --> 00:20:04.393
It's probably the main framework
of choice for many people.

00:20:04.393 --> 00:20:06.310
So that's a big change.

00:20:07.342 --> 00:20:09.833
We've also seen a ton of new frameworks

00:20:09.833 --> 00:20:12.282
sort of popping up like
mushrooms in the last year.

00:20:12.282 --> 00:20:15.732
So in particular Caffe2 and
PyTorch are new frameworks

00:20:15.732 --> 00:20:18.052
from Facebook that I think
are pretty interesting.

00:20:18.052 --> 00:20:20.409
There's also a ton of other frameworks.

00:20:20.409 --> 00:20:24.089
Paddle, Baidu has Paddle,
Microsoft has CNTK,

00:20:24.089 --> 00:20:28.592
Amazon is mostly using
MXNet and there's a ton

00:20:28.592 --> 00:20:30.337
of other frameworks as well,
but I'm less familiar with,

00:20:30.337 --> 00:20:33.449
and really don't have time to get into.

00:20:33.449 --> 00:20:37.110
But one interesting thing to
point out from this picture

00:20:37.110 --> 00:20:39.757
is that kind of the first
generation of deep learning

00:20:39.757 --> 00:20:41.839
frameworks that really saw wide adoption

00:20:41.839 --> 00:20:43.572
were built in academia.

00:20:43.572 --> 00:20:45.572
So Caffe was from Berkeley,
Torch was developed

00:20:45.572 --> 00:20:49.388
originally NYU and also in
collaboration with Facebook.

00:20:49.388 --> 00:20:52.077
And Theana was mostly build
at the University of Montreal.

00:20:52.077 --> 00:20:54.491
But these kind of next
generation deep learning

00:20:54.491 --> 00:20:56.491
frameworks all originated in industry.

00:20:56.491 --> 00:20:58.989
So Caffe2 is from Facebook,
PyTorch is from Facebook.

00:20:58.989 --> 00:21:00.659
TensorFlow is from Google.

00:21:00.659 --> 00:21:02.557
So it's kind of an interesting
shift that we've seen

00:21:02.557 --> 00:21:04.727
in the landscape over
the last couple of years

00:21:04.727 --> 00:21:06.866
is that these ideas
have really moved a lot

00:21:06.866 --> 00:21:08.925
from academia into industry.

00:21:08.925 --> 00:21:10.770
And now industry is kind of
giving us these big powerful

00:21:10.770 --> 00:21:13.187
nice frameworks to work with.

00:21:14.147 --> 00:21:17.092
So today I wanted to
mostly talk about PyTorch

00:21:17.092 --> 00:21:19.988
and TensorFlow 'cause I
personally think that those

00:21:19.988 --> 00:21:22.601
are probably the ones you
should be focusing on for

00:21:22.601 --> 00:21:24.850
a lot of research type
problems these days.

00:21:24.850 --> 00:21:28.470
I'll also talk a bit
about Caffe and Caffe2.

00:21:28.470 --> 00:21:32.192
But probably a little bit
less emphasis on those.

00:21:32.192 --> 00:21:34.341
And before we move any farther,
I thought I should make

00:21:34.341 --> 00:21:36.705
my own biases a little bit more explicit.

00:21:36.705 --> 00:21:39.058
So I have mostly, I've
worked with Torch mostly

00:21:39.058 --> 00:21:40.315
for the last several years.

00:21:40.315 --> 00:21:43.501
And I've used it quite
a lot, I like it a lot.

00:21:43.501 --> 00:21:46.306
And then in the last year I've
mostly switched to PyTorch

00:21:46.306 --> 00:21:48.568
as my main research framework.

00:21:48.568 --> 00:21:50.487
So I have a little bit
less experience with some

00:21:50.487 --> 00:21:52.306
of these others, especially TensorFlow,

00:21:52.306 --> 00:21:54.087
but I'll still try to do
my best to give you a fair

00:21:54.087 --> 00:21:58.382
picture and a decent
overview of these things.

00:21:58.382 --> 00:22:02.507
So, remember that in the
last several lectures

00:22:02.507 --> 00:22:04.725
we've hammered this idea
of computational graphs in

00:22:04.725 --> 00:22:06.807
sort of over and over.

00:22:06.807 --> 00:22:08.217
That whenever you're doing deep learning,

00:22:08.217 --> 00:22:09.970
you want to think about building
some computational graph

00:22:09.970 --> 00:22:13.176
that computes whatever function
that you want to compute.

00:22:13.176 --> 00:22:15.090
So in the case of a linear
classifier you'll combine

00:22:15.090 --> 00:22:18.778
your data X and your weights
W with a matrix multiply.

00:22:18.778 --> 00:22:21.554
You'll do some kind of
hinge loss to maybe have,

00:22:21.554 --> 00:22:22.832
compute your loss.

00:22:22.832 --> 00:22:24.386
You'll have some regularization term

00:22:24.386 --> 00:22:26.397
and you imagine stitching
together all these different

00:22:26.397 --> 00:22:28.909
operations into some graph structure.

00:22:28.909 --> 00:22:31.069
Remember that these graph
structures can get pretty

00:22:31.069 --> 00:22:33.191
complex in the case of a big neural net,

00:22:33.191 --> 00:22:34.680
now there's many different layers,

00:22:34.680 --> 00:22:36.167
many different activations.

00:22:36.167 --> 00:22:38.087
Many different weights
spread all around in a pretty

00:22:38.087 --> 00:22:39.687
complex graph.

00:22:39.687 --> 00:22:41.490
And as you move to things
like neural turing machines

00:22:41.490 --> 00:22:44.130
then you can get these really
crazy computational graphs

00:22:44.130 --> 00:22:45.911
that you can't even really
draw because they're

00:22:45.911 --> 00:22:47.328
so big and messy.

00:22:48.349 --> 00:22:52.127
So the point of deep learning
frameworks is really,

00:22:52.127 --> 00:22:54.392
there's really kind of three
main reasons why you might

00:22:54.392 --> 00:22:56.425
want to use one of these
deep learning frameworks

00:22:56.425 --> 00:22:58.727
rather than just writing your own code.

00:22:58.727 --> 00:23:01.414
So the first would be that
these frameworks enable

00:23:01.414 --> 00:23:03.255
you to easily build and
work with these big hairy

00:23:03.255 --> 00:23:05.950
computational graphs
without kind of worrying

00:23:05.950 --> 00:23:08.610
about a lot of those
bookkeeping details yourself.

00:23:08.610 --> 00:23:10.860
Another major idea is that,

00:23:11.716 --> 00:23:13.479
whenever we're working in deep learning

00:23:13.479 --> 00:23:14.812
we always need to compute gradients.

00:23:14.812 --> 00:23:16.211
We're always computing some loss,

00:23:16.211 --> 00:23:17.629
we're always computer
gradient of our weight

00:23:17.629 --> 00:23:18.900
with respect to the loss.

00:23:18.900 --> 00:23:22.973
And we'd like to make this
automatically computing gradient,

00:23:22.973 --> 00:23:26.115
you don't want to have to
write that code yourself.

00:23:26.115 --> 00:23:28.287
You want that framework to
handle all these back propagation

00:23:28.287 --> 00:23:30.485
details for you so you
can just think about

00:23:30.485 --> 00:23:32.526
writing down the forward
pass of your network

00:23:32.526 --> 00:23:34.725
and have the backward pass
sort of come out for free

00:23:34.725 --> 00:23:36.539
without any additional work.

00:23:36.539 --> 00:23:38.905
And finally you want all
this stuff to run efficiently

00:23:38.905 --> 00:23:42.000
on GPUs so you don't have to
worry too much about these

00:23:42.000 --> 00:23:44.973
low level hardware details
about cuBLAS and cuDNN

00:23:44.973 --> 00:23:48.389
and CUDA and moving data
between the CPU and GPU memory.

00:23:48.389 --> 00:23:51.030
You kind of want all those messy
details to be taken care of

00:23:51.030 --> 00:23:52.439
for you.

00:23:52.439 --> 00:23:54.483
So those are kind of
some of the major reasons

00:23:54.483 --> 00:23:56.930
why you might choose to
use frameworks rather than

00:23:56.930 --> 00:23:59.450
writing your own stuff from scratch.

00:23:59.450 --> 00:24:02.969
So as kind of a concrete
example of a computational graph

00:24:02.969 --> 00:24:05.231
we can maybe write down
this super simple thing.

00:24:05.231 --> 00:24:08.367
Where we have three inputs, X, Y, and Z.

00:24:08.367 --> 00:24:09.908
We're going to combine
X and Y to produce A.

00:24:09.908 --> 00:24:13.071
Then we're going to combine
A and Z to produce B

00:24:13.071 --> 00:24:15.406
and then finally we're going
to do some maybe summing out

00:24:15.406 --> 00:24:18.630
operation on B to give
some scaler final result C.

00:24:18.630 --> 00:24:21.638
So you've probably written
enough Numpy code at this point

00:24:21.638 --> 00:24:24.310
to realize that it's
super easy to write down,

00:24:24.310 --> 00:24:27.216
to implement this computational graph,

00:24:27.216 --> 00:24:30.798
or rather to implement this
bit of computation in Numpy,

00:24:30.798 --> 00:24:31.631
right?

00:24:31.631 --> 00:24:33.724
You can just kind of write
down in Numpy that you want to

00:24:33.724 --> 00:24:36.508
generate some random data, you
want to multiply two things,

00:24:36.508 --> 00:24:38.547
you want to add two things, you
want to sum out a couple things.

00:24:38.547 --> 00:24:41.923
And it's really easy to do this in Numpy.

00:24:41.923 --> 00:24:44.360
But then the question is
like suppose that we want

00:24:44.360 --> 00:24:48.355
to compute the gradient of C
with respect to X, Y, and Z.

00:24:48.355 --> 00:24:51.149
So, if you're working in Numpy,
you kind of need to write out

00:24:51.149 --> 00:24:52.725
this backward pass yourself.

00:24:52.725 --> 00:24:54.965
And you've gotten a lot of
practice with this on the

00:24:54.965 --> 00:24:58.127
homeworks, but it can be kind of a pain

00:24:58.127 --> 00:25:00.306
and a little bit annoying
and messy once you get to

00:25:00.306 --> 00:25:02.859
really big complicated things.

00:25:02.859 --> 00:25:04.487
The other problem with
Numpy is that it doesn't run

00:25:04.487 --> 00:25:05.675
on the GPU.

00:25:05.675 --> 00:25:08.189
So Numpy is definitely CPU only.

00:25:08.189 --> 00:25:10.514
And you're never going
to be able to experience

00:25:10.514 --> 00:25:13.112
or take advantage of these
GPU accelerated speedups

00:25:13.112 --> 00:25:14.920
if you're stuck working in Numpy.

00:25:14.920 --> 00:25:17.011
And it's, again, it's a
pain to have to compute

00:25:17.011 --> 00:25:19.527
your own gradients in
all these situations.

00:25:19.527 --> 00:25:22.807
So, kind of the goal of most
deep learning frameworks

00:25:22.807 --> 00:25:26.829
these days is to let you
write code in the forward pass

00:25:26.829 --> 00:25:29.047
that looks very similar to Numpy,

00:25:29.047 --> 00:25:31.170
but lets you run it on the GPU

00:25:31.170 --> 00:25:33.069
and lets you automatically
compute gradients.

00:25:33.069 --> 00:25:34.967
And that's kind of the big
picture goal of most of these

00:25:34.967 --> 00:25:36.397
frameworks.

00:25:36.397 --> 00:25:38.533
So if you imagine looking
at, if we look at an example

00:25:38.533 --> 00:25:42.226
in TensorFlow of the exact
same computational graph,

00:25:42.226 --> 00:25:44.314
we now see that in this forward pass,

00:25:44.314 --> 00:25:47.241
you write this code that ends
up looking very very similar

00:25:47.241 --> 00:25:49.334
to the Numpy forward pass
where you're kind of doing

00:25:49.334 --> 00:25:52.687
these multiplication and
these addition operations.

00:25:52.687 --> 00:25:55.669
But now TensorFlow has
this magic line that just

00:25:55.669 --> 00:25:57.623
computes all the gradients for you.

00:25:57.623 --> 00:25:59.686
So now you don't have go in and
write your own backward pass

00:25:59.686 --> 00:26:02.235
and that's much more convenient.

00:26:02.235 --> 00:26:04.095
The other nice thing about
TensorFlow is you can really

00:26:04.095 --> 00:26:06.841
just, like with one line you
can switch all this computation

00:26:06.841 --> 00:26:08.926
between CPU and GPU.

00:26:08.926 --> 00:26:11.016
So here, if you just
add this with statement

00:26:11.016 --> 00:26:13.037
before you're doing this forward pass,

00:26:13.037 --> 00:26:14.866
you just can explicitly
tell the framework,

00:26:14.866 --> 00:26:16.668
hey I want to run this code on the CPU.

00:26:16.668 --> 00:26:19.537
But now if we just change that
with statement a little bit

00:26:19.537 --> 00:26:21.527
with just with a one
character change in this case,

00:26:21.527 --> 00:26:24.866
changing that C to a G,
now the code runs on GPU.

00:26:24.866 --> 00:26:27.868
And now in this little code snippet,

00:26:27.868 --> 00:26:29.539
we've solved these two problems.

00:26:29.539 --> 00:26:31.388
We're running our code on the GPU

00:26:31.388 --> 00:26:33.127
and we're having the framework
compute all the gradients

00:26:33.127 --> 00:26:35.685
for us, so that's really nice.

00:26:35.685 --> 00:26:38.459
And PyTorch kind looks
almost exactly the same.

00:26:38.459 --> 00:26:40.349
So again, in PyTorch
you kind of write down,

00:26:40.349 --> 00:26:42.509
you define some variables,

00:26:42.509 --> 00:26:45.149
you have some forward pass
and the forward pass again

00:26:45.149 --> 00:26:47.640
looks very similar to like,
in this case identical

00:26:47.640 --> 00:26:49.262
to the Numpy code.

00:26:49.262 --> 00:26:52.146
And then again, you can
just use PyTorch to compute

00:26:52.146 --> 00:26:56.251
gradients, all your
gradients with just one line.

00:26:56.251 --> 00:26:58.084
And now in PyTorch again,
it's really easy to switch

00:26:58.084 --> 00:27:00.263
to GPU, you just need to
cast all your stuff to the

00:27:00.263 --> 00:27:03.210
CUDA data type before
you rung your computation

00:27:03.210 --> 00:27:06.781
and now everything runs
transparently on the GPU for you.

00:27:06.781 --> 00:27:09.091
So if you kind of just look
at these three examples,

00:27:09.091 --> 00:27:11.321
these three snippets of code side by side,

00:27:11.321 --> 00:27:13.878
the Numpy, the TensorFlow and the PyTorch

00:27:13.878 --> 00:27:17.579
you see that the TensorFlow
and the PyTorch code

00:27:17.579 --> 00:27:20.564
in the forward pass looks
almost exactly like Numpy

00:27:20.564 --> 00:27:23.048
which is great 'cause
Numpy has a beautiful API,

00:27:23.048 --> 00:27:24.349
it's really easy to work with.

00:27:24.349 --> 00:27:26.109
But we can compute gradients automatically

00:27:26.109 --> 00:27:29.192
and we can run the GPU automatically.

00:27:30.186 --> 00:27:31.873
So after that kind of introduction,

00:27:31.873 --> 00:27:33.654
I wanted to dive in and
talk in a little bit more

00:27:33.654 --> 00:27:35.758
detail about kind of
what's going on inside this

00:27:35.758 --> 00:27:37.502
TensorFlow example.

00:27:37.502 --> 00:27:40.384
So as a running example throughout
the rest of the lecture,

00:27:40.384 --> 00:27:44.028
I'm going to use the training
a two-layer fully connected

00:27:44.028 --> 00:27:48.377
ReLU network on random data
as kind of a running example

00:27:48.377 --> 00:27:50.662
throughout the rest of the examples here.

00:27:50.662 --> 00:27:53.353
And we're going to train this
thing with an L2 Euclidean

00:27:53.353 --> 00:27:55.289
loss on random data.

00:27:55.289 --> 00:27:57.633
So this is kind of a silly
network, it's not really doing

00:27:57.633 --> 00:27:59.836
anything useful, but it does give you the,

00:27:59.836 --> 00:28:01.723
it's relatively small, self contained,

00:28:01.723 --> 00:28:04.444
the code fits on the slide
without being too small,

00:28:04.444 --> 00:28:06.428
and it lets you demonstrate
kind of a lot of the useful

00:28:06.428 --> 00:28:08.966
ideas inside these frameworks.

00:28:08.966 --> 00:28:10.814
So here on the right, oh,
and then another note,

00:28:10.814 --> 00:28:13.397
I'm kind of assuming
that Numpy and TensorFlow

00:28:13.397 --> 00:28:15.900
have already been imported
in all these code snippets.

00:28:15.900 --> 00:28:19.308
So in TensorFlow you would
typically divide your computation

00:28:19.308 --> 00:28:21.163
into two major stages.

00:28:21.163 --> 00:28:23.996
First, we're going to write
some code that defines

00:28:23.996 --> 00:28:26.852
our computational graph,
and that's this red code

00:28:26.852 --> 00:28:28.363
up in the top half.

00:28:28.363 --> 00:28:30.347
And then after you define your graph,

00:28:30.347 --> 00:28:32.360
you're going to run the
graph over and over again

00:28:32.360 --> 00:28:34.113
and actually feed data into the graph

00:28:34.113 --> 00:28:36.851
to perform whatever computation
you want it to perform.

00:28:36.851 --> 00:28:38.772
So this is the really,
this is kind of the big

00:28:38.772 --> 00:28:40.961
common pattern in TensorFlow.

00:28:40.961 --> 00:28:42.907
You'll first have a bunch of
code that builds the graph

00:28:42.907 --> 00:28:45.282
and then you'll go and
run the graph and reuse it

00:28:45.282 --> 00:28:46.615
many many times.

00:28:48.099 --> 00:28:50.827
So if you kind of dive
into the code of building

00:28:50.827 --> 00:28:52.763
the graph in this case.

00:28:52.763 --> 00:28:56.542
Up at the top you see that
we're defining this X, Y,

00:28:56.542 --> 00:29:00.709
w1 and w2, and we're creating
these tf.placeholder objects.

00:29:01.637 --> 00:29:05.193
So these are going to be
input nodes to the graph.

00:29:05.193 --> 00:29:08.360
These are going to be sort
of entry points to the graph

00:29:08.360 --> 00:29:11.101
where when we run the graph,
we're going to feed in data

00:29:11.101 --> 00:29:13.379
and put them in through
these input slots in our

00:29:13.379 --> 00:29:15.379
computational graph.

00:29:15.379 --> 00:29:17.218
So this is not actually
like allocating any memory

00:29:17.218 --> 00:29:18.051
right now.

00:29:19.044 --> 00:29:20.861
We're just sort of setting
up these input slots

00:29:20.861 --> 00:29:21.944
to the graph.

00:29:23.272 --> 00:29:25.560
Then we're going to use those
input slots which are now

00:29:25.560 --> 00:29:28.665
kind of like these symbolic variables

00:29:28.665 --> 00:29:31.021
and we're going to perform
different TensorFlow operations

00:29:31.021 --> 00:29:33.917
on these symbolic variables
in order to set up

00:29:33.917 --> 00:29:37.135
what computation we want
to run on those variables.

00:29:37.135 --> 00:29:39.379
So in this case we're doing
a matrix multiplication

00:29:39.379 --> 00:29:43.904
between X and w1, we're
doing some tf.maximum to do a

00:29:43.904 --> 00:29:46.109
ReLU nonlinearity and
then we're doing another

00:29:46.109 --> 00:29:49.240
matrix multiplication to
compute our output predictions.

00:29:49.240 --> 00:29:50.955
And then we're again using
a sort of basic Tensor

00:29:50.955 --> 00:29:53.356
operations to compute
our Euclidean distance,

00:29:53.356 --> 00:29:58.175
our L2 loss between our
prediction and the target Y.

00:29:58.175 --> 00:30:00.099
Another thing to point out here is that

00:30:00.099 --> 00:30:03.647
these lines of code are not
actually computing anything.

00:30:03.647 --> 00:30:05.824
There's no data in the system right now.

00:30:05.824 --> 00:30:07.571
We're just building up this
computational graph data

00:30:07.571 --> 00:30:10.799
structure telling
TensorFlow which operations

00:30:10.799 --> 00:30:15.001
we want to eventually run
once we put in real data.

00:30:15.001 --> 00:30:16.393
So this is just building the graph,

00:30:16.393 --> 00:30:18.648
this is not actually doing anything.

00:30:18.648 --> 00:30:21.658
Then we have this magical line
where after we've computed

00:30:21.658 --> 00:30:24.739
our loss with these symbolic operations,

00:30:24.739 --> 00:30:27.181
then we can just ask TensorFlow to compute

00:30:27.181 --> 00:30:31.186
the gradient of the loss
with respect to w1 and w2

00:30:31.186 --> 00:30:33.135
in this one magical, beautiful line.

00:30:33.135 --> 00:30:35.619
And this avoids you writing
all your own backprop code

00:30:35.619 --> 00:30:37.981
that you had to do in the assignments.

00:30:37.981 --> 00:30:40.439
But again there's no actual
computation happening here.

00:30:40.439 --> 00:30:42.521
This is just sort of
adding extra operations

00:30:42.521 --> 00:30:46.009
to the computational graph
where now the computational

00:30:46.009 --> 00:30:47.950
graph has these additional
operations which will end up

00:30:47.950 --> 00:30:51.108
computing these gradients for you.

00:30:51.108 --> 00:30:53.129
So now at this point we've
computed our computational

00:30:53.129 --> 00:30:56.638
graph, we have this big graph
in this graph data structure

00:30:56.638 --> 00:30:59.039
in memory that knows what
operations we want to perform

00:30:59.039 --> 00:31:01.421
to compute the loss in gradients.

00:31:01.421 --> 00:31:03.705
And now we enter a TensorFlow
session to actually run

00:31:03.705 --> 00:31:06.843
this graph and feed it with data.

00:31:06.843 --> 00:31:09.160
So then, once we've entered the session,

00:31:09.160 --> 00:31:11.943
then we actually need to
construct some concrete values

00:31:11.943 --> 00:31:13.859
that will be fed to the graph.

00:31:13.859 --> 00:31:17.227
So TensorFlow just expects
to receive data from

00:31:17.227 --> 00:31:19.459
Numpy arrays in most cases.

00:31:19.459 --> 00:31:23.701
So here we're just creating
concrete actual values

00:31:23.701 --> 00:31:28.066
for X, Y, w1 and w2 using
Numpy and then storing these

00:31:28.066 --> 00:31:30.226
in some dictionary.

00:31:30.226 --> 00:31:32.743
And now here is where we're
actually running the graph.

00:31:32.743 --> 00:31:36.206
So you can see that we're
calling a session.run

00:31:36.206 --> 00:31:38.120
to actually execute
some part of the graph.

00:31:38.120 --> 00:31:41.603
The first argument loss, tells
us which part of the graph

00:31:41.603 --> 00:31:43.899
do we actually want as output.

00:31:43.899 --> 00:31:45.979
And that, so we actually want the graph,

00:31:45.979 --> 00:31:47.597
in this case we need to
tell it that we actually

00:31:47.597 --> 00:31:50.950
want to compute loss and grad1 and grad w2

00:31:50.950 --> 00:31:53.880
and we need to pass in with
this feed dict parameter

00:31:53.880 --> 00:31:57.140
the actual concrete values
that will be fed to the graph.

00:31:57.140 --> 00:32:00.043
And then after, in this one line,

00:32:00.043 --> 00:32:02.888
it's going and running the
graph and then computing

00:32:02.888 --> 00:32:06.541
those values for loss grad1 to grad w2

00:32:06.541 --> 00:32:09.097
and then returning the
actual concrete values

00:32:09.097 --> 00:32:12.003
for those in Numpy arrays again.

00:32:12.003 --> 00:32:14.398
So now after you unpack this
output in the second line,

00:32:14.398 --> 00:32:18.446
you get Numpy arrays, or you
get Numpy arrays with the loss

00:32:18.446 --> 00:32:19.859
and the gradients.

00:32:19.859 --> 00:32:21.720
So then you can go and
do whatever you want

00:32:21.720 --> 00:32:23.697
with these values.

00:32:23.697 --> 00:32:28.655
So then, this has only run sort
of one forward and backward

00:32:28.655 --> 00:32:29.599
pass through our graph,

00:32:29.599 --> 00:32:31.468
and it only takes a couple
extra lines if we actually

00:32:31.468 --> 00:32:33.167
want to train the network.

00:32:33.167 --> 00:32:36.225
So here we're, now we're
running the graph many times

00:32:36.225 --> 00:32:38.577
in a loop so we're doing a four loop

00:32:38.577 --> 00:32:40.739
and in each iteration of the loop,

00:32:40.739 --> 00:32:43.635
we're calling session.run
asking it to compute

00:32:43.635 --> 00:32:45.511
the loss and the gradients.

00:32:45.511 --> 00:32:48.360
And now we're doing a
manual gradient discent step

00:32:48.360 --> 00:32:50.852
using those computed gradients
to now update our current

00:32:50.852 --> 00:32:52.291
values of the weights.

00:32:52.291 --> 00:32:56.159
So if you actually run this
code and plot the losses,

00:32:56.159 --> 00:32:57.770
then you'll see that the loss goes down

00:32:57.770 --> 00:33:00.749
and the network is training and
this is working pretty well.

00:33:00.749 --> 00:33:03.449
So this is kind of like a
super bare bones example

00:33:03.449 --> 00:33:06.113
of training a fully connected
network in TensorFlow.

00:33:06.113 --> 00:33:08.046
But there's a problem here.

00:33:08.046 --> 00:33:11.437
So here, remember that
on the forward pass,

00:33:11.437 --> 00:33:13.256
every time we execute this graph,

00:33:13.256 --> 00:33:15.086
we're actually feeding in the weights.

00:33:15.086 --> 00:33:16.659
We have the weights as Numpy arrays

00:33:16.659 --> 00:33:18.835
and we're explicitly
feeding them into the graph.

00:33:18.835 --> 00:33:21.395
And now when the graph finishes executing

00:33:21.395 --> 00:33:23.230
it's going to give us these gradients.

00:33:23.230 --> 00:33:24.979
And remember the gradients
are the same size

00:33:24.979 --> 00:33:26.339
as the weights.

00:33:26.339 --> 00:33:28.192
So this means that every time
we're running the graph here,

00:33:28.192 --> 00:33:30.675
we're copying the weights
from Numpy arrays into

00:33:30.675 --> 00:33:32.665
TensorFlow then getting the gradients

00:33:32.665 --> 00:33:34.583
and then copying the
gradients from TensorFlow

00:33:34.583 --> 00:33:36.419
back out to Numpy arrays.

00:33:36.419 --> 00:33:37.886
So if you're just running on CPU,

00:33:37.886 --> 00:33:39.849
this is maybe not a huge deal,

00:33:39.849 --> 00:33:42.676
but remember we talked
about CPU GPU bottleneck

00:33:42.676 --> 00:33:44.867
and how it's very expensive
actually to copy data

00:33:44.867 --> 00:33:47.235
between CPU memory and GPU memory.

00:33:47.235 --> 00:33:49.840
So if your network is very
large and your weights

00:33:49.840 --> 00:33:51.096
and gradients were very big,

00:33:51.096 --> 00:33:52.975
then doing something like
this would be super expensive

00:33:52.975 --> 00:33:55.900
and super slow because we'd
be copying all kinds of data

00:33:55.900 --> 00:33:58.423
back and forth between the
CPU and the GPU at every

00:33:58.423 --> 00:33:59.256
time step.

00:33:59.256 --> 00:34:00.441
So that's bad, we don't want to do that.

00:34:00.441 --> 00:34:01.689
We need to fix that.

00:34:01.689 --> 00:34:06.027
So, obviously TensorFlow
has some solution to this.

00:34:06.027 --> 00:34:08.342
And the idea is that
now we want our weights,

00:34:08.342 --> 00:34:11.437
w1 and w2, rather than being
placeholders where we're

00:34:11.437 --> 00:34:14.456
going to, where we expect to
feed them in to the network

00:34:14.456 --> 00:34:17.969
on every forward pass, instead
we define them as variables.

00:34:17.969 --> 00:34:20.477
So a variable is something
is a value that lives inside

00:34:20.477 --> 00:34:23.176
the computational graph
and it's going to persist

00:34:23.176 --> 00:34:25.601
inside the computational
graph across different times

00:34:25.601 --> 00:34:27.347
when you run the same graph.

00:34:27.347 --> 00:34:31.300
So now instead of declaring
these w1 and w2 as placeholders,

00:34:31.300 --> 00:34:33.094
instead we just construct
them as variables.

00:34:33.094 --> 00:34:35.514
But now since they live inside the graph,

00:34:35.514 --> 00:34:38.041
we also need to tell
TensorFlow how they should be

00:34:38.041 --> 00:34:39.219
initialized, right?

00:34:39.219 --> 00:34:40.815
Because in the previous
case we were feeding in

00:34:40.815 --> 00:34:42.697
their values from outside the graph,

00:34:42.697 --> 00:34:44.606
so we initialized them in Numpy,

00:34:44.606 --> 00:34:47.437
but now because these things
live inside the graph,

00:34:47.437 --> 00:34:50.569
TensorFlow is responsible
for initializing them.

00:34:50.569 --> 00:34:53.149
So we need to pass in a
tf.randomnormal operation,

00:34:53.149 --> 00:34:55.689
which again is not
actually initializing them

00:34:55.689 --> 00:34:58.220
when we run this line, this
is just telling TensorFlow

00:34:58.220 --> 00:35:00.627
how we want them to be initialized.

00:35:00.627 --> 00:35:02.048
So it's a little bit of
confusing misdirection

00:35:02.048 --> 00:35:03.215
going on here.

00:35:04.869 --> 00:35:07.478
And now, remember in the previous example

00:35:07.478 --> 00:35:10.207
we were actually updating
the weights outside

00:35:10.207 --> 00:35:11.862
of the computational graph.

00:35:11.862 --> 00:35:14.554
We, in the previous example,
we were computing the gradients

00:35:14.554 --> 00:35:17.219
and then using them to update
the weights as Numpy arrays

00:35:17.219 --> 00:35:19.431
and then feeding in the
updated weights at the next

00:35:19.431 --> 00:35:20.264
time step.

00:35:20.264 --> 00:35:22.742
But now because we want
these weights to live inside

00:35:22.742 --> 00:35:25.818
the graph, this operation
of updating the weights

00:35:25.818 --> 00:35:28.004
needs to also be an operation inside

00:35:28.004 --> 00:35:29.402
the computational graph.

00:35:29.402 --> 00:35:34.242
So now we used this assign
function which mutates

00:35:34.242 --> 00:35:37.020
these variables inside
the computational graph

00:35:37.020 --> 00:35:39.407
and now the mutated value will
persist across multiple runs

00:35:39.407 --> 00:35:41.487
of the same graph.

00:35:41.487 --> 00:35:44.195
So now when we run this graph

00:35:44.195 --> 00:35:45.976
and when we train the network,

00:35:45.976 --> 00:35:48.420
now we need to run the graph
once with a little bit of

00:35:48.420 --> 00:35:50.830
special incantation to tell
TensorFlow to set up these

00:35:50.830 --> 00:35:53.825
variables that are going
to live inside the graph.

00:35:53.825 --> 00:35:55.779
And then once we've done
that initialization,

00:35:55.779 --> 00:35:58.574
now we can run the graph
over and over again.

00:35:58.574 --> 00:36:02.149
And here, we're now only
feeding in the data and labels

00:36:02.149 --> 00:36:05.091
X and Y and the weights are
living inside the graph.

00:36:05.091 --> 00:36:07.035
And here we've asked the network to,

00:36:07.035 --> 00:36:09.517
we've asked TensorFlow to
compute the loss for us.

00:36:09.517 --> 00:36:13.001
And then you might think that
this would train the network,

00:36:13.001 --> 00:36:15.627
but there's actually a bug here.

00:36:15.627 --> 00:36:17.574
So, if you actually run this code,

00:36:17.574 --> 00:36:19.964
and you plot the loss, it doesn't train.

00:36:19.964 --> 00:36:23.401
So that's bad, it's confusing,
like what's going on?

00:36:23.401 --> 00:36:25.385
We wrote this assign
code, we ran the thing,

00:36:25.385 --> 00:36:26.902
like we computed the
loss and the gradients

00:36:26.902 --> 00:36:29.957
and our loss is flat, what's going on?

00:36:29.957 --> 00:36:31.460
Any ideas?

00:36:31.460 --> 00:36:34.595
[student's words obscured
due to lack of microphone]

00:36:34.595 --> 00:36:38.654
Yeah so one hypothesis is
that maybe we're accidentally

00:36:38.654 --> 00:36:41.749
re-initializing the w's
every time we call the graph.

00:36:41.749 --> 00:36:43.854
That's a good hypothesis,
that's actually not the problem

00:36:43.854 --> 00:36:44.979
in this case.

00:36:44.979 --> 00:36:48.057
[student's words obscured
due to lack of microphone]

00:36:48.057 --> 00:36:51.777
Yeah, so the answer is that
we actually need to explicitly

00:36:51.777 --> 00:36:54.339
tell TensorFlow that we
want to run these new w1

00:36:54.339 --> 00:36:56.318
and new w2 operations.

00:36:56.318 --> 00:36:58.835
So we've built up this big
computational graph data

00:36:58.835 --> 00:37:01.699
structure in memory and
now when we call run,

00:37:01.699 --> 00:37:04.894
we only told TensorFlow that
we wanted to compute loss.

00:37:04.894 --> 00:37:07.361
And if you look at the
dependencies among these different

00:37:07.361 --> 00:37:09.155
operations inside the graph,

00:37:09.155 --> 00:37:11.277
you see that in order to compute loss

00:37:11.277 --> 00:37:13.715
we don't actually need to
perform this update operation.

00:37:13.715 --> 00:37:16.366
So TensorFlow is smart and
it only computes the parts

00:37:16.366 --> 00:37:19.416
of the graph that are necessary
for computing the output

00:37:19.416 --> 00:37:21.496
that you asked it to compute.

00:37:21.496 --> 00:37:24.499
So that's kind of a nice thing
because it means it's only

00:37:24.499 --> 00:37:26.656
doing as much work as it needs to,

00:37:26.656 --> 00:37:29.729
but in situations like this it
can be a little bit confusing

00:37:29.729 --> 00:37:32.739
and lead to behavior
that you didn't expect.

00:37:32.739 --> 00:37:34.936
So the solution in this case
is that we actually need to

00:37:34.936 --> 00:37:37.656
explicitly tell TensorFlow
to perform those

00:37:37.656 --> 00:37:39.141
update operations.

00:37:39.141 --> 00:37:41.475
So one thing we could do,
which is what was suggested

00:37:41.475 --> 00:37:45.603
is we could add new w1
and new w2 as outputs

00:37:45.603 --> 00:37:47.761
and just tell TensorFlow
that we want to produce

00:37:47.761 --> 00:37:49.531
these values as outputs.

00:37:49.531 --> 00:37:53.199
But that's a problem
too because the values,

00:37:53.199 --> 00:37:57.366
those new w1, new w2 values
are again these big tensors.

00:37:58.891 --> 00:38:01.123
So now if we tell TensorFlow
we want those as output,

00:38:01.123 --> 00:38:03.068
we're going to again get
this copying behavior

00:38:03.068 --> 00:38:05.138
between CPU and GPU at ever iteration.

00:38:05.138 --> 00:38:07.316
So that's bad, we don't want that.

00:38:07.316 --> 00:38:09.217
So there's a little
trick you can do instead.

00:38:09.217 --> 00:38:11.742
Which is that we add kind of
a dummy node to the graph.

00:38:11.742 --> 00:38:14.255
With these fake data dependencies

00:38:14.255 --> 00:38:16.986
and we just say that
this dummy node updates,

00:38:16.986 --> 00:38:20.307
has these data dependencies
of new w1 and new w2.

00:38:20.307 --> 00:38:22.410
And now when we actually run the graph,

00:38:22.410 --> 00:38:25.803
we tell it to compute both
the loss and this dummy node.

00:38:25.803 --> 00:38:27.840
And this dummy node
doesn't actually return

00:38:27.840 --> 00:38:31.169
any value it just returns
none, but because of this

00:38:31.169 --> 00:38:33.952
dependency that we've put
into the node it ensures

00:38:33.952 --> 00:38:35.980
that when we run the updates value,

00:38:35.980 --> 00:38:38.468
we actually also run
these update operations.

00:38:38.468 --> 00:38:39.551
So, question?

00:38:40.788 --> 00:38:44.955
[student's words obscured
due to lack of microphone]

00:38:45.854 --> 00:38:48.548
Is there a reason why we didn't
put X and Y into the graph?

00:38:48.548 --> 00:38:51.370
And that it stayed as Numpy.

00:38:51.370 --> 00:38:54.725
So in this example we're
reusing X and Y on every,

00:38:54.725 --> 00:38:57.151
we're reusing the same X
and Y on every iteration.

00:38:57.151 --> 00:38:59.156
So you're right, we could
have just also stuck those

00:38:59.156 --> 00:39:02.211
in the graph, but in a
more realistic scenario,

00:39:02.211 --> 00:39:05.301
X and Y will be minibatches
of data so those will actually

00:39:05.301 --> 00:39:07.526
change at every iteration
and we will want to feed

00:39:07.526 --> 00:39:10.122
different values for
those at every iteration.

00:39:10.122 --> 00:39:12.190
So in this case, they could
have stayed in the graph,

00:39:12.190 --> 00:39:14.330
but in most cases they will change,

00:39:14.330 --> 00:39:17.913
so we don't want them
to live in the graph.

00:39:19.388 --> 00:39:21.290
Oh, another question?

00:39:21.290 --> 00:39:25.457
[student's words obscured
due to lack of microphone]

00:39:37.046 --> 00:39:40.927
Yeah, so we've told it,
we had put into TensorFlow

00:39:40.927 --> 00:39:44.305
that the outputs we want
are loss and updates.

00:39:44.305 --> 00:39:47.388
Updates is not actually a real value.

00:39:48.666 --> 00:39:51.801
So when updates evaluates
it just returns none.

00:39:51.801 --> 00:39:54.570
But because of this dependency
we've told it that updates

00:39:54.570 --> 00:39:57.416
depends on these assign operations.

00:39:57.416 --> 00:39:59.356
But these assign operations live inside

00:39:59.356 --> 00:40:02.358
the computational graph and
all live inside GPU memory.

00:40:02.358 --> 00:40:04.426
So then we're doing
these update operations

00:40:04.426 --> 00:40:07.107
entirely on the GPU and
we're no longer copying the

00:40:07.107 --> 00:40:10.190
updated values back out of the graph.

00:40:11.723 --> 00:40:15.112
[student's words obscured
due to lack of microphone]

00:40:15.112 --> 00:40:18.195
So the question is does
tf.group return none?

00:40:18.195 --> 00:40:21.824
So this gets into the
trickiness of TensorFlow.

00:40:21.824 --> 00:40:25.923
So tf.group returns some
crazy TensorFlow value.

00:40:25.923 --> 00:40:29.371
It sort of returns some like
internal TensorFlow node

00:40:29.371 --> 00:40:32.658
operation that we need to
continue building the graph.

00:40:32.658 --> 00:40:34.266
But when you execute the graph,

00:40:34.266 --> 00:40:37.417
and when you tell, inside the session.run,

00:40:37.417 --> 00:40:40.250
when we told it we want it
to compute the concrete value

00:40:40.250 --> 00:40:43.333
from updates, then that returns none.

00:40:43.333 --> 00:40:45.482
So whenever you're working with TensorFlow

00:40:45.482 --> 00:40:47.907
you have this funny indirection
between building the graph

00:40:47.907 --> 00:40:50.781
and the actual output values
during building the graph

00:40:50.781 --> 00:40:53.487
is some funny weird object,
and then you actually get

00:40:53.487 --> 00:40:55.466
a concrete value when you run the graph.

00:40:55.466 --> 00:40:58.658
So here after you run updates,
then the output is none.

00:40:58.658 --> 00:40:59.967
Does that clear it up a little bit?

00:40:59.967 --> 00:41:04.134
[student's words obscured
due to lack of microphone]

00:41:18.796 --> 00:41:20.792
So the question is why is loss a value

00:41:20.792 --> 00:41:22.334
and why is updates none?

00:41:22.334 --> 00:41:24.068
That's just the way that updates works.

00:41:24.068 --> 00:41:25.988
So loss is a value when we compute,

00:41:25.988 --> 00:41:28.597
when we tell TensorFlow
we want to run a tensor,

00:41:28.597 --> 00:41:30.176
then we get the concrete value.

00:41:30.176 --> 00:41:33.147
Updates is this kind of
special other data type

00:41:33.147 --> 00:41:35.753
that does not return a value,
it instead returns none.

00:41:35.753 --> 00:41:38.703
So it's kind of some TensorFlow
magic that's going on there.

00:41:38.703 --> 00:41:40.602
Maybe we can talk offline
if you're still confused.

00:41:40.602 --> 00:41:42.678
[student's words obscured
due to lack of microphone]

00:41:42.678 --> 00:41:46.186
Yeah, yeah, that behavior is
coming from the group method.

00:41:46.186 --> 00:41:48.388
So now, we kind of have
this weird pattern where we

00:41:48.388 --> 00:41:50.548
wanted to do these
different assign operations,

00:41:50.548 --> 00:41:52.492
we have to use this funny tf.group thing.

00:41:52.492 --> 00:41:56.248
That's kind of a pain, so
thankfully TensorFlow gives

00:41:56.248 --> 00:41:58.058
you some convenience
operations that kind of do that

00:41:58.058 --> 00:42:00.004
kind of stuff for you.

00:42:00.004 --> 00:42:01.706
And that's called an optimizer.

00:42:01.706 --> 00:42:06.047
So here we're using a
tf.train.GradientDescentOptimizer

00:42:06.047 --> 00:42:08.458
and we're telling it what
learning rate we want to use.

00:42:08.458 --> 00:42:10.964
And you can imagine that
there's, there's RMSprop,

00:42:10.964 --> 00:42:12.784
there's all kinds of different
optimization algorithms here.

00:42:12.784 --> 00:42:16.284
And now we call optimizer.minimize of loss

00:42:17.311 --> 00:42:19.067
and now this is a pretty magical,

00:42:19.067 --> 00:42:21.204
this is a pretty magical thing,

00:42:21.204 --> 00:42:24.527
because now this call is
aware that these variables

00:42:24.527 --> 00:42:28.106
w1 and w2 are marked as
trainable by default,

00:42:28.106 --> 00:42:30.586
so then internally, inside
this optimizer.minimize

00:42:30.586 --> 00:42:33.104
it's going in and adding
nodes to the graph

00:42:33.104 --> 00:42:35.184
which will compute gradient
of loss with respect

00:42:35.184 --> 00:42:38.159
to w1 and w2 and then it's
also performing that update

00:42:38.159 --> 00:42:40.287
operation for you and it's
doing the grouping operation

00:42:40.287 --> 00:42:42.219
for you and it's doing the assigns.

00:42:42.219 --> 00:42:44.206
It's like doing a lot of
magical stuff inside there.

00:42:44.206 --> 00:42:46.506
But then it ends up giving
you this magical updates value

00:42:46.506 --> 00:42:49.542
which, if you dig through the
code they're actually using

00:42:49.542 --> 00:42:52.344
tf.group so it looks very
similar internally to what

00:42:52.344 --> 00:42:53.518
we saw before.

00:42:53.518 --> 00:42:55.946
And now when we run the
graph inside our loop

00:42:55.946 --> 00:42:58.607
we do the same pattern of
telling it to compute loss

00:42:58.607 --> 00:43:00.004
and updates.

00:43:00.004 --> 00:43:03.444
And every time we tell the
graph to compute updates,

00:43:03.444 --> 00:43:07.450
then it'll actually go
and update the graph.

00:43:07.450 --> 00:43:08.593
Question?

00:43:08.593 --> 00:43:10.959
[student's words obscured
due to lack of microphone]

00:43:10.959 --> 00:43:14.249
Yeah, so what is the
tf.GlobalVariablesInitializer?

00:43:14.249 --> 00:43:18.076
So that's initializing w1
and w2 because these are

00:43:18.076 --> 00:43:20.502
variables which live inside the graph.

00:43:20.502 --> 00:43:22.823
So we need to, when we
saw this, when we create

00:43:22.823 --> 00:43:25.244
the tf.variable we have
this tf.randomnormal

00:43:25.244 --> 00:43:28.366
which is this initialization so the

00:43:28.366 --> 00:43:30.771
tf.GlobalVariablesInitializer
is causing the

00:43:30.771 --> 00:43:34.946
tf.randomnormal to actually run
and generate concrete values

00:43:34.946 --> 00:43:37.733
to initialize those variables.

00:43:37.733 --> 00:43:40.794
[student's words obscured
due to lack of microphone]

00:43:40.794 --> 00:43:42.271
Sorry, what was the question?

00:43:42.271 --> 00:43:45.233
[student's words obscured
due to lack of microphone]

00:43:45.233 --> 00:43:47.935
So it knows that a
placeholder is going to be fed

00:43:47.935 --> 00:43:49.978
outside of the graph and a
variable is something that

00:43:49.978 --> 00:43:51.385
lives inside the graph.

00:43:51.385 --> 00:43:53.770
So I don't know all the
details about how it decides,

00:43:53.770 --> 00:43:56.371
what exactly it decides
to run with that call.

00:43:56.371 --> 00:43:57.680
I think you'd need to dig
through the code to figure

00:43:57.680 --> 00:44:00.384
that out, or maybe it's
documented somewhere.

00:44:00.384 --> 00:44:01.941
So but now we've kind of got this,

00:44:01.941 --> 00:44:04.656
again we've got this full
example of training a

00:44:04.656 --> 00:44:06.130
network in TensorFlow
and we're kind of adding

00:44:06.130 --> 00:44:09.328
bells and whistles to make it
a little bit more convenient.

00:44:09.328 --> 00:44:11.893
So we can also here,
in the previous example

00:44:11.893 --> 00:44:14.027
we were computing the loss
explicitly using our own

00:44:14.027 --> 00:44:16.954
tensor operations, TensorFlow
you can always do that,

00:44:16.954 --> 00:44:19.148
you can use basic tensor
operations to compute

00:44:19.148 --> 00:44:20.739
just about anything you want.

00:44:20.739 --> 00:44:22.730
But TensorFlow also gives
you a bunch of convenience

00:44:22.730 --> 00:44:25.901
functions that compute these
common neural network things

00:44:25.901 --> 00:44:26.734
for you.

00:44:26.734 --> 00:44:30.040
So in this case we can use
tf.losses.mean_squared_error

00:44:30.040 --> 00:44:32.531
and it just does the L2
loss for us so we don't have

00:44:32.531 --> 00:44:36.273
to compute it ourself in terms
of basic tensor operations.

00:44:36.273 --> 00:44:39.194
So another kind of weirdness
here is that it was kind of

00:44:39.194 --> 00:44:42.606
annoying that we had to
explicitly define our inputs

00:44:42.606 --> 00:44:44.729
and define our weights and
then like chain them together

00:44:44.729 --> 00:44:46.667
in the forward pass
using a matrix multiply.

00:44:46.667 --> 00:44:49.958
And in this example we've
actually not put biases

00:44:49.958 --> 00:44:52.820
in the layer because that
would be kind of an extra,

00:44:52.820 --> 00:44:54.291
then we'd have to initialize biases,

00:44:54.291 --> 00:44:56.330
we'd have to get them in the right shape,

00:44:56.330 --> 00:44:58.494
we'd have to broadcast the
biases against the output

00:44:58.494 --> 00:45:00.568
of the matrix multiply
and you can see that that

00:45:00.568 --> 00:45:01.966
would kind of be a lot of code.

00:45:01.966 --> 00:45:03.664
It would be kind of annoying write.

00:45:03.664 --> 00:45:05.231
And once you get to like convolutions

00:45:05.231 --> 00:45:07.626
and batch normalizations
and other types of layers

00:45:07.626 --> 00:45:09.653
this kind of basic way of working,

00:45:09.653 --> 00:45:12.511
of having these variables,
having these inputs and outputs

00:45:12.511 --> 00:45:14.626
and combining them all together with basic

00:45:14.626 --> 00:45:17.403
computational graph operations
could be a little bit

00:45:17.403 --> 00:45:19.749
unwieldy and it could
be really annoying to

00:45:19.749 --> 00:45:21.274
make sure you initialize
the weights with the right

00:45:21.274 --> 00:45:22.954
shapes and all that sort of stuff.

00:45:22.954 --> 00:45:25.353
So as a result, there's a
bunch of sort of higher level

00:45:25.353 --> 00:45:27.535
libraries that wrap around TensorFlow

00:45:27.535 --> 00:45:30.615
and handle some of these details for you.

00:45:30.615 --> 00:45:33.190
So one example that ships with TensorFlow,

00:45:33.190 --> 00:45:35.965
is this tf.layers inside.

00:45:35.965 --> 00:45:38.554
So now in this code example
you can see that our code

00:45:38.554 --> 00:45:41.455
is only explicitly
declaring the X and the Y

00:45:41.455 --> 00:45:44.060
which are the placeholders
for the data and the labels.

00:45:44.060 --> 00:45:48.474
And now we say that H=tf.layers.dense,

00:45:48.474 --> 00:45:53.036
we give it the input X
and we tell it units=H.

00:45:53.036 --> 00:45:55.171
This is again kind of a magical line

00:45:55.171 --> 00:45:57.782
because inside this line,
it's kind of setting up

00:45:57.782 --> 00:46:02.048
w1 and b1, the bias, it's
setting up variables for those

00:46:02.048 --> 00:46:05.222
with the right shapes that
are kind of inside the graph

00:46:05.222 --> 00:46:07.411
but a little bit hidden from us.

00:46:07.411 --> 00:46:10.012
And it's using this
xavier initializer object

00:46:10.012 --> 00:46:12.931
to set up an initialization
strategy for those.

00:46:12.931 --> 00:46:14.730
So before we were doing
that explicitly ourselves

00:46:14.730 --> 00:46:17.200
with the tf.randomnormal business,

00:46:17.200 --> 00:46:19.335
but now here it's kind of
handling some of those details

00:46:19.335 --> 00:46:22.266
for us and it's just spitting out an H,

00:46:22.266 --> 00:46:23.989
which is again the same
sort of H that we saw

00:46:23.989 --> 00:46:26.265
in the previous layer, it's
just doing some of those

00:46:26.265 --> 00:46:27.515
details for us.

00:46:28.487 --> 00:46:30.354
And you can see here,
we're also passing an

00:46:30.354 --> 00:46:33.857
activation=tf.nn.relu so it's
even doing the activation,

00:46:33.857 --> 00:46:36.910
the relu activation function
inside this layer for us.

00:46:36.910 --> 00:46:39.541
So it's taking care of a
lot of these architectural

00:46:39.541 --> 00:46:41.370
details for us.

00:46:41.370 --> 00:46:42.784
Question?

00:46:42.784 --> 00:46:46.446
[student's words obscured
due to lack of microphone]

00:46:46.446 --> 00:46:49.032
Question is does the
xavier initializer default

00:46:49.032 --> 00:46:51.168
to particular distribution?

00:46:51.168 --> 00:46:53.887
I'm sure it has some default,
I'm not sure what it is.

00:46:53.887 --> 00:46:55.850
I think you'll have to
look at the documentation.

00:46:55.850 --> 00:46:58.010
But it seems to be a
reasonable strategy, I guess.

00:46:58.010 --> 00:46:59.625
And in fact if you run this code,

00:46:59.625 --> 00:47:01.303
it converges much faster
than the previous one

00:47:01.303 --> 00:47:04.111
because the initialization is better.

00:47:04.111 --> 00:47:06.047
And you can see that
we're using two calls to

00:47:06.047 --> 00:47:08.037
tf.layers and this lets us build our model

00:47:08.037 --> 00:47:10.465
without doing all these
explicit bookkeeping details

00:47:10.465 --> 00:47:11.911
ourself.

00:47:11.911 --> 00:47:14.273
So this is maybe a little
bit more convenient.

00:47:14.273 --> 00:47:18.682
But tf.contrib.layer is really
not the only game in town.

00:47:18.682 --> 00:47:21.260
There's like a lot of different
higher level libraries

00:47:21.260 --> 00:47:23.349
that people build on top of TensorFlow.

00:47:23.349 --> 00:47:26.841
And it's kind of due to this
basic impotence mis-match

00:47:26.841 --> 00:47:30.315
where the computational graph
is relatively low level thing,

00:47:30.315 --> 00:47:32.356
but when we're working
with neural networks

00:47:32.356 --> 00:47:34.309
we have this concept of layers and weights

00:47:34.309 --> 00:47:36.426
and some layers have weights
associated with them,

00:47:36.426 --> 00:47:38.949
and we typically think at
a slightly higher level

00:47:38.949 --> 00:47:41.866
of abstraction than this
raw computational graph.

00:47:41.866 --> 00:47:44.899
So that's what these various
packages are trying to

00:47:44.899 --> 00:47:46.563
help you out and let you
work at this higher layer

00:47:46.563 --> 00:47:48.503
of abstraction.

00:47:48.503 --> 00:47:50.781
So another very popular
package that you may have

00:47:50.781 --> 00:47:52.460
seen before is Keras.

00:47:52.460 --> 00:47:56.275
Keras is a very beautiful,
nice API that sits on top of

00:47:56.275 --> 00:47:59.051
TensorFlow and handles
sort of building up these

00:47:59.051 --> 00:48:02.806
computational graph for
you up in the back end.

00:48:02.806 --> 00:48:05.029
By the way, Keras also
supports Theano as a back end,

00:48:05.029 --> 00:48:07.704
so that's also kind of nice.

00:48:07.704 --> 00:48:09.693
And in this example you
can see we build the model

00:48:09.693 --> 00:48:10.958
as a sequence of layers.

00:48:10.958 --> 00:48:12.738
We build some optimizer object

00:48:12.738 --> 00:48:15.589
and we call model.compile
and this does a lot of magic

00:48:15.589 --> 00:48:17.910
in the back end to build the graph.

00:48:17.910 --> 00:48:20.759
And now we can call model.fit
and that does the whole

00:48:20.759 --> 00:48:22.797
training procedure for us magically.

00:48:22.797 --> 00:48:24.914
So I don't know all the
details of how this works,

00:48:24.914 --> 00:48:26.211
but I know Keras is very popular,

00:48:26.211 --> 00:48:27.606
so you might consider using
it if you're talking about

00:48:27.606 --> 00:48:28.523
TensorFlow.

00:48:29.797 --> 00:48:31.270
Question?

00:48:31.270 --> 00:48:35.437
[student's words obscured
due to lack of microphone]

00:48:41.717 --> 00:48:43.899
Yeah, so the question is
like why there's no explicit

00:48:43.899 --> 00:48:45.525
CPU, GPU going on here.

00:48:45.525 --> 00:48:48.409
So I've kind of left that
out to keep the code clean.

00:48:48.409 --> 00:48:50.042
But you saw at the beginning examples

00:48:50.042 --> 00:48:52.147
it was pretty easy to
flop all these things

00:48:52.147 --> 00:48:54.607
between CPU and GPU and there
was either some global flag

00:48:54.607 --> 00:48:56.369
or some different data type

00:48:56.369 --> 00:48:59.449
or some with statement and
it's usually relatively simple

00:48:59.449 --> 00:49:01.635
and just about one line
to swap in each case.

00:49:01.635 --> 00:49:03.349
But exactly what that line looks like

00:49:03.349 --> 00:49:06.149
differs a bit depending on the situation.

00:49:06.149 --> 00:49:09.456
So there's actually like
this whole large set

00:49:09.456 --> 00:49:12.527
of higher level TensorFlow
wrappers that you might see

00:49:12.527 --> 00:49:14.186
out there in the wild.

00:49:14.186 --> 00:49:17.109
And it seems that like
even people within Google

00:49:17.109 --> 00:49:21.276
can't really agree on which
one is the right one to use.

00:49:22.230 --> 00:49:24.690
So Keras and TFLearn are
third party libraries

00:49:24.690 --> 00:49:26.829
that are out there on the
internet by other people.

00:49:26.829 --> 00:49:29.768
But there's these three different ones,

00:49:29.768 --> 00:49:32.563
tf.layers, TF-Slim and tf.contrib.learn

00:49:32.563 --> 00:49:35.355
that all ship with TensorFlow,
that are all kind of

00:49:35.355 --> 00:49:37.890
doing a slightly different version of this

00:49:37.890 --> 00:49:39.727
higher level wrapper thing.

00:49:39.727 --> 00:49:41.765
There's another framework
also from Google,

00:49:41.765 --> 00:49:44.109
but not shipping with
TensorFlow called Pretty Tensor

00:49:44.109 --> 00:49:46.291
that does the same sort of thing.

00:49:46.291 --> 00:49:48.599
And I guess none of these
were good enough for DeepMind,

00:49:48.599 --> 00:49:50.269
because they went ahead a couple weeks ago

00:49:50.269 --> 00:49:52.488
and wrote and released
their very own high level

00:49:52.488 --> 00:49:54.530
TensorFlow wrapper called Sonnet.

00:49:54.530 --> 00:49:57.570
So I wouldn't begrudge you
if you were kind of confused

00:49:57.570 --> 00:49:59.506
by all these things.

00:49:59.506 --> 00:50:00.715
There's a lot of different choices.

00:50:00.715 --> 00:50:03.113
They don't always play
nicely with each other.

00:50:03.113 --> 00:50:07.423
But you have a lot of
options, so that's good.

00:50:07.423 --> 00:50:09.123
TensorFlow has pretrained models.

00:50:09.123 --> 00:50:11.112
There's some examples in
TF-Slim, and in Keras.

00:50:11.112 --> 00:50:14.072
'Cause remember retrained
models are super important

00:50:14.072 --> 00:50:15.874
when you're training your own things.

00:50:15.874 --> 00:50:17.931
There's also this idea of Tensorboard

00:50:17.931 --> 00:50:19.634
where you can load up your,

00:50:19.634 --> 00:50:21.072
I don't want to get into details,

00:50:21.072 --> 00:50:22.834
but Tensorboard you can
add sort of instrumentation

00:50:22.834 --> 00:50:24.733
to your code and then
plot losses and things

00:50:24.733 --> 00:50:27.747
as you go through the training process.

00:50:27.747 --> 00:50:29.535
TensorFlow also let's you run distributed

00:50:29.535 --> 00:50:31.372
where you can break up
a computational graph

00:50:31.372 --> 00:50:32.760
run on different machines.

00:50:32.760 --> 00:50:35.222
That's super cool but I
think probably not anyone

00:50:35.222 --> 00:50:37.613
outside of Google is really
using that to great success

00:50:37.613 --> 00:50:40.733
these days, but if you do
want to run distributed stuff

00:50:40.733 --> 00:50:44.193
probably TensorFlow is the
main game in town for that.

00:50:44.193 --> 00:50:46.834
A side note is that a lot
of the design of TensorFlow

00:50:46.834 --> 00:50:49.992
is kind of spiritually inspired
by this earlier framework

00:50:49.992 --> 00:50:51.533
called Theano from Montreal.

00:50:51.533 --> 00:50:54.008
I don't want to go
through the details here,

00:50:54.008 --> 00:50:55.933
just if you go through
these slides on your own,

00:50:55.933 --> 00:50:58.127
you can see that the code
for Theano ends up looking

00:50:58.127 --> 00:50:59.979
very similar to TensorFlow.

00:50:59.979 --> 00:51:01.420
Where we define some variables,

00:51:01.420 --> 00:51:03.512
we do some forward pass,
we compute some gradients,

00:51:03.512 --> 00:51:05.978
and we compile some function,
then we run the function

00:51:05.978 --> 00:51:08.034
over and over to train the network.

00:51:08.034 --> 00:51:10.290
So it kind of looks a lot like TensorFlow.

00:51:10.290 --> 00:51:13.010
So we still have a lot to get through,

00:51:13.010 --> 00:51:14.462
so I'm going to move on to PyTorch

00:51:14.462 --> 00:51:16.671
and maybe take questions at the end.

00:51:16.671 --> 00:51:20.770
So, PyTorch from Facebook
is kind of different from

00:51:20.770 --> 00:51:22.868
TensorFlow in that we have
sort of three explicit

00:51:22.868 --> 00:51:26.397
different layers of
abstraction inside PyTorch.

00:51:26.397 --> 00:51:29.415
So PyTorch has this tensor
object which is just like a

00:51:29.415 --> 00:51:30.619
Numpy array.

00:51:30.619 --> 00:51:33.603
It's just an imperative array,
it doesn't know anything

00:51:33.603 --> 00:51:36.770
about deep learning,
but it can run with GPU.

00:51:36.770 --> 00:51:38.676
We have this variable
object which is a node in a

00:51:38.676 --> 00:51:42.274
computational graph which
builds up computational graphs,

00:51:42.274 --> 00:51:44.093
lets you compute gradients,
that sort of thing.

00:51:44.093 --> 00:51:46.312
And we have a module object
which is a neural network

00:51:46.312 --> 00:51:48.573
layer that you can compose
together these modules

00:51:48.573 --> 00:51:50.766
to build big networks.

00:51:50.766 --> 00:51:52.973
So if you kind of want to
think about rough equivalents

00:51:52.973 --> 00:51:55.971
between PyTorch and TensorFlow
you can think of the

00:51:55.971 --> 00:51:58.759
PyTorch tensor as fulfilling the same role

00:51:58.759 --> 00:52:01.457
as the Numpy array in TensorFlow.

00:52:01.457 --> 00:52:04.602
The PyTorch variable is similar
to the TensorFlow tensor

00:52:04.602 --> 00:52:07.549
or variable or placeholder,
which are all sort of nodes

00:52:07.549 --> 00:52:08.803
in a computational graph.

00:52:08.803 --> 00:52:11.970
And now the PyTorch module
is kind of equivalent

00:52:11.970 --> 00:52:16.288
to these higher level things
from tf.slim or tf.layers

00:52:16.288 --> 00:52:18.448
or sonnet or these other
higher level frameworks.

00:52:18.448 --> 00:52:21.102
So right away one thing
to notice about PyTorch

00:52:21.102 --> 00:52:24.072
is that because it ships with
this high level abstraction

00:52:24.072 --> 00:52:26.696
and like one really nice
higher level abstraction

00:52:26.696 --> 00:52:28.947
called modules on its own,
there's sort of less choice

00:52:28.947 --> 00:52:29.780
involved.

00:52:29.780 --> 00:52:32.534
Just stick with nnmodules
and you'll be good to go.

00:52:32.534 --> 00:52:35.809
You don't need to worry about
which higher level wrapper

00:52:35.809 --> 00:52:36.642
to use.

00:52:37.777 --> 00:52:41.944
So PyTorch tensors, as I said,
are just like Numpy arrays

00:52:43.660 --> 00:52:46.181
so here on the right we've done
an entire two layer network

00:52:46.181 --> 00:52:47.787
using entirely PyTorch tensors.

00:52:47.787 --> 00:52:50.279
One thing to note is that
we're not importing Numpy here

00:52:50.279 --> 00:52:51.379
at all anymore.

00:52:51.379 --> 00:52:53.910
We're just doing all these
operations using PyTorch tensors.

00:52:53.910 --> 00:52:58.624
And this code looks exactly
like the two layer net code

00:52:58.624 --> 00:53:01.245
that you wrote in Numpy
on the first homework.

00:53:01.245 --> 00:53:05.774
So you set up some random
data, you use some operations

00:53:05.774 --> 00:53:07.127
to compute the forward pass.

00:53:07.127 --> 00:53:09.332
And then we're explicitly
viewing the backward pass

00:53:09.332 --> 00:53:10.165
ourself.

00:53:10.165 --> 00:53:12.794
Just sort of backhopping
through the network,

00:53:12.794 --> 00:53:15.980
through the operations, just
as you did on homework one.

00:53:15.980 --> 00:53:18.241
And now we're doing a
manual update of the weights

00:53:18.241 --> 00:53:22.672
using a learning rate and
using our computed gradients.

00:53:22.672 --> 00:53:24.681
But the major difference
between the PyTorch tensor

00:53:24.681 --> 00:53:27.785
and Numpy arrays is that they run on GPU

00:53:27.785 --> 00:53:30.651
so all you have to do
to make this code run on

00:53:30.651 --> 00:53:33.034
GPU is use a different data type.

00:53:33.034 --> 00:53:35.092
Rather than using torch.FloatTensor,

00:53:35.092 --> 00:53:39.259
you do torch.cuda.FloatTensor,
cast all of your tensors

00:53:40.152 --> 00:53:42.174
to this new datatype and
everything runs magically

00:53:42.174 --> 00:53:43.709
on the GPU.

00:53:43.709 --> 00:53:47.637
You should think of PyTorch
tensors as just Numpy plus GPU.

00:53:47.637 --> 00:53:49.401
That's exactly what it
is, nothing specific

00:53:49.401 --> 00:53:50.818
to deep learning.

00:53:52.638 --> 00:53:55.278
So the next layer of abstraction
in PyTorch is the variable.

00:53:55.278 --> 00:53:58.440
So this is, once we moved
from tensors to variables

00:53:58.440 --> 00:54:00.702
now we're building computational graphs

00:54:00.702 --> 00:54:02.194
and we're able to take
gradients automatically

00:54:02.194 --> 00:54:03.460
and everything like that.

00:54:03.460 --> 00:54:07.627
So here, if X is a variable,
then x.data is a tensor

00:54:08.890 --> 00:54:12.164
and x.grad is another variable
containing the gradients

00:54:12.164 --> 00:54:14.007
of the loss with respect to that tensor.

00:54:14.007 --> 00:54:15.913
So x.grad.data is an
actual tensor containing

00:54:15.913 --> 00:54:17.246
those gradients.

00:54:18.972 --> 00:54:22.387
And PyTorch tensors and variables
have the exact same API.

00:54:22.387 --> 00:54:25.606
So any code that worked on
PyTorch tensors you can just

00:54:25.606 --> 00:54:28.457
make them variables instead
and run the same code,

00:54:28.457 --> 00:54:30.292
except now you're building
up a computational graph

00:54:30.292 --> 00:54:34.459
rather than just doing
these imperative operations.

00:54:35.943 --> 00:54:38.553
So here when we create these variables

00:54:38.553 --> 00:54:41.234
each call to the variable
constructor wraps a PyTorch

00:54:41.234 --> 00:54:43.652
tensor and then also gives
a flag whether or not

00:54:43.652 --> 00:54:47.461
we want to compute gradients
with respect to this variable.

00:54:47.461 --> 00:54:49.412
And now in the forward
pass it looks exactly like

00:54:49.412 --> 00:54:52.012
it did before in the variable
in the case with tensors

00:54:52.012 --> 00:54:54.073
because they have the same API.

00:54:54.073 --> 00:54:55.667
So now we're computing our predictions,

00:54:55.667 --> 00:54:58.431
we're computing our loss
in kind of this imperative

00:54:58.431 --> 00:54:59.683
kind of way.

00:54:59.683 --> 00:55:03.492
And then we call loss.backwards
and now all these gradients

00:55:03.492 --> 00:55:05.251
come out for us.

00:55:05.251 --> 00:55:06.790
And then we can make
a gradient update step

00:55:06.790 --> 00:55:09.214
on our weights using the
gradients that are now present

00:55:09.214 --> 00:55:11.528
in the w1.grad.data.

00:55:11.528 --> 00:55:16.227
So this ends up looking
quite like the Numpy case,

00:55:16.227 --> 00:55:18.137
except all the gradients come for free.

00:55:18.137 --> 00:55:20.596
One thing to note that's
kind of different between

00:55:20.596 --> 00:55:23.353
PyTorch and TensorFlow is
that in a TensorFlow case

00:55:23.353 --> 00:55:25.289
we were building up this explicit graph,

00:55:25.289 --> 00:55:27.132
then running the graph many times.

00:55:27.132 --> 00:55:30.308
Here in PyTorch, instead
we're building up a new graph

00:55:30.308 --> 00:55:32.152
every time we do a forward pass.

00:55:32.152 --> 00:55:33.887
And this makes the code
look a bit cleaner.

00:55:33.887 --> 00:55:35.284
And it has some other
implications that we'll

00:55:35.284 --> 00:55:37.058
get to in a bit.

00:55:37.058 --> 00:55:40.630
So in PyTorch you can define
your own new autograd functions

00:55:40.630 --> 00:55:42.933
by defining the forward and
backward in terms of tensors.

00:55:42.933 --> 00:55:45.954
This ends up looking kind
of like the module layers

00:55:45.954 --> 00:55:48.303
code that you write for homework two.

00:55:48.303 --> 00:55:50.404
Where you can implement
forward and backward using

00:55:50.404 --> 00:55:52.676
tensor operations and then
stick these things inside

00:55:52.676 --> 00:55:54.433
computational graph.

00:55:54.433 --> 00:55:56.297
So here we're defining our own relu

00:55:56.297 --> 00:56:00.654
and then we can actually
go in and use our own relu

00:56:00.654 --> 00:56:02.754
operation and now stick it
inside our computational graph

00:56:02.754 --> 00:56:05.214
and define our own operations this way.

00:56:05.214 --> 00:56:06.857
But most of the time you
will probably not need

00:56:06.857 --> 00:56:09.097
to define your own autograd operations.

00:56:09.097 --> 00:56:10.814
Most of the times the
operations you need will

00:56:10.814 --> 00:56:14.246
mostly be already implemented for you.

00:56:14.246 --> 00:56:16.293
So in TensorFlow we saw,

00:56:16.293 --> 00:56:19.054
if we can move to something
like Keras or TF.Learn

00:56:19.054 --> 00:56:21.253
and this gives us a higher
level API to work with,

00:56:21.253 --> 00:56:23.349
rather than this raw computational graphs.

00:56:23.349 --> 00:56:25.433
The equivalent in PyTorch
is the nn package.

00:56:25.433 --> 00:56:29.448
Where it provides these high
level wrappers for working

00:56:29.448 --> 00:56:30.948
with these things.

00:56:31.882 --> 00:56:33.454
But unlike TensorFlow
there's only one of them.

00:56:33.454 --> 00:56:35.837
And it works pretty well,
so just use that if you're

00:56:35.837 --> 00:56:37.772
using PyTorch.

00:56:37.772 --> 00:56:39.374
So here, this ends up
kind of looking like Keras

00:56:39.374 --> 00:56:42.196
where we define our model
as some sequence of layers.

00:56:42.196 --> 00:56:44.436
Our linear and relu operations.

00:56:44.436 --> 00:56:47.574
And we use some loss function
defined in the nn package

00:56:47.574 --> 00:56:49.816
that's our mean squared error loss.

00:56:49.816 --> 00:56:51.593
And now inside each iteration of our loop

00:56:51.593 --> 00:56:53.716
we can run data forward
through the model to get

00:56:53.716 --> 00:56:55.214
our predictions.

00:56:55.214 --> 00:56:57.511
We can run the predictions
forward through the loss function

00:56:57.511 --> 00:56:59.054
to get our scale or loss,

00:56:59.054 --> 00:57:01.177
then we can call loss.backward,
get all our gradients

00:57:01.177 --> 00:57:04.021
for free and then loop over
the parameters of the models

00:57:04.021 --> 00:57:06.143
and do our explicit gradient
descent step to update

00:57:06.143 --> 00:57:07.273
the models.

00:57:07.273 --> 00:57:09.054
And again we see that we're
sort of building up this

00:57:09.054 --> 00:57:12.749
new computational graph every
time we do a forward pass.

00:57:12.749 --> 00:57:14.714
And just like we saw in TensorFlow,

00:57:14.714 --> 00:57:17.017
PyTorch provides these
optimizer operations

00:57:17.017 --> 00:57:19.655
that kind of abstract
away this updating logic

00:57:19.655 --> 00:57:21.758
and implement fancier
update rules like Adam

00:57:21.758 --> 00:57:23.000
and whatnot.

00:57:23.000 --> 00:57:25.038
So here we're constructing
an optimizer object

00:57:25.038 --> 00:57:27.034
telling it that we want
it to optimize over the

00:57:27.034 --> 00:57:28.771
parameters of the model.

00:57:28.771 --> 00:57:31.115
Giving it some learning rate
under the hyper parameters.

00:57:31.115 --> 00:57:33.438
And now after we compute our gradients

00:57:33.438 --> 00:57:35.356
we can just call
optimizer.step and it updates

00:57:35.356 --> 00:57:39.810
all the parameters of the
model for us right here.

00:57:39.810 --> 00:57:41.951
So another common thing
you'll do in PyTorch

00:57:41.951 --> 00:57:44.714
a lot is define your own nn modules.

00:57:44.714 --> 00:57:47.268
So typically you'll write your own class

00:57:47.268 --> 00:57:49.961
which defines you entire model as a single

00:57:49.961 --> 00:57:51.801
new nn module class.

00:57:51.801 --> 00:57:54.979
And a module is just kind
of a neural network layer

00:57:54.979 --> 00:57:57.678
that can contain either
other other modules

00:57:57.678 --> 00:58:01.043
or trainable weights or
other other kinds of state.

00:58:01.043 --> 00:58:04.142
So in this case we can redo
the two layer net example

00:58:04.142 --> 00:58:07.051
by defining our own nn module class.

00:58:07.051 --> 00:58:09.925
So now here in the
initializer of the class

00:58:09.925 --> 00:58:11.672
we're assigning this linear1 and linear2.

00:58:11.672 --> 00:58:13.853
We're constructing
these new module objects

00:58:13.853 --> 00:58:17.257
and then store them
inside of our own class.

00:58:17.257 --> 00:58:20.335
And now in the forward pass
we can use both our own

00:58:20.335 --> 00:58:22.832
internal modules as well as
arbitrary autograd operations

00:58:22.832 --> 00:58:26.466
on variables to compute
the output of our network.

00:58:26.466 --> 00:58:29.782
So here we receive the, inside
this forward method here,

00:58:29.782 --> 00:58:31.594
the input acts as a variable,

00:58:31.594 --> 00:58:34.213
then we pass the variable
to our self.linear1

00:58:34.213 --> 00:58:35.817
for the first layer.

00:58:35.817 --> 00:58:38.129
We use an autograd op
clamp to complete the relu,

00:58:38.129 --> 00:58:40.233
we pass the output of
that to the second linear

00:58:40.233 --> 00:58:42.233
and then that gives us our output.

00:58:42.233 --> 00:58:44.732
And now the rest of this
code for training this thing

00:58:44.732 --> 00:58:46.633
looks pretty much the same.

00:58:46.633 --> 00:58:48.455
Where we build an optimizer and loop over

00:58:48.455 --> 00:58:50.916
and on ever iteration
feed data to the model,

00:58:50.916 --> 00:58:52.777
compute the gradients with loss.backwards,

00:58:52.777 --> 00:58:54.676
call optimizer.step.

00:58:54.676 --> 00:58:57.924
So this is like relatively characteristic

00:58:57.924 --> 00:59:00.233
of what you might see
in a lot of PyTorch type

00:59:00.233 --> 00:59:01.817
training scenarios.

00:59:01.817 --> 00:59:02.964
Where you define your own class,

00:59:02.964 --> 00:59:04.932
defining your own model
that contains other modules

00:59:04.932 --> 00:59:07.103
and whatnot and then you
have some explicit training

00:59:07.103 --> 00:59:11.166
loop like this that
runs it and updates it.

00:59:11.166 --> 00:59:13.353
One kind of nice quality
of life thing that you have

00:59:13.353 --> 00:59:16.057
in PyTorch is a dataloader.

00:59:16.057 --> 00:59:18.873
So a dataloader can handle
building minibatches for you.

00:59:18.873 --> 00:59:21.273
It can handle some of the
multi-threading that we talked

00:59:21.273 --> 00:59:23.876
about for you, where it can
actually use multiple threads

00:59:23.876 --> 00:59:25.934
in the background to
build many batches for you

00:59:25.934 --> 00:59:27.273
and stream off disk.

00:59:27.273 --> 00:59:30.777
So here a dataloader wraps
a dataset and provides

00:59:30.777 --> 00:59:33.221
some of these abstractions for you.

00:59:33.221 --> 00:59:35.562
And in practice when you
want to run your own data,

00:59:35.562 --> 00:59:38.138
you typically will write
your own dataset class

00:59:38.138 --> 00:59:40.208
which knows how to read
your particular type of data

00:59:40.208 --> 00:59:42.251
off whatever source you
want and then wrap it in

00:59:42.251 --> 00:59:44.458
a data loader and train with that.

00:59:44.458 --> 00:59:47.631
So, here we can see that
now we're iterating over

00:59:47.631 --> 00:59:50.444
the dataloader object
and at every iteration

00:59:50.444 --> 00:59:52.233
this is yielding minibatches of data.

00:59:52.233 --> 00:59:55.527
And it's internally handling
the shuffling of the data

00:59:55.527 --> 00:59:57.576
and multithreaded dataloading
and all this sort of stuff

00:59:57.576 --> 00:59:58.409
for you.

00:59:58.409 --> 01:00:00.654
So this is kind of a
completely PyTorch example

01:00:00.654 --> 01:00:02.494
and a lot of PyTorch
training code ends up looking

01:00:02.494 --> 01:00:04.161
something like this.

01:00:05.583 --> 01:00:07.587
PyTorch provides pretrained models.

01:00:07.587 --> 01:00:09.469
And this is probably the
slickest pretrained model

01:00:09.469 --> 01:00:11.521
experience I've ever seen.

01:00:11.521 --> 01:00:14.268
You just say torchvision.models.alexnet
pretained=true.

01:00:14.268 --> 01:00:16.951
That'll go down in the background,
download the pretrained

01:00:16.951 --> 01:00:18.759
weights for you if you
don't already have them,

01:00:18.759 --> 01:00:21.052
and then it's right
there, you're good to go.

01:00:21.052 --> 01:00:24.242
So this is super easy to use.

01:00:24.242 --> 01:00:27.094
PyTorch also has, there's
also a package called Visdom

01:00:27.094 --> 01:00:30.253
that lets you visualize some
of these loss statistics

01:00:30.253 --> 01:00:33.600
somewhat similar to Tensorboard.

01:00:33.600 --> 01:00:35.168
So that's kind of nice,
I haven't actually gotten

01:00:35.168 --> 01:00:36.934
a chance to play around with
this myself so I can't really

01:00:36.934 --> 01:00:38.569
speak to how useful it is,

01:00:38.569 --> 01:00:40.927
but one of the major
differences between Tensorboard

01:00:40.927 --> 01:00:43.769
and Visdom is that Tensorboard
actually lets you visualize

01:00:43.769 --> 01:00:45.907
the structure of the computational graph.

01:00:45.907 --> 01:00:47.984
Which is really cool, a really
useful debugging strategy.

01:00:47.984 --> 01:00:50.989
And Visdom does not have
that functionality yet.

01:00:50.989 --> 01:00:53.011
But I've never really used
this myself so I can't really

01:00:53.011 --> 01:00:54.761
speak to its utility.

01:00:56.350 --> 01:00:58.627
As a bit of an aside, PyTorch
is kind of an evolution of,

01:00:58.627 --> 01:01:01.747
kind of a newer updated
version of an older framework

01:01:01.747 --> 01:01:04.086
called Torch which I worked
with a lot in the last

01:01:04.086 --> 01:01:05.491
couple of years.

01:01:05.491 --> 01:01:07.577
And I don't want to go
through the details here,

01:01:07.577 --> 01:01:10.569
but PyTorch is pretty much
better in a lot of ways

01:01:10.569 --> 01:01:13.280
than the old Lua Torch, but
they actually share a lot

01:01:13.280 --> 01:01:15.657
of the same back end C code
for computing with tensors

01:01:15.657 --> 01:01:18.100
and GPU operations on tensors and whatnot.

01:01:18.100 --> 01:01:19.554
So if you look through this Torch example,

01:01:19.554 --> 01:01:21.906
some of it ends up looking
kind of similar to PyTorch,

01:01:21.906 --> 01:01:23.369
some of it's a bit different.

01:01:23.369 --> 01:01:25.957
Maybe you can step through this offline.

01:01:25.957 --> 01:01:28.361
But kind of the high
level differences between

01:01:28.361 --> 01:01:31.229
Torch and PyTorch are that
Torch is actually in Lua,

01:01:31.229 --> 01:01:33.011
not Python, unlike these other things.

01:01:33.011 --> 01:01:37.748
So learning Lua is a bit of
a turn off for some people.

01:01:37.748 --> 01:01:40.009
Torch doesn't have autograd.

01:01:40.009 --> 01:01:41.710
Torch is also older, so it's more stable,

01:01:41.710 --> 01:01:43.491
less susceptible to bugs,
there's maybe more example code

01:01:43.491 --> 01:01:44.324
for Torch.

01:01:45.230 --> 01:01:47.214
They're about the same speeds,
that's not really a concern.

01:01:47.214 --> 01:01:49.827
But in PyTorch it's in
Python which is great,

01:01:49.827 --> 01:01:52.270
you've got autograd which
makes it a lot simpler

01:01:52.270 --> 01:01:54.531
to write complex models.

01:01:54.531 --> 01:01:56.422
In Lua Torch you end up
writing a lot of your own

01:01:56.422 --> 01:01:59.670
back prop code sometimes, so
that's a little bit annoying.

01:01:59.670 --> 01:02:01.650
But PyTorch is newer,
there's less existing code,

01:02:01.650 --> 01:02:03.689
it's still subject to change.

01:02:03.689 --> 01:02:06.051
So it's a little bit more of an adventure.

01:02:06.051 --> 01:02:08.145
But at least for me, I kind of prefer,

01:02:08.145 --> 01:02:10.162
I don't really see much reason for myself

01:02:10.162 --> 01:02:13.228
to use Torch over PyTorch
anymore at this time.

01:02:13.228 --> 01:02:15.848
So I'm pretty much using
PyTorch exclusively for

01:02:15.848 --> 01:02:17.765
all my work these days.

01:02:18.606 --> 01:02:20.557
We talked about this a
little bit about this idea

01:02:20.557 --> 01:02:22.531
of static versus dynamic graphs.

01:02:22.531 --> 01:02:24.350
And this is one of the main
distinguishing features

01:02:24.350 --> 01:02:26.291
between PyTorch and TensorFlow.

01:02:26.291 --> 01:02:29.416
So we saw in TensorFlow
you have these two stages

01:02:29.416 --> 01:02:31.667
of operation where first you build up this

01:02:31.667 --> 01:02:34.371
computational graph, then you
run the computational graph

01:02:34.371 --> 01:02:37.246
over and over again many
many times reusing that same

01:02:37.246 --> 01:02:38.145
graph.

01:02:38.145 --> 01:02:40.209
That's called a static
computational graph 'cause there's

01:02:40.209 --> 01:02:42.403
only one of them.

01:02:42.403 --> 01:02:44.940
And we saw PyTorch is quite
different where we're actually

01:02:44.940 --> 01:02:46.829
building up this new computational graph,

01:02:46.829 --> 01:02:48.771
this new fresh thing
on every forward pass.

01:02:48.771 --> 01:02:52.259
That's called a dynamic
computational graph.

01:02:52.259 --> 01:02:54.075
For kind of simple cases,
with kind of feed forward

01:02:54.075 --> 01:02:57.053
neural networks, it doesn't
really make a huge difference,

01:02:57.053 --> 01:02:58.467
the code ends up kind of similarly

01:02:58.467 --> 01:03:00.225
and they work kind of similarly,

01:03:00.225 --> 01:03:02.079
but I do want to talk a bit
about some of the implications

01:03:02.079 --> 01:03:04.227
of static versus dynamic.

01:03:04.227 --> 01:03:07.102
And what are the tradeoffs of those two.

01:03:07.102 --> 01:03:08.947
So one kind of nice
idea with static graphs

01:03:08.947 --> 01:03:11.331
is that because we're
kind of building up one

01:03:11.331 --> 01:03:15.286
computational graph once, and
then reusing it many times,

01:03:15.286 --> 01:03:17.355
the framework might have
the opportunity to go in

01:03:17.355 --> 01:03:19.571
and do optimizations on that graph.

01:03:19.571 --> 01:03:22.821
And kind of fuse some operations,
reorder some operations,

01:03:22.821 --> 01:03:24.520
figure out the most
efficient way to operate

01:03:24.520 --> 01:03:26.809
that graph so it can be really efficient.

01:03:26.809 --> 01:03:28.725
And because we're going
to reuse that graph

01:03:28.725 --> 01:03:31.790
many times, maybe that
optimization process

01:03:31.790 --> 01:03:33.039
is expensive up front,

01:03:33.039 --> 01:03:34.947
but we can amortize that
cost with the speedups

01:03:34.947 --> 01:03:37.230
that we've gotten when we run
the graph many many times.

01:03:37.230 --> 01:03:40.162
So as kind of a concrete example,

01:03:40.162 --> 01:03:41.814
maybe if you write some
graph which has convolution

01:03:41.814 --> 01:03:44.085
and relu operations kind
of one after another,

01:03:44.085 --> 01:03:48.250
you might imagine that
some fancy graph optimizer

01:03:48.250 --> 01:03:51.865
could go in and actually
output, like emit custom code

01:03:51.865 --> 01:03:54.530
which has fused operations,
fusing the convolution

01:03:54.530 --> 01:03:56.371
and the relu so now it's
computing the same thing

01:03:56.371 --> 01:04:00.525
as the code you wrote, but
now might be able to be

01:04:00.525 --> 01:04:03.445
executed more efficiently.

01:04:03.445 --> 01:04:07.909
So I'm not too sure on exactly
what the state in practice

01:04:07.909 --> 01:04:10.419
of TensorFlow graph
optimization is right now,

01:04:10.419 --> 01:04:14.469
but at least in principle,
this is one place where

01:04:14.469 --> 01:04:17.747
static graph really, you
can have the potential for

01:04:17.747 --> 01:04:20.131
doing this optimization in static graphs

01:04:20.131 --> 01:04:24.298
where maybe it would be not so
tractable for dynamic graphs.

01:04:25.504 --> 01:04:26.937
Another kind of subtle point
about static versus dynamic

01:04:26.937 --> 01:04:28.931
is this idea of serialization.

01:04:28.931 --> 01:04:32.347
So with a static graph you
can imagine that you write

01:04:32.347 --> 01:04:34.026
this code that builds up the graph

01:04:34.026 --> 01:04:35.641
and then once you've built the graph,

01:04:35.641 --> 01:04:37.667
you have this data structure
in memory that represents

01:04:37.667 --> 01:04:39.571
the entire structure of your network.

01:04:39.571 --> 01:04:41.226
And now you could take that data structure

01:04:41.226 --> 01:04:42.428
and just serialize it to disk.

01:04:42.428 --> 01:04:44.528
And now you've got the whole
structure of your network

01:04:44.528 --> 01:04:45.996
saved in some file.

01:04:45.996 --> 01:04:48.711
And then you could later
rear load that thing

01:04:48.711 --> 01:04:51.627
and then run that computational
graph without access

01:04:51.627 --> 01:04:53.630
to the original code that built it.

01:04:53.630 --> 01:04:55.450
So this would be kind of nice
in a deployment scenario.

01:04:55.450 --> 01:04:57.606
You might imagine that you
might want to train your

01:04:57.606 --> 01:05:00.424
network in Python because it's
maybe easier to work with,

01:05:00.424 --> 01:05:01.788
but then after you serialize that network

01:05:01.788 --> 01:05:04.170
and then you could deploy
it now in maybe a C++

01:05:04.170 --> 01:05:06.409
environment where you don't
need to use the original

01:05:06.409 --> 01:05:07.759
code that built the graph.

01:05:07.759 --> 01:05:10.909
So that's kind of a nice
advantage of static graphs.

01:05:10.909 --> 01:05:12.510
Whereas with a dynamic graph,
because we're interleaving

01:05:12.510 --> 01:05:15.793
these processes of graph
building and graph execution,

01:05:15.793 --> 01:05:17.822
you kind of need the
original code at all times

01:05:17.822 --> 01:05:22.012
if you want to reuse
that model in the future.

01:05:22.012 --> 01:05:24.390
On the other hand, some
advantages for dynamic graphs

01:05:24.390 --> 01:05:26.921
are that it kind of makes,
it just makes your code

01:05:26.921 --> 01:05:29.163
a lot cleaner and a lot
easier in a lot of scenarios.

01:05:29.163 --> 01:05:31.264
So for example, suppose
that we want to do some

01:05:31.264 --> 01:05:34.501
conditional operation where
depending on the value

01:05:34.501 --> 01:05:37.541
of some variable Z, we want
to do different operations

01:05:37.541 --> 01:05:38.624
to compute Y.

01:05:39.723 --> 01:05:42.123
Where if Z is positive, we
want to use one weight matrix,

01:05:42.123 --> 01:05:45.070
if Z is negative we want to
use a different weight matrix.

01:05:45.070 --> 01:05:47.981
And we just want to switch off
between these two alternatives.

01:05:47.981 --> 01:05:50.720
In PyTorch because we're
using dynamic graphs,

01:05:50.720 --> 01:05:52.011
it's super simple.

01:05:52.011 --> 01:05:54.101
Your code kind of looks
exactly like you would expect,

01:05:54.101 --> 01:05:56.400
exactly what you would do in Numpy.

01:05:56.400 --> 01:05:58.877
You can just use normal
Python control flow

01:05:58.877 --> 01:06:00.795
to handle this thing.

01:06:00.795 --> 01:06:03.264
And now because we're building
up the graph each time,

01:06:03.264 --> 01:06:05.563
each time we perform this
operation will take one

01:06:05.563 --> 01:06:08.021
of the two paths and build
up maybe a different graph

01:06:08.021 --> 01:06:10.864
on each forward pass, but
for any graph that we do

01:06:10.864 --> 01:06:13.104
end up building up, we can
back propagate through it

01:06:13.104 --> 01:06:14.337
just fine.

01:06:14.337 --> 01:06:15.941
And the code is very
clean, easy to work with.

01:06:15.941 --> 01:06:18.843
Now in TensorFlow the
situations is a little bit more

01:06:18.843 --> 01:06:23.201
complicated because we
build the graph once,

01:06:23.201 --> 01:06:25.219
this control flow operator
kind of needs to be

01:06:25.219 --> 01:06:28.400
an explicit operator in
the TensorFlow graph.

01:06:28.400 --> 01:06:31.301
And now, so them you can
see that we have this

01:06:31.301 --> 01:06:34.319
tf.cond call which is kind
of like a TensorFlow version

01:06:34.319 --> 01:06:36.818
of an if statement,
but now it's baked into

01:06:36.818 --> 01:06:38.838
the computational graph
rather than using sort of

01:06:38.838 --> 01:06:40.741
Python control flow.

01:06:40.741 --> 01:06:43.473
And the problem is that
because we only build the graph

01:06:43.473 --> 01:06:46.123
once, all the potential
paths of control flow that

01:06:46.123 --> 01:06:48.729
our program might flow
through need to be baked

01:06:48.729 --> 01:06:51.200
into the graph at the time we
construct it before we ever

01:06:51.200 --> 01:06:52.523
run it.

01:06:52.523 --> 01:06:54.353
So that means that any kind
of control flow operators

01:06:54.353 --> 01:06:58.394
that you want to have need
to be not Python control flow

01:06:58.394 --> 01:07:00.409
operators, you need to
use some kind of magic,

01:07:00.409 --> 01:07:03.360
special tensor flow
operations to do control flow.

01:07:03.360 --> 01:07:05.527
In this case this tf.cond.

01:07:06.713 --> 01:07:09.400
Another kind of similar
situation happens if you want to

01:07:09.400 --> 01:07:10.763
have loops.

01:07:10.763 --> 01:07:12.684
So suppose that we want to
compute some kind of recurrent

01:07:12.684 --> 01:07:16.607
relationships where maybe Y
T is equal to Y T minus one

01:07:16.607 --> 01:07:19.839
plus X T times some weight
matrix W and depending on

01:07:19.839 --> 01:07:23.077
each time we do this,
every time we compute this,

01:07:23.077 --> 01:07:26.436
we might have a different
sized sequence of data.

01:07:26.436 --> 01:07:28.265
And no matter the length
of our sequence of data,

01:07:28.265 --> 01:07:30.217
we just want to compute this
same recurrence relation

01:07:30.217 --> 01:07:33.371
no matter the size of the input sequence.

01:07:33.371 --> 01:07:35.796
So in PyTorch this is super easy.

01:07:35.796 --> 01:07:39.489
We can just kind of use a
normal for loop in Python

01:07:39.489 --> 01:07:41.534
to just loop over the number
of times that we want to

01:07:41.534 --> 01:07:44.436
unroll and now depending on
the size of the input data,

01:07:44.436 --> 01:07:47.095
our computational graph will
end up as different sizes,

01:07:47.095 --> 01:07:49.737
but that's fine, we can
just back propagate through

01:07:49.737 --> 01:07:51.694
each one, one at a time.

01:07:51.694 --> 01:07:55.782
Now in TensorFlow this
becomes a little bit uglier.

01:07:55.782 --> 01:07:58.494
And again, because we need
to construct the graph

01:07:58.494 --> 01:08:02.398
all at once up front, this
control flow looping construct

01:08:02.398 --> 01:08:06.364
again needs to be an explicit
node in the TensorFlow graph.

01:08:06.364 --> 01:08:08.084
So I hope you remember
your functional programming

01:08:08.084 --> 01:08:10.432
because you'll have to use
those kinds of operators

01:08:10.432 --> 01:08:13.517
to implement looping
constructs in TensorFlow.

01:08:13.517 --> 01:08:16.292
So in this case, for this
particular recurrence relationship

01:08:16.292 --> 01:08:18.857
you can use a foldl operation and pass in,

01:08:18.857 --> 01:08:23.024
sort of implement this particular
loop in terms of a foldl.

01:08:24.101 --> 01:08:26.201
But what this basically means
is that you have this sense

01:08:26.201 --> 01:08:28.734
that TensorFlow is almost
building its own entire

01:08:28.734 --> 01:08:31.214
programming language,
using the language of

01:08:31.214 --> 01:08:33.212
computational graphs.

01:08:33.212 --> 01:08:34.821
And any kind of control flow operator,

01:08:34.821 --> 01:08:37.215
or any kind of data
structure needs to be rolled

01:08:37.215 --> 01:08:40.014
into the computational graph
so you can't really utilize

01:08:40.014 --> 01:08:42.596
all your favorite paradigms
for working imperatively

01:08:42.596 --> 01:08:44.217
in Python.

01:08:44.217 --> 01:08:46.196
You kind of need to relearn
a whole separate set

01:08:46.196 --> 01:08:47.956
of control flow operators.

01:08:47.956 --> 01:08:49.991
And if you want to do
any kinds of control flow

01:08:49.991 --> 01:08:52.804
inside your computational
graph using TensorFlow.

01:08:52.804 --> 01:08:56.252
So at least for me, I find
that kind of confusing,

01:08:56.252 --> 01:08:58.238
a little bit hard to wrap
my head around sometimes,

01:08:58.238 --> 01:09:01.259
and I kind of like that
using PyTorch dynamic graphs,

01:09:01.259 --> 01:09:03.556
you can just use your favorite
imperative programming

01:09:03.556 --> 01:09:06.723
constructs and it all works just fine.

01:09:07.737 --> 01:09:12.051
By the way, there actually
is some very new library

01:09:12.051 --> 01:09:15.732
called TensorFlow Fold which
is another one of these

01:09:15.732 --> 01:09:17.663
layers on top of TensorFlow
that lets you implement

01:09:17.663 --> 01:09:21.580
dynamic graphs, you kind
of write your own code

01:09:22.416 --> 01:09:24.986
using TensorFlow Fold that
looks kind of like a dynamic

01:09:24.986 --> 01:09:27.977
graph operation and then
TensorFlow Fold does some magic

01:09:27.977 --> 01:09:30.617
for you and somehow implements
that in terms of the

01:09:30.617 --> 01:09:32.277
static TensorFlow graphs.

01:09:32.277 --> 01:09:35.226
This is a super new paper
that's being presented

01:09:35.226 --> 01:09:37.358
at ICLR this week in France.

01:09:37.358 --> 01:09:39.737
So I haven't had the chance
to like dive in and play

01:09:39.737 --> 01:09:41.694
with this yet.

01:09:41.694 --> 01:09:44.252
But my initial impression
was that it does add some

01:09:44.252 --> 01:09:46.455
amount of dynamic graphs to
TensorFlow but it is still

01:09:46.455 --> 01:09:48.798
a bit more awkward to work
with than the sort of native

01:09:48.798 --> 01:09:51.952
dynamic graphs you have in PyTorch.

01:09:51.952 --> 01:09:54.527
So then, I thought it
might be nice to motivate

01:09:54.527 --> 01:09:57.257
like why would we care about
dynamic graphs in general?

01:09:57.257 --> 01:10:00.257
So one option is recurrent networks.

01:10:01.177 --> 01:10:03.256
So you can see that for
something like image captioning

01:10:03.256 --> 01:10:05.715
we use a recurrent network
which operates over

01:10:05.715 --> 01:10:07.612
sequences of different lengths.

01:10:07.612 --> 01:10:10.798
In this case, the sentence
that we want to generate

01:10:10.798 --> 01:10:13.337
as a caption is a sequence
and that sequence can vary

01:10:13.337 --> 01:10:15.636
depending on our input data.

01:10:15.636 --> 01:10:18.414
So now you can see that we
have this dynamism in the thing

01:10:18.414 --> 01:10:21.694
where depending on the
size of the sentence,

01:10:21.694 --> 01:10:24.136
our computational graph
might need to have more

01:10:24.136 --> 01:10:25.716
or fewer elements.

01:10:25.716 --> 01:10:29.920
So that's one kind of common
application of dynamic graphs.

01:10:29.920 --> 01:10:34.115
For those of you who
took CS224N last quarter,

01:10:34.115 --> 01:10:36.377
you saw this idea of recursive networks

01:10:36.377 --> 01:10:38.674
where sometimes in natural
language processing

01:10:38.674 --> 01:10:41.316
you might, for example,
compute a parsed tree

01:10:41.316 --> 01:10:43.934
of a sentence and then
you want to have a neural

01:10:43.934 --> 01:10:47.337
network kind of operate
recursively up this parse tree.

01:10:47.337 --> 01:10:49.291
So having a neural network
that kind of works,

01:10:49.291 --> 01:10:51.817
it's not just a sequential
sequence of layers,

01:10:51.817 --> 01:10:54.516
but instead it's kind of
working over some graph

01:10:54.516 --> 01:10:56.856
or tree structure instead
where now each data point

01:10:56.856 --> 01:10:58.732
might have a different
graph or tree structure

01:10:58.732 --> 01:11:00.756
so the structure of
the computational graph

01:11:00.756 --> 01:11:03.194
then kind of mirrors the
structure of the input data.

01:11:03.194 --> 01:11:05.714
And it could vary from
data point to data point.

01:11:05.714 --> 01:11:07.934
So this type of thing seems
kind of complicated and

01:11:07.934 --> 01:11:10.316
hairy to implement using TensorFlow,

01:11:10.316 --> 01:11:12.478
but in PyTorch you can just kind of use

01:11:12.478 --> 01:11:14.054
like normal Python control
flow and it'll work out

01:11:14.054 --> 01:11:14.887
just fine.

01:11:16.574 --> 01:11:19.198
Another bit of more researchy
application is this really

01:11:19.198 --> 01:11:21.614
cool idea that I like
called neuromodule networks

01:11:21.614 --> 01:11:23.678
for visual question answering.

01:11:23.678 --> 01:11:26.718
So here the idea is that we
want to ask some questions

01:11:26.718 --> 01:11:29.278
about images where we
maybe input this image

01:11:29.278 --> 01:11:31.737
of cats and dogs, there's some question,

01:11:31.737 --> 01:11:34.756
what color is the cat, and
then internally the system

01:11:34.756 --> 01:11:37.614
can read the question and
that has these different

01:11:37.614 --> 01:11:39.758
specialized neural network
modules for performing

01:11:39.758 --> 01:11:43.594
operations like asking for
colors and finding cats.

01:11:43.594 --> 01:11:45.915
And then depending on
the text of the question,

01:11:45.915 --> 01:11:48.193
it can compile this custom
architecture for answering

01:11:48.193 --> 01:11:49.838
the question.

01:11:49.838 --> 01:11:52.294
And now if we asked a different question,

01:11:52.294 --> 01:11:55.094
like are there more cats than dogs?

01:11:55.094 --> 01:11:58.241
Now we have maybe the
same basic set of modules

01:11:58.241 --> 01:12:00.534
for doing things like finding
cats and dogs and counting,

01:12:00.534 --> 01:12:03.076
but they're arranged in a different order.

01:12:03.076 --> 01:12:05.177
So we get this dynamism again
where different data points

01:12:05.177 --> 01:12:07.716
might give rise to different
computational graphs.

01:12:07.716 --> 01:12:09.635
But this is a bit more
of a researchy thing

01:12:09.635 --> 01:12:12.574
and maybe not so main stream right now.

01:12:12.574 --> 01:12:15.037
But as kind of a bigger
point, I think that there's

01:12:15.037 --> 01:12:17.198
a lot of cool, creative
applications that people

01:12:17.198 --> 01:12:19.214
could do with dynamic computational graphs

01:12:19.214 --> 01:12:21.577
and maybe there aren't so many right now,

01:12:21.577 --> 01:12:23.471
just because it's been so
painful to work with them.

01:12:23.471 --> 01:12:25.577
So I think that there's
a lot of opportunity

01:12:25.577 --> 01:12:27.396
for doing cool, creative things with

01:12:27.396 --> 01:12:30.596
dynamic computational graphs.

01:12:30.596 --> 01:12:32.297
And maybe if you come up with cool ideas,

01:12:32.297 --> 01:12:34.078
we'll feature it in lecture next year.

01:12:34.078 --> 01:12:37.612
So I wanted to talk
very briefly about Caffe

01:12:37.612 --> 01:12:39.854
which is this framework from Berkeley.

01:12:39.854 --> 01:12:43.795
Which Caffe is somewhat
different from the other

01:12:43.795 --> 01:12:45.774
deep learning frameworks
where you in many cases

01:12:45.774 --> 01:12:47.454
you can actually train
networks without writing

01:12:47.454 --> 01:12:48.815
any code yourself.

01:12:48.815 --> 01:12:50.798
You kind of just call into
these pre-existing binaries,

01:12:50.798 --> 01:12:53.214
set up some configuration
files and in many cases

01:12:53.214 --> 01:12:56.697
you can train on data without
writing any of your own code.

01:12:56.697 --> 01:13:00.078
So, you may be first,
you convert your data

01:13:00.078 --> 01:13:03.054
into some format like HDF5
or LMDB and there exists

01:13:03.054 --> 01:13:06.014
some scripts inside Caffe
that can just convert like

01:13:06.014 --> 01:13:08.638
folders of images and text files
into these formats for you.

01:13:08.638 --> 01:13:12.537
You need to define, now
instead of writing code

01:13:12.537 --> 01:13:14.478
to define the structure of
your computational graph,

01:13:14.478 --> 01:13:17.414
instead you edit some text
file called a prototxt

01:13:17.414 --> 01:13:19.934
which sets up the structure
of the computational graph.

01:13:19.934 --> 01:13:22.997
Here the structure is that
we read from some input

01:13:22.997 --> 01:13:26.537
HDF5 file, we perform some inner product,

01:13:26.537 --> 01:13:28.974
we compute some loss
and the whole structure

01:13:28.974 --> 01:13:30.875
of the graph is set up in this text file.

01:13:30.875 --> 01:13:33.653
One kind of downside
here is that these files

01:13:33.653 --> 01:13:35.956
can get really ugly for
very large networks.

01:13:35.956 --> 01:13:38.455
So for something like the
152 layer ResNet model,

01:13:38.455 --> 01:13:41.535
which by the way was
trained in Caffe originally,

01:13:41.535 --> 01:13:44.253
then this prototxt file ends
up almost 7000 lines long.

01:13:44.253 --> 01:13:47.278
So people are not writing these by hand.

01:13:47.278 --> 01:13:49.887
People will sometimes will
like write python scripts

01:13:49.887 --> 01:13:51.817
to generate these prototxt files.

01:13:51.817 --> 01:13:53.275
[laughter]

01:13:53.275 --> 01:13:55.154
Then you're kind in the
realm of rolling your own

01:13:55.154 --> 01:13:56.318
computational graph abstraction.

01:13:56.318 --> 01:13:58.974
That's probably not a good
idea, but I've seen that before.

01:13:58.974 --> 01:14:02.238
Then, rather than having
some optimizer object,

01:14:02.238 --> 01:14:05.316
instead there's some solver,
you define some solver things

01:14:05.316 --> 01:14:07.497
inside another prototxt.

01:14:07.497 --> 01:14:09.118
This defines your learning rate,

01:14:09.118 --> 01:14:11.036
your optimization algorithm and whatnot.

01:14:11.036 --> 01:14:12.334
And then once you do all these things,

01:14:12.334 --> 01:14:14.174
you can just run the Caffe
binary with the train command

01:14:14.174 --> 01:14:17.278
and it all happens magically.

01:14:17.278 --> 01:14:19.577
Cafee has a model zoo with a
bunch of pretrained models,

01:14:19.577 --> 01:14:21.294
that's pretty useful.

01:14:21.294 --> 01:14:23.454
Caffe has a Python
interface but it's not super

01:14:23.454 --> 01:14:25.438
well documented.

01:14:25.438 --> 01:14:27.358
You kind of need to read the
source code of the python

01:14:27.358 --> 01:14:29.017
interface to see what it can do,

01:14:29.017 --> 01:14:30.116
so that's kind of annoying.

01:14:30.116 --> 01:14:31.455
But it does work.

01:14:31.455 --> 01:14:35.622
So, kind of my general thing
about Caffe is that it's

01:14:36.596 --> 01:14:38.334
maybe good for feed forward models,

01:14:38.334 --> 01:14:40.174
it's maybe good for production scenarios,

01:14:40.174 --> 01:14:42.796
because it doesn't depend on Python.

01:14:42.796 --> 01:14:45.075
But probably for research
these days, I've seen Caffe

01:14:45.075 --> 01:14:47.358
being used maybe a little bit less.

01:14:47.358 --> 01:14:49.597
Although I think it is
still pretty commonly used

01:14:49.597 --> 01:14:51.417
in industry again for production.

01:14:51.417 --> 01:14:54.410
I promise one slide, one
or two slides on Caffe 2.

01:14:54.410 --> 01:14:58.596
So Caffe 2 is the successor to
Caffe which is from Facebook.

01:14:58.596 --> 01:15:02.432
It's super new, it was
only released a week ago.

01:15:02.432 --> 01:15:04.436
[laughter]

01:15:04.436 --> 01:15:06.617
So I really haven't had
the time to form a super

01:15:06.617 --> 01:15:09.314
educated opinion about Caffe 2 yet,

01:15:09.314 --> 01:15:12.318
but it uses static graphs
kind of similar to TensorFlow.

01:15:12.318 --> 01:15:15.198
Kind of like Caffe one
the core is written in C++

01:15:15.198 --> 01:15:17.817
and they have some Python interface.

01:15:17.817 --> 01:15:19.694
The difference is that
now you no longer need to

01:15:19.694 --> 01:15:21.518
write your own Python scripts
to generate prototxt files.

01:15:21.518 --> 01:15:25.312
You can kind of define your
computational graph structure

01:15:25.312 --> 01:15:28.170
all in Python, kind of
looking with an API that looks

01:15:28.170 --> 01:15:29.657
kind of like TensorFlow.

01:15:29.657 --> 01:15:31.854
But then you can spit out,
you can serialize this

01:15:31.854 --> 01:15:34.596
computational graph
structure to a prototxt file.

01:15:34.596 --> 01:15:36.777
And then once your model
is trained and whatnot,

01:15:36.777 --> 01:15:38.676
then we get this benefit that
we talked about of static

01:15:38.676 --> 01:15:41.257
graphs where you can, you
don't need the original

01:15:41.257 --> 01:15:43.534
training code now in order
to deploy a trained model.

01:15:43.534 --> 01:15:46.958
So one interesting thing
is that you've seen Google

01:15:46.958 --> 01:15:49.417
maybe has one major
deep running framework,

01:15:49.417 --> 01:15:52.094
which is TensorFlow, where
Facebook has these two,

01:15:52.094 --> 01:15:53.761
PyTorch and Caffe 2.

01:15:54.596 --> 01:15:57.252
So these are kind of
different philosophies.

01:15:57.252 --> 01:15:59.751
Google's kind of trying to
build one framework to rule

01:15:59.751 --> 01:16:01.569
them all that maybe works
for every possible scenario

01:16:01.569 --> 01:16:02.847
for deep learning.

01:16:02.847 --> 01:16:04.609
This is kind of nice because
it consolidates all efforts

01:16:04.609 --> 01:16:06.209
onto one framework.

01:16:06.209 --> 01:16:07.852
It means you only need to learn one thing

01:16:07.852 --> 01:16:09.464
and it'll work across
many different scenarios

01:16:09.464 --> 01:16:11.708
including like distributed
systems, production,

01:16:11.708 --> 01:16:13.772
deployment, mobile, research, everything.

01:16:13.772 --> 01:16:15.706
Only need to learn one framework
to do all these things.

01:16:15.706 --> 01:16:18.151
Whereas Facebook is taking a
bit of a different approach.

01:16:18.151 --> 01:16:20.849
Where PyTorch is really more specialized,

01:16:20.849 --> 01:16:23.591
more geared towards research
so in terms of writing

01:16:23.591 --> 01:16:26.071
research code and quickly
iterating on your ideas,

01:16:26.071 --> 01:16:27.948
that's super easy in
PyTorch, but for things like

01:16:27.948 --> 01:16:30.869
running in production,
running on mobile devices,

01:16:30.869 --> 01:16:32.951
PyTorch doesn't have a
lot of great support.

01:16:32.951 --> 01:16:35.210
Instead, Caffe 2 is kind
of geared toward those more

01:16:35.210 --> 01:16:37.710
production oriented use cases.

01:16:39.567 --> 01:16:42.929
So my kind of general study,
my general, overall advice

01:16:42.929 --> 01:16:45.409
about like which framework
to use for which problems

01:16:45.409 --> 01:16:47.350
is kind of that both,

01:16:47.350 --> 01:16:50.172
I think TensorFlow is a
pretty safe bet for just about

01:16:50.172 --> 01:16:53.510
any project that you
want to start new, right?

01:16:53.510 --> 01:16:56.168
Because it is sort of one
framework to rule them all,

01:16:56.168 --> 01:16:58.849
it can be used for just
about any circumstance.

01:16:58.849 --> 01:17:01.166
However, you probably
need to pair it with a

01:17:01.166 --> 01:17:03.510
higher level wrapper and
if you want dynamic graphs,

01:17:03.510 --> 01:17:05.207
you're maybe out of luck.

01:17:05.207 --> 01:17:07.152
Some of the code ends up
looking a little bit uglier

01:17:07.152 --> 01:17:10.226
in my opinion, but maybe that's
kind of a cosmetic detail

01:17:10.226 --> 01:17:13.190
and it doesn't really matter that much.

01:17:13.190 --> 01:17:15.809
I personally think PyTorch
is really great for research.

01:17:15.809 --> 01:17:18.675
If you're focused on just
writing research code,

01:17:18.675 --> 01:17:21.233
I think PyTorch is a great choice.

01:17:21.233 --> 01:17:23.830
But it's a bit newer, has
less community support,

01:17:23.830 --> 01:17:25.649
less code out there, so it
could be a bit of an adventure.

01:17:25.649 --> 01:17:28.412
If you want more of a well
trodden path, TensorFlow

01:17:28.412 --> 01:17:29.969
might be a better choice.

01:17:29.969 --> 01:17:32.365
If you're interested in
production deployment,

01:17:32.365 --> 01:17:34.710
you should probably look at
Caffe, Caffe 2 or TensorFlow.

01:17:34.710 --> 01:17:37.017
And if you're really focused
on mobile deployment,

01:17:37.017 --> 01:17:39.312
I think TensorFlow and Caffe
2 both have some built in

01:17:39.312 --> 01:17:41.270
support for that.

01:17:41.270 --> 01:17:43.325
So it's kind of unfortunately,
there's not just like

01:17:43.325 --> 01:17:45.009
one global best framework,
it kind of depends

01:17:45.009 --> 01:17:47.393
on what you're actually trying to do,

01:17:47.393 --> 01:17:49.212
what applications you anticipate
but theses are kind of

01:17:49.212 --> 01:17:52.045
my general advice on those things.

01:17:53.169 --> 01:17:55.691
So next time we'll talk
about some case studies

01:17:55.691 --> 01:17:58.441
about various CNN architectures.

