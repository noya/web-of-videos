WEBVTT
Kind: captions
Language: en

00:00:08.840 --> 00:00:10.009
- Okay, it's after 12, so I think

00:00:10.009 --> 00:00:11.842
we should get started.

00:00:15.129 --> 00:00:16.262
Today we're going to kind of pick up

00:00:16.262 --> 00:00:17.904
where we left off last time.

00:00:17.904 --> 00:00:19.205
Last time we talked about a lot of

00:00:19.205 --> 00:00:20.875
sort of tips and tricks involved

00:00:20.875 --> 00:00:22.628
in the nitty gritty details of training

00:00:22.628 --> 00:00:23.885
neural networks.

00:00:23.885 --> 00:00:25.545
Today we'll pick up where we left off,

00:00:25.545 --> 00:00:26.920
and talk about a lot more of these

00:00:26.920 --> 00:00:28.868
sort of nitty gritty details about

00:00:28.868 --> 00:00:30.924
training these things.

00:00:30.924 --> 00:00:32.717
As usual, a couple administrative notes

00:00:32.717 --> 00:00:35.192
before we get into the material.

00:00:35.192 --> 00:00:38.123
As you all know, assignment
one is already due.

00:00:38.123 --> 00:00:40.130
Hopefully you all turned it in.

00:00:40.130 --> 00:00:42.723
Did it go okay? Was it not okay?

00:00:42.723 --> 00:00:44.056
Rough sentiment?

00:00:45.937 --> 00:00:46.989
Mostly okay.

00:00:46.989 --> 00:00:48.215
Okay, that's good.

00:00:48.215 --> 00:00:51.314
Awesome. [laughs]

00:00:51.314 --> 00:00:52.687
We're in the process of grading those,

00:00:52.687 --> 00:00:53.605
so stay turned.

00:00:53.605 --> 00:00:55.705
We're hoping to get grades back for those

00:00:55.705 --> 00:00:57.807
before A two is due.

00:00:57.807 --> 00:01:00.038
Another reminder, that
your project proposals

00:01:00.038 --> 00:01:02.189
are due tomorrow.

00:01:02.189 --> 00:01:04.606
Actually, no, today at 11:59.

00:01:05.444 --> 00:01:06.843
Make sure you send those in.

00:01:06.843 --> 00:01:09.559
Details are on the website and on Piazza.

00:01:09.559 --> 00:01:12.750
Also a reminder, assignment
two is already out.

00:01:12.750 --> 00:01:15.754
That'll be due a week from Thursday.

00:01:15.754 --> 00:01:17.757
Historically, assignment two has been the

00:01:17.757 --> 00:01:20.102
longest one in the class, so if you

00:01:20.102 --> 00:01:22.442
haven't started already on assignment two,

00:01:22.442 --> 00:01:25.345
I'd recommend you take a look at that

00:01:25.345 --> 00:01:26.345
pretty soon.

00:01:27.607 --> 00:01:30.058
Another reminder is
that for assignment two,

00:01:30.058 --> 00:01:31.304
I think of a lot of you will be using

00:01:31.304 --> 00:01:32.969
Google Cloud.

00:01:32.969 --> 00:01:35.235
Big reminder, make sure
to stop your instances

00:01:35.235 --> 00:01:36.987
when you're not using them because

00:01:36.987 --> 00:01:39.071
whenever your instance
is on, you get charged,

00:01:39.071 --> 00:01:40.514
and we only have so many coupons

00:01:40.514 --> 00:01:43.384
to distribute to you guys.

00:01:43.384 --> 00:01:44.966
Anytime your instance is on, even if

00:01:44.966 --> 00:01:46.998
you're not SSH to it, even if you're not

00:01:46.998 --> 00:01:49.884
running things immediately
in your Jupyter Notebook,

00:01:49.884 --> 00:01:52.708
any time that instance is on,
you're going to be charged.

00:01:52.708 --> 00:01:54.569
Just make sure that you explicitly stop

00:01:54.569 --> 00:01:57.603
your instances when you're not using them.

00:01:57.603 --> 00:01:59.399
In this example, I've
got a little screenshot

00:01:59.399 --> 00:02:01.940
of my dashboard on Google Cloud.

00:02:01.940 --> 00:02:03.089
I need to go in there and explicitly

00:02:03.089 --> 00:02:05.455
go to the dropdown and click stop.

00:02:05.455 --> 00:02:06.712
Just make sure that you do this when

00:02:06.712 --> 00:02:09.129
you're done working each day.

00:02:09.966 --> 00:02:12.566
Another thing to remember is it's kind of

00:02:12.566 --> 00:02:14.340
up to you guys to keep
track of your spending

00:02:14.340 --> 00:02:15.845
on Google Cloud.

00:02:15.845 --> 00:02:18.212
In particular, instances that use GPUs

00:02:18.212 --> 00:02:21.338
are a lot more expensive
than those with CPUs.

00:02:21.338 --> 00:02:23.720
Rough order of magnitude,
those GPU instances

00:02:23.720 --> 00:02:26.635
are around 90 cents to a dollar an hour.

00:02:26.635 --> 00:02:28.807
Those are actually quite pricey.

00:02:28.807 --> 00:02:31.023
The CPU instances are much cheaper.

00:02:31.023 --> 00:02:32.732
The general strategy is that you probably

00:02:32.732 --> 00:02:34.983
want to make two instances, one with a GPU

00:02:34.983 --> 00:02:36.874
and one without, and then only use that

00:02:36.874 --> 00:02:40.224
GPU instance when you really need the GPU.

00:02:40.224 --> 00:02:43.271
For example, on assignment two,

00:02:43.271 --> 00:02:44.766
most of the assignment,
you should only need

00:02:44.766 --> 00:02:46.198
the CPU, so you should only use your

00:02:46.198 --> 00:02:47.862
CPU instance for that.

00:02:47.862 --> 00:02:50.213
But then the final
question, about TensorFlow

00:02:50.213 --> 00:02:53.475
or PyTorch, that will need a GPU.

00:02:53.475 --> 00:02:54.688
This'll give you a little bit of practice

00:02:54.688 --> 00:02:56.354
with switching between multiple instances

00:02:56.354 --> 00:02:59.382
and only using that GPU
when it's really necessary.

00:02:59.382 --> 00:03:01.310
Again, just kind of watch your spending.

00:03:01.310 --> 00:03:04.792
Try not to go too crazy on these things.

00:03:04.792 --> 00:03:06.733
Any questions on the administrative stuff

00:03:06.733 --> 00:03:08.233
before we move on?

00:03:11.665 --> 00:03:12.667
Question.

00:03:12.667 --> 00:03:14.387
- [Student] How much RAM should we use?

00:03:14.387 --> 00:03:16.618
- Question is how much RAM should we use?

00:03:16.618 --> 00:03:18.533
I think eight or 16 gigs is probably good

00:03:18.533 --> 00:03:22.348
for everything that
you need in this class.

00:03:22.348 --> 00:03:23.883
As you scale up the number of CPUs

00:03:23.883 --> 00:03:25.355
and the number of RAM, you also end up

00:03:25.355 --> 00:03:27.599
spending more money.

00:03:27.599 --> 00:03:29.584
If you stick with two or four CPUs

00:03:29.584 --> 00:03:31.111
and eight or 16 gigs of RAM, that should

00:03:31.111 --> 00:03:33.360
be plenty for all the
homework-related stuff

00:03:33.360 --> 00:03:35.027
that you need to do.

00:03:37.121 --> 00:03:39.440
As a quick recap, last
time we talked about

00:03:39.440 --> 00:03:40.902
activation functions.

00:03:40.902 --> 00:03:42.017
We talked about this whole zoo

00:03:42.017 --> 00:03:43.714
of different activation functions and some

00:03:43.714 --> 00:03:45.447
of their different properties.

00:03:45.447 --> 00:03:47.171
We saw that the sigmoid, which used

00:03:47.171 --> 00:03:50.110
to be quite popular when
training neural networks

00:03:50.110 --> 00:03:52.254
maybe 10 years ago or so, has this problem

00:03:52.254 --> 00:03:54.729
with vanishing gradients near the two ends

00:03:54.729 --> 00:03:57.480
of the activation function.

00:03:57.480 --> 00:04:00.221
tanh has this similar sort of problem.

00:04:00.221 --> 00:04:01.900
Kind of the general recommendation is that

00:04:01.900 --> 00:04:03.188
you probably want to stick with ReLU

00:04:03.188 --> 00:04:05.951
for most cases as sort of a default choice

00:04:05.951 --> 00:04:07.242
'cause it tends to work well for a lot

00:04:07.242 --> 00:04:09.715
of different architectures.

00:04:09.715 --> 00:04:12.072
We also talked about
weight initialization.

00:04:12.072 --> 00:04:14.545
Remember that up on the
top, we have this idea

00:04:14.545 --> 00:04:17.306
that when you initialize your weights

00:04:17.306 --> 00:04:19.147
at the start of training, if those weights

00:04:19.147 --> 00:04:21.362
are initialized to be
too small, then if you

00:04:21.362 --> 00:04:24.273
look at, then the activations will vanish

00:04:24.273 --> 00:04:25.915
as you go through the network because

00:04:25.915 --> 00:04:27.408
as you multiply by these small numbers

00:04:27.408 --> 00:04:28.810
over and over again, they'll all

00:04:28.810 --> 00:04:30.068
sort of decay to zero.

00:04:30.068 --> 00:04:32.160
Then everything will be
zero, learning won't happen,

00:04:32.160 --> 00:04:33.557
you'll be sad.

00:04:33.557 --> 00:04:34.808
On the other hand, if you initialize

00:04:34.808 --> 00:04:37.108
your weights too big,
then as you go through

00:04:37.108 --> 00:04:38.910
the network and multiply
by your weight matrix

00:04:38.910 --> 00:04:41.693
over and over again,
eventually they'll explode.

00:04:41.693 --> 00:04:43.440
You'll be unhappy,
there'll be no learning,

00:04:43.440 --> 00:04:45.874
it will be very bad.

00:04:45.874 --> 00:04:48.349
But if you get that
initialization just right,

00:04:48.349 --> 00:04:51.041
for example, using the
Xavier initialization

00:04:51.041 --> 00:04:54.590
or the MSRA initialization,
then you kind of keep

00:04:54.590 --> 00:04:56.249
a nice distribution of activations

00:04:56.249 --> 00:04:59.016
as you go through the network.

00:04:59.016 --> 00:05:00.940
Remember that this kind of gets more

00:05:00.940 --> 00:05:02.947
and more important and
more and more critical

00:05:02.947 --> 00:05:04.813
as your networks get deeper and deeper

00:05:04.813 --> 00:05:06.386
because as your network gets deeper,

00:05:06.386 --> 00:05:07.979
you're multiplying by
those weight matrices

00:05:07.979 --> 00:05:12.105
over and over again with these
more multiplicative terms.

00:05:12.105 --> 00:05:14.965
We also talked last time
about data preprocessing.

00:05:14.965 --> 00:05:16.870
We talked about how it's pretty typical

00:05:16.870 --> 00:05:19.943
in conv nets to zero center and normalize

00:05:19.943 --> 00:05:24.151
your data so it has zero
mean and unit variance.

00:05:24.151 --> 00:05:27.016
I wanted to provide a little
bit of extra intuition

00:05:27.016 --> 00:05:30.453
about why you might
actually want to do this.

00:05:30.453 --> 00:05:33.742
Imagine a simple setup where we have

00:05:33.742 --> 00:05:35.500
a binary classification problem where we

00:05:35.500 --> 00:05:38.369
want to draw a line to
separate these red points

00:05:38.369 --> 00:05:40.017
from these blue points.

00:05:40.017 --> 00:05:42.078
On the left, you have this idea where

00:05:42.078 --> 00:05:44.832
if those data points are
kind of not normalized

00:05:44.832 --> 00:05:47.433
and not centered and far
away from the origin,

00:05:47.433 --> 00:05:50.044
then we can still use a
line to separate them,

00:05:50.044 --> 00:05:52.388
but now if that line
wiggles just a little bit,

00:05:52.388 --> 00:05:54.022
then our classification is going to get

00:05:54.022 --> 00:05:55.492
totally destroyed.

00:05:55.492 --> 00:05:57.729
That kind of means that in the example

00:05:57.729 --> 00:06:00.356
on the left, the loss function is now

00:06:00.356 --> 00:06:02.560
extremely sensitive to small perturbations

00:06:02.560 --> 00:06:06.477
in that linear classifier
in our weight matrix.

00:06:07.800 --> 00:06:09.920
We can still represent the same functions,

00:06:09.920 --> 00:06:12.349
but that might make
learning quite difficult

00:06:12.349 --> 00:06:15.039
because, again, their
loss is very sensitive

00:06:15.039 --> 00:06:18.804
to our parameter vector,
whereas in the situation

00:06:18.804 --> 00:06:21.067
on the right, if you take that data cloud

00:06:21.067 --> 00:06:22.963
and you move it into the
origin and you make it

00:06:22.963 --> 00:06:25.836
unit variance, then
now, again, we can still

00:06:25.836 --> 00:06:28.511
classify that data quite well, but now

00:06:28.511 --> 00:06:30.894
as we wiggle that line a little bit,

00:06:30.894 --> 00:06:33.442
then our loss function is less sensitive

00:06:33.442 --> 00:06:36.008
to small perturbations
in the parameter values.

00:06:36.008 --> 00:06:38.579
That maybe makes optimization
a little bit easier,

00:06:38.579 --> 00:06:41.549
as we'll see a little bit going forward.

00:06:41.549 --> 00:06:44.301
By the way, this situation is not only

00:06:44.301 --> 00:06:47.024
in the linear classification case.

00:06:47.024 --> 00:06:49.400
Inside a neural network,
remember we kind of have

00:06:49.400 --> 00:06:53.041
these interleavings of these linear

00:06:53.041 --> 00:06:55.574
matrix multiplies, or
convolutions, followed by

00:06:55.574 --> 00:06:58.241
non-linear activation functions.

00:06:59.563 --> 00:07:01.751
If the input to some layer in your

00:07:01.751 --> 00:07:03.943
neural network is not centered or not

00:07:03.943 --> 00:07:06.172
zero mean, not unit variance, then again,

00:07:06.172 --> 00:07:08.538
small perturbations in the weight matrix

00:07:08.538 --> 00:07:10.802
of that layer of the network could cause

00:07:10.802 --> 00:07:13.553
large perturbations in
the output of that layer,

00:07:13.553 --> 00:07:16.117
which, again, might
make learning difficult.

00:07:16.117 --> 00:07:17.874
This is kind of a little bit of extra

00:07:17.874 --> 00:07:19.383
intuition about why normalization

00:07:19.383 --> 00:07:20.966
might be important.

00:07:22.349 --> 00:07:24.760
Because we have this
intuition that normalization

00:07:24.760 --> 00:07:27.347
is so important, we talked
about batch normalization,

00:07:27.347 --> 00:07:29.583
which is where we just
add this additional layer

00:07:29.583 --> 00:07:31.616
inside our networks to just force all

00:07:31.616 --> 00:07:34.293
of the intermediate
activations to be zero mean

00:07:34.293 --> 00:07:36.515
and unit variance.

00:07:36.515 --> 00:07:38.107
I've sort of resummarized the

00:07:38.107 --> 00:07:39.586
batch normalization equations here

00:07:39.586 --> 00:07:41.950
with the shapes a little
bit more explicitly.

00:07:41.950 --> 00:07:43.161
Hopefully this can help you out

00:07:43.161 --> 00:07:44.429
when you're implementing this thing

00:07:44.429 --> 00:07:45.657
on assignment two.

00:07:45.657 --> 00:07:47.376
But again, in batch normalization, we have

00:07:47.376 --> 00:07:49.270
this idea that in the forward pass,

00:07:49.270 --> 00:07:51.621
we use the statistics of the mini batch

00:07:51.621 --> 00:07:53.998
to compute a mean and
a standard deviation,

00:07:53.998 --> 00:07:57.101
and then use those estimates to normalize

00:07:57.101 --> 00:07:59.739
our data on the forward pass.

00:07:59.739 --> 00:08:01.357
Then we also reintroduce the scale

00:08:01.357 --> 00:08:03.133
and shift parameters to increase

00:08:03.133 --> 00:08:06.126
the expressivity of the layer.

00:08:06.126 --> 00:08:07.221
You might want to refer back to this

00:08:07.221 --> 00:08:10.475
when working on assignment two.

00:08:10.475 --> 00:08:12.552
We also talked last
time a little bit about

00:08:12.552 --> 00:08:15.396
babysitting the learning process, how you

00:08:15.396 --> 00:08:16.918
should probably be looking
at your loss curves

00:08:16.918 --> 00:08:18.631
during training.

00:08:18.631 --> 00:08:20.140
Here's an example of some networks

00:08:20.140 --> 00:08:23.557
I was actually training over the weekend.

00:08:24.481 --> 00:08:25.964
This is usually my setup when I'm working

00:08:25.964 --> 00:08:27.168
on these things.

00:08:27.168 --> 00:08:28.673
On the left, I have some plot showing

00:08:28.673 --> 00:08:30.924
the training loss over time.

00:08:30.924 --> 00:08:32.506
You can see it's kind of going down,

00:08:32.506 --> 00:08:34.700
which means my network
is reducing the loss.

00:08:34.700 --> 00:08:36.280
It's doing well.

00:08:36.280 --> 00:08:38.910
On the right, there's this plot where

00:08:38.910 --> 00:08:42.354
the X axis is, again, time,
or the iteration number,

00:08:42.354 --> 00:08:45.601
and the Y axis is my performance measure

00:08:45.601 --> 00:08:48.950
both on my training set
and on my validation set.

00:08:48.950 --> 00:08:51.011
You can see that as we go over time,

00:08:51.011 --> 00:08:52.674
then my training set performance goes up

00:08:52.674 --> 00:08:54.595
and up and up and up and
up as my loss function

00:08:54.595 --> 00:08:57.111
goes down, but at some
point, my validation set

00:08:57.111 --> 00:08:59.165
performance kind of plateaus.

00:08:59.165 --> 00:09:00.368
This kind of suggests that maybe

00:09:00.368 --> 00:09:02.109
I'm overfitting in this situation.

00:09:02.109 --> 00:09:03.384
Maybe I should have been trying to add

00:09:03.384 --> 00:09:05.551
additional regularization.

00:09:06.802 --> 00:09:08.273
We also talked a bit last time about

00:09:08.273 --> 00:09:09.989
hyperparameter search.

00:09:09.989 --> 00:09:12.023
All these networks have
sort of a large zoo

00:09:12.023 --> 00:09:13.167
of hyperparameters.

00:09:13.167 --> 00:09:15.283
It's pretty important
to set them correctly.

00:09:15.283 --> 00:09:16.802
We talked a little bit about grid search

00:09:16.802 --> 00:09:19.186
versus random search,
and how random search

00:09:19.186 --> 00:09:21.210
is maybe a little bit nicer in theory

00:09:21.210 --> 00:09:24.750
because in the situation
where your performance

00:09:24.750 --> 00:09:26.170
might be more sensitive, with respect

00:09:26.170 --> 00:09:28.714
to one hyperparameter than
other, and random search

00:09:28.714 --> 00:09:31.154
lets you cover that space
a little bit better.

00:09:31.154 --> 00:09:33.102
We also talked about the idea of coarse

00:09:33.102 --> 00:09:35.248
to fine search, where when you're doing

00:09:35.248 --> 00:09:37.490
this hyperparameter
optimization, probably you

00:09:37.490 --> 00:09:39.588
want to start with very wide ranges

00:09:39.588 --> 00:09:41.357
for your hyperparameters, only train

00:09:41.357 --> 00:09:43.893
for a couple iterations, and then based on

00:09:43.893 --> 00:09:46.004
those results, you kind of narrow in

00:09:46.004 --> 00:09:48.458
on the range of
hyperparameters that are good.

00:09:48.458 --> 00:09:50.686
Now, again, redo your
search in a smaller range

00:09:50.686 --> 00:09:52.151
for more iterations.

00:09:52.151 --> 00:09:53.574
You can kind of iterate this process

00:09:53.574 --> 00:09:55.413
to kind of hone in on the right region

00:09:55.413 --> 00:09:57.193
for hyperparameters.

00:09:57.193 --> 00:09:58.658
But again, it's really important to,

00:09:58.658 --> 00:10:00.912
at the start, have a very coarse range

00:10:00.912 --> 00:10:02.928
to start with, where
you want very, very wide

00:10:02.928 --> 00:10:04.940
ranges for all your hyperparameters.

00:10:04.940 --> 00:10:07.924
Ideally, those ranges should be so wide

00:10:07.924 --> 00:10:09.395
that your network is kind of blowing up

00:10:09.395 --> 00:10:11.341
at either end of the
range so that you know

00:10:11.341 --> 00:10:12.814
that you've searched a wide enough range

00:10:12.814 --> 00:10:14.231
for those things.

00:10:17.947 --> 00:10:18.780
Question?

00:10:20.529 --> 00:10:22.409
- [Student] How many
[speaks too low to hear]

00:10:22.409 --> 00:10:23.740
optimize at once?

00:10:23.740 --> 00:10:27.157
[speaks too low to hear]

00:10:32.325 --> 00:10:33.611
- The question is how many hyperparameters

00:10:33.611 --> 00:10:35.039
do we typically search at a time?

00:10:35.039 --> 00:10:36.702
Here is two, but there's
a lot more than two

00:10:36.702 --> 00:10:38.729
in these typical things.

00:10:38.729 --> 00:10:40.263
It kind of depends on the exact model

00:10:40.263 --> 00:10:42.705
and the exact architecture, but because

00:10:42.705 --> 00:10:44.715
the number of possibilities is exponential

00:10:44.715 --> 00:10:45.927
in the number of hyperparameters,

00:10:45.927 --> 00:10:48.497
you can't really test too many at a time.

00:10:48.497 --> 00:10:50.577
It also kind of depends
on how many machines

00:10:50.577 --> 00:10:52.222
you have available.

00:10:52.222 --> 00:10:54.236
It kind of varies from person to person

00:10:54.236 --> 00:10:56.230
and from experiment to experiment.

00:10:56.230 --> 00:10:59.410
But generally, I try
not to do this over more

00:10:59.410 --> 00:11:01.535
than maybe two or three or four at a time

00:11:01.535 --> 00:11:03.840
at most because, again,
this exponential search

00:11:03.840 --> 00:11:05.838
just gets out of control.

00:11:05.838 --> 00:11:08.118
Typically, learning rate
is the really important one

00:11:08.118 --> 00:11:10.891
that you need to nail first.

00:11:10.891 --> 00:11:13.136
Then other things, like regularization,

00:11:13.136 --> 00:11:16.633
like learning rate decay, model size,

00:11:16.633 --> 00:11:17.847
these other types of things tend to be a

00:11:17.847 --> 00:11:20.027
little bit less sensitive
than learning rate.

00:11:20.027 --> 00:11:21.479
Sometimes you might do kind of a block

00:11:21.479 --> 00:11:23.208
coordinate descent, where you go and find

00:11:23.208 --> 00:11:25.082
the good learning rate,
then you go back and try

00:11:25.082 --> 00:11:27.944
to look at different model sizes.

00:11:27.944 --> 00:11:29.343
This can help you cut down on the

00:11:29.343 --> 00:11:31.244
exponential search a little bit,

00:11:31.244 --> 00:11:32.644
but it's a little bit problem dependent

00:11:32.644 --> 00:11:34.605
on exactly which ones you
should be searching over

00:11:34.605 --> 00:11:35.855
in which order.

00:11:36.738 --> 00:11:38.605
More questions?

00:11:38.605 --> 00:11:42.772
- [Student] [speaks too low to hear]

00:11:45.184 --> 00:11:48.725
Another parameter, but then changing that

00:11:48.725 --> 00:11:51.224
other parameter, two or
three other parameters,

00:11:51.224 --> 00:11:54.189
makes it so that your
learning rate or the ideal

00:11:54.189 --> 00:11:57.526
learning rate is still
[speaks too low to hear].

00:11:57.526 --> 00:11:59.557
- Question is how often
does it happen where

00:11:59.557 --> 00:12:01.001
when you change one hyperparameter,

00:12:01.001 --> 00:12:03.023
then the other, the
optimal values of the other

00:12:03.023 --> 00:12:05.022
hyperparameters change?

00:12:05.022 --> 00:12:09.844
That does happen sometimes,
although for learning rates,

00:12:09.844 --> 00:12:11.824
that's typically less of a problem.

00:12:11.824 --> 00:12:13.416
For learning rates,
typically you want to get

00:12:13.416 --> 00:12:15.932
in a good range, and
then set it maybe even

00:12:15.932 --> 00:12:17.431
a little bit lower than
optimal, and let it go

00:12:17.431 --> 00:12:18.615
for a long time.

00:12:18.615 --> 00:12:20.418
Then if you do that, combined with some

00:12:20.418 --> 00:12:22.199
of the fancier optimization
strategies that

00:12:22.199 --> 00:12:25.373
we'll talk about today,
then a lot of models

00:12:25.373 --> 00:12:26.787
tend to be a little bit less sensitive

00:12:26.787 --> 00:12:31.776
to learning rate once you
get them in a good range.

00:12:31.776 --> 00:12:33.447
Sorry, did you have a
question in front, as well?

00:12:33.447 --> 00:12:37.793
- [Student] [speaks too low to hear]

00:12:37.793 --> 00:12:39.075
- The question is what's wrong with having

00:12:39.075 --> 00:12:40.386
a small learning rate and increasing

00:12:40.386 --> 00:12:41.777
the number of epochs?

00:12:41.777 --> 00:12:43.488
The answer is that it might take

00:12:43.488 --> 00:12:45.624
a very long time. [laughs]

00:12:45.624 --> 00:12:48.868
- [Student] [speaks too low to hear]

00:12:48.868 --> 00:12:50.346
- Intuitively, if you
set the learning rate

00:12:50.346 --> 00:12:53.010
very low and let it go
for a very long time,

00:12:53.010 --> 00:12:55.338
then this should, in theory, always work.

00:12:55.338 --> 00:12:58.490
But in practice, those
factors of 10 or 100

00:12:58.490 --> 00:12:59.883
actually matter a lot when you're training

00:12:59.883 --> 00:13:00.976
these things.

00:13:00.976 --> 00:13:02.487
Maybe if you got the right learning rate,

00:13:02.487 --> 00:13:04.416
you could train it in six hours, 12 hours

00:13:04.416 --> 00:13:06.719
or a day, but then if
you just were super safe

00:13:06.719 --> 00:13:08.064
and dropped it by a factor of 10

00:13:08.064 --> 00:13:10.930
or by a factor of 100,
now that one-day training

00:13:10.930 --> 00:13:12.396
becomes 100 days of training.

00:13:12.396 --> 00:13:14.318
That's three months.

00:13:14.318 --> 00:13:16.885
That's not going to be good.

00:13:16.885 --> 00:13:17.747
When you're taking these intro

00:13:17.747 --> 00:13:19.179
computer science classes,
they always kind of sweep

00:13:19.179 --> 00:13:21.153
the constants under the rug, but when

00:13:21.153 --> 00:13:22.954
you're actually thinking
about training things,

00:13:22.954 --> 00:13:25.929
those constants end up mattering a lot.

00:13:25.929 --> 00:13:27.346
Another question?

00:13:28.362 --> 00:13:29.763
- [Student] If you have
a low learning rate,

00:13:29.763 --> 00:13:33.870
[speaks too low to hear].

00:13:33.870 --> 00:13:35.344
- Question is for a low learning rate,

00:13:35.344 --> 00:13:38.292
are you more likely to
be stuck in local optima?

00:13:38.292 --> 00:13:40.191
I think that makes some intuitive sense,

00:13:40.191 --> 00:13:42.232
but in practice, that seems not to be much

00:13:42.232 --> 00:13:43.086
of a problem.

00:13:43.086 --> 00:13:47.515
I think we'll talk a bit
more about that later today.

00:13:47.515 --> 00:13:49.084
Today I wanted to talk about a couple

00:13:49.084 --> 00:13:51.843
other really interesting
and important topics

00:13:51.843 --> 00:13:53.636
when we're training neural networks.

00:13:53.636 --> 00:13:55.223
In particular, I wanted to talk,

00:13:55.223 --> 00:13:56.327
we've kind of alluded to this fact

00:13:56.327 --> 00:13:58.709
of fancier, more powerful
optimization algorithms

00:13:58.709 --> 00:14:00.140
a couple times.

00:14:00.140 --> 00:14:01.560
I wanted to spend some
time today and really

00:14:01.560 --> 00:14:03.877
dig into those and talk about what are the

00:14:03.877 --> 00:14:05.592
actual optimization
algorithms that most people

00:14:05.592 --> 00:14:07.552
are using these days.

00:14:07.552 --> 00:14:09.576
We also touched on regularization

00:14:09.576 --> 00:14:10.849
in earlier lectures.

00:14:10.849 --> 00:14:12.736
This concept of making your network

00:14:12.736 --> 00:14:14.599
do additional things to reduce the gap

00:14:14.599 --> 00:14:16.291
between train and test error.

00:14:16.291 --> 00:14:18.133
I wanted to talk about
some more strategies

00:14:18.133 --> 00:14:19.686
that people are using in practice

00:14:19.686 --> 00:14:22.628
of regularization, with
respect to neural networks.

00:14:22.628 --> 00:14:24.411
Finally, I also wanted to talk a bit

00:14:24.411 --> 00:14:26.886
about transfer learning, where you can

00:14:26.886 --> 00:14:28.431
sometimes get away with using less data

00:14:28.431 --> 00:14:30.058
than you think by transferring from

00:14:30.058 --> 00:14:31.975
one problem to another.

00:14:33.306 --> 00:14:36.005
If you recall from a few lectures ago,

00:14:36.005 --> 00:14:37.710
the kind of core strategy in training

00:14:37.710 --> 00:14:40.370
neural networks is an optimization problem

00:14:40.370 --> 00:14:42.711
where we write down some loss function,

00:14:42.711 --> 00:14:46.059
which defines, for each
value of the network weights,

00:14:46.059 --> 00:14:48.201
the loss function tells us how good or bad

00:14:48.201 --> 00:14:51.467
is that value of the weights
doing on our problem.

00:14:51.467 --> 00:14:54.074
Then we imagine that this loss function

00:14:54.074 --> 00:14:56.993
gives us some nice
landscape over the weights,

00:14:56.993 --> 00:14:59.411
where on the right, I've shown this maybe

00:14:59.411 --> 00:15:01.335
small, two-dimensional
problem, where the X

00:15:01.335 --> 00:15:04.627
and Y axes are two values of the weights.

00:15:04.627 --> 00:15:06.394
Then the color of the
plot kind of represents

00:15:06.394 --> 00:15:08.469
the value of the loss.

00:15:08.469 --> 00:15:10.072
In this kind of cartoon picture

00:15:10.072 --> 00:15:12.754
of a two-dimensional problem, we're only

00:15:12.754 --> 00:15:15.680
optimizing over these
two values, W one, W two.

00:15:15.680 --> 00:15:19.205
The goal is to find the most red region

00:15:19.205 --> 00:15:21.242
in this case, which
corresponds to the setting

00:15:21.242 --> 00:15:23.688
of the weights with the lowest loss.

00:15:23.688 --> 00:15:25.030
Remember, we've been working so far

00:15:25.030 --> 00:15:27.616
with this extremely simple
optimization algorithm,

00:15:27.616 --> 00:15:29.584
stochastic gradient descent,

00:15:29.584 --> 00:15:32.878
where it's super simple, it's three lines.

00:15:32.878 --> 00:15:36.095
While true, we first evaluate the loss

00:15:36.095 --> 00:15:39.664
in the gradient on some
mini batch of data.

00:15:39.664 --> 00:15:43.043
Then we step, updating
our parameter vector

00:15:43.043 --> 00:15:45.141
in the negative direction of the gradient

00:15:45.141 --> 00:15:46.891
because this gives, again, the direction

00:15:46.891 --> 00:15:49.283
of greatest decrease of the loss function.

00:15:49.283 --> 00:15:50.836
Then we repeat this over and over again,

00:15:50.836 --> 00:15:53.315
and hopefully we converge
to the red region

00:15:53.315 --> 00:15:56.767
and we get great errors
and we're very happy.

00:15:56.767 --> 00:15:58.933
But unfortunately, this relatively simple

00:15:58.933 --> 00:16:02.197
optimization algorithm has
quite a lot of problems

00:16:02.197 --> 00:16:05.947
that actually could come up in practice.

00:16:05.947 --> 00:16:09.198
One problem with stochastic
gradient descent,

00:16:09.198 --> 00:16:12.003
imagine what happens if
our objective function

00:16:12.003 --> 00:16:15.336
looks something like this, where, again,

00:16:16.696 --> 00:16:19.454
we're plotting two
values, W one and W two.

00:16:19.454 --> 00:16:21.417
As we change one of those values,

00:16:21.417 --> 00:16:23.957
the loss function changes very slowly.

00:16:23.957 --> 00:16:25.922
As we change the horizontal
value, then our loss

00:16:25.922 --> 00:16:27.172
changes slowly.

00:16:28.637 --> 00:16:30.328
As we go up and down in this landscape,

00:16:30.328 --> 00:16:32.724
now our loss is very sensitive to changes

00:16:32.724 --> 00:16:35.415
in the vertical direction.

00:16:35.415 --> 00:16:38.268
By the way, this is
referred to as the loss

00:16:38.268 --> 00:16:41.242
having a bad condition
number at this point,

00:16:41.242 --> 00:16:43.511
which is the ratio between
the largest and smallest

00:16:43.511 --> 00:16:45.331
singular values of the Hessian matrix

00:16:45.331 --> 00:16:46.535
at that point.

00:16:46.535 --> 00:16:48.814
But the intuitive idea is
that the loss landscape

00:16:48.814 --> 00:16:50.982
kind of looks like a taco shell.

00:16:50.982 --> 00:16:52.877
It's sort of very
sensitive in one direction,

00:16:52.877 --> 00:16:54.878
not sensitive in the other direction.

00:16:54.878 --> 00:16:57.381
The question is what might SGD,

00:16:57.381 --> 00:16:59.368
stochastic gradient
descent, do on a function

00:16:59.368 --> 00:17:01.118
that looks like this?

00:17:05.795 --> 00:17:07.535
If you run stochastic gradient descent

00:17:07.535 --> 00:17:10.216
on this type of function, you might get

00:17:10.216 --> 00:17:12.682
this characteristic zigzagging behavior,

00:17:12.682 --> 00:17:16.503
where because for this
type of objective function,

00:17:16.503 --> 00:17:19.829
the direction of the
gradient does not align

00:17:19.829 --> 00:17:22.597
with the direction towards the minima.

00:17:22.597 --> 00:17:24.547
When you compute the
gradient and take a step,

00:17:24.547 --> 00:17:27.264
you might step sort of over this line

00:17:27.264 --> 00:17:29.820
and sort of zigzag back and forth.

00:17:29.820 --> 00:17:32.588
In effect, you get very
slow progress along

00:17:32.588 --> 00:17:34.455
the horizontal dimension, which is the

00:17:34.455 --> 00:17:36.480
less sensitive dimension, and you get this

00:17:36.480 --> 00:17:39.837
zigzagging, nasty, nasty
zigzagging behavior

00:17:39.837 --> 00:17:42.036
across the fast-changing dimension.

00:17:42.036 --> 00:17:45.032
This is undesirable behavior.

00:17:45.032 --> 00:17:47.624
By the way, this problem actually becomes

00:17:47.624 --> 00:17:50.624
much more common in high dimensions.

00:17:51.671 --> 00:17:53.065
In this kind of cartoon
picture, we're only

00:17:53.065 --> 00:17:55.605
showing a two-dimensional
optimization landscape,

00:17:55.605 --> 00:17:57.892
but in practice, our
neural networks might have

00:17:57.892 --> 00:17:59.830
millions, tens of millions,
hundreds of millions

00:17:59.830 --> 00:18:01.102
of parameters.

00:18:01.102 --> 00:18:02.811
That's hundreds of millions of directions

00:18:02.811 --> 00:18:04.986
along which this thing can move.

00:18:04.986 --> 00:18:06.737
Now among those hundreds of millions

00:18:06.737 --> 00:18:08.942
of different directions to move,

00:18:08.942 --> 00:18:10.742
if the ratio between the largest one

00:18:10.742 --> 00:18:12.925
and the smallest one is bad, then SGD

00:18:12.925 --> 00:18:14.706
will not perform so nicely.

00:18:14.706 --> 00:18:17.332
You can imagine that if we
have 100 million parameters,

00:18:17.332 --> 00:18:19.601
probably the maximum
ratio between those two

00:18:19.601 --> 00:18:21.058
will be quite large.

00:18:21.058 --> 00:18:22.966
I think this is actually
quite a big problem

00:18:22.966 --> 00:18:26.883
in practice for many
high-dimensional problems.

00:18:28.278 --> 00:18:31.041
Another problem with SGD
has to do with this idea

00:18:31.041 --> 00:18:34.049
of local minima or saddle points.

00:18:34.049 --> 00:18:37.157
Here I've sort of swapped
the graph a little bit.

00:18:37.157 --> 00:18:39.186
Now the X axis is showing the value

00:18:39.186 --> 00:18:41.641
of one parameter, and then the Y axis

00:18:41.641 --> 00:18:44.488
is showing the value of the loss.

00:18:44.488 --> 00:18:47.812
In this top example, we have kind of this

00:18:47.812 --> 00:18:50.073
curvy objective function, where there's a

00:18:50.073 --> 00:18:52.068
valley in the middle.

00:18:52.068 --> 00:18:55.521
What happens to SGD in this situation?

00:18:55.521 --> 00:18:57.516
- [Student] [speaks too low to hear]

00:18:57.516 --> 00:18:59.866
- In this situation, SGD will get stuck

00:18:59.866 --> 00:19:02.570
because at this local minima, the gradient

00:19:02.570 --> 00:19:04.939
is zero because it's locally flat.

00:19:04.939 --> 00:19:07.507
Now remember with SGD,
we compute the gradient

00:19:07.507 --> 00:19:09.679
and step in the direction
of opposite gradient,

00:19:09.679 --> 00:19:11.980
so if at our current point,
the opposite gradient

00:19:11.980 --> 00:19:14.440
is zero, then we're not
going to make any progress,

00:19:14.440 --> 00:19:16.347
and we'll get stuck at this point.

00:19:16.347 --> 00:19:18.420
There's another problem with this idea

00:19:18.420 --> 00:19:19.891
of saddle points.

00:19:19.891 --> 00:19:21.361
Rather than being a local minima,

00:19:21.361 --> 00:19:23.970
you can imagine a point
where in one direction

00:19:23.970 --> 00:19:26.625
we go up, and in the other
direction we go down.

00:19:26.625 --> 00:19:29.438
Then at our current point,
the gradient is zero.

00:19:29.438 --> 00:19:32.386
Again, in this situation, the function

00:19:32.386 --> 00:19:33.826
will get stuck at the saddle point because

00:19:33.826 --> 00:19:36.384
the gradient is zero.

00:19:36.384 --> 00:19:37.943
Although one thing I'd like to point out

00:19:37.943 --> 00:19:40.856
is that in one dimension,
in a one-dimensional problem

00:19:40.856 --> 00:19:44.378
like this, local minima
seem like a big problem

00:19:44.378 --> 00:19:46.218
and saddle points seem like kind of not

00:19:46.218 --> 00:19:48.607
something to worry about, but in fact,

00:19:48.607 --> 00:19:50.140
it's the opposite once you move to very

00:19:50.140 --> 00:19:52.495
high-dimensional problems because, again,

00:19:52.495 --> 00:19:54.329
if you think about you're in this

00:19:54.329 --> 00:19:56.090
100 million dimensional space,

00:19:56.090 --> 00:19:57.656
what does a saddle point mean?

00:19:57.656 --> 00:19:59.523
That means that at my current point,

00:19:59.523 --> 00:20:01.523
some directions the loss goes up,

00:20:01.523 --> 00:20:03.620
and some directions the loss goes down.

00:20:03.620 --> 00:20:05.371
If you have 100 million dimensions,

00:20:05.371 --> 00:20:07.700
that's probably going to
happen more frequently than,

00:20:07.700 --> 00:20:10.076
that's probably going to happen
almost everywhere, basically.

00:20:10.076 --> 00:20:12.983
Whereas a local minima
says that of all those

00:20:12.983 --> 00:20:14.851
100 million directions that I can move,

00:20:14.851 --> 00:20:17.229
every one of them causes
the loss to go up.

00:20:17.229 --> 00:20:19.369
In fact, that seems pretty rare when

00:20:19.369 --> 00:20:20.634
you're thinking about, again, these very

00:20:20.634 --> 00:20:22.801
high-dimensional problems.

00:20:23.755 --> 00:20:25.766
Really, the idea that has come to light

00:20:25.766 --> 00:20:27.672
in the last few years is that when

00:20:27.672 --> 00:20:29.623
you're training these very
large neural networks,

00:20:29.623 --> 00:20:31.515
the problem is more about saddle points

00:20:31.515 --> 00:20:33.768
and less about local minima.

00:20:33.768 --> 00:20:36.050
By the way, this also is a problem

00:20:36.050 --> 00:20:38.259
not just exactly at the saddle point,

00:20:38.259 --> 00:20:40.625
but also near the saddle point.

00:20:40.625 --> 00:20:42.439
If you look at the example on the bottom,

00:20:42.439 --> 00:20:44.259
you see that in the regions around

00:20:44.259 --> 00:20:46.620
the saddle point, the gradient isn't zero,

00:20:46.620 --> 00:20:48.420
but the slope is very small.

00:20:48.420 --> 00:20:50.493
That means that if we're,
again, just stepping

00:20:50.493 --> 00:20:52.120
in the direction of the gradient,

00:20:52.120 --> 00:20:54.096
and that gradient is very
small, we're going to make

00:20:54.096 --> 00:20:58.003
very, very slow progress whenever our

00:20:58.003 --> 00:21:00.039
current parameter value
is near a saddle point

00:21:00.039 --> 00:21:02.357
in the objective landscape.

00:21:02.357 --> 00:21:04.940
This is actually a big problem.

00:21:06.877 --> 00:21:10.600
Another problem with SGD comes from the S.

00:21:10.600 --> 00:21:14.006
Remember that SGD is
stochastic gradient descent.

00:21:14.006 --> 00:21:15.830
Recall that our loss function is typically

00:21:15.830 --> 00:21:18.868
defined by computing the loss over many,

00:21:18.868 --> 00:21:21.071
many different examples.

00:21:21.071 --> 00:21:24.415
In this case, if N is
your whole training set,

00:21:24.415 --> 00:21:26.604
then that could be
something like a million.

00:21:26.604 --> 00:21:28.513
Each time computing
the loss would be very,

00:21:28.513 --> 00:21:29.832
very expensive.

00:21:29.832 --> 00:21:33.039
In practice, remember
that we often estimate

00:21:33.039 --> 00:21:34.940
the loss and estimate the gradient

00:21:34.940 --> 00:21:37.442
using a small mini batch of examples.

00:21:37.442 --> 00:21:38.965
What this means is that we're not actually

00:21:38.965 --> 00:21:41.301
getting the true information
about the gradient

00:21:41.301 --> 00:21:42.633
at every time step.

00:21:42.633 --> 00:21:44.654
Instead, we're just
getting some noisy estimate

00:21:44.654 --> 00:21:47.258
of the gradient at our current point.

00:21:47.258 --> 00:21:49.224
Here on the right, I've kind of faked

00:21:49.224 --> 00:21:51.060
this plot a little bit.

00:21:51.060 --> 00:21:54.044
I've just added random uniform noise

00:21:54.044 --> 00:21:56.277
to the gradient at every point, and then

00:21:56.277 --> 00:22:00.412
run SGD with these noisy,
messed up gradients.

00:22:00.412 --> 00:22:01.992
This is maybe not exactly what happens

00:22:01.992 --> 00:22:03.992
with the SGD process,
but it still give you

00:22:03.992 --> 00:22:06.021
the sense that if there's noise in your

00:22:06.021 --> 00:22:08.472
gradient estimates, then vanilla SGD

00:22:08.472 --> 00:22:10.449
kind of meanders around the space

00:22:10.449 --> 00:22:12.354
and might actually take a long time

00:22:12.354 --> 00:22:14.521
to get towards the minima.

00:22:16.208 --> 00:22:17.740
Now that we've talked about a lot

00:22:17.740 --> 00:22:19.451
of these problems.

00:22:19.451 --> 00:22:21.441
Sorry, was there a question?

00:22:21.441 --> 00:22:25.608
- [Student] [speaks too low to hear]

00:22:29.584 --> 00:22:31.003
- The question is do all of these

00:22:31.003 --> 00:22:34.920
just go away if we use
normal gradient descent?

00:22:35.766 --> 00:22:37.293
Let's see.

00:22:37.293 --> 00:22:40.462
I think that the taco shell problem

00:22:40.462 --> 00:22:42.318
of high condition numbers
is still a problem

00:22:42.318 --> 00:22:44.591
with full batch gradient descent.

00:22:44.591 --> 00:22:45.963
The noise.

00:22:45.963 --> 00:22:47.823
As we'll see, we might sometimes introduce

00:22:47.823 --> 00:22:49.190
additional noise into the network,

00:22:49.190 --> 00:22:51.377
not only due to sampling mini batches,

00:22:51.377 --> 00:22:54.605
but also due to explicit
stochasticity in the network,

00:22:54.605 --> 00:22:55.687
so we'll see that later.

00:22:55.687 --> 00:22:58.221
That can still be a problem.

00:22:58.221 --> 00:23:00.228
Saddle points, that's still a problem

00:23:00.228 --> 00:23:02.447
for full batch gradient descent because

00:23:02.447 --> 00:23:03.757
there can still be saddle points

00:23:03.757 --> 00:23:05.586
in the full objective landscape.

00:23:05.586 --> 00:23:07.383
Basically, even if we go to full batch

00:23:07.383 --> 00:23:08.980
gradient descent, it doesn't really solve

00:23:08.980 --> 00:23:10.734
these problems.

00:23:10.734 --> 00:23:12.773
We kind of need to think
about a slightly fancier

00:23:12.773 --> 00:23:14.513
optimization algorithm that can try

00:23:14.513 --> 00:23:17.089
to address these concerns.

00:23:17.089 --> 00:23:19.253
Thankfully, there's a really,
really simple strategy

00:23:19.253 --> 00:23:20.960
that works pretty well at addressing

00:23:20.960 --> 00:23:22.451
many of these problems.

00:23:22.451 --> 00:23:24.962
That's this idea of adding a momentum term

00:23:24.962 --> 00:23:27.463
to our stochastic gradient descent.

00:23:27.463 --> 00:23:29.455
Here on the left, we have our classic

00:23:29.455 --> 00:23:31.863
old friend, SGD, where we just always step

00:23:31.863 --> 00:23:33.408
in the direction of the gradient.

00:23:33.408 --> 00:23:35.383
But now on the right, we have this minor,

00:23:35.383 --> 00:23:38.089
minor variance called SGD plus momentum,

00:23:38.089 --> 00:23:40.987
which is now two equations
and five lines of code,

00:23:40.987 --> 00:23:43.547
so it's twice as complicated.

00:23:43.547 --> 00:23:44.867
But it's very simple.

00:23:44.867 --> 00:23:47.315
The idea is that we maintain a velocity

00:23:47.315 --> 00:23:50.273
over time, and we add
our gradient estimates

00:23:50.273 --> 00:23:51.816
to the velocity.

00:23:51.816 --> 00:23:54.143
Then we step in the
direction of the velocity,

00:23:54.143 --> 00:23:58.296
rather than stepping in the
direction of the gradient.

00:23:58.296 --> 00:24:00.463
This is very, very simple.

00:24:01.563 --> 00:24:04.562
We also have this hyperparameter rho now

00:24:04.562 --> 00:24:06.410
which corresponds to friction.

00:24:06.410 --> 00:24:07.774
Now at every time step, we take our

00:24:07.774 --> 00:24:10.326
current velocity, we
decay the current velocity

00:24:10.326 --> 00:24:13.069
by the friction constant,
rho, which is often

00:24:13.069 --> 00:24:17.333
something high, like
.9 is a common choice.

00:24:17.333 --> 00:24:19.115
We take our current velocity, we decay it

00:24:19.115 --> 00:24:21.658
by friction and we add in our gradient.

00:24:21.658 --> 00:24:24.506
Now we step in the direction
of our velocity vector,

00:24:24.506 --> 00:24:25.817
rather than the direction of our

00:24:25.817 --> 00:24:27.484
raw gradient vector.

00:24:28.812 --> 00:24:30.710
This super, super simple strategy

00:24:30.710 --> 00:24:32.656
actually helps for all of these problems

00:24:32.656 --> 00:24:35.033
that we just talked about.

00:24:35.033 --> 00:24:37.303
If you think about what
happens at local minima

00:24:37.303 --> 00:24:40.608
or saddle points, then if we're imagining

00:24:40.608 --> 00:24:42.819
velocity in this system,
then you kind of have

00:24:42.819 --> 00:24:45.294
this physical interpretation of this ball

00:24:45.294 --> 00:24:47.349
kind of rolling down the
hill, picking up speed

00:24:47.349 --> 00:24:48.700
as it comes down.

00:24:48.700 --> 00:24:51.870
Now once we have velocity,
then even when we

00:24:51.870 --> 00:24:53.837
pass that point of local minima,

00:24:53.837 --> 00:24:55.859
the point will still have velocity,

00:24:55.859 --> 00:24:57.407
even if it doesn't have gradient.

00:24:57.407 --> 00:24:59.625
Then we can hopefully get
over this local minima

00:24:59.625 --> 00:25:01.524
and continue downward.

00:25:01.524 --> 00:25:04.294
There's this similar
intuition near saddle points,

00:25:04.294 --> 00:25:05.945
where even though the gradient around

00:25:05.945 --> 00:25:08.001
the saddle point is very small, we have

00:25:08.001 --> 00:25:09.758
this velocity vector that we've built up

00:25:09.758 --> 00:25:11.219
as we roll downhill.

00:25:11.219 --> 00:25:12.625
That can hopefully carry us through

00:25:12.625 --> 00:25:14.720
the saddle point and
let us continue rolling

00:25:14.720 --> 00:25:16.947
all the way down.

00:25:16.947 --> 00:25:18.563
If you think about what happens in

00:25:18.563 --> 00:25:22.434
poor conditioning, now if we were to have

00:25:22.434 --> 00:25:24.282
these kind of zigzagging approximations

00:25:24.282 --> 00:25:27.407
to the gradient, then those zigzags

00:25:27.407 --> 00:25:29.960
will hopefully cancel
each other out pretty fast

00:25:29.960 --> 00:25:31.590
once we're using momentum.

00:25:31.590 --> 00:25:33.916
This will effectively reduce the amount

00:25:33.916 --> 00:25:36.360
by which we step in the
sensitive direction,

00:25:36.360 --> 00:25:39.255
whereas in the horizontal direction,

00:25:39.255 --> 00:25:40.926
our velocity will just keep building up,

00:25:40.926 --> 00:25:43.918
and will actually accelerate our descent

00:25:43.918 --> 00:25:46.491
across that less sensitive dimension.

00:25:46.491 --> 00:25:48.340
Adding momentum here can actually help us

00:25:48.340 --> 00:25:51.829
with this high condition
number problem, as well.

00:25:51.829 --> 00:25:53.428
Finally, on the right, we've repeated

00:25:53.428 --> 00:25:57.919
the same visualization of
gradient descent with noise.

00:25:57.919 --> 00:26:00.403
Here, the black is this vanilla SGD,

00:26:00.403 --> 00:26:02.547
which is sort of zigzagging
all over the place,

00:26:02.547 --> 00:26:04.342
where the blue line is showing now SGD

00:26:04.342 --> 00:26:05.692
with momentum.

00:26:05.692 --> 00:26:07.597
You can see that because we're adding it,

00:26:07.597 --> 00:26:09.645
we're building up this velocity over time,

00:26:09.645 --> 00:26:12.022
the noise kind of gets averaged out

00:26:12.022 --> 00:26:13.129
in our gradient estimates.

00:26:13.129 --> 00:26:15.395
Now SGD ends up taking
a much smoother path

00:26:15.395 --> 00:26:17.938
towards the minima, compared with the SGD,

00:26:17.938 --> 00:26:20.822
which is kind of meandering due to noise.

00:26:20.822 --> 00:26:22.017
Question?

00:26:22.017 --> 00:26:26.184
- [Student] [speaks too low to hear]

00:26:35.261 --> 00:26:37.920
- The question is how does SGD momentum

00:26:37.920 --> 00:26:40.950
help with the poorly
conditioned coordinate?

00:26:40.950 --> 00:26:42.899
The idea is that if you go back and look

00:26:42.899 --> 00:26:44.613
at this velocity estimate and look

00:26:44.613 --> 00:26:46.906
at the velocity
computation, we're adding in

00:26:46.906 --> 00:26:49.610
the gradient at every time step.

00:26:49.610 --> 00:26:51.580
It kind of depends on your setting of rho,

00:26:51.580 --> 00:26:53.344
that hyperparameter, but you can imagine

00:26:53.344 --> 00:26:57.088
that if the gradient is relatively small,

00:26:57.088 --> 00:26:59.739
and if rho is well
behaved in this situation,

00:26:59.739 --> 00:27:02.048
then our velocity could
actually monotonically increase

00:27:02.048 --> 00:27:03.957
up to a point where the velocity could now

00:27:03.957 --> 00:27:05.997
be larger than the actual gradient.

00:27:05.997 --> 00:27:07.612
Then we might actually
make faster progress

00:27:07.612 --> 00:27:10.862
along the poorly conditioned dimension.

00:27:13.054 --> 00:27:16.145
Kind of one picture that
you can have in mind

00:27:16.145 --> 00:27:18.505
when we're doing SGD plus momentum is that

00:27:18.505 --> 00:27:20.758
the red here is our current point.

00:27:20.758 --> 00:27:23.158
At our current point,
we have some red vector,

00:27:23.158 --> 00:27:24.847
which is the direction of the gradient,

00:27:24.847 --> 00:27:26.415
or rather our estimate of the gradient

00:27:26.415 --> 00:27:27.753
at the current point.

00:27:27.753 --> 00:27:30.560
Green is now the direction
of our velocity vector.

00:27:30.560 --> 00:27:33.232
Now when we do the momentum update,

00:27:33.232 --> 00:27:34.549
we're actually stepping according to a

00:27:34.549 --> 00:27:36.802
weighted average of these two.

00:27:36.802 --> 00:27:38.574
This helps overcome some noise in our

00:27:38.574 --> 00:27:40.534
gradient estimate.

00:27:40.534 --> 00:27:42.124
There's a slight variation of momentum

00:27:42.124 --> 00:27:44.041
that you sometimes see, called

00:27:44.041 --> 00:27:45.640
Nesterov accelerated gradient,

00:27:45.640 --> 00:27:48.209
also sometimes called Nesterov momentum.

00:27:48.209 --> 00:27:50.690
That switches up this order of things

00:27:50.690 --> 00:27:52.222
a little bit.

00:27:52.222 --> 00:27:56.309
In sort of normal SGD momentum, we imagine

00:27:56.309 --> 00:27:58.075
that we estimate the gradient
at our current point,

00:27:58.075 --> 00:27:59.698
and then take a mix of our velocity

00:27:59.698 --> 00:28:00.770
and our gradient.

00:28:00.770 --> 00:28:02.513
With Nesterov accelerated gradient, you do

00:28:02.513 --> 00:28:04.714
something a little bit different.

00:28:04.714 --> 00:28:07.382
Here, you start at the red point.

00:28:07.382 --> 00:28:10.098
You step in the direction
of where the velocity

00:28:10.098 --> 00:28:11.250
would take you.

00:28:11.250 --> 00:28:14.844
You evaluate the gradient at that point.

00:28:14.844 --> 00:28:16.022
Then you go back to your original point

00:28:16.022 --> 00:28:19.217
and kind of mix together those two.

00:28:19.217 --> 00:28:21.304
This is kind of a funny interpretation,

00:28:21.304 --> 00:28:23.127
but you can imagine that you're kind of

00:28:23.127 --> 00:28:26.164
mixing together information
a little bit more.

00:28:26.164 --> 00:28:28.089
If your velocity direction was actually

00:28:28.089 --> 00:28:29.857
a little bit wrong, it
lets you incorporate

00:28:29.857 --> 00:28:31.774
gradient information
from a little bit larger

00:28:31.774 --> 00:28:35.187
parts of the objective landscape.

00:28:35.187 --> 00:28:37.369
This also has some really
nice theoretical properties

00:28:37.369 --> 00:28:39.836
when it comes to convex optimization,

00:28:39.836 --> 00:28:42.211
but those guarantees go a little bit

00:28:42.211 --> 00:28:44.295
out the window once it
comes to non-convex problems

00:28:44.295 --> 00:28:46.431
like neural networks.

00:28:46.431 --> 00:28:48.553
Writing it down in
equations, Nesterov momentum

00:28:48.553 --> 00:28:51.546
looks something like this, where now

00:28:51.546 --> 00:28:54.379
to update our velocity, we take a step,

00:28:54.379 --> 00:28:56.230
according to our previous
velocity, and evaluate

00:28:56.230 --> 00:28:57.640
that gradient there.

00:28:57.640 --> 00:29:00.223
Now when we take our next step,

00:29:01.388 --> 00:29:03.144
we actually step in the
direction of our velocity

00:29:03.144 --> 00:29:04.457
that's incorporating information from

00:29:04.457 --> 00:29:06.707
these multiple points.

00:29:06.707 --> 00:29:07.540
Question?

00:29:08.922 --> 00:29:12.008
- [Student] [speaks too low to hear]

00:29:12.008 --> 00:29:12.842
- Oh, sorry.

00:29:12.842 --> 00:29:14.243
The question is what's
a good initialization

00:29:14.243 --> 00:29:15.228
for the velocity?

00:29:15.228 --> 00:29:17.483
This is almost always zero.

00:29:17.483 --> 00:29:18.595
It's not even a hyperparameter.

00:29:18.595 --> 00:29:20.581
Just set it to zero and don't worry.

00:29:20.581 --> 00:29:21.800
Another question?

00:29:21.800 --> 00:29:25.967
- [Student] [speaks too low to hear]

00:29:32.477 --> 00:29:35.817
- Intuitively, the velocity
is kind of a weighted sum

00:29:35.817 --> 00:29:38.553
of your gradients that
you've seen over time.

00:29:38.553 --> 00:29:41.951
- [Student] [speaks too low to hear]

00:29:41.951 --> 00:29:44.512
- With more recent gradients
being weighted heavier.

00:29:44.512 --> 00:29:46.947
At every time step, we
take our old velocity,

00:29:46.947 --> 00:29:48.559
we decay by friction and we add in

00:29:48.559 --> 00:29:50.201
our current gradient.

00:29:50.201 --> 00:29:51.417
You can kind of think of this as a

00:29:51.417 --> 00:29:55.147
smooth moving average
of your recent gradients

00:29:55.147 --> 00:29:57.511
with kind of a exponentially
decaying weight

00:29:57.511 --> 00:30:00.594
on your gradients going back in time.

00:30:03.112 --> 00:30:05.359
This Nesterov formulation
is a little bit annoying

00:30:05.359 --> 00:30:07.837
'cause if you look at
this, normally when you

00:30:07.837 --> 00:30:09.933
have your loss function,
you want to evaluate

00:30:09.933 --> 00:30:12.117
your loss and your
gradient at the same point.

00:30:12.117 --> 00:30:14.645
Nesterov breaks this a little bit.

00:30:14.645 --> 00:30:17.259
It's a little bit annoying to work with.

00:30:17.259 --> 00:30:18.852
Thankfully, there's a
cute change of variables

00:30:18.852 --> 00:30:19.768
you can do.

00:30:19.768 --> 00:30:20.918
If you do the change of variables

00:30:20.918 --> 00:30:22.868
and reshuffle a little bit, then you can

00:30:22.868 --> 00:30:25.230
write Nesterov momentum in
a slightly different way

00:30:25.230 --> 00:30:27.243
that now, again, lets
you evaluate the loss

00:30:27.243 --> 00:30:29.877
and the gradient at the same point always.

00:30:29.877 --> 00:30:31.857
Once you make this change of variables,

00:30:31.857 --> 00:30:34.578
you get kind of a nice
interpretation of Nesterov,

00:30:34.578 --> 00:30:36.880
which is that here in the first step,

00:30:36.880 --> 00:30:39.371
this looks exactly like
updating the velocity

00:30:39.371 --> 00:30:42.224
in the vanilla SGD momentum case, where we

00:30:42.224 --> 00:30:44.493
have our current velocity,
we evaluate gradient

00:30:44.493 --> 00:30:47.081
at the current point and
mix these two together

00:30:47.081 --> 00:30:48.663
in a decaying way.

00:30:48.663 --> 00:30:50.977
Now in the second update,
now when we're actually

00:30:50.977 --> 00:30:52.436
updating our parameter vector, if you look

00:30:52.436 --> 00:30:55.199
at the second equation,
we have our current point

00:30:55.199 --> 00:30:58.077
plus our current velocity plus

00:30:58.077 --> 00:31:00.245
a weighted difference
between our current velocity

00:31:00.245 --> 00:31:01.939
and our previous velocity.

00:31:01.939 --> 00:31:06.051
Here, Nesterov momentum
is kind of incorporating

00:31:06.051 --> 00:31:07.673
some kind of error-correcting term between

00:31:07.673 --> 00:31:11.756
your current velocity and
your previous velocity.

00:31:13.514 --> 00:31:15.211
If we look at SGD, SGD momentum

00:31:15.211 --> 00:31:17.230
and Nesterov momentum
on this kind of simple

00:31:17.230 --> 00:31:20.730
problem, compared with SGD, we notice that

00:31:21.827 --> 00:31:24.145
SGD kind of takes this,
SGD is in the black,

00:31:24.145 --> 00:31:26.831
kind of taking this slow
progress toward the minima.

00:31:26.831 --> 00:31:28.734
The blue and the green show momentum

00:31:28.734 --> 00:31:30.083
and Nesterov.

00:31:30.083 --> 00:31:32.235
These have this behavior
of kind of overshooting

00:31:32.235 --> 00:31:35.492
the minimum 'cause they're
building up velocity

00:31:35.492 --> 00:31:37.288
going past the minimum, and then kind of

00:31:37.288 --> 00:31:39.302
correcting themselves and coming back

00:31:39.302 --> 00:31:40.334
towards the minima.

00:31:40.334 --> 00:31:41.167
Question?

00:31:42.508 --> 00:31:46.675
- [Student] [speaks too low to hear]

00:31:52.509 --> 00:31:54.849
- The question is this picture looks good,

00:31:54.849 --> 00:31:56.470
but what happens if your minima call

00:31:56.470 --> 00:31:58.535
lies in this very narrow basin?

00:31:58.535 --> 00:32:00.019
Will the velocity just cause you

00:32:00.019 --> 00:32:02.012
to skip right over that minima?

00:32:02.012 --> 00:32:03.461
That's actually a really
interesting point,

00:32:03.461 --> 00:32:05.717
and the subject of some
recent theoretical work,

00:32:05.717 --> 00:32:07.969
but the idea is that maybe those really

00:32:07.969 --> 00:32:09.556
sharp minima are actually bad minima.

00:32:09.556 --> 00:32:11.778
We don't want to even land in those

00:32:11.778 --> 00:32:13.660
'cause the idea is that maybe if you have

00:32:13.660 --> 00:32:15.525
a very sharp minima, that actually could

00:32:15.525 --> 00:32:18.086
be a minima that overfits more.

00:32:18.086 --> 00:32:20.356
If you imagine that we
doubled our training set,

00:32:20.356 --> 00:32:22.511
the whole optimization
landscape would change,

00:32:22.511 --> 00:32:24.083
and maybe that very sensitive minima

00:32:24.083 --> 00:32:25.970
would actually disappear
if we were to collect

00:32:25.970 --> 00:32:27.905
more training data.

00:32:27.905 --> 00:32:29.279
We kind of have this intuition that we

00:32:29.279 --> 00:32:31.674
maybe want to land in very flat minima

00:32:31.674 --> 00:32:33.904
because those very flat
minima are probably

00:32:33.904 --> 00:32:36.418
more robust as we change
the training data.

00:32:36.418 --> 00:32:38.354
Those flat minima might
actually generalize

00:32:38.354 --> 00:32:40.938
better to testing data.

00:32:40.938 --> 00:32:42.453
This is again, sort of very recent

00:32:42.453 --> 00:32:44.907
theoretical work, but that's actually

00:32:44.907 --> 00:32:46.769
a really good point that you bring it up.

00:32:46.769 --> 00:32:48.869
In some sense, it's actually a feature

00:32:48.869 --> 00:32:51.922
and not a bug that SGD momentum actually

00:32:51.922 --> 00:32:54.839
skips over those very sharp minima.

00:32:56.464 --> 00:33:00.464
That's actually a good
thing, believe it or not.

00:33:01.310 --> 00:33:02.717
Another thing you can see is if you look

00:33:02.717 --> 00:33:04.801
at the difference between
momentum and Nesterov here,

00:33:04.801 --> 00:33:06.832
you can see that because of the

00:33:06.832 --> 00:33:09.247
correction factor in
Nesterov, maybe it's not

00:33:09.247 --> 00:33:10.783
overshooting quite as drastically,

00:33:10.783 --> 00:33:13.200
compared to vanilla momentum.

00:33:15.168 --> 00:33:17.714
There's another kind
of common optimization

00:33:17.714 --> 00:33:20.553
strategy is this algorithm called AdaGrad,

00:33:20.553 --> 00:33:23.172
which John Duchi, who's
now a professor here,

00:33:23.172 --> 00:33:25.777
worked on during his Ph.D.

00:33:25.777 --> 00:33:28.860
The idea with AdaGrad is that as you,

00:33:30.766 --> 00:33:32.453
during the course of the optimization,

00:33:32.453 --> 00:33:35.371
you're going to keep a running estimate

00:33:35.371 --> 00:33:37.690
or a running sum of all
the squared gradients

00:33:37.690 --> 00:33:40.054
that you see during training.

00:33:40.054 --> 00:33:42.039
Now rather than having a velocity term,

00:33:42.039 --> 00:33:44.442
instead we have this grad squared term.

00:33:44.442 --> 00:33:46.544
During training, we're
going to just keep adding

00:33:46.544 --> 00:33:49.684
the squared gradients to
this grad squared term.

00:33:49.684 --> 00:33:52.378
Now when we update our parameter vector,

00:33:52.378 --> 00:33:55.545
we'll divide by this grad squared term

00:33:57.430 --> 00:33:59.819
when we're making our update step.

00:33:59.819 --> 00:34:02.688
The question is what
does this kind of scaling

00:34:02.688 --> 00:34:05.521
do in this situation where we have

00:34:06.653 --> 00:34:08.878
a very high condition number?

00:34:08.878 --> 00:34:13.045
- [Student] [speaks too low to hear]

00:34:16.741 --> 00:34:18.791
- The idea is that if
we have two coordinates,

00:34:18.791 --> 00:34:21.214
one that always has a very high gradient

00:34:21.214 --> 00:34:23.389
and one that always has
a very small gradient,

00:34:23.389 --> 00:34:25.008
then as we add the sum of the squares

00:34:25.008 --> 00:34:27.127
of the small gradient,
we're going to be dividing

00:34:27.127 --> 00:34:30.698
by a small number, so
we'll accelerate movement

00:34:30.698 --> 00:34:33.610
along the slow dimension,

00:34:33.610 --> 00:34:35.666
along the one dimension.

00:34:35.666 --> 00:34:37.812
Then along the other
dimension, where the gradients

00:34:37.812 --> 00:34:40.729
tend to be very large,
then we'll be dividing

00:34:40.729 --> 00:34:42.927
by a large number, so
we'll kind of slow down

00:34:42.927 --> 00:34:46.409
our progress along the wiggling dimension.

00:34:46.409 --> 00:34:48.515
But there's kind of a problem here.

00:34:48.515 --> 00:34:50.943
That's the question of
what happens with AdaGrad

00:34:50.943 --> 00:34:52.976
over the course of training, as t

00:34:52.976 --> 00:34:56.579
gets larger and larger and larger?

00:34:56.579 --> 00:34:58.876
- [Student] [speaks too low to hear]

00:34:58.876 --> 00:35:00.891
- With AdaGrad, the steps
actually get smaller

00:35:00.891 --> 00:35:02.724
and smaller and smaller because we just

00:35:02.724 --> 00:35:04.501
continue updating this estimate of the

00:35:04.501 --> 00:35:06.645
squared gradients over
time, so this estimate

00:35:06.645 --> 00:35:08.896
just grows and grows
and grows monotonically

00:35:08.896 --> 00:35:10.380
over the course of training.

00:35:10.380 --> 00:35:12.962
Now this causes our
step size to get smaller

00:35:12.962 --> 00:35:15.844
and smaller and smaller over time.

00:35:15.844 --> 00:35:18.038
Again, in the convex case, there's some

00:35:18.038 --> 00:35:20.819
really nice theory showing
that this is actually

00:35:20.819 --> 00:35:23.965
really good 'cause in the convex case,

00:35:23.965 --> 00:35:25.812
as you approach a
minimum, you kind of want

00:35:25.812 --> 00:35:28.610
to slow down so you actually converge.

00:35:28.610 --> 00:35:30.404
That's actually kind of a feature

00:35:30.404 --> 00:35:31.677
in the convex case.

00:35:31.677 --> 00:35:33.865
But in the non-convex
case, that's a little bit

00:35:33.865 --> 00:35:36.562
problematic because as you come towards

00:35:36.562 --> 00:35:38.694
a saddle point, you might
get stuck with AdaGrad,

00:35:38.694 --> 00:35:42.492
and then you kind of no
longer make any progress.

00:35:42.492 --> 00:35:44.773
There's a slight variation of AdaGrad,

00:35:44.773 --> 00:35:47.370
called RMSProp, that actually addresses

00:35:47.370 --> 00:35:49.163
this concern a little bit.

00:35:49.163 --> 00:35:52.006
Now with RMSProp, we
still keep this estimate

00:35:52.006 --> 00:35:53.875
of the squared gradients, but instead

00:35:53.875 --> 00:35:55.428
of just letting that squared estimate

00:35:55.428 --> 00:35:57.595
continually accumulate over training,

00:35:57.595 --> 00:36:01.570
instead, we let that squared
estimate actually decay.

00:36:01.570 --> 00:36:03.969
This ends up looking kind
of like a momentum update,

00:36:03.969 --> 00:36:06.057
except we're having kind of momentum over

00:36:06.057 --> 00:36:08.114
the squared gradients,
rather than momentum

00:36:08.114 --> 00:36:09.825
over the actual gradients.

00:36:09.825 --> 00:36:12.872
Now with RMSProp, after
we compute our gradient,

00:36:12.872 --> 00:36:15.354
we take our current estimate
of the grad squared,

00:36:15.354 --> 00:36:16.983
we multiply it by this decay rate,

00:36:16.983 --> 00:36:20.846
which is commonly
something like .9 or .99.

00:36:20.846 --> 00:36:24.477
Then we add in this one
minus the decay rate

00:36:24.477 --> 00:36:27.086
of our current squared gradient.

00:36:27.086 --> 00:36:30.866
Now over time, you can imagine that.

00:36:30.866 --> 00:36:32.814
Then again, when we
make our step, the step

00:36:32.814 --> 00:36:36.080
looks exactly the same as AdaGrad,

00:36:36.080 --> 00:36:37.678
where we divide by the squared gradient

00:36:37.678 --> 00:36:39.592
in the step to again
have this nice property

00:36:39.592 --> 00:36:42.080
of accelerating movement
along the one dimension,

00:36:42.080 --> 00:36:44.555
and slowing down movement
along the other dimension.

00:36:44.555 --> 00:36:46.609
But now with RMSProp,
because these estimates

00:36:46.609 --> 00:36:49.614
are leaky, then it kind
of addresses the problem

00:36:49.614 --> 00:36:51.396
of maybe always slowing down where you

00:36:51.396 --> 00:36:52.896
might not want to.

00:36:56.940 --> 00:36:58.964
Here again, we're kind
of showing our favorite

00:36:58.964 --> 00:37:01.817
toy problem with SGD
in black, SGD momentum

00:37:01.817 --> 00:37:04.658
in blue and RMSProp in red.

00:37:04.658 --> 00:37:07.807
You can see that RMSProp and SGD momentum

00:37:07.807 --> 00:37:09.965
are both doing much better than SGD,

00:37:09.965 --> 00:37:12.748
but their qualitative behavior
is a little bit different.

00:37:12.748 --> 00:37:16.005
With SGD momentum, it kind of overshoots

00:37:16.005 --> 00:37:17.973
the minimum and comes back, whereas with

00:37:17.973 --> 00:37:21.061
RMSProp, it's kind of adjusting

00:37:21.061 --> 00:37:22.914
its trajectory such that we're making

00:37:22.914 --> 00:37:24.549
approximately equal progress among

00:37:24.549 --> 00:37:26.877
all the dimensions.

00:37:26.877 --> 00:37:28.544
By the way, you can't actually tell,

00:37:28.544 --> 00:37:32.967
but this plot is also
showing AdaGrad in green

00:37:32.967 --> 00:37:34.897
with the same learning rate, but it just

00:37:34.897 --> 00:37:37.244
gets stuck due to this
problem of continually

00:37:37.244 --> 00:37:39.091
decaying learning rates.

00:37:39.091 --> 00:37:41.354
In practice, AdaGrad
is maybe not so common

00:37:41.354 --> 00:37:43.437
for many of these things.

00:37:44.276 --> 00:37:45.874
That's a little bit of
an unfair comparison

00:37:45.874 --> 00:37:46.877
of AdaGrad.

00:37:46.877 --> 00:37:48.715
Probably you need to
increase the learning rate

00:37:48.715 --> 00:37:50.495
with AdaGrad, and then
it would end up looking

00:37:50.495 --> 00:37:53.043
kind of like RMSProp in this case.

00:37:53.043 --> 00:37:55.639
But in general, we tend not to use AdaGrad

00:37:55.639 --> 00:37:57.633
so much when training neural networks.

00:37:57.633 --> 00:37:58.466
Question?

00:37:58.466 --> 00:38:00.281
- [Student] [speaks too low to hear]

00:38:00.281 --> 00:38:03.455
- The answer is yes,
this problem is convex,

00:38:03.455 --> 00:38:04.872
but in this case,

00:38:07.631 --> 00:38:09.128
it's a little bit of an unfair comparison

00:38:09.128 --> 00:38:11.315
because the learning rates
are not so comparable

00:38:11.315 --> 00:38:12.488
among the methods.

00:38:12.488 --> 00:38:14.114
I've been a little bit unfair to AdaGrad

00:38:14.114 --> 00:38:15.892
in this visualization by showing the same

00:38:15.892 --> 00:38:17.775
learning rate between
the different algorithms,

00:38:17.775 --> 00:38:20.284
when probably you should have separately

00:38:20.284 --> 00:38:23.617
turned the learning rates per algorithm.

00:38:28.455 --> 00:38:29.921
We saw in momentum, we had this idea

00:38:29.921 --> 00:38:32.203
of velocity, where we're
building up velocity

00:38:32.203 --> 00:38:34.249
by adding in the gradients,
and then stepping

00:38:34.249 --> 00:38:35.888
in the direction of the velocity.

00:38:35.888 --> 00:38:38.200
We saw with AdaGrad and RMSProp that we

00:38:38.200 --> 00:38:40.279
had this other idea, of
building up an estimate

00:38:40.279 --> 00:38:42.296
of the squared gradients,
and then dividing

00:38:42.296 --> 00:38:44.229
by the squared gradients.

00:38:44.229 --> 00:38:46.252
Then these both seem like good ideas

00:38:46.252 --> 00:38:47.143
on their own.

00:38:47.143 --> 00:38:48.439
Why don't we just stick 'em together

00:38:48.439 --> 00:38:49.465
and use them both?

00:38:49.465 --> 00:38:51.383
Maybe that would be even better.

00:38:51.383 --> 00:38:53.796
That brings us to this
algorithm called Adam,

00:38:53.796 --> 00:38:57.226
or rather brings us very close to Adam.

00:38:57.226 --> 00:38:59.812
We'll see in a couple
slides that there's a slight

00:38:59.812 --> 00:39:01.604
correction we need to make here.

00:39:01.604 --> 00:39:03.914
Here with Adam, we maintain an estimate

00:39:03.914 --> 00:39:07.373
of the first moment and the second moment.

00:39:07.373 --> 00:39:10.706
Now in the red, we make this estimate

00:39:10.706 --> 00:39:13.654
of the first moment as a weighed sum

00:39:13.654 --> 00:39:15.152
of our gradients.

00:39:15.152 --> 00:39:18.385
We have this moving estimate
of the second moment,

00:39:18.385 --> 00:39:20.904
like AdaGrad and like RMSProp, which is a

00:39:20.904 --> 00:39:23.226
moving estimate of our squared gradients.

00:39:23.226 --> 00:39:27.032
Now when we make our update step, we step

00:39:27.032 --> 00:39:29.106
using both the first
moment, which is kind of our

00:39:29.106 --> 00:39:31.680
velocity, and also divide
by the second moment,

00:39:31.680 --> 00:39:34.766
or rather the square root
of the second moment,

00:39:34.766 --> 00:39:37.766
which is this squared gradient term.

00:39:38.613 --> 00:39:40.302
This idea of Adam ends
up looking a little bit

00:39:40.302 --> 00:39:43.064
like RMSProp plus momentum, or ends up

00:39:43.064 --> 00:39:46.754
looking like momentum plus
second squared gradients.

00:39:46.754 --> 00:39:50.304
It kind of incorporates the
nice properties of both.

00:39:50.304 --> 00:39:52.474
But there's a little
bit of a problem here.

00:39:52.474 --> 00:39:55.227
That's the question of what happens

00:39:55.227 --> 00:39:57.560
at the very first time step?

00:40:00.683 --> 00:40:02.492
At the very first time step, you can see

00:40:02.492 --> 00:40:04.536
that at the beginning, we've initialized

00:40:04.536 --> 00:40:06.619
our second moment with zero.

00:40:06.619 --> 00:40:10.119
Now after one update of the second moment,

00:40:11.551 --> 00:40:14.007
typically this beta two, second moment

00:40:14.007 --> 00:40:17.038
decay rate, is something like .9 or .99,

00:40:17.038 --> 00:40:18.720
something very close to one.

00:40:18.720 --> 00:40:21.556
After one update, our
second moment is still

00:40:21.556 --> 00:40:23.352
very, very close to zero.

00:40:23.352 --> 00:40:25.751
Now when we're making our update step here

00:40:25.751 --> 00:40:27.906
and we divide by our second moment,

00:40:27.906 --> 00:40:30.224
now we're dividing by a very small number.

00:40:30.224 --> 00:40:31.681
We're making a very, very large step

00:40:31.681 --> 00:40:32.862
at the beginning.

00:40:32.862 --> 00:40:35.189
This very, very large
step at the beginning

00:40:35.189 --> 00:40:38.253
is not really due to the
geometry of the problem.

00:40:38.253 --> 00:40:40.189
It's kind of an artifact
of the fact that we

00:40:40.189 --> 00:40:43.907
initialized our second
moment estimate was zero.

00:40:43.907 --> 00:40:44.807
Question?

00:40:44.807 --> 00:40:48.974
- [Student] [speaks too low to hear]

00:40:53.317 --> 00:40:55.165
- That's true.

00:40:55.165 --> 00:40:56.573
The comment is that if your first moment

00:40:56.573 --> 00:40:58.973
is also very small,
then you're multiplying

00:40:58.973 --> 00:41:00.850
by small and you're
dividing by square root

00:41:00.850 --> 00:41:03.391
of small squared, so
what's going to happen?

00:41:03.391 --> 00:41:06.231
They might cancel each other
out, you might be okay.

00:41:06.231 --> 00:41:07.738
That's true.

00:41:07.738 --> 00:41:09.366
Sometimes these cancel each other out

00:41:09.366 --> 00:41:11.616
and you're okay, but
sometimes this ends up

00:41:11.616 --> 00:41:14.117
in taking very large steps
right at the beginning.

00:41:14.117 --> 00:41:16.730
That can be quite bad.

00:41:16.730 --> 00:41:18.762
Maybe you initialize a little bit poorly.

00:41:18.762 --> 00:41:20.018
You take a very large step.

00:41:20.018 --> 00:41:21.917
Now your initialization
is completely messed up,

00:41:21.917 --> 00:41:23.171
and then you're in a very bad part

00:41:23.171 --> 00:41:24.500
of the objective landscape
and you just can't

00:41:24.500 --> 00:41:26.630
converge from there.

00:41:26.630 --> 00:41:27.650
Question?

00:41:27.650 --> 00:41:31.400
- [Student] [speaks too low to hear]

00:41:31.400 --> 00:41:32.779
- The idea is what is this 10

00:41:32.779 --> 00:41:35.616
to the minus seven term
in the last equation?

00:41:35.616 --> 00:41:37.016
That's actually appeared in AdaGrad,

00:41:37.016 --> 00:41:38.332
RMSProp and Adam.

00:41:38.332 --> 00:41:40.728
The idea is that we're
dividing by something.

00:41:40.728 --> 00:41:42.672
We want to make sure we're
not dividing by zero,

00:41:42.672 --> 00:41:44.510
so we always add a small positive constant

00:41:44.510 --> 00:41:46.058
to the denominator, just to make sure

00:41:46.058 --> 00:41:49.094
we're not dividing by zero.

00:41:49.094 --> 00:41:50.094
That's technically a hyperparameter,

00:41:50.094 --> 00:41:51.807
but it tends not to matter too much,

00:41:51.807 --> 00:41:53.262
so just setting 10 to minus seven,

00:41:53.262 --> 00:41:54.914
10 to minus eight, something like that,

00:41:54.914 --> 00:41:56.497
tends to work well.

00:41:58.452 --> 00:42:00.713
With Adam, remember we just talked about

00:42:00.713 --> 00:42:02.585
this idea of at the first couple steps,

00:42:02.585 --> 00:42:03.791
it gets very large, and we might take

00:42:03.791 --> 00:42:05.996
very large steps and mess ourselves up.

00:42:05.996 --> 00:42:08.378
Adam also adds this bias correction term

00:42:08.378 --> 00:42:11.074
to avoid this problem of
taking very large steps

00:42:11.074 --> 00:42:12.995
at the beginning.

00:42:12.995 --> 00:42:15.129
You can see that after we update our first

00:42:15.129 --> 00:42:17.775
and second moments, we
create an unbiased estimate

00:42:17.775 --> 00:42:21.196
of those first and second
moments by incorporating

00:42:21.196 --> 00:42:23.104
the current time step, t.

00:42:23.104 --> 00:42:24.662
Now we actually make our step using these

00:42:24.662 --> 00:42:27.749
unbiased estimates,
rather than the original

00:42:27.749 --> 00:42:30.035
first and second moment estimates.

00:42:30.035 --> 00:42:33.652
This gives us our full form of Adam.

00:42:33.652 --> 00:42:37.106
By the way, Adam is a
really, [laughs] really good

00:42:37.106 --> 00:42:39.177
optimization algorithm,
and it works really well

00:42:39.177 --> 00:42:41.342
for a lot of different
problems, so that's kind of

00:42:41.342 --> 00:42:43.708
my default optimization
algorithm for just about

00:42:43.708 --> 00:42:46.035
any new problem that I'm tackling.

00:42:46.035 --> 00:42:48.445
In particular, if you
set beta one equals .9,

00:42:48.445 --> 00:42:51.599
beta two equals .999, learning rate one e

00:42:51.599 --> 00:42:53.573
minus three or five e minus four,

00:42:53.573 --> 00:42:55.426
that's a great staring
point for just about

00:42:55.426 --> 00:42:59.282
all the architectures
I've ever worked with.

00:42:59.282 --> 00:43:00.473
Try that.

00:43:00.473 --> 00:43:04.003
That's a really good place
to start, in general.

00:43:04.003 --> 00:43:06.434
[laughs]

00:43:06.434 --> 00:43:07.798
If we actually plot these things out

00:43:07.798 --> 00:43:09.780
and look at SGD, SGD momentum,

00:43:09.780 --> 00:43:12.119
RMSProp and Adam on the same problem,

00:43:12.119 --> 00:43:14.355
you can see that Adam, in the purple here,

00:43:14.355 --> 00:43:16.869
kind of combines elements
of both SGD momentum

00:43:16.869 --> 00:43:18.579
and RMSProp.

00:43:18.579 --> 00:43:20.467
Adam kind of overshoots the minimum

00:43:20.467 --> 00:43:23.430
a little bit like SGD
momentum, but it doesn't

00:43:23.430 --> 00:43:25.660
overshoot quite as much as momentum.

00:43:25.660 --> 00:43:27.610
Adam also has this similar behavior

00:43:27.610 --> 00:43:30.007
of RMSProp of kind of trying to curve

00:43:30.007 --> 00:43:33.753
to make equal progress
along all dimensions.

00:43:33.753 --> 00:43:35.519
Maybe in this small
two-dimensional example,

00:43:35.519 --> 00:43:38.191
Adam converged about
similarly to other ones,

00:43:38.191 --> 00:43:39.483
but you can see qualitatively that

00:43:39.483 --> 00:43:41.567
it's kind of combining
the behaviors of both

00:43:41.567 --> 00:43:43.317
momentum and RMSProp.

00:43:45.527 --> 00:43:49.194
Any questions about
optimization algorithms?

00:43:50.533 --> 00:43:52.830
- [Student] [speaks too low to hear]

00:43:52.830 --> 00:43:54.657
They still take a very long time to train.

00:43:54.657 --> 00:43:57.091
[speaks too low to hear]

00:43:57.091 --> 00:43:59.289
- The question is what does Adam not fix?

00:43:59.289 --> 00:44:00.595
Would these neural
networks are still large,

00:44:00.595 --> 00:44:03.678
they still take a long time to train.

00:44:05.229 --> 00:44:07.583
There can still be a problem.

00:44:07.583 --> 00:44:09.464
In this picture where
we have this landscape

00:44:09.464 --> 00:44:12.464
of things looking like
ovals, if you imagine

00:44:12.464 --> 00:44:15.659
that we're kind of making estimates along

00:44:15.659 --> 00:44:18.004
each dimension independently to allow us

00:44:18.004 --> 00:44:19.704
to speed up or slow down along different

00:44:19.704 --> 00:44:22.389
coordinate axes, but one problem is that

00:44:22.389 --> 00:44:24.641
if that taco shell is kind of tilted

00:44:24.641 --> 00:44:27.061
and is not axis aligned, then we're still

00:44:27.061 --> 00:44:29.205
only making estimates
along the individual axes

00:44:29.205 --> 00:44:30.372
independently.

00:44:31.420 --> 00:44:33.602
That corresponds to taking your rotated

00:44:33.602 --> 00:44:35.605
taco shell and squishing it horizontally

00:44:35.605 --> 00:44:38.616
and vertically, but you
can't actually unrotate it.

00:44:38.616 --> 00:44:41.347
In cases where you have this kind

00:44:41.347 --> 00:44:44.091
of rotated picture of poor conditioning,

00:44:44.091 --> 00:44:45.884
then Adam or any of these other algorithms

00:44:45.884 --> 00:44:49.217
really can't address that, that concern.

00:44:51.841 --> 00:44:54.046
Another thing that we've seen in all

00:44:54.046 --> 00:44:56.229
these optimization
algorithms is learning rate

00:44:56.229 --> 00:44:58.191
as a hyperparameter.

00:44:58.191 --> 00:45:00.111
We've seen this picture
before a couple times,

00:45:00.111 --> 00:45:02.313
that as you use different learning rates,

00:45:02.313 --> 00:45:04.298
sometimes if it's too
high, it might explode

00:45:04.298 --> 00:45:05.582
in the yellow.

00:45:05.582 --> 00:45:08.077
If it's a very low
learning rate, in the blue,

00:45:08.077 --> 00:45:10.114
it might take a very
long time to converge.

00:45:10.114 --> 00:45:11.251
It's kind of tricky to pick the right

00:45:11.251 --> 00:45:12.418
learning rate.

00:45:14.197 --> 00:45:15.571
This is a little bit of a trick question

00:45:15.571 --> 00:45:17.066
because we don't actually have to stick

00:45:17.066 --> 00:45:18.540
with one learning rate
throughout the course

00:45:18.540 --> 00:45:19.793
of training.

00:45:19.793 --> 00:45:21.832
Sometimes you'll see people
decay the learning rates

00:45:21.832 --> 00:45:24.546
over time, where we can kind of combine

00:45:24.546 --> 00:45:27.323
the effects of these different curves

00:45:27.323 --> 00:45:30.190
on the left, and get the
nice properties of each.

00:45:30.190 --> 00:45:31.907
Sometimes you'll start
with a higher learning rate

00:45:31.907 --> 00:45:34.395
near the start of training, and then decay

00:45:34.395 --> 00:45:35.588
the learning rate and make it smaller

00:45:35.588 --> 00:45:39.851
and smaller throughout
the course of training.

00:45:39.851 --> 00:45:42.691
A couple strategies for
these would be a step decay,

00:45:42.691 --> 00:45:45.483
where at 100,000th
iteration, you just decay

00:45:45.483 --> 00:45:47.280
by some factor and you keep going.

00:45:47.280 --> 00:45:48.594
You might see an exponential decay,

00:45:48.594 --> 00:45:53.064
where you continually
decay during training.

00:45:53.064 --> 00:45:54.607
You might see different variations

00:45:54.607 --> 00:45:56.345
of continually decaying the learning rate

00:45:56.345 --> 00:45:58.083
during training.

00:45:58.083 --> 00:46:00.855
If you look at papers,
especially the resonate paper,

00:46:00.855 --> 00:46:03.232
you often see plots that
look kind of like this,

00:46:03.232 --> 00:46:04.832
where the loss is kind of going down,

00:46:04.832 --> 00:46:07.265
then dropping, then flattening again,

00:46:07.265 --> 00:46:08.383
then dropping again.

00:46:08.383 --> 00:46:09.791
What's going on in these plots is that

00:46:09.791 --> 00:46:11.797
they're using a step decay learning rate,

00:46:11.797 --> 00:46:14.039
where at these parts where it plateaus

00:46:14.039 --> 00:46:15.599
and then suddenly drops
again, those are the

00:46:15.599 --> 00:46:17.371
iterations where they
dropped the learning rate

00:46:17.371 --> 00:46:18.886
by some factor.

00:46:18.886 --> 00:46:22.828
This idea of dropping the learning rate,

00:46:22.828 --> 00:46:24.090
you might imagine that it got near

00:46:24.090 --> 00:46:26.728
some good region, but now
the gradients got smaller,

00:46:26.728 --> 00:46:28.551
it's kind of bouncing around too much.

00:46:28.551 --> 00:46:29.728
Then if we drop the learning rate,

00:46:29.728 --> 00:46:31.066
it lets it slow down and continue

00:46:31.066 --> 00:46:33.230
to make progress down the landscape.

00:46:33.230 --> 00:46:36.960
This tends to help in practice sometimes.

00:46:36.960 --> 00:46:38.909
Although one thing to point out is that

00:46:38.909 --> 00:46:40.912
learning rate decay is
a little bit more common

00:46:40.912 --> 00:46:44.084
with SGD momentum, and
a little bit less common

00:46:44.084 --> 00:46:45.458
with something like Adam.

00:46:45.458 --> 00:46:47.707
Another thing I'd like
to point out is that

00:46:47.707 --> 00:46:49.699
learning rate decay is kind of a

00:46:49.699 --> 00:46:50.943
second-order hyperparameter.

00:46:50.943 --> 00:46:52.247
You typically should not optimize

00:46:52.247 --> 00:46:53.809
over this thing from the start.

00:46:53.809 --> 00:46:55.271
Usually when you're
kind of getting networks

00:46:55.271 --> 00:46:58.589
to work at the beginning, you want to pick

00:46:58.589 --> 00:47:00.097
a good learning rate with
no learning rate decay

00:47:00.097 --> 00:47:01.362
from the start.

00:47:01.362 --> 00:47:02.714
Trying to cross-validate jointly over

00:47:02.714 --> 00:47:04.629
learning rate decay and
initial learning rate

00:47:04.629 --> 00:47:06.553
and other things, you'll
just get confused.

00:47:06.553 --> 00:47:08.259
What you do for setting
learning rate decay

00:47:08.259 --> 00:47:11.066
is try with no decay, see what happens.

00:47:11.066 --> 00:47:12.829
Then kind of eyeball
the loss curve and see

00:47:12.829 --> 00:47:15.912
where you think you might need decay.

00:47:17.345 --> 00:47:19.006
Another thing I wanted to mention briefly

00:47:19.006 --> 00:47:22.039
is this idea of all these algorithms

00:47:22.039 --> 00:47:22.998
that we've talked about

00:47:22.998 --> 00:47:25.433
are first-order optimization algorithms.

00:47:25.433 --> 00:47:27.992
In this picture, in this
one-dimensional picture,

00:47:27.992 --> 00:47:32.238
we have this kind of
curvy objective function

00:47:32.238 --> 00:47:33.549
at our current point in red.

00:47:33.549 --> 00:47:34.995
What we're basically doing is computing

00:47:34.995 --> 00:47:36.542
the gradient at that point.

00:47:36.542 --> 00:47:38.619
We're using the gradient
information to compute

00:47:38.619 --> 00:47:41.207
some linear approximation to our function,

00:47:41.207 --> 00:47:43.568
which is kind of a first-order
Taylor approximation

00:47:43.568 --> 00:47:44.693
to our function.

00:47:44.693 --> 00:47:47.416
Now we pretend that the
first-order approximation

00:47:47.416 --> 00:47:49.778
is our actual function, and we make a step

00:47:49.778 --> 00:47:52.299
to try to minimize the approximation.

00:47:52.299 --> 00:47:54.535
But this approximation doesn't hold

00:47:54.535 --> 00:47:56.377
for very large regions, so we can't step

00:47:56.377 --> 00:47:57.838
too far in that direction.

00:47:57.838 --> 00:47:59.521
But really, the idea
here is that we're only

00:47:59.521 --> 00:48:01.129
incorporating information about the first

00:48:01.129 --> 00:48:03.002
derivative of the function.

00:48:03.002 --> 00:48:04.994
You can actually go a little bit fancier.

00:48:04.994 --> 00:48:07.433
There's this idea of
second-order approximation,

00:48:07.433 --> 00:48:09.716
where we take into account
both first derivative

00:48:09.716 --> 00:48:11.733
and second derivative information.

00:48:11.733 --> 00:48:14.812
Now we make a second-order
Taylor approximation

00:48:14.812 --> 00:48:17.194
to our function and kind
of locally approximate

00:48:17.194 --> 00:48:18.934
our function with a quadratic.

00:48:18.934 --> 00:48:20.498
Now with a quadratic, you can step right

00:48:20.498 --> 00:48:22.766
to the minimum, and you're really happy.

00:48:22.766 --> 00:48:26.254
That's this idea of
second-order optimization.

00:48:26.254 --> 00:48:28.687
When you generalize this
to multiple dimensions,

00:48:28.687 --> 00:48:30.974
you get something called the Newton step,

00:48:30.974 --> 00:48:33.137
where you compute this Hessian matrix,

00:48:33.137 --> 00:48:35.551
which is a matrix of second derivatives,

00:48:35.551 --> 00:48:37.753
and you end up inverting
this Hessian matrix

00:48:37.753 --> 00:48:39.759
in order to step directly to the minimum

00:48:39.759 --> 00:48:44.174
of this quadratic
approximation to your function.

00:48:44.174 --> 00:48:45.869
Does anyone spot something
that's quite different

00:48:45.869 --> 00:48:47.872
about this update rule,
compared to the other ones

00:48:47.872 --> 00:48:49.395
that we've seen?

00:48:49.395 --> 00:48:51.592
- [Student] [speaks too low to hear]

00:48:51.592 --> 00:48:53.313
- This doesn't have a learning rate.

00:48:53.313 --> 00:48:54.813
That's kind of cool.

00:48:56.948 --> 00:48:58.463
We're making this quadratic approximation

00:48:58.463 --> 00:48:59.638
and we're stepping right to the minimum

00:48:59.638 --> 00:49:01.149
of the quadratic.

00:49:01.149 --> 00:49:03.755
At least in this vanilla
version of Newton's method,

00:49:03.755 --> 00:49:05.166
you don't actually need a learning rate.

00:49:05.166 --> 00:49:06.405
You just always step to the minimum

00:49:06.405 --> 00:49:08.334
at every time step.

00:49:08.334 --> 00:49:09.988
However, in practice, you might end up,

00:49:09.988 --> 00:49:11.492
have a learning rate
anyway because, again,

00:49:11.492 --> 00:49:13.750
that quadratic approximation
might not be perfect,

00:49:13.750 --> 00:49:15.405
so you might only want
to step in the direction

00:49:15.405 --> 00:49:17.101
towards the minimum, rather than actually

00:49:17.101 --> 00:49:18.670
stepping to the minimum, but at least

00:49:18.670 --> 00:49:19.790
in this vanilla version, it doesn't

00:49:19.790 --> 00:49:21.540
have a learning rate.

00:49:24.479 --> 00:49:25.685
But unfortunately, this is maybe

00:49:25.685 --> 00:49:27.851
a little bit impractical for deep learning

00:49:27.851 --> 00:49:30.061
because this Hessian matrix

00:49:30.061 --> 00:49:33.413
is N by N, where N is
the number of parameters

00:49:33.413 --> 00:49:35.004
in your network.

00:49:35.004 --> 00:49:37.829
If N is 100 million,
then 100 million squared

00:49:37.829 --> 00:49:38.983
is way too big.

00:49:38.983 --> 00:49:40.419
You definitely can't store that in memory,

00:49:40.419 --> 00:49:42.531
and you definitely can't invert it.

00:49:42.531 --> 00:49:44.796
In practice, people sometimes use these

00:49:44.796 --> 00:49:46.971
quasi-Newton methods
that, rather than working

00:49:46.971 --> 00:49:48.275
with the full Hessian and inverting

00:49:48.275 --> 00:49:50.983
the full Hessian, they
work with approximations.

00:49:50.983 --> 00:49:53.210
Low-rank approximations are common.

00:49:53.210 --> 00:49:57.577
You'll sometimes see
these for some problems.

00:49:57.577 --> 00:50:00.242
L-BFGS is one particular
second-order optimizer

00:50:00.242 --> 00:50:02.647
that has this approximate second,

00:50:02.647 --> 00:50:03.972
keeps this approximation of the Hessian

00:50:03.972 --> 00:50:06.458
that you'll sometimes
see, but in practice,

00:50:06.458 --> 00:50:08.418
it doesn't work too well for many

00:50:08.418 --> 00:50:11.690
deep learning problems
because these approximations,

00:50:11.690 --> 00:50:13.774
these second-order
approximations, don't really

00:50:13.774 --> 00:50:15.772
handle the stochastic case very much,

00:50:15.772 --> 00:50:16.895
very nicely.

00:50:16.895 --> 00:50:18.772
They also tend not to work so well with

00:50:18.772 --> 00:50:21.101
non-convex problems.

00:50:21.101 --> 00:50:23.627
I don't want to get into
that right now too much.

00:50:23.627 --> 00:50:24.963
In practice, what you should really do

00:50:24.963 --> 00:50:27.435
is probably Adam is a really good choice

00:50:27.435 --> 00:50:29.507
for many different neural network things,

00:50:29.507 --> 00:50:31.737
but if you're in a situation where you

00:50:31.737 --> 00:50:33.557
can afford to do full batch updates,

00:50:33.557 --> 00:50:34.956
and you know that your
problem doesn't have

00:50:34.956 --> 00:50:37.377
really any stochasticity, then L-BFGS

00:50:37.377 --> 00:50:39.459
is kind of a good choice.

00:50:39.459 --> 00:50:41.370
L-BFGS doesn't really
get used for training

00:50:41.370 --> 00:50:43.666
neural networks too much, but as we'll see

00:50:43.666 --> 00:50:45.208
in a couple of lectures, it does sometimes

00:50:45.208 --> 00:50:47.736
get used for things like style transfer,

00:50:47.736 --> 00:50:49.848
where you actually have less stochasticity

00:50:49.848 --> 00:50:52.091
and fewer parameters, but you still want

00:50:52.091 --> 00:50:54.841
to solve an optimization problem.

00:50:56.319 --> 00:50:58.060
All of these strategies we've talked about

00:50:58.060 --> 00:51:01.477
so far are about reducing training error.

00:51:02.829 --> 00:51:04.439
All these optimization
algorithms are really

00:51:04.439 --> 00:51:05.927
about driving down your training error

00:51:05.927 --> 00:51:07.937
and minimizing your objective function,

00:51:07.937 --> 00:51:09.123
but we don't really care about

00:51:09.123 --> 00:51:10.888
training error that much.

00:51:10.888 --> 00:51:12.521
Instead, we really care
about our performance

00:51:12.521 --> 00:51:13.688
on unseen data.

00:51:13.688 --> 00:51:15.456
We really care about reducing this gap

00:51:15.456 --> 00:51:17.302
between train and test error.

00:51:17.302 --> 00:51:19.716
The question is once we're already

00:51:19.716 --> 00:51:21.713
good at optimizing our objective function,

00:51:21.713 --> 00:51:23.533
what can we do to try to reduce this gap

00:51:23.533 --> 00:51:24.770
and make our model perform better

00:51:24.770 --> 00:51:26.020
on unseen data?

00:51:28.982 --> 00:51:30.733
One really quick and dirty, easy thing

00:51:30.733 --> 00:51:34.102
to try is this idea of model ensembles

00:51:34.102 --> 00:51:35.984
that sometimes works
across many different areas

00:51:35.984 --> 00:51:37.252
in machine learning.

00:51:37.252 --> 00:51:38.674
The idea is pretty simple.

00:51:38.674 --> 00:51:40.337
Rather than having just one model,

00:51:40.337 --> 00:51:42.638
we'll train 10 different
models independently

00:51:42.638 --> 00:51:45.073
from different initial random restarts.

00:51:45.073 --> 00:51:47.086
Now at test time, we'll run our data

00:51:47.086 --> 00:51:48.901
through all of the 10 models and average

00:51:48.901 --> 00:51:51.818
the predictions of those 10 models.

00:51:54.047 --> 00:51:55.677
Adding these multiple models together

00:51:55.677 --> 00:51:57.769
tends to reduce overfitting a little bit

00:51:57.769 --> 00:52:00.163
and tend to improve
performance a little bit,

00:52:00.163 --> 00:52:02.040
typically by a couple percent.

00:52:02.040 --> 00:52:04.204
This is generally not
a drastic improvement,

00:52:04.204 --> 00:52:05.787
but it is a consistent improvement.

00:52:05.787 --> 00:52:07.498
You'll see that in competitions, like

00:52:07.498 --> 00:52:09.783
ImageNet and other things like that,

00:52:09.783 --> 00:52:11.498
using model ensembles is very common

00:52:11.498 --> 00:52:13.748
to get maximal performance.

00:52:14.973 --> 00:52:17.065
You can actually get a little
bit creative with this.

00:52:17.065 --> 00:52:19.128
Sometimes rather than
training separate models

00:52:19.128 --> 00:52:20.967
independently, you can just keep multiple

00:52:20.967 --> 00:52:22.595
snapshots of your model during the course

00:52:22.595 --> 00:52:24.863
of training, and then use these

00:52:24.863 --> 00:52:26.413
as your ensembles.

00:52:26.413 --> 00:52:28.262
Then you still, at test
time, need to average

00:52:28.262 --> 00:52:30.289
the predictions of these
multiple snapshots,

00:52:30.289 --> 00:52:31.812
but you can collect the snapshots during

00:52:31.812 --> 00:52:33.729
the course of training.

00:52:34.618 --> 00:52:36.441
There's actually a very
nice paper being presented

00:52:36.441 --> 00:52:39.178
at ICLR this week that kind of has

00:52:39.178 --> 00:52:42.092
a fancy version of this idea, where we use

00:52:42.092 --> 00:52:43.695
a crazy learning rate schedule,

00:52:43.695 --> 00:52:45.800
where our learning rate goes very slow,

00:52:45.800 --> 00:52:48.481
then very fast, then very
slow, then very fast.

00:52:48.481 --> 00:52:49.756
The idea is that with this crazy

00:52:49.756 --> 00:52:51.817
learning rate schedule,
then over the course

00:52:51.817 --> 00:52:53.705
of training, the model
might be able to converge

00:52:53.705 --> 00:52:55.699
to different regions in
the objective landscape

00:52:55.699 --> 00:52:58.116
that all are reasonably good.

00:52:59.202 --> 00:53:00.373
If you do an ensemble over these

00:53:00.373 --> 00:53:02.256
different snapshots, then you can improve

00:53:02.256 --> 00:53:03.738
your performance quite nicely,

00:53:03.738 --> 00:53:06.017
even though you're only
training the model once.

00:53:06.017 --> 00:53:07.516
Questions?

00:53:07.516 --> 00:53:11.683
- [Student] [speaks too low to hear]

00:53:25.873 --> 00:53:28.273
- The question is, it's bad when

00:53:28.273 --> 00:53:29.558
there's a large gap between error 'cause

00:53:29.558 --> 00:53:30.737
that means you're overfitting, but if

00:53:30.737 --> 00:53:33.898
there's no gap, then
is that also maybe bad?

00:53:33.898 --> 00:53:36.131
Do we actually want
some small, optimal gap

00:53:36.131 --> 00:53:37.931
between the two?

00:53:37.931 --> 00:53:39.617
We don't really care about the gap.

00:53:39.617 --> 00:53:41.496
What we really care about is maximizing

00:53:41.496 --> 00:53:44.504
the performance on the validation set.

00:53:44.504 --> 00:53:46.369
What tends to happen is that if you

00:53:46.369 --> 00:53:48.871
don't see a gap, then
you could have improved

00:53:48.871 --> 00:53:52.288
your absolute performance, in many cases,

00:53:53.709 --> 00:53:55.480
by overfitting a little bit more.

00:53:55.480 --> 00:53:56.931
There's this weird correlation between

00:53:56.931 --> 00:53:58.698
the absolute performance
on the validation set

00:53:58.698 --> 00:54:00.280
and the size of that gap.

00:54:00.280 --> 00:54:03.205
We only care about absolute performance.

00:54:03.205 --> 00:54:04.220
Question in the back?

00:54:04.220 --> 00:54:05.769
- [Student] Are hyperparameters the same

00:54:05.769 --> 00:54:07.489
for the ensemble?

00:54:07.489 --> 00:54:08.739
- Are the hyperparameters the same

00:54:08.739 --> 00:54:10.013
for the ensembles?

00:54:10.013 --> 00:54:11.135
That's a good question.

00:54:11.135 --> 00:54:12.719
Sometimes they're not.

00:54:12.719 --> 00:54:15.891
You might want to try
different sizes of the model,

00:54:15.891 --> 00:54:16.998
different learning rates, different

00:54:16.998 --> 00:54:19.144
regularization strategies
and ensemble across

00:54:19.144 --> 00:54:20.099
these different things.

00:54:20.099 --> 00:54:23.099
That actually does happen sometimes.

00:54:23.981 --> 00:54:25.615
Another little trick you can do sometimes

00:54:25.615 --> 00:54:27.832
is that during training,
you might actually keep

00:54:27.832 --> 00:54:29.472
an exponentially decaying average

00:54:29.472 --> 00:54:32.254
of your parameter vector
itself to kind of have

00:54:32.254 --> 00:54:34.365
a smooth ensemble of your own network

00:54:34.365 --> 00:54:36.263
during training.

00:54:36.263 --> 00:54:38.083
Then use this smoothly decaying average

00:54:38.083 --> 00:54:39.902
of your parameter vector, rather than

00:54:39.902 --> 00:54:42.134
the actual checkpoints themselves.

00:54:42.134 --> 00:54:43.457
This is called Polyak averaging,

00:54:43.457 --> 00:54:45.747
and it sometimes helps a little bit.

00:54:45.747 --> 00:54:47.163
It's just another one
of these small tricks

00:54:47.163 --> 00:54:48.819
you can sometimes add, but it's not maybe

00:54:48.819 --> 00:54:51.323
too common in practice.

00:54:51.323 --> 00:54:53.295
Another question you might have is that

00:54:53.295 --> 00:54:54.846
how can we actually
improve the performance

00:54:54.846 --> 00:54:56.263
of single models?

00:54:57.714 --> 00:54:59.518
When we have ensembles,
we still need to run,

00:54:59.518 --> 00:55:01.224
like, 10 models at test time.

00:55:01.224 --> 00:55:02.988
That's not so great.

00:55:02.988 --> 00:55:04.638
We really want some strategies to improve

00:55:04.638 --> 00:55:06.704
the performance of our single models.

00:55:06.704 --> 00:55:08.722
That's really this idea of regularization,

00:55:08.722 --> 00:55:10.702
where we add something to our model

00:55:10.702 --> 00:55:12.439
to prevent it from
fitting the training data

00:55:12.439 --> 00:55:14.877
too well in the attempts
to make it perform better

00:55:14.877 --> 00:55:16.688
on unseen data.

00:55:16.688 --> 00:55:18.789
We've seen a couple
ideas, a couple methods

00:55:18.789 --> 00:55:20.730
for regularization already, where we add

00:55:20.730 --> 00:55:24.000
some explicit extra term to the loss.

00:55:24.000 --> 00:55:25.911
Where we have this one
term telling the model

00:55:25.911 --> 00:55:27.999
to fit the data, and another term

00:55:27.999 --> 00:55:30.223
that's a regularization term.

00:55:30.223 --> 00:55:32.017
You saw this in homework
one, where we used

00:55:32.017 --> 00:55:33.517
L2 regularization.

00:55:35.289 --> 00:55:37.533
As we talked about in lecture a couple

00:55:37.533 --> 00:55:39.853
lectures ago, this L2
regularization doesn't

00:55:39.853 --> 00:55:41.903
really make maybe a lot
of sense in the context

00:55:41.903 --> 00:55:43.486
of neural networks.

00:55:44.407 --> 00:55:48.467
Sometimes we use other
things for neural networks.

00:55:48.467 --> 00:55:50.321
One regularization strategy that's super,

00:55:50.321 --> 00:55:51.812
super common for neural networks

00:55:51.812 --> 00:55:53.861
is this idea of dropout.

00:55:53.861 --> 00:55:55.565
Dropout is super simple.

00:55:55.565 --> 00:55:57.074
Every time we do a forward pass through

00:55:57.074 --> 00:55:59.530
the network, at every
layer, we're going to

00:55:59.530 --> 00:56:02.749
randomly set some neurons to zero.

00:56:02.749 --> 00:56:04.027
Every time we do a forward pass,

00:56:04.027 --> 00:56:05.510
we'll set a different random subset

00:56:05.510 --> 00:56:07.135
of the neurons to zero.

00:56:07.135 --> 00:56:09.173
This kind of proceeds one layer at a time.

00:56:09.173 --> 00:56:11.145
We run through one layer, we compute

00:56:11.145 --> 00:56:12.873
the value of the layer, we randomly set

00:56:12.873 --> 00:56:14.433
some of them to zero,
and then we continue up

00:56:14.433 --> 00:56:15.678
through the network.

00:56:15.678 --> 00:56:17.893
Now if you look at this
fully connected network

00:56:17.893 --> 00:56:21.090
on the left versus a dropout version

00:56:21.090 --> 00:56:22.930
of the same network on
the right, you can see

00:56:22.930 --> 00:56:25.988
that after we do dropout, it kind of looks

00:56:25.988 --> 00:56:28.308
like a smaller version
of the same network,

00:56:28.308 --> 00:56:30.885
where we're only using
some subset of the neurons.

00:56:30.885 --> 00:56:34.774
This subset that we use
varies at each iteration,

00:56:34.774 --> 00:56:36.231
at each forward pass.

00:56:36.231 --> 00:56:37.217
Question?

00:56:37.217 --> 00:56:41.384
- [Student] [speaks too low to hear]

00:56:44.179 --> 00:56:45.717
- The question is what
are we setting to zero?

00:56:45.717 --> 00:56:46.860
It's the activations.

00:56:46.860 --> 00:56:49.082
Each layer is computing
previous activation

00:56:49.082 --> 00:56:50.305
times the weight matrix gives you

00:56:50.305 --> 00:56:52.216
our next activation.

00:56:52.216 --> 00:56:53.881
Then you just take that activation,

00:56:53.881 --> 00:56:55.759
set some of them to zero, and then

00:56:55.759 --> 00:56:59.024
your next layer will be
partially zeroed activations

00:56:59.024 --> 00:57:02.077
times another matrix give
you your next activations.

00:57:02.077 --> 00:57:03.640
Question?

00:57:03.640 --> 00:57:07.187
- [Student] [speaks too low to hear]

00:57:07.187 --> 00:57:09.236
- Question is which
layers do you do this on?

00:57:09.236 --> 00:57:11.708
It's more common in
fully connected layers,

00:57:11.708 --> 00:57:14.939
but you sometimes see this in
convolutional layers, as well.

00:57:14.939 --> 00:57:16.386
When you're working in
convolutional layers,

00:57:16.386 --> 00:57:18.521
sometimes instead of dropping

00:57:18.521 --> 00:57:20.575
each activation randomly,
instead you sometimes

00:57:20.575 --> 00:57:23.908
might drop entire feature maps randomly.

00:57:24.940 --> 00:57:26.731
In convolutions, you have
this channel dimension,

00:57:26.731 --> 00:57:28.269
and you might drop out entire channels,

00:57:28.269 --> 00:57:30.602
rather than random elements.

00:57:32.544 --> 00:57:34.405
Dropout is kind of super
simple in practice.

00:57:34.405 --> 00:57:37.022
It only requires adding two lines,

00:57:37.022 --> 00:57:38.965
one line per dropout call.

00:57:38.965 --> 00:57:40.884
Here we have a three-layer neural network,

00:57:40.884 --> 00:57:42.057
and we've added dropout.

00:57:42.057 --> 00:57:44.375
You can see that all we needed to do

00:57:44.375 --> 00:57:46.149
was add this extra line where we randomly

00:57:46.149 --> 00:57:47.641
set some things to zero.

00:57:47.641 --> 00:57:49.945
This is super easy to implement.

00:57:49.945 --> 00:57:52.623
But the question is why
is this even a good idea?

00:57:52.623 --> 00:57:54.481
We're seriously messing with the network

00:57:54.481 --> 00:57:56.537
at training time by setting a bunch

00:57:56.537 --> 00:57:58.552
of its values to zero.

00:57:58.552 --> 00:58:01.473
How can this possibly make sense?

00:58:01.473 --> 00:58:04.390
One sort of slightly hand wavy idea

00:58:05.480 --> 00:58:07.474
that people have is that
dropout helps prevent

00:58:07.474 --> 00:58:10.107
co-adaptation of features.

00:58:10.107 --> 00:58:11.377
Maybe if you imagine that we're trying

00:58:11.377 --> 00:58:14.281
to classify cats, maybe in some universe,

00:58:14.281 --> 00:58:16.338
the network might learn one neuron

00:58:16.338 --> 00:58:18.602
for having an ear, one
neuron for having a tail,

00:58:18.602 --> 00:58:21.551
one neuron for the input being furry.

00:58:21.551 --> 00:58:23.284
Then it kind of combines
these things together

00:58:23.284 --> 00:58:25.236
to decide whether or not it's a cat.

00:58:25.236 --> 00:58:27.391
But now if we have dropout, then in making

00:58:27.391 --> 00:58:30.427
the final decision about
catness, the network

00:58:30.427 --> 00:58:32.176
cannot depend too much on any of these

00:58:32.176 --> 00:58:33.316
one features.

00:58:33.316 --> 00:58:34.929
Instead, it kind of needs to distribute

00:58:34.929 --> 00:58:38.210
its idea of catness across
many different features.

00:58:38.210 --> 00:58:42.690
This might help prevent
overfitting somehow.

00:58:42.690 --> 00:58:44.687
Another interpretation of dropout

00:58:44.687 --> 00:58:46.682
that's come out a little bit more recently

00:58:46.682 --> 00:58:48.999
is that it's kind of like
doing model ensembling

00:58:48.999 --> 00:58:50.832
within a single model.

00:58:52.175 --> 00:58:53.689
If you look at the picture on the left,

00:58:53.689 --> 00:58:55.546
after you apply dropout to the network,

00:58:55.546 --> 00:58:57.245
we're kind of computing this subnetwork

00:58:57.245 --> 00:58:59.230
using some subset of the neurons.

00:58:59.230 --> 00:59:01.625
Now every different potential dropout mask

00:59:01.625 --> 00:59:03.876
leads to a different potential subnetwork.

00:59:03.876 --> 00:59:06.419
Now dropout is kind of
learning a whole ensemble

00:59:06.419 --> 00:59:08.115
of networks all at the same time that all

00:59:08.115 --> 00:59:09.630
share parameters.

00:59:09.630 --> 00:59:12.077
By the way, because of
the number of potential

00:59:12.077 --> 00:59:14.275
dropout masks grows
exponentially in the number

00:59:14.275 --> 00:59:15.970
of neurons, you're never going to sample

00:59:15.970 --> 00:59:17.637
all of these things.

00:59:18.574 --> 00:59:21.106
This is really a gigantic,
gigantic ensemble

00:59:21.106 --> 00:59:25.273
of networks that are all
being trained simultaneously.

00:59:26.107 --> 00:59:29.613
Then the question is what
happens at test time?

00:59:29.613 --> 00:59:31.733
Once we move to dropout,
we've kind of fundamentally

00:59:31.733 --> 00:59:34.643
changed the operation
of our neural network.

00:59:34.643 --> 00:59:37.642
Previously, we've had
our neural network, f,

00:59:37.642 --> 00:59:39.201
be a function of the weights, w,

00:59:39.201 --> 00:59:42.190
and the inputs, x, and then produce

00:59:42.190 --> 00:59:43.335
the output, y.

00:59:43.335 --> 00:59:45.141
But now, our network is also taking

00:59:45.141 --> 00:59:47.314
this additional input, z, which is some

00:59:47.314 --> 00:59:48.753
random dropout mask.

00:59:48.753 --> 00:59:50.490
That z is random.

00:59:50.490 --> 00:59:53.217
Having randomness at
test time is maybe bad.

00:59:53.217 --> 00:59:55.130
Imagine that you're working at Facebook,

00:59:55.130 --> 00:59:56.674
and you want to classify the images

00:59:56.674 --> 00:59:57.929
that people are uploading.

00:59:57.929 --> 01:00:00.397
Then today, your image
gets classified as a cat,

01:00:00.397 --> 01:00:01.530
and tomorrow it doesn't.

01:00:01.530 --> 01:00:03.577
That would be really weird and really bad.

01:00:03.577 --> 01:00:05.964
You'd probably want to eliminate this

01:00:05.964 --> 01:00:08.240
stochasticity at test
time once the network

01:00:08.240 --> 01:00:09.808
is already trained.

01:00:09.808 --> 01:00:11.318
Then we kind of want to average out

01:00:11.318 --> 01:00:12.578
this randomness.

01:00:12.578 --> 01:00:14.790
If you write this out, you can imagine

01:00:14.790 --> 01:00:16.757
actually marginalizing out this randomness

01:00:16.757 --> 01:00:18.616
with some integral, but in practice,

01:00:18.616 --> 01:00:20.738
this integral is totally intractable.

01:00:20.738 --> 01:00:23.086
We don't know how to evaluate this thing.

01:00:23.086 --> 01:00:24.853
You're in bad shape.

01:00:24.853 --> 01:00:26.224
One thing you might imagine doing

01:00:26.224 --> 01:00:28.558
is approximating this
integral via sampling,

01:00:28.558 --> 01:00:30.278
where you draw multiple samples of z

01:00:30.278 --> 01:00:31.969
and then average them out at test time,

01:00:31.969 --> 01:00:34.326
but this still would
introduce some randomness,

01:00:34.326 --> 01:00:36.525
which is little bit bad.

01:00:36.525 --> 01:00:38.029
Thankfully, in the case of dropout, we can

01:00:38.029 --> 01:00:39.520
actually approximate this integral

01:00:39.520 --> 01:00:41.908
in kind of a cheap way locally.

01:00:41.908 --> 01:00:44.427
If we consider a single
neuron, the output is a,

01:00:44.427 --> 01:00:46.147
the inputs are x and y, with two weights,

01:00:46.147 --> 01:00:47.713
w one, w two.

01:00:47.713 --> 01:00:51.107
Then at test time, our value a is just

01:00:51.107 --> 01:00:53.107
w one x plus w two y.

01:00:54.075 --> 01:00:56.510
Now imagine that we
trained to this network.

01:00:56.510 --> 01:00:58.997
During training, we used
dropout with probability

01:00:58.997 --> 01:01:01.130
1/2 of dropping our neurons.

01:01:01.130 --> 01:01:03.853
Now the expected value
of a during training,

01:01:03.853 --> 01:01:05.135
we can kind of compute analytically

01:01:05.135 --> 01:01:06.802
for this small case.

01:01:08.197 --> 01:01:09.824
There's four possible dropout masks,

01:01:09.824 --> 01:01:11.007
and we're going to average out the values

01:01:11.007 --> 01:01:12.734
across these four masks.

01:01:12.734 --> 01:01:14.522
We can see that the expected value of a

01:01:14.522 --> 01:01:18.689
during training is 1/2
w one x plus w two y.

01:01:19.560 --> 01:01:22.211
There's this disconnect between

01:01:22.211 --> 01:01:24.842
this average value of w one x plus w two y

01:01:24.842 --> 01:01:26.567
at test time, and at training time,

01:01:26.567 --> 01:01:29.485
the average value is only 1/2 as much.

01:01:29.485 --> 01:01:31.593
One cheap thing we can do is that

01:01:31.593 --> 01:01:35.368
at test time, we don't
have any stochasticity.

01:01:35.368 --> 01:01:37.195
Instead, we just multiply this output

01:01:37.195 --> 01:01:38.772
by the dropout probability.

01:01:38.772 --> 01:01:41.221
Now these expected values are the same.

01:01:41.221 --> 01:01:43.745
This is kind of like a
local cheap approximation

01:01:43.745 --> 01:01:45.218
to this complex integral.

01:01:45.218 --> 01:01:46.978
This is what people really commonly do

01:01:46.978 --> 01:01:49.061
in practice with dropout.

01:01:50.200 --> 01:01:51.922
At dropout, we have this predict function,

01:01:51.922 --> 01:01:53.722
and we just multiply
our outputs of the layer

01:01:53.722 --> 01:01:56.754
by the dropout probability.

01:01:56.754 --> 01:01:58.853
The summary of dropout is
that it's really simple

01:01:58.853 --> 01:01:59.878
on the forward pass.

01:01:59.878 --> 01:02:02.178
You're just adding two
lines to your implementation

01:02:02.178 --> 01:02:04.292
to randomly zero out some nodes.

01:02:04.292 --> 01:02:06.694
Then at the test time prediction function,

01:02:06.694 --> 01:02:09.375
you just added one little multiplication

01:02:09.375 --> 01:02:10.694
by your probability.

01:02:10.694 --> 01:02:11.814
Dropout is super simple.

01:02:11.814 --> 01:02:14.881
It tends to work well sometimes

01:02:14.881 --> 01:02:17.098
for regularizing neural networks.

01:02:17.098 --> 01:02:19.189
By the way, one common
trick you see sometimes

01:02:19.189 --> 01:02:21.939
is this idea of inverted dropout.

01:02:23.150 --> 01:02:25.489
Maybe at test time, you
care more about efficiency,

01:02:25.489 --> 01:02:27.500
so you want to eliminate
that extra multiplication

01:02:27.500 --> 01:02:29.220
by p at test time.

01:02:29.220 --> 01:02:31.248
Then what you can do is, at test time,

01:02:31.248 --> 01:02:33.410
you use the entire weight matrix, but now

01:02:33.410 --> 01:02:35.838
at training time, instead you divide by p

01:02:35.838 --> 01:02:38.162
because training is
probably happening on a GPU.

01:02:38.162 --> 01:02:39.198
You don't really care if you do one

01:02:39.198 --> 01:02:41.259
extra multiply at training time, but then

01:02:41.259 --> 01:02:42.527
at test time, you kind of want this thing

01:02:42.527 --> 01:02:45.218
to be as efficient as possible.

01:02:45.218 --> 01:02:46.051
Question?

01:02:46.901 --> 01:02:51.068
- [Student] [speaks too low to hear]

01:02:53.095 --> 01:02:57.262
Now the gradient [speaks too low to hear].

01:02:58.163 --> 01:02:59.710
- The question is what
happens to the gradient

01:02:59.710 --> 01:03:02.697
during training with dropout?

01:03:02.697 --> 01:03:03.530
You're right.

01:03:03.530 --> 01:03:04.532
We only end up propagating the gradients

01:03:04.532 --> 01:03:07.068
through the nodes that were not dropped.

01:03:07.068 --> 01:03:09.494
This has the consequence that

01:03:09.494 --> 01:03:10.851
when you're training with dropout,

01:03:10.851 --> 01:03:12.711
typically training takes longer because

01:03:12.711 --> 01:03:14.034
at each step, you're only updating

01:03:14.034 --> 01:03:15.841
some subparts of the network.

01:03:15.841 --> 01:03:17.047
When you're using dropout, it typically

01:03:17.047 --> 01:03:19.022
takes longer to train, but you might have

01:03:19.022 --> 01:03:22.772
a better generalization
after it's converged.

01:03:24.894 --> 01:03:27.578
Dropout, we kind of saw is like this one

01:03:27.578 --> 01:03:28.859
concrete instantiation.

01:03:28.859 --> 01:03:30.394
There's a little bit more general strategy

01:03:30.394 --> 01:03:33.295
for regularization where during training

01:03:33.295 --> 01:03:35.570
we add some kind of
randomness to the network

01:03:35.570 --> 01:03:37.967
to prevent it from fitting
the training data too well.

01:03:37.967 --> 01:03:39.619
To kind of mess it up and prevent it

01:03:39.619 --> 01:03:41.522
from fitting the training data perfectly.

01:03:41.522 --> 01:03:43.010
Now at test time, we want to average out

01:03:43.010 --> 01:03:45.076
all that randomness to hopefully improve

01:03:45.076 --> 01:03:46.645
our generalization.

01:03:46.645 --> 01:03:48.506
Dropout is probably
the most common example

01:03:48.506 --> 01:03:50.734
of this type of strategy, but actually

01:03:50.734 --> 01:03:54.412
batch normalization kind
of fits this idea, as well.

01:03:54.412 --> 01:03:57.218
Remember in batch
normalization, during training,

01:03:57.218 --> 01:03:59.481
one data point might appear
in different mini batches

01:03:59.481 --> 01:04:01.240
with different other data points.

01:04:01.240 --> 01:04:02.798
There's a bit of
stochasticity with respect

01:04:02.798 --> 01:04:04.912
to a single data point with how exactly

01:04:04.912 --> 01:04:07.685
that point gets normalized
during training.

01:04:07.685 --> 01:04:09.792
But now at test time,
we kind of average out

01:04:09.792 --> 01:04:11.531
this stochasticity by using some

01:04:11.531 --> 01:04:13.272
global estimates to normalize, rather than

01:04:13.272 --> 01:04:15.220
the per mini batch estimates.

01:04:15.220 --> 01:04:17.021
Actually batch normalization tends to have

01:04:17.021 --> 01:04:18.910
kind of a similar regularizing effect

01:04:18.910 --> 01:04:20.708
as dropout because they both introduce

01:04:20.708 --> 01:04:22.707
some kind of stochasticity or noise

01:04:22.707 --> 01:04:24.736
at training time, but then average it out

01:04:24.736 --> 01:04:25.963
at test time.

01:04:25.963 --> 01:04:28.473
Actually, when you train networks with

01:04:28.473 --> 01:04:30.485
batch normalization,
sometimes you don't use

01:04:30.485 --> 01:04:32.467
dropout at all, and just
the batch normalization

01:04:32.467 --> 01:04:34.244
adds enough of a regularizing effect

01:04:34.244 --> 01:04:36.229
to your network.

01:04:36.229 --> 01:04:37.697
Dropout is somewhat nice because you can

01:04:37.697 --> 01:04:39.341
actually tune the regularization strength

01:04:39.341 --> 01:04:41.481
by varying that parameter
p, and there's no such

01:04:41.481 --> 01:04:44.318
control in batch normalization.

01:04:44.318 --> 01:04:46.357
Another kind of strategy that fits in

01:04:46.357 --> 01:04:49.413
this paradigm is this
idea of data augmentation.

01:04:49.413 --> 01:04:51.494
During training, in a vanilla version

01:04:51.494 --> 01:04:53.816
for training, we have our
data, we have our label.

01:04:53.816 --> 01:04:57.563
We use it to update our
CNN at each time step.

01:04:57.563 --> 01:04:59.166
But instead, what we can do is randomly

01:04:59.166 --> 01:05:02.233
transform the image in
some way during training

01:05:02.233 --> 01:05:04.040
such that the label is preserved.

01:05:04.040 --> 01:05:06.572
Now we train on these
random transformations

01:05:06.572 --> 01:05:09.903
of the image rather than
the original images.

01:05:09.903 --> 01:05:12.464
Sometimes you might see
random horizontal flips

01:05:12.464 --> 01:05:14.055
'cause if you take a cat and flip it

01:05:14.055 --> 01:05:16.638
horizontally, it's still a cat.

01:05:18.175 --> 01:05:20.309
You'll randomly sample
crops of different sizes

01:05:20.309 --> 01:05:22.081
from the image because the random crop

01:05:22.081 --> 01:05:24.248
of the cat is still a cat.

01:05:25.673 --> 01:05:27.677
Then during testing,
you kind of average out

01:05:27.677 --> 01:05:30.802
this stochasticity by evaluating with some

01:05:30.802 --> 01:05:32.993
fixed set of crops, often the four corners

01:05:32.993 --> 01:05:34.794
and the middle and their flips.

01:05:34.794 --> 01:05:36.443
What's very common is that when you read,

01:05:36.443 --> 01:05:38.526
for example, papers on
ImageNet, they'll report

01:05:38.526 --> 01:05:40.452
a single crop performance of their model,

01:05:40.452 --> 01:05:41.919
which is just like the whole image,

01:05:41.919 --> 01:05:43.560
and a 10 crop performance of their model,

01:05:43.560 --> 01:05:46.376
which are these five standard crops

01:05:46.376 --> 01:05:47.793
plus their flips.

01:05:48.723 --> 01:05:50.891
Also with data augmentation,
you'll sometimes

01:05:50.891 --> 01:05:53.104
use color jittering,
where you might randomly

01:05:53.104 --> 01:05:55.361
vary the contrast or
brightness of your image

01:05:55.361 --> 01:05:56.830
during training.

01:05:56.830 --> 01:05:58.168
You can get a little bit more complex

01:05:58.168 --> 01:05:59.998
with color jittering,
as well, where you try

01:05:59.998 --> 01:06:01.910
to make color jitters that are maybe in

01:06:01.910 --> 01:06:05.127
the PCA directions of your
data space or whatever,

01:06:05.127 --> 01:06:07.168
where you do some color jittering

01:06:07.168 --> 01:06:10.024
in some data-dependent way, but that's a

01:06:10.024 --> 01:06:11.941
little bit less common.

01:06:12.977 --> 01:06:15.135
In general, data
augmentation is this really

01:06:15.135 --> 01:06:16.402
general thing that you can apply

01:06:16.402 --> 01:06:18.522
to just about any problem.

01:06:18.522 --> 01:06:20.206
Whatever problem you're trying to solve,

01:06:20.206 --> 01:06:22.054
you kind of think about what are the ways

01:06:22.054 --> 01:06:24.146
that I can transform my data without

01:06:24.146 --> 01:06:25.425
changing the label?

01:06:25.425 --> 01:06:26.721
Now during training, you just apply

01:06:26.721 --> 01:06:29.630
these random transformations
to your input data.

01:06:29.630 --> 01:06:31.703
This sort of has a regularizing effect

01:06:31.703 --> 01:06:33.683
on the network because
you're, again, adding

01:06:33.683 --> 01:06:35.856
some kind of stochasticity
during training,

01:06:35.856 --> 01:06:39.439
and then marginalizing
it out at test time.

01:06:40.540 --> 01:06:43.131
Now we've seen three
examples of this pattern,

01:06:43.131 --> 01:06:45.717
dropout, batch normalization,
data augmentation,

01:06:45.717 --> 01:06:47.639
but there's many other examples, as well.

01:06:47.639 --> 01:06:49.456
Once you have this pattern in your mind,

01:06:49.456 --> 01:06:51.067
you'll kind of recognize this thing

01:06:51.067 --> 01:06:53.534
as you read other papers sometimes.

01:06:53.534 --> 01:06:55.520
There's another kind of
related idea to dropout

01:06:55.520 --> 01:06:57.207
called DropConnect.

01:06:57.207 --> 01:06:59.339
With DropConnect, it's the same idea,

01:06:59.339 --> 01:07:01.978
but rather than zeroing
out the activations

01:07:01.978 --> 01:07:03.952
at every forward pass, instead we randomly

01:07:03.952 --> 01:07:06.750
zero out some of the values
of the weight matrix instead.

01:07:06.750 --> 01:07:10.137
Again, it kind of has this similar flavor.

01:07:10.137 --> 01:07:13.367
Another kind of cool idea that I like,

01:07:13.367 --> 01:07:14.913
this one's not so commonly used, but I

01:07:14.913 --> 01:07:16.766
just think it's a really cool idea,

01:07:16.766 --> 01:07:19.885
is this idea of fractional max pooling.

01:07:19.885 --> 01:07:22.057
Normally when you do
two-by-two max pooling,

01:07:22.057 --> 01:07:24.294
you have these fixed two-by-two regions

01:07:24.294 --> 01:07:26.642
over which you pool over
in the forward pass,

01:07:26.642 --> 01:07:29.552
but now with fractional max pooling,

01:07:29.552 --> 01:07:32.185
every time we have our pooling layer,

01:07:32.185 --> 01:07:33.872
we're going to randomize exactly the pool

01:07:33.872 --> 01:07:36.336
that the regions over which we pool.

01:07:36.336 --> 01:07:38.197
Here in the example on the right,

01:07:38.197 --> 01:07:39.971
I've shown three different sets

01:07:39.971 --> 01:07:41.954
of random pooling regions
that you might see

01:07:41.954 --> 01:07:43.555
during training.

01:07:43.555 --> 01:07:46.720
Now during test time, you kind of average

01:07:46.720 --> 01:07:49.342
the stochasticity out by
trying many different,

01:07:49.342 --> 01:07:52.882
by either sticking to some
fixed set of pooling regions.

01:07:52.882 --> 01:07:55.189
or drawing many samples
and averaging over them.

01:07:55.189 --> 01:07:56.519
That's kind of a cool idea, even though

01:07:56.519 --> 01:07:59.512
it's not so commonly used.

01:07:59.512 --> 01:08:01.793
Another really kind of surprising paper

01:08:01.793 --> 01:08:04.878
in this paradigm that actually came out

01:08:04.878 --> 01:08:06.375
in the last year, so this is new since

01:08:06.375 --> 01:08:08.612
the last time we taught
the class, is this idea

01:08:08.612 --> 01:08:10.396
of stochastic depth.

01:08:10.396 --> 01:08:13.087
Here we have a network on the left.

01:08:13.087 --> 01:08:15.975
The idea is that we have
a very deep network.

01:08:15.975 --> 01:08:17.760
We're going to randomly
drop layers from the network

01:08:17.760 --> 01:08:19.015
during training.

01:08:19.015 --> 01:08:21.459
During training, we're going to eliminate

01:08:21.459 --> 01:08:23.047
some layers and only use some subset

01:08:23.047 --> 01:08:24.599
of the layers during training.

01:08:24.599 --> 01:08:27.339
Now during test time, we'll
use the whole network.

01:08:27.339 --> 01:08:28.747
This is kind of crazy.

01:08:28.747 --> 01:08:30.736
It's kind of amazing that this works,

01:08:30.736 --> 01:08:32.337
but this tends to have kind of a similar

01:08:32.337 --> 01:08:33.955
regularizing effect as dropout

01:08:33.955 --> 01:08:35.795
and these other strategies.

01:08:35.795 --> 01:08:38.471
But again, this is super,
super cutting-edge research.

01:08:38.471 --> 01:08:40.694
This is not super
commonly used in practice,

01:08:40.694 --> 01:08:42.527
but it is a cool idea.

01:08:45.179 --> 01:08:49.096
Any last minute questions
about regularization?

01:08:50.429 --> 01:08:52.201
No? Use it. It's a good idea.

01:08:52.201 --> 01:08:53.158
Yeah?

01:08:53.158 --> 01:08:57.531
- [Student] [speaks too low to hear]

01:08:57.531 --> 01:08:58.669
- The question is do you usually use

01:08:58.669 --> 01:09:01.669
more than one regularization method?

01:09:04.811 --> 01:09:07.066
You should generally be
using batch normalization

01:09:07.066 --> 01:09:08.515
as kind of a good thing to have

01:09:08.515 --> 01:09:10.237
in most networks nowadays because it

01:09:10.237 --> 01:09:13.135
helps you converge, especially
for very deep things.

01:09:13.135 --> 01:09:15.399
In many cases, batch normalization alone

01:09:15.399 --> 01:09:18.228
tends to be enough, but then sometimes

01:09:18.228 --> 01:09:20.608
if batch normalization
alone is not enough,

01:09:20.608 --> 01:09:22.227
then you can consider adding dropout

01:09:22.227 --> 01:09:25.690
or other thing once you see
your network overfitting.

01:09:25.690 --> 01:09:27.883
You generally don't do
a blind cross-validation

01:09:27.883 --> 01:09:29.012
over these things.

01:09:29.012 --> 01:09:31.010
Instead, you add them in in a targeted way

01:09:31.010 --> 01:09:34.427
once you see your network is overfitting.

01:09:36.885 --> 01:09:39.466
One quick thing, it's this
idea of transfer learning.

01:09:39.466 --> 01:09:41.225
We've kind of seen with regularization,

01:09:41.225 --> 01:09:43.004
we can help reduce the gap between

01:09:43.004 --> 01:09:45.336
train and test error by
adding these different

01:09:45.336 --> 01:09:47.503
regularization strategies.

01:09:49.388 --> 01:09:51.576
One problem with overfitting is sometimes

01:09:51.576 --> 01:09:53.498
you overfit 'cause you
don't have enough data.

01:09:53.498 --> 01:09:55.117
You want to use a big, powerful model,

01:09:55.117 --> 01:09:57.400
but that big, powerful
model just is going to

01:09:57.400 --> 01:10:00.929
overfit too much on your small dataset.

01:10:00.929 --> 01:10:03.316
Regularization is one way to combat that,

01:10:03.316 --> 01:10:06.394
but another way is through
using transfer learning.

01:10:06.394 --> 01:10:08.667
Transfer learning kind of busts this myth

01:10:08.667 --> 01:10:11.180
that you don't need a huge amount of data

01:10:11.180 --> 01:10:13.215
in order to train a CNN.

01:10:13.215 --> 01:10:15.785
The idea is really simple.

01:10:15.785 --> 01:10:18.296
You'll maybe first take some CNN.

01:10:18.296 --> 01:10:21.283
Here is kind of a VGG style architecture.

01:10:21.283 --> 01:10:23.078
You'll take your CNN, you'll train it

01:10:23.078 --> 01:10:25.516
in a very large dataset, like ImageNet,

01:10:25.516 --> 01:10:26.709
where you actually have enough data

01:10:26.709 --> 01:10:28.524
to train the whole network.

01:10:28.524 --> 01:10:30.488
Now the idea is that you want to apply

01:10:30.488 --> 01:10:32.715
the features from this dataset to some

01:10:32.715 --> 01:10:35.081
small dataset that you care about.

01:10:35.081 --> 01:10:37.078
Maybe instead of classifying the 1,000

01:10:37.078 --> 01:10:39.627
ImageNet categories,
now you want to classify

01:10:39.627 --> 01:10:41.775
10 dog breeds or something like that.

01:10:41.775 --> 01:10:43.349
You only have a small dataset.

01:10:43.349 --> 01:10:46.402
Here, our small dataset
only has C classes.

01:10:46.402 --> 01:10:48.900
Then what you'll typically
do is for this last

01:10:48.900 --> 01:10:51.421
fully connected layer that is going from

01:10:51.421 --> 01:10:54.453
the last layer features
to the final class scores,

01:10:54.453 --> 01:10:58.620
this now, you need to
reinitialize that matrix randomly.

01:11:00.136 --> 01:11:02.191
For ImageNet, it was a 4,096-by-1,000

01:11:02.191 --> 01:11:03.437
dimensional matrix.

01:11:03.437 --> 01:11:06.542
Now for your new classes, it might

01:11:06.542 --> 01:11:09.667
be 4,096-by-C or by 10 or whatever.

01:11:09.667 --> 01:11:12.154
You reinitialize this
last matrix randomly,

01:11:12.154 --> 01:11:14.470
freeze the weights of
all the previous layers

01:11:14.470 --> 01:11:17.406
and now just basically
train a linear classifier,

01:11:17.406 --> 01:11:19.682
and only train the
parameters of this last layer

01:11:19.682 --> 01:11:22.432
and let it converge on your data.

01:11:24.273 --> 01:11:25.750
This tends to work pretty well if you only

01:11:25.750 --> 01:11:29.241
have a very small dataset to work with.

01:11:29.241 --> 01:11:31.373
Now if you have a little bit more data,

01:11:31.373 --> 01:11:32.874
another thing you can try is actually

01:11:32.874 --> 01:11:35.651
fine tuning the whole network.

01:11:35.651 --> 01:11:37.910
After that top layer converges and after

01:11:37.910 --> 01:11:40.051
you learn that last layer for your data,

01:11:40.051 --> 01:11:42.683
then you can consider
actually trying to update

01:11:42.683 --> 01:11:45.420
the whole network, as well.

01:11:45.420 --> 01:11:47.532
If you have more data,
then you might consider

01:11:47.532 --> 01:11:49.919
updating larger parts of the network.

01:11:49.919 --> 01:11:52.304
A general strategy here is that when

01:11:52.304 --> 01:11:54.461
you're updating the
network, you want to drop

01:11:54.461 --> 01:11:56.628
the learning rate from
its initial learning rate

01:11:56.628 --> 01:11:59.972
because probably the original parameters

01:11:59.972 --> 01:12:02.129
in this network that converged on ImageNet

01:12:02.129 --> 01:12:03.458
probably worked pretty well generally,

01:12:03.458 --> 01:12:05.222
and you just want to change
them a very small amount

01:12:05.222 --> 01:12:09.090
to tune performance for your dataset.

01:12:09.090 --> 01:12:10.708
Then when you're working
with transfer learning,

01:12:10.708 --> 01:12:12.728
you kind of imagine this two-by-two grid

01:12:12.728 --> 01:12:15.975
of scenarios where on
the one side, you have

01:12:15.975 --> 01:12:17.605
maybe very small amounts of data for your

01:12:17.605 --> 01:12:19.181
dataset, or very large amount of data

01:12:19.181 --> 01:12:20.598
for your dataset.

01:12:21.673 --> 01:12:24.786
Then maybe your data is
very similar to images.

01:12:24.786 --> 01:12:27.023
Like, ImageNet has a lot
of pictures of animals

01:12:27.023 --> 01:12:29.265
and plants and stuff like that.

01:12:29.265 --> 01:12:31.043
If you want to just classify other types

01:12:31.043 --> 01:12:33.046
of animals and plants
and other types of images

01:12:33.046 --> 01:12:35.820
like that, then you're
in pretty good shape.

01:12:35.820 --> 01:12:38.025
Then generally what you do is if your data

01:12:38.025 --> 01:12:42.233
is very similar to
something like ImageNet,

01:12:42.233 --> 01:12:43.820
if you have a very small amount of data,

01:12:43.820 --> 01:12:46.026
you can just basically
train a linear classifier

01:12:46.026 --> 01:12:47.685
on top of features, extracted using

01:12:47.685 --> 01:12:49.346
an ImageNet model.

01:12:49.346 --> 01:12:52.121
If you have a little bit
more data to work with,

01:12:52.121 --> 01:12:55.271
then you might imagine
fine tuning your data.

01:12:55.271 --> 01:12:56.730
However, you sometimes get in trouble

01:12:56.730 --> 01:12:59.240
if your data looks very
different from ImageNet.

01:12:59.240 --> 01:13:01.108
Maybe if you're working with maybe

01:13:01.108 --> 01:13:03.635
medical images that
are X-rays or CAT scans

01:13:03.635 --> 01:13:05.258
or something that looks very different

01:13:05.258 --> 01:13:07.266
from images in ImageNet, in that case,

01:13:07.266 --> 01:13:09.557
you maybe need to get a
little bit more creative.

01:13:09.557 --> 01:13:11.332
Sometimes it still works well here,

01:13:11.332 --> 01:13:13.634
but those last layer features might not

01:13:13.634 --> 01:13:14.893
be so informative.

01:13:14.893 --> 01:13:17.174
You might consider
reinitializing larger parts

01:13:17.174 --> 01:13:18.305
of the network and getting a little bit

01:13:18.305 --> 01:13:21.992
more creative and trying
more experiments here.

01:13:21.992 --> 01:13:23.417
This is somewhat mitigated if you have

01:13:23.417 --> 01:13:25.759
a large amount of data in
your very different dataset

01:13:25.759 --> 01:13:26.966
'cause then you can actually fine tune

01:13:26.966 --> 01:13:29.500
larger parts of the network.

01:13:29.500 --> 01:13:31.193
Another point I'd like
to make is this idea

01:13:31.193 --> 01:13:33.072
of transfer learning is super pervasive.

01:13:33.072 --> 01:13:36.145
It's actually the norm,
rather than the exception.

01:13:36.145 --> 01:13:37.677
As you read computer vision papers,

01:13:37.677 --> 01:13:39.523
you'll often see system diagrams like this

01:13:39.523 --> 01:13:41.047
for different tasks.

01:13:41.047 --> 01:13:42.587
On the left, we're working
with object detection.

01:13:42.587 --> 01:13:45.191
On the right, we're working
with image captioning.

01:13:45.191 --> 01:13:46.764
Both of these models have a CNN

01:13:46.764 --> 01:13:48.872
that's kind of processing the image.

01:13:48.872 --> 01:13:51.118
In almost all applications
of computer vision

01:13:51.118 --> 01:13:53.131
these days, most people are not training

01:13:53.131 --> 01:13:54.398
these things from scratch.

01:13:54.398 --> 01:13:56.344
Almost always, that CNN will be pretrained

01:13:56.344 --> 01:13:58.273
on ImageNet, and then
potentially fine tuned

01:13:58.273 --> 01:14:00.458
for the task at hand.

01:14:00.458 --> 01:14:03.098
Also, in the captioning
sense, sometimes you can

01:14:03.098 --> 01:14:05.872
actually pretrain some word vectors

01:14:05.872 --> 01:14:07.574
relating to the language, as well.

01:14:07.574 --> 01:14:09.847
You maybe pretrain the CNN on ImageNet,

01:14:09.847 --> 01:14:11.214
pretrain some word vectors on a large

01:14:11.214 --> 01:14:12.954
text corpus, and then
fine tune the whole thing

01:14:12.954 --> 01:14:14.628
for your dataset.

01:14:14.628 --> 01:14:16.188
Although in the case
of captioning, I think

01:14:16.188 --> 01:14:17.927
this pretraining with word vectors tends

01:14:17.927 --> 01:14:19.392
to be a little bit less common

01:14:19.392 --> 01:14:22.763
and a little bit less critical.

01:14:22.763 --> 01:14:24.384
The takeaway for your projects,

01:14:24.384 --> 01:14:26.554
and more generally as you
work on different models,

01:14:26.554 --> 01:14:29.988
is that whenever you
have some large dataset,

01:14:29.988 --> 01:14:31.105
whenever you have some problem that you

01:14:31.105 --> 01:14:33.710
want to tackle, but you
don't have a large dataset,

01:14:33.710 --> 01:14:37.601
then what you should
generally do is download

01:14:37.601 --> 01:14:39.679
some pretrained model
that's relatively close

01:14:39.679 --> 01:14:42.158
to the task you care
about, and then either

01:14:42.158 --> 01:14:43.859
reinitialize parts of
that model or fine tune

01:14:43.859 --> 01:14:45.344
that model for your data.

01:14:45.344 --> 01:14:48.404
That tends to work pretty
well, even if you have

01:14:48.404 --> 01:14:49.693
only a modest amount of training data

01:14:49.693 --> 01:14:51.161
to work with.

01:14:51.161 --> 01:14:52.998
Because this is such a common strategy,

01:14:52.998 --> 01:14:54.167
all of the different deep learning

01:14:54.167 --> 01:14:56.314
software packages out there provide

01:14:56.314 --> 01:14:58.223
a model zoo where you can just download

01:14:58.223 --> 01:15:01.584
pretrained versions of various models.

01:15:01.584 --> 01:15:04.009
In summary today, we
talked about optimization,

01:15:04.009 --> 01:15:06.528
which is about how to
improve the training loss.

01:15:06.528 --> 01:15:08.568
We talked about regularization,
which is improving

01:15:08.568 --> 01:15:11.369
your performance on the test data.

01:15:11.369 --> 01:15:13.323
Model ensembling kind of fit into there.

01:15:13.323 --> 01:15:14.724
We also talked about transfer learning,

01:15:14.724 --> 01:15:16.223
which is how you can actually do better

01:15:16.223 --> 01:15:17.925
with less data.

01:15:17.925 --> 01:15:19.551
These are all super useful strategies.

01:15:19.551 --> 01:15:22.425
You should use them in
your projects and beyond.

01:15:22.425 --> 01:15:24.717
Next time, we'll talk
more concretely about

01:15:24.717 --> 01:15:25.723
some of the different deep learning

01:15:25.723 --> 01:15:28.140
software packages out there.

