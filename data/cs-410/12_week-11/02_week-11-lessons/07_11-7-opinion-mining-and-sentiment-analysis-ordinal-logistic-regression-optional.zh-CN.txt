本节课是关于有序逻辑回归 在情感分析中的使用 那么这是我们对情感分类问题的典型设定 更准确地说
是评价预测的一个设定 我们以含有评价的文本d作为输入
然后想得到 一个从1到k的评分 这是个离散评分
且这是个归类问题 我们这有k类 现在我们可以用正则文本 使用分类技术来解决这个问题 但这样一个解决方式不考虑分类的顺序和依赖性 直觉上
区分分类1或2的特征 或者从1分或2分的特征 可能与区分k与k-1的特征相似 比如正面的文字通常代表一个更高的分数 当我们训练归类问题时 我们将这些类别独立对待
因而不会获取到这些特征 那么解决方法是什么? 通常来说我们可以排序分类
这有很多方法 我们将谈到其中的一个 称作有序逻辑回归 现在首先让我们来思考如何将逻辑回归 用在二元情感上 一个分类问题 假设我们只想区分正面和负面 那这就是一个两个分类的问题 自变量用X来表示
这些是特征 总共有M个特征 特征值是一个实数 这些可以表征文本 为什么这有两个数字
二元响应变量0或1 1指X是正面的
0指X是负面的 当然这是一个标准的二元分类问题 我们可以使用逻辑回归 你可以回忆一下在逻辑回归里
我们假设 Y为1的概率的对数 是这些特征表征的线性函数
如此所示 这使我们可以将
给定X时Y等于1的概率改写为 下方显示的式子 那是逻辑函数 你们能看到这将概率 y=1的概率和这些值相联系 当然贝塔i在这是参数 这是一个二元分类在逻辑回归中的直接运用 如果我们有多个类别或者多个层级呢? 那么我们还是要用二元逻辑回归的问题 来预测这种多级评分 方法是我们可以引入多个二元类文件 每种情况我们都让类文件去预测 评分比j高还是低 当Yj等于1时，表示评分高于或等于j 等于0时表示低于j 简单来说
如果我们想要在1到k的区间里预测评分 我们首先要有个分类来区分k和其他值 那是我们的分类1 然后我们要有另一个分类来区分它 从k-1开始 那是分类2 最后我们需要一个分类来区分2和1 所以加起来一共有k-1个分类器 现在如果我们这么做‑我们就可以解决这个问题 而逻辑回归语言会是个很直接的方法 如前一页所见 只是这里我们有了更多的参数 因为对每个分类
我们需要一个不同的参数组 逻辑回归用J来作为指数区分分类 即所对应的评分层级 之后我使用阿尔法j来取代贝塔0 这是为了 使标记更一致 与我们在有序逻辑回归中展示的相一致 这里我们已经有了k-1个逻辑回归分类器 且每个都有自己的参数组 通过这个方法
我们现在可以计算评分如下 在我们训练这k-1个逻辑回归分类之后 当然它们是分开进行的
之后我们可以有个新样本 通过依次调用这些分类器来做决定 首先让我们看一下对应K级分数的分类器 这会告诉我们这项是否应该 有一个K或者更高的评分 如果该逻辑回归分类器给出的概率 高于0.5的话
我们可以说结果成立 评分为K 现在如果它比0.5小呢? 那么这意味着评分小于K
对吧? 所以现在我们需要借助下一个分类器 来告诉我们它是否高于K-1 至少是K-1 如果概率比0.5大 那我们能够说它是K-1 如果还要小呢? 那就意味评分比K-1还要低 我们就继续使用下一个分类器 直到我们到达终点决定是2还是1 这将帮我们解决问题 对吧? 我们会有个分类器告诉我们评分的预测 在1到K的区间里 不幸的是
现在这个策略不是最佳的解决方式 具体来讲是这个方法有两个问题 这些等式也是一样 你们之前已经见过了 现在第一个问题是参数太多 这么多参数 现在你们能来数一下这里有几个参数? 这可能是个有趣的练习 做这个 也许你们想暂停播放来数一数 每个分类器我有多少个参数? 以及我们有多少个分类器? 你可以看到
每个分类器我们有 n+1个参数
一共k-1个分类 所以参数的总数为(k-1)*(n+1) 太多了 太多参数了
当分类器有很多参数时 我们需要很多训练数据
来帮我们 来决定这样一个复杂模型里的最优参数 那不是很理想 第二个问题是 这些k-1个分类器并不彼此独立 它们可能是有相关性的 一般来说
正面文字能够 在这些分类里的任何一个中使评分更高 所有分类 所以我们应该利用这个事实 有序逻辑回归就是针对这个问题的 关键点就是在 k-1个独立逻辑回归分类器上 我们要做的是把这些贝塔参数连起来 那意味着我们要假设这些贝塔参数 是表明这些权重推断的参数 我们假设这些贝塔值对于k-1个分类器 都是相同的 这只是印证了我们的直觉 即一般来说
正面文字更有可能得到一个高分 当然这是直觉上的对意见的假设
看起来是问题很合理的开端 当我们在这些类别里有这个顺序的时候 实际上
这有两个好处 一是这将大大减少参数数量 二是这使我们能分享训练数据 因为这些参数差不多 所以这些在不同分类器当中的训练数据可以 通过分享来使我们获得贝塔的最优值 我们也因此有更多数据来帮我们选择一个好的贝塔值 结果是什么呢 公式会非常像你之前看到的 现在贝塔参数的区分只会对应到每一个特征 它不再拥有对应评分级别的其他指标值 意味着我们已经将它们捆绑在一起了 也就是所有分类器里只有一组更好的值 然而每个分类依旧有各自的阿尔法值 阿尔法参数 除了它比较特殊 这会被用于预测不同的评分级别 阿尔法j不同
它取决于j 不同的j会有不同的阿尔法值 但余下的参数贝塔i是相同的 现在你可以问
我们现在有多少个参数呢? 它依然会是个有趣的问题 如果你考虑一下 你会发现现在我们的参数少了很多 即是M+K-1个 因为我们有M个贝塔值然后K-1个阿尔法值 让我们简单来看 这就是有序逻辑回归的主要思路 现在我们来看看如果把这种方法运用在评分上 结果是用它不仅连接起了参数
贝塔值 我们最后也得到了一个类似的判定方法 更准确地说
预测概率 从是否至少为0.5
现在变成了 这个对象的分数是否大于 或等于负的阿尔法j
如这里所示 得分函数只需要利用这些贝塔值 对所有特征取线性组合即可 这意味着我们可以基于得分函数 看它落在哪个区间
然后简单地做出评分决定 现在你知道大致的决策规则了 当分数在我们的取值范围内 我们就给该文本打相应的分数 用这种方法，我们可以给对象打分 使用它的特征和训练过的参数 这个值会和一组训练过的 阿尔法值比较
来看得分在哪个区间 然后呢 根据区间来决定分数 这些alpha值对应不同的 分数级别
来自我们的训练值 每个都和某些评分级别相连 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community