1
00:00:00,025 --> 00:00:05,824
本节课是关于有序逻辑回归

2
00:00:05,824 --> 00:00:12,859
在情感分析中的使用

3
00:00:12,859 --> 00:00:18,730
那么这是我们对情感分类问题的典型设定

4
00:00:18,730 --> 00:00:21,460
更准确地说
是评价预测的一个设定

5
00:00:21,460 --> 00:00:27,430
我们以含有评价的文本d作为输入
然后想得到

6
00:00:27,430 --> 00:00:30,770
一个从1到k的评分

7
00:00:30,770 --> 00:00:37,110
这是个离散评分
且这是个归类问题

8
00:00:37,110 --> 00:00:38,960
我们这有k类

9
00:00:38,960 --> 00:00:40,330
现在我们可以用正则文本

10
00:00:40,330 --> 00:00:42,950
使用分类技术来解决这个问题

11
00:00:42,950 --> 00:00:48,890
但这样一个解决方式不考虑分类的顺序和依赖性

12
00:00:48,890 --> 00:00:53,400
直觉上
区分分类1或2的特征

13
00:00:53,400 --> 00:00:58,380
或者从1分或2分的特征

14
00:00:58,380 --> 00:01:02,749
可能与区分k与k-1的特征相似

15
00:01:02,749 --> 00:01:08,610
比如正面的文字通常代表一个更高的分数

16
00:01:08,610 --> 00:01:11,950
当我们训练归类问题时

17
00:01:11,950 --> 00:01:16,670
我们将这些类别独立对待
因而不会获取到这些特征

18
00:01:17,700 --> 00:01:18,760
那么解决方法是什么?

19
00:01:18,760 --> 00:01:23,830
通常来说我们可以排序分类
这有很多方法

20
00:01:23,830 --> 00:01:26,330
我们将谈到其中的一个

21
00:01:26,330 --> 00:01:29,030
称作有序逻辑回归

22
00:01:29,030 --> 00:01:33,218
现在首先让我们来思考如何将逻辑回归

23
00:01:33,218 --> 00:01:34,410
用在二元情感上

24
00:01:34,410 --> 00:01:36,460
一个分类问题

25
00:01:36,460 --> 00:01:40,700
假设我们只想区分正面和负面

26
00:01:40,700 --> 00:01:44,070
那这就是一个两个分类的问题

27
00:01:44,070 --> 00:01:47,660
自变量用X来表示
这些是特征

28
00:01:47,660 --> 00:01:50,080
总共有M个特征

29
00:01:50,080 --> 00:01:52,390
特征值是一个实数

30
00:01:52,390 --> 00:01:55,360
这些可以表征文本

31
00:01:56,790 --> 00:02:02,150
为什么这有两个数字
二元响应变量0或1

32
00:02:02,150 --> 00:02:04,940
1指X是正面的
0指X是负面的

33
00:02:04,940 --> 00:02:09,940
当然这是一个标准的二元分类问题

34
00:02:09,940 --> 00:02:11,990
我们可以使用逻辑回归

35
00:02:11,990 --> 00:02:17,620
你可以回忆一下在逻辑回归里
我们假设

36
00:02:17,620 --> 00:02:22,820
Y为1的概率的对数

37
00:02:22,820 --> 00:02:28,490
是这些特征表征的线性函数
如此所示

38
00:02:28,490 --> 00:02:35,000
这使我们可以将
给定X时Y等于1的概率改写为

39
00:02:36,030 --> 00:02:41,820
下方显示的式子

40
00:02:43,020 --> 00:02:47,306
那是逻辑函数

41
00:02:47,306 --> 00:02:52,421
你们能看到这将概率

42
00:02:52,421 --> 00:02:57,970
y=1的概率和这些值相联系

43
00:02:57,970 --> 00:03:02,960
当然贝塔i在这是参数

44
00:03:02,960 --> 00:03:07,400
这是一个二元分类在逻辑回归中的直接运用

45
00:03:08,790 --> 00:03:11,730
如果我们有多个类别或者多个层级呢?

46
00:03:11,730 --> 00:03:16,600
那么我们还是要用二元逻辑回归的问题

47
00:03:16,600 --> 00:03:20,000
来预测这种多级评分

48
00:03:21,170 --> 00:03:26,215
方法是我们可以引入多个二元类文件

49
00:03:26,215 --> 00:03:29,790
每种情况我们都让类文件去预测

50
00:03:29,790 --> 00:03:35,210
评分比j高还是低

51
00:03:35,210 --> 00:03:41,550
当Yj等于1时，表示评分高于或等于j

52
00:03:41,550 --> 00:03:44,080
等于0时表示低于j

53
00:03:45,360 --> 00:03:51,520
简单来说
如果我们想要在1到k的区间里预测评分

54
00:03:51,520 --> 00:03:57,070
我们首先要有个分类来区分k和其他值

55
00:03:57,070 --> 00:03:59,220
那是我们的分类1

56
00:03:59,220 --> 00:04:02,275
然后我们要有另一个分类来区分它

57
00:04:02,275 --> 00:04:05,850
从k-1开始

58
00:04:05,850 --> 00:04:06,700
那是分类2

59
00:04:06,700 --> 00:04:11,989
最后我们需要一个分类来区分2和1

60
00:04:11,989 --> 00:04:16,281
所以加起来一共有k-1个分类器

61
00:04:17,830 --> 00:04:23,580
现在如果我们这么做‑我们就可以解决这个问题

62
00:04:23,580 --> 00:04:27,750
而逻辑回归语言会是个很直接的方法

63
00:04:27,750 --> 00:04:30,910
如前一页所见

64
00:04:30,910 --> 00:04:33,760
只是这里我们有了更多的参数

65
00:04:33,760 --> 00:04:37,566
因为对每个分类
我们需要一个不同的参数组

66
00:04:37,566 --> 00:04:41,889
逻辑回归用J来作为指数区分分类

67
00:04:41,889 --> 00:04:44,780
即所对应的评分层级

68
00:04:46,190 --> 00:04:51,910
之后我使用阿尔法j来取代贝塔0

69
00:04:51,910 --> 00:04:54,390
这是为了

70
00:04:54,390 --> 00:04:57,451
使标记更一致

71
00:04:57,451 --> 00:05:02,800
与我们在有序逻辑回归中展示的相一致

72
00:05:02,800 --> 00:05:09,000
这里我们已经有了k-1个逻辑回归分类器

73
00:05:09,000 --> 00:05:12,380
且每个都有自己的参数组

74
00:05:12,380 --> 00:05:18,349
通过这个方法
我们现在可以计算评分如下

75
00:05:19,350 --> 00:05:23,760
在我们训练这k-1个逻辑回归分类之后

76
00:05:23,760 --> 00:05:30,160
当然它们是分开进行的
之后我们可以有个新样本

77
00:05:30,160 --> 00:05:37,085
通过依次调用这些分类器来做决定

78
00:05:37,085 --> 00:05:43,955
首先让我们看一下对应K级分数的分类器

79
00:05:43,955 --> 00:05:49,010
这会告诉我们这项是否应该

80
00:05:49,010 --> 00:05:54,230
有一个K或者更高的评分

81
00:05:54,230 --> 00:05:58,360
如果该逻辑回归分类器给出的概率

82
00:05:58,360 --> 00:06:00,650
高于0.5的话
我们可以说结果成立

83
00:06:00,650 --> 00:06:01,310
评分为K

84
00:06:02,540 --> 00:06:06,750
现在如果它比0.5小呢?

85
00:06:06,750 --> 00:06:10,020
那么这意味着评分小于K
对吧?

86
00:06:11,050 --> 00:06:13,750
所以现在我们需要借助下一个分类器

87
00:06:13,750 --> 00:06:17,530
来告诉我们它是否高于K-1

88
00:06:18,690 --> 00:06:20,660
至少是K-1

89
00:06:20,660 --> 00:06:23,140
如果概率比0.5大

90
00:06:23,140 --> 00:06:26,400
那我们能够说它是K-1

91
00:06:26,400 --> 00:06:27,960
如果还要小呢?

92
00:06:27,960 --> 00:06:30,280
那就意味评分比K-1还要低

93
00:06:30,280 --> 00:06:34,990
我们就继续使用下一个分类器

94
00:06:34,990 --> 00:06:41,340
直到我们到达终点决定是2还是1

95
00:06:41,340 --> 00:06:43,510
这将帮我们解决问题

96
00:06:43,510 --> 00:06:44,350
对吧?

97
00:06:44,350 --> 00:06:49,320
我们会有个分类器告诉我们评分的预测

98
00:06:49,320 --> 00:06:51,120
在1到K的区间里

99
00:06:51,120 --> 00:06:55,510
不幸的是
现在这个策略不是最佳的解决方式

100
00:06:55,510 --> 00:07:01,661
具体来讲是这个方法有两个问题

101
00:07:01,661 --> 00:07:03,850
这些等式也是一样

102
00:07:03,850 --> 00:07:05,110
你们之前已经见过了

103
00:07:06,250 --> 00:07:10,100
现在第一个问题是参数太多

104
00:07:10,100 --> 00:07:11,630
这么多参数

105
00:07:11,630 --> 00:07:15,540
现在你们能来数一下这里有几个参数?

106
00:07:15,540 --> 00:07:18,680
这可能是个有趣的练习

107
00:07:18,680 --> 00:07:19,440
做这个

108
00:07:19,440 --> 00:07:24,250
也许你们想暂停播放来数一数

109
00:07:24,250 --> 00:07:27,060
每个分类器我有多少个参数?

110
00:07:28,580 --> 00:07:30,280
以及我们有多少个分类器?

111
00:07:31,840 --> 00:07:37,310
你可以看到
每个分类器我们有

112
00:07:37,310 --> 00:07:42,680
n+1个参数
一共k-1个分类

113
00:07:42,680 --> 00:07:49,030
所以参数的总数为(k-1)*(n+1)

114
00:07:49,030 --> 00:07:49,820
太多了

115
00:07:49,820 --> 00:07:54,096
太多参数了
当分类器有很多参数时

116
00:07:54,096 --> 00:07:58,530
我们需要很多训练数据
来帮我们

117
00:07:58,530 --> 00:08:03,220
来决定这样一个复杂模型里的最优参数

118
00:08:04,450 --> 00:08:05,785
那不是很理想

119
00:08:07,225 --> 00:08:10,751
第二个问题是

120
00:08:10,751 --> 00:08:15,595
这些k-1个分类器并不彼此独立

121
00:08:15,595 --> 00:08:17,305
它们可能是有相关性的

122
00:08:18,372 --> 00:08:23,172
一般来说
正面文字能够

123
00:08:25,042 --> 00:08:27,082
在这些分类里的任何一个中使评分更高

124
00:08:27,082 --> 00:08:28,752
所有分类

125
00:08:28,752 --> 00:08:31,896
所以我们应该利用这个事实

126
00:08:33,016 --> 00:08:37,846
有序逻辑回归就是针对这个问题的

127
00:08:37,846 --> 00:08:42,007
关键点就是在

128
00:08:42,007 --> 00:08:46,390
k-1个独立逻辑回归分类器上

129
00:08:46,390 --> 00:08:51,590
我们要做的是把这些贝塔参数连起来

130
00:08:51,590 --> 00:08:59,070
那意味着我们要假设这些贝塔参数

131
00:08:59,070 --> 00:09:05,290
是表明这些权重推断的参数

132
00:09:05,290 --> 00:09:09,490
我们假设这些贝塔值对于k-1个分类器

133
00:09:09,490 --> 00:09:10,920
都是相同的

134
00:09:10,920 --> 00:09:13,678
这只是印证了我们的直觉

135
00:09:13,678 --> 00:09:17,980
即一般来说
正面文字更有可能得到一个高分

136
00:09:19,550 --> 00:09:25,220
当然这是直觉上的对意见的假设
看起来是问题很合理的开端

137
00:09:25,220 --> 00:09:27,410
当我们在这些类别里有这个顺序的时候

138
00:09:28,630 --> 00:09:34,370
实际上
这有两个好处

139
00:09:34,370 --> 00:09:37,450
一是这将大大减少参数数量

140
00:09:38,750 --> 00:09:42,880
二是这使我们能分享训练数据

141
00:09:42,880 --> 00:09:45,860
因为这些参数差不多

142
00:09:45,860 --> 00:09:51,200
所以这些在不同分类器当中的训练数据可以

143
00:09:51,200 --> 00:09:55,230
通过分享来使我们获得贝塔的最优值

144
00:09:56,280 --> 00:10:00,010
我们也因此有更多数据来帮我们选择一个好的贝塔值

145
00:10:01,790 --> 00:10:02,840
结果是什么呢

146
00:10:02,840 --> 00:10:08,010
公式会非常像你之前看到的

147
00:10:08,010 --> 00:10:13,440
现在贝塔参数的区分只会对应到每一个特征

148
00:10:13,440 --> 00:10:17,820
它不再拥有对应评分级别的其他指标值

149
00:10:19,260 --> 00:10:21,340
意味着我们已经将它们捆绑在一起了

150
00:10:21,340 --> 00:10:26,340
也就是所有分类器里只有一组更好的值

151
00:10:26,340 --> 00:10:31,180
然而每个分类依旧有各自的阿尔法值

152
00:10:31,180 --> 00:10:33,060
阿尔法参数

153
00:10:33,060 --> 00:10:35,290
除了它比较特殊

154
00:10:35,290 --> 00:10:39,950
这会被用于预测不同的评分级别

155
00:10:39,950 --> 00:10:43,840
阿尔法j不同
它取决于j

156
00:10:43,840 --> 00:10:46,020
不同的j会有不同的阿尔法值

157
00:10:46,020 --> 00:10:48,890
但余下的参数贝塔i是相同的

158
00:10:48,890 --> 00:10:53,940
现在你可以问
我们现在有多少个参数呢?

159
00:10:53,940 --> 00:10:57,140
它依然会是个有趣的问题

160
00:10:57,140 --> 00:11:00,910
如果你考虑一下

161
00:11:00,910 --> 00:11:05,415
你会发现现在我们的参数少了很多

162
00:11:05,415 --> 00:11:08,320
即是M+K-1个

163
00:11:08,320 --> 00:11:13,720
因为我们有M个贝塔值然后K-1个阿尔法值

164
00:11:15,550 --> 00:11:17,575
让我们简单来看

165
00:11:17,575 --> 00:11:21,931
这就是有序逻辑回归的主要思路

166
00:11:24,695 --> 00:11:31,290
现在我们来看看如果把这种方法运用在评分上

167
00:11:31,290 --> 00:11:39,730
结果是用它不仅连接起了参数
贝塔值

168
00:11:39,730 --> 00:11:44,710
我们最后也得到了一个类似的判定方法

169
00:11:44,710 --> 00:11:50,220
更准确地说
预测概率

170
00:11:50,220 --> 00:11:55,440
从是否至少为0.5
现在变成了

171
00:11:55,440 --> 00:12:00,810
这个对象的分数是否大于

172
00:12:00,810 --> 00:12:06,390
或等于负的阿尔法j
如这里所示

173
00:12:06,390 --> 00:12:11,130
得分函数只需要利用这些贝塔值

174
00:12:11,130 --> 00:12:14,380
对所有特征取线性组合即可

175
00:12:15,790 --> 00:12:21,820
这意味着我们可以基于得分函数

176
00:12:21,820 --> 00:12:27,900
看它落在哪个区间
然后简单地做出评分决定

177
00:12:27,900 --> 00:12:33,121
现在你知道大致的决策规则了

178
00:12:33,121 --> 00:12:39,584
当分数在我们的取值范围内

179
00:12:39,584 --> 00:12:46,569
我们就给该文本打相应的分数

180
00:12:49,960 --> 00:12:53,760
用这种方法，我们可以给对象打分

181
00:12:55,140 --> 00:12:59,090
使用它的特征和训练过的参数

182
00:13:00,150 --> 00:13:04,490
这个值会和一组训练过的

183
00:13:04,490 --> 00:13:09,020
阿尔法值比较
来看得分在哪个区间

184
00:13:09,020 --> 00:13:09,540
然后呢

185
00:13:09,540 --> 00:13:14,220
根据区间来决定分数

186
00:13:14,220 --> 00:13:19,750
这些alpha值对应不同的

187
00:13:19,750 --> 00:13:24,840
分数级别
来自我们的训练值

188
00:13:24,840 --> 00:13:30,909
每个都和某些评分级别相连

189
00:13:30,909 --> 00:13:40,909
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community