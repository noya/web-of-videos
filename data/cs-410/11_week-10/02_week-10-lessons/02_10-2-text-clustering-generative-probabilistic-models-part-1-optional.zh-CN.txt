这次讲座 是关于为文本聚类生成概率模型 这次讲座 我们会继续讨论文本聚类 和介绍生成概率模型 来做文本聚类 因此 这就是为了覆盖文本聚类的整个计划 在过去的讲座
我们曾讨论什么是文本聚类 以及为什么文本聚类很有趣 这个讲座 我们会讨论如何做文本聚类 一般而言 如你所见 这里有两种方法 第一个是生成概率模型 也就是我们这节课的主题 稍后 我们会讨论以相似度为基础的方法 所以 来讨论为文本聚类生成的模型 用这个主题模型来回顾文本挖掘的问题 会很有帮助
因为这两个问题很相似 这个课件你曾在先前那个主题模型讲座看过 这里我们会展示我们如何输入文本集合C 和一系列主题K 和单词表V 我们希望输出两个东西 一个是一系列主题 有Theta i 表示 每个都是有贡献的
另一个是pi i j 这里是每个文件都覆盖每个主题的概率 所以这是一个文本覆盖
和这也是在这个课件看到的 你可以看到我们通过主题模型可以得到什么 现在 这个和文本聚类问题的主要区别在这里 一个文件 被假设为可能覆盖不同主题 而且一般而言 一个文件会覆盖 一个以上主题 并且概率非零 在文本聚类 然而 我们只允许一个文件 覆盖一个主题 如果我们假设一个主题是一个聚类的话 这意味着如果我们稍微改变题目的定义 假设每个文件只能被一个主题组成 这样 我们会拥有一个聚类问题的定义 所以这里的输出也会改变 我们不在拥有细节的覆盖贡献 pi i 然而 我们会聚类课题决定 C Ci 是一个文件i的决定 然后C 下面会有从1到k的值 表示其中一个 k 聚类 基本上告诉我们 
d i 实在聚类里 如这里所描述 我们不再拥有多个主题被一个文档覆盖 只有准确的一个主题 尽管是哪个主题仍不确定 这里有一个连接 与一个先前讨论过的挖掘问题 这个课件你们之前看过 这里我希望估计一个主题模型 或者分布 基于一个文档 然后当我们假设这个文档精确覆盖一个主题 不过我们仍考虑问题的变化 比如 我们可以认为这里有N个文档 每个覆盖不容的主题
所以 这里有 N文档 和主题 当然 在这个案例 这些文档时独立的 而且主题也是独立的 不过 我们会让这些文档分享主题 而且我们会假设这里的主题数量会比 文档的数量少
所以这些文档会分享多个主题 如果我们拥有N个分享K个主题的文档 我们会有一个精确地文档聚类问题 所以因为这些关联
我们会想如何 用概率生成模型来解决文本聚类问题 现在问题是 怎样的生成模型可以被用来聚类 在所有设计生成模型的案例里 我们希望生成一个 能够输出我们想要的结果的模型
或者我们希望模型的结构如此 所以在这案例
这是一个聚类结构 主题们 还有每个文档覆盖一个主题 然后我们希望嵌入我们的优先选择于生成模型 不过 如果你们想到这个问题和我们 之前讨论过的一个模型的主要区别 你会看见一个主要的要求 是
我们如何迫使所有 文档都精确的由一个主题生成 而不是k主题 在这个主题模型里 让我们更为细节地回顾这个模型 这就是两个成分混合模型的细节 当我们拥有K个成分时 看起来很相似 所以我们会看见当我们生成一个文档时 我们生成的每个词都是独立的 而当我们生成这些词的时候
我们首先在这些分布中做出选择 我们按概率选择其中一个 这里p(θ1)是选择上面这个分布的概率 现在我们先来选择 生成词的分布 然后我们用这个分布来抽样产生单词 现在注意
在这样一个生成模型当中 对于每个词所用分布的选择是独立的 也就是说
比如 here这个词可以从第二个分布θ2当中产生 而text这个词在第一个分布中更容易出现 也就是说文档中的词 可能来自多个分布 现在这不是我们想要的
如我们之前说的 文本聚类或者说文档聚类 我们希望文档是精确地从一个话题中产生的 现在意味着我们要改变模型 怎么改呢? 首先我们来想这个模型为什么不能用在聚类上 像我说的
原因就是 它允许多个话题对文档产生同一个词 这就会带来混乱
因为我们不知道 文档究竟来自哪个聚类 更重要的是
它违背了我们 对于集聚中文档分区的假设 如果我们让一个话题对应到文档中的一个聚类 那么我们需要文档完全从一个话题中生成 也就是说文档中的所有词 都需要从一个分布中生成 对于我们看到的这样的主题模型
这点并不成立 这也是为什么
这个模型不能用来做聚类 因为它不能保证一个文档中的所有词都来自一个分布 如果你意识到这个问题 我们就可以可以设计另外的混合模型来做聚类 这就是你会看到的内容 我们还是要在分布上做出选择 来生成文档
因为文档可以 从我们有的任何k词分布中产生 但这次
当我们选出一个话题之后 我们会用这个话题生成文档中的所有词 也就是说
一旦我们选定了产生第一个词的分布 我们会一直用这个分布 生成文档中的其他所有词 换句话说
我们只做出一次选择 基本上我们只对文档做一次选择 这个状态会用以生成所有词 同样地如果我选择了第二个分布
这里的θ2 你可以看到状态就是这个 然后生成整个文档d 现在如果你将这张图与之前比较 你会看到对于分布的决定 在文档聚类当中
对于这篇文档只进行了一次 但是在话题模型当中 我们需要进行决定的次数是文档中单词的个数 因为每一个词都可能做出一个不同的决定 这就是两个模型的关键不同 但是这个很明显也是个混合模型 所以我们可以将其一同分组 来说明这个模型能够以一定概率生成文档 现在在模型当中 还有一个选择不同分部的开关 但是我们看不到它
所以这是一个混合模型 当然文档聚类的主要问题是推断 文档生成使用了哪个分布 让我们能够还原文档的聚类情况 所以需要来考虑与话题模型的不同 像我之前多次提过的那样 这里有两个主要的不同 一个是选择特定分布的决定 在文本聚类当中只进行一次 而在话题模型当中
不同的词可能会有多次选择 第二个是词的分布 这里会是用来重新生成文档中所有的词 但是在话题模型当中
一个分布 不需要生成文档中所有的词 文档可以使用多个分布来生成词 我们来考虑一个特别的情况 当选择某个特定分布的概率等于1时 也就是我们现在没有不确定因素了 我们只用一个特定的分布 在这情况下
显然我们看到这不再是混合模型 因为没有不确定性 我们看到的只是一个分布生成整个文档 现在我们回到 通过一个文档估计分布的问题当中 我们之前提到过 现在你可以看得更清楚 如在所有利用生成模型解决问题的示例当中 我们先看了数据
然后思考如何去设计模型 但一旦我们设定了模型 下一步就是来写似然函数 之后我们要来考虑如何去估计参数 这里的似然函数是什么呢? 它会跟你之前在话题模型中看到的很相似 但是也会有所不同 现在如果你还想得起来之前的似然函数 你会意识到
从混合模型中观测到某个数据点的概率 等于生成数据所有可能性之和 这里就会是所有k个话题之和 因为每一个都可用来生成文档 在总和当中
你还能想到公式是怎样的 它会是两个概率的乘积 一个是选择分布的概率 一个是从该分布观测到该数据点的概率 如果你写出我们问题的式子 你会看到观测到文档d的概率 基本上等于两个不同分布的和 因为我们这里情况很简单
只有两个聚类 所以这里只是两个情况的总和 每个情形下
就是由选择分布的概率 θ1或者θ2 然后乘以 从该分布观测到文档的概率 如果你进一步展开 观测到整个文档的概率
我们看它就是观测到每个词Xi的(概率)之积 现在我们做一个假设
假设每个词的生成过程是独立的 也就是说整个文档的概率就是 文档中每个词的概率的乘积 这个形式跟话题模型是十分类似的 但是我们也要考虑其中的区别
出于这一点 我也把话题模型概率的这两类元素拷贝到了这里 你可以看到
公式从许多方面看都很相似 但是其中也有一些不同 尤其是在顶部这里的不同 你可以看到文档聚类的混合模型上
我们先是取积 然后求和 对应到我们的假设 我们先是选择一个分布 然后我们一直用这个分布生成所有的词 这就是为什么乘法是在求和之内的 求和对应的选择 现在话题模型当中呢
和是在乘积当中的 这是因为我们每个词是独立生成的 这就是为什么乘法是在外面 但是当我们生成每一个词的时候 我们需要选择使用的分布
所以这里有个对每个词的求和 但是总的来讲
这些都是混合模型 我们可以利用EM算法来估计这些模型 之后我们还会更多讲到这一点 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community