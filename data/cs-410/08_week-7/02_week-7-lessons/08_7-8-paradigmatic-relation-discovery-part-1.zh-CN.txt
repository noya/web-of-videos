这次讲座 是关于寻找聚合关系的方法 在这堂课里我们将会讲到如何寻找 一种特别的叫聚合关系的词义联想关系 这种关系的定义是
如果两个词有相同的语境 那么它们是聚合关系 直接地说
它们在文本中位置相似 自然我们能想到要找到这种关系
需要观察每个词的语境 然后试着计算出这些语境的相似度 这里有一个词例
cat 这里我把cat从语境中删掉了 现在我们看到的是剩下的词句中还有cat
这个词 我们也可以用同样的手段处理带dog这个词的句子 总的来说我们要找到关键词的语境
然后比较cat这个词 和dog这个词语境的相似度 现在问题就是
我们如何以数学方式表现语境 然后定义相似度函数 首先我们注意到语境其实包含很多词 所以我们可以把它看成一个伪文档
一个想象中的文档 同时也可从不同的角度来查看语境 比如我们可以看在cat前边出现的词 我们把它定义为Left1 好的，这样我们就有像my,his,or, big,a,the,等等的词 这些都是cat左边相邻的词 我们有my cat, his cat, big cat, a cat,等等 同样的我们也可以收集出现在cat右边的词 我们可以定义这个为Right1 这样我们就能看到像eats,ate,is,has,等等 或者，更广的看 我们可以看cat附近的一段词 这里我们可以看cat附近的8个词 我们把这个定义为Window8 现在
我们可以来看全部左边和右边的词 全用上我们就有一堆来表达语境了 现在，这种以词为基准的表达其实可以 给我们带来一种估计相似度的有趣方法 因为如果我们只关注Left1的相似度 等于我们只关注左边语境中的词 我们基本上是并没有关注其它也在语境中的词 所以这给我们了一个测量相似度的角度 如果我们只使用Right1语境 我们就又捕获到了另一个的角度 如果同时使用Left1和Right1
显然可以让我们 用更严格的条件去抓取相似度 总的来说
语境可能包括相邻词 比如eats,my,像这边看到的这样
或者非连接词 比如Saturday,Tuesday,或者其他这样语境中的词 这种可以考虑不同方面的灵活性允许我们 很多种不同的方法 有时候这是有用的 比如我们有时需要基于一般语境捕捉相似度 这会给我们泛泛的聚合关系 然而如果你只是用直接左相邻 或右相邻的词
你可能可以捕获 在句法和语义方面非常契合的词 所以寻找聚合关系的基本思想是 计算两个词语境的相似度 所以这里举的例子中
我们可以通过估量 cat和dog的语境相似度来判断这两个词的相似度 总的来说
我们可以组合不同种类对语境的观察 所以相似度函数
总的来说 就是不同语境相似度函数的组合 并且当然
我们也可以给不同的相似度函数 赋予不同权重
这样就可以允许我们着眼于某种特定的语境 这当然是就事论事的
但是再强调一次 这里主要的思想是通过计算关键词语境的相似度 来寻找有聚合关系的词 现在我们来看如何具体计算这些相似度方程 为了回答这个问题
把词袋 看作向量空间模型中的向量是很有用的 现在如果你曾经了解过信息检索 或者文本检索相关技术
你可能反应过来了 向量空间模型经常在搜索技术中被用于文档建模和查询建模 这里我们发现它也是一种 建立一个词聚合关系语境模型的简便方法 这里建模的思路是 把每一个我们词库中的词当作一个高维空间中的一个维度 我们词库中有N个词 那么我们就有N个维度
像这里画的一样 底下你能看见是一个表现语境的词频向量 这里能看到eats这个词在语境中出现了5次 ate出现了3次之类的 那么这个向量就可以被放在这个向量空间模型中 总的来说我们可以通过这一个向量d1 表现cat这个词的语境
另外那个词 dog可能给我们一个不同的语境
所以有d2 然后我们就可以估量这两个向量的相似度 通过把语境看作向量空间模型 我们把这个寻找聚合关系的问题转化成 计算向量及其相似度的问题 所以这里我们要提出两个问题 第一，怎么计算每个向量
即怎么计算xi或yi 第二个问题是怎么计算相似度 通常来说
有很多解决这两个问题的办法 大部分都是为解决信息检索问题 这些方法在匹配查询向量和文档向量方面 被证明是很不错的 这里我们可以改造很多这种办法来达到我们 计算语境文本的相似度的目的 因此让我们先看看一个似乎可行的方法 我们尝试根据共有词期望 来匹配语境的相似性
我们把这个叫做EOWC 这里的想法是用词向量来表达语境 其中每一个词的权重都等于其概率 这个概率就是任意在文本中选择一个词即为当前词的概率 换句话说xi就是 文本中词wi正则化的词频 这可以被解释成 随机在d1中选择一个词就是这个词的概率 现在xi的和必然是1
因为它们是正则化后的概率 这意味着这个向量 其实也是一个文档中文字的概率分布 那么向量d2也可以以同样的方法算出来啦 这会给我们代表两个文档的文本概率分布 这样就讲了如何计算向量的问题 现在我们看看用这个方法怎么定义相似度 这里我们就简单定义相似度为两个向量的点击 即两个向量内 每个相对应元素乘积的和 现在其实这个相似度方程很有趣 它有一个很好的解读 这个点积其实是告诉我们文档中 两个随意选择的词完全一样的概率 这就是说如果我们从一个文本中选一个词
然后从另一个文本中 选一个词
然后我们可以问它们一样吗 如果两个文本非常相似
我们可以期待 我们选的词大部分情况下是完全一样的 如果它们非常不一样
那么在两个文本中 选到完全一样的词的概率会很小 所以测量文档相似度方面
这个公式感觉很有道理 现在你可能同时想看看公式 看看为什么它是 两个随机选择的词完全一样的概率表现 如果你盯着公式想一会这个和里都包含着什么 那么你能看到基本上每一个项 都是两个文档中某一个共有词wi的概率 xi给我们在d1中选到wi的概率 而yi给我们在d2中选到wi的概率 当我们在两个文档中选到的词一样时 我们就选了完全一样的词对吧 这就介绍了一种可能的办法，即EOWC
共有词期望 现在像以前一样
我们像评估这个方法是不是有用 当然最终我们必须用真实数据来测试这个方法 看它是不是真的给我们语义相关的词 真的给我们聚合关系 但是我们也可以用分析的眼光分析下这个公式 那么首先我也说过
这个确实有道理对吧 因为这个公式里
如果两个文档的共有词越多值就越大 这就是我们想要的 但是如果你更仔细的分析一下这个公式 你会发现其实有一些隐藏的问题 更准确的说这里有两个隐藏的问题 第一它可能在匹配常见词方面 比匹配特有词做的更好 因为在点积中
如果一个项有很高的值，而且 这个项还都在两个文档里
那么它对最后的和贡献是非常大的 相比于两个有很多不同词频低共有词的向量点积 这一项很有可能让前者最后的值更高 所以这并不是我们想要的 当然这应用在其他方面可能很不错 但是在我们这里
直觉上我们应该更偏好 匹配文档中不一样的词更多的向量
这样我们才更有自信说 这两个词确实经常出现在相似的语境里 如果仅仅靠一个词 那么最终结果是有点问题的
最终模型不一定稳定 现在第二个问题是这个方法对每个词都一样 所以如果我们匹配the这样的词 它的概率可能跟eats差不多 但是直觉上我们知道
一个句子里有the 并不是多吃惊的事
the这个词哪里都会出现 所以相比匹配到像eats这样并不常出现的词 匹配到了the并不算是很有力的证据 这就是这个方法另一个问题 下一讲我们会谈谈如何解决这两个问题 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community