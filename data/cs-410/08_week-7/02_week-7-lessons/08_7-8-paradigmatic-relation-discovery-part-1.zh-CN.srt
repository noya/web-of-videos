1
00:00:00,025 --> 00:00:07,935
这次讲座

2
00:00:07,935 --> 00:00:14,253
是关于寻找聚合关系的方法

3
00:00:14,253 --> 00:00:19,131
在这堂课里我们将会讲到如何寻找

4
00:00:19,131 --> 00:00:22,160
一种特别的叫聚合关系的词义联想关系

5
00:00:25,400 --> 00:00:30,307
这种关系的定义是
如果两个词有相同的语境

6
00:00:30,307 --> 00:00:34,503
那么它们是聚合关系

7
00:00:34,503 --> 00:00:39,086
直接地说
它们在文本中位置相似

8
00:00:39,086 --> 00:00:44,280
自然我们能想到要找到这种关系
需要观察每个词的语境

9
00:00:44,280 --> 00:00:49,080
然后试着计算出这些语境的相似度

10
00:00:50,160 --> 00:00:54,360
这里有一个词例
cat

11
00:00:55,800 --> 00:01:01,690
这里我把cat从语境中删掉了

12
00:01:01,690 --> 00:01:08,080
现在我们看到的是剩下的词句中还有cat
这个词

13
00:01:09,610 --> 00:01:12,479
我们也可以用同样的手段处理带dog这个词的句子

14
00:01:13,660 --> 00:01:18,370
总的来说我们要找到关键词的语境
然后比较cat这个词

15
00:01:18,370 --> 00:01:23,340
和dog这个词语境的相似度

16
00:01:24,790 --> 00:01:29,970
现在问题就是
我们如何以数学方式表现语境

17
00:01:29,970 --> 00:01:31,458
然后定义相似度函数

18
00:01:33,340 --> 00:01:38,560
首先我们注意到语境其实包含很多词

19
00:01:38,560 --> 00:01:43,637
所以我们可以把它看成一个伪文档
一个想象中的文档

20
00:01:43,637 --> 00:01:49,370
同时也可从不同的角度来查看语境

21
00:01:49,370 --> 00:01:57,470
比如我们可以看在cat前边出现的词

22
00:01:57,470 --> 00:02:00,440
我们把它定义为Left1

23
00:02:00,440 --> 00:02:04,980
好的，这样我们就有像my,his,or,

24
00:02:04,980 --> 00:02:07,430
big,a,the,等等的词

25
00:02:07,430 --> 00:02:12,690
这些都是cat左边相邻的词

26
00:02:12,690 --> 00:02:19,280
我们有my cat, his cat, big cat, a cat,等等

27
00:02:19,280 --> 00:02:24,180
同样的我们也可以收集出现在cat右边的词

28
00:02:24,180 --> 00:02:28,156
我们可以定义这个为Right1

29
00:02:28,156 --> 00:02:34,128
这样我们就能看到像eats,ate,is,has,等等

30
00:02:34,128 --> 00:02:35,907
或者，更广的看

31
00:02:35,907 --> 00:02:41,253
我们可以看cat附近的一段词

32
00:02:41,253 --> 00:02:46,960
这里我们可以看cat附近的8个词

33
00:02:46,960 --> 00:02:48,720
我们把这个定义为Window8

34
00:02:49,850 --> 00:02:54,680
现在
我们可以来看全部左边和右边的词

35
00:02:54,680 --> 00:02:58,829
全用上我们就有一堆来表达语境了

36
00:03:01,270 --> 00:03:06,410
现在，这种以词为基准的表达其实可以

37
00:03:06,410 --> 00:03:12,230
给我们带来一种估计相似度的有趣方法

38
00:03:12,230 --> 00:03:15,911
因为如果我们只关注Left1的相似度

39
00:03:15,911 --> 00:03:21,750
等于我们只关注左边语境中的词

40
00:03:21,750 --> 00:03:27,650
我们基本上是并没有关注其它也在语境中的词

41
00:03:27,650 --> 00:03:32,380
所以这给我们了一个测量相似度的角度

42
00:03:32,380 --> 00:03:34,244
如果我们只使用Right1语境

43
00:03:34,244 --> 00:03:38,420
我们就又捕获到了另一个的角度

44
00:03:38,420 --> 00:03:43,040
如果同时使用Left1和Right1
显然可以让我们

45
00:03:43,040 --> 00:03:47,720
用更严格的条件去抓取相似度

46
00:03:49,910 --> 00:03:54,744
总的来说
语境可能包括相邻词

47
00:03:54,744 --> 00:03:59,575
比如eats,my,像这边看到的这样
或者非连接词

48
00:03:59,575 --> 00:04:02,961
比如Saturday,Tuesday,或者其他这样语境中的词

49
00:04:05,461 --> 00:04:10,174
这种可以考虑不同方面的灵活性允许我们

50
00:04:10,174 --> 00:04:11,660
很多种不同的方法

51
00:04:11,660 --> 00:04:13,500
有时候这是有用的

52
00:04:13,500 --> 00:04:19,130
比如我们有时需要基于一般语境捕捉相似度

53
00:04:19,130 --> 00:04:25,270
这会给我们泛泛的聚合关系

54
00:04:25,270 --> 00:04:29,340
然而如果你只是用直接左相邻

55
00:04:29,340 --> 00:04:35,520
或右相邻的词
你可能可以捕获

56
00:04:35,520 --> 00:04:39,950
在句法和语义方面非常契合的词

57
00:04:41,170 --> 00:04:46,304
所以寻找聚合关系的基本思想是

58
00:04:46,304 --> 00:04:50,754
计算两个词语境的相似度

59
00:04:50,754 --> 00:04:55,264
所以这里举的例子中
我们可以通过估量

60
00:04:55,264 --> 00:04:59,110
cat和dog的语境相似度来判断这两个词的相似度

61
00:04:59,110 --> 00:05:02,890
总的来说
我们可以组合不同种类对语境的观察

62
00:05:02,890 --> 00:05:06,395
所以相似度函数
总的来说

63
00:05:06,395 --> 00:05:10,336
就是不同语境相似度函数的组合

64
00:05:10,336 --> 00:05:14,849
并且当然
我们也可以给不同的相似度函数

65
00:05:14,849 --> 00:05:20,170
赋予不同权重
这样就可以允许我们着眼于某种特定的语境

66
00:05:20,170 --> 00:05:24,395
这当然是就事论事的
但是再强调一次

67
00:05:24,395 --> 00:05:28,935
这里主要的思想是通过计算关键词语境的相似度

68
00:05:28,935 --> 00:05:32,470
来寻找有聚合关系的词

69
00:05:32,470 --> 00:05:37,670
现在我们来看如何具体计算这些相似度方程

70
00:05:37,670 --> 00:05:42,235
为了回答这个问题
把词袋

71
00:05:42,235 --> 00:05:46,520
看作向量空间模型中的向量是很有用的

72
00:05:48,340 --> 00:05:53,016
现在如果你曾经了解过信息检索

73
00:05:53,016 --> 00:05:57,936
或者文本检索相关技术
你可能反应过来了

74
00:05:57,936 --> 00:06:02,711
向量空间模型经常在搜索技术中被用于文档建模和查询建模

75
00:06:02,711 --> 00:06:08,115
这里我们发现它也是一种

76
00:06:08,115 --> 00:06:11,130
建立一个词聚合关系语境模型的简便方法

77
00:06:11,130 --> 00:06:15,440
这里建模的思路是

78
00:06:15,440 --> 00:06:20,140
把每一个我们词库中的词当作一个高维空间中的一个维度

79
00:06:20,140 --> 00:06:23,615
我们词库中有N个词

80
00:06:23,615 --> 00:06:27,462
那么我们就有N个维度
像这里画的一样

81
00:06:27,462 --> 00:06:34,311
底下你能看见是一个表现语境的词频向量

82
00:06:34,311 --> 00:06:39,855
这里能看到eats这个词在语境中出现了5次

83
00:06:39,855 --> 00:06:43,140
ate出现了3次之类的

84
00:06:43,140 --> 00:06:48,003
那么这个向量就可以被放在这个向量空间模型中

85
00:06:48,003 --> 00:06:53,347
总的来说我们可以通过这一个向量d1

86
00:06:53,347 --> 00:06:58,933
表现cat这个词的语境
另外那个词

87
00:06:58,933 --> 00:07:04,045
dog可能给我们一个不同的语境
所以有d2

88
00:07:04,045 --> 00:07:07,880
然后我们就可以估量这两个向量的相似度

89
00:07:07,880 --> 00:07:10,980
通过把语境看作向量空间模型

90
00:07:10,980 --> 00:07:15,100
我们把这个寻找聚合关系的问题转化成

91
00:07:15,100 --> 00:07:18,820
计算向量及其相似度的问题

92
00:07:20,300 --> 00:07:24,170
所以这里我们要提出两个问题

93
00:07:24,170 --> 00:07:28,750
第一，怎么计算每个向量
即怎么计算xi或yi

94
00:07:31,050 --> 00:07:33,579
第二个问题是怎么计算相似度

95
00:07:35,580 --> 00:07:40,515
通常来说
有很多解决这两个问题的办法

96
00:07:40,515 --> 00:07:43,795
大部分都是为解决信息检索问题

97
00:07:43,795 --> 00:07:47,821
这些方法在匹配查询向量和文档向量方面

98
00:07:47,821 --> 00:07:52,712
被证明是很不错的

99
00:07:52,712 --> 00:07:57,555
这里我们可以改造很多这种办法来达到我们

100
00:07:57,555 --> 00:08:01,378
计算语境文本的相似度的目的

101
00:08:01,378 --> 00:08:05,829
因此让我们先看看一个似乎可行的方法

102
00:08:05,829 --> 00:08:10,481
我们尝试根据共有词期望

103
00:08:10,481 --> 00:08:15,150
来匹配语境的相似性
我们把这个叫做EOWC

104
00:08:17,020 --> 00:08:22,495
这里的想法是用词向量来表达语境

105
00:08:22,495 --> 00:08:28,438
其中每一个词的权重都等于其概率

106
00:08:28,438 --> 00:08:35,336
这个概率就是任意在文本中选择一个词即为当前词的概率

107
00:08:35,336 --> 00:08:39,956
换句话说xi就是

108
00:08:39,956 --> 00:08:43,476
文本中词wi正则化的词频

109
00:08:43,476 --> 00:08:48,756
这可以被解释成

110
00:08:48,756 --> 00:08:54,600
随机在d1中选择一个词就是这个词的概率

111
00:08:56,760 --> 00:09:01,620
现在xi的和必然是1
因为它们是正则化后的概率

112
00:09:02,930 --> 00:09:05,750
这意味着这个向量

113
00:09:05,750 --> 00:09:08,193
其实也是一个文档中文字的概率分布

114
00:09:10,500 --> 00:09:15,883
那么向量d2也可以以同样的方法算出来啦

115
00:09:15,883 --> 00:09:23,540
这会给我们代表两个文档的文本概率分布

116
00:09:24,840 --> 00:09:28,220
这样就讲了如何计算向量的问题

117
00:09:28,220 --> 00:09:31,760
现在我们看看用这个方法怎么定义相似度

118
00:09:31,760 --> 00:09:35,668
这里我们就简单定义相似度为两个向量的点击

119
00:09:35,668 --> 00:09:39,890
即两个向量内

120
00:09:41,410 --> 00:09:43,960
每个相对应元素乘积的和

121
00:09:46,630 --> 00:09:51,847
现在其实这个相似度方程很有趣

122
00:09:51,847 --> 00:09:57,360
它有一个很好的解读

123
00:09:57,360 --> 00:10:02,548
这个点积其实是告诉我们文档中

124
00:10:02,548 --> 00:10:08,570
两个随意选择的词完全一样的概率

125
00:10:08,570 --> 00:10:12,630
这就是说如果我们从一个文本中选一个词
然后从另一个文本中

126
00:10:12,630 --> 00:10:17,860
选一个词
然后我们可以问它们一样吗

127
00:10:17,860 --> 00:10:22,650
如果两个文本非常相似
我们可以期待

128
00:10:22,650 --> 00:10:27,390
我们选的词大部分情况下是完全一样的

129
00:10:27,390 --> 00:10:30,900
如果它们非常不一样
那么在两个文本中

130
00:10:30,900 --> 00:10:34,890
选到完全一样的词的概率会很小

131
00:10:34,890 --> 00:10:39,865
所以测量文档相似度方面
这个公式感觉很有道理

132
00:10:41,490 --> 00:10:46,819
现在你可能同时想看看公式

133
00:10:46,819 --> 00:10:51,627
看看为什么它是

134
00:10:51,627 --> 00:10:55,410
两个随机选择的词完全一样的概率表现

135
00:10:57,440 --> 00:11:04,550
如果你盯着公式想一会这个和里都包含着什么

136
00:11:04,550 --> 00:11:12,034
那么你能看到基本上每一个项

137
00:11:12,034 --> 00:11:17,170
都是两个文档中某一个共有词wi的概率

138
00:11:17,170 --> 00:11:23,661
xi给我们在d1中选到wi的概率

139
00:11:23,661 --> 00:11:28,503
而yi给我们在d2中选到wi的概率

140
00:11:28,503 --> 00:11:32,024
当我们在两个文档中选到的词一样时

141
00:11:32,024 --> 00:11:34,920
我们就选了完全一样的词对吧

142
00:11:34,920 --> 00:11:42,380
这就介绍了一种可能的办法，即EOWC
共有词期望

143
00:11:42,380 --> 00:11:49,440
现在像以前一样
我们像评估这个方法是不是有用

144
00:11:49,440 --> 00:11:52,880
当然最终我们必须用真实数据来测试这个方法

145
00:11:52,880 --> 00:11:56,259
看它是不是真的给我们语义相关的词

146
00:11:57,730 --> 00:12:01,010
真的给我们聚合关系

147
00:12:01,010 --> 00:12:05,380
但是我们也可以用分析的眼光分析下这个公式

148
00:12:05,380 --> 00:12:11,020
那么首先我也说过
这个确实有道理对吧

149
00:12:11,020 --> 00:12:15,802
因为这个公式里
如果两个文档的共有词越多值就越大

150
00:12:15,802 --> 00:12:17,988
这就是我们想要的

151
00:12:17,988 --> 00:12:21,170
但是如果你更仔细的分析一下这个公式

152
00:12:21,170 --> 00:12:24,286
你会发现其实有一些隐藏的问题

153
00:12:24,286 --> 00:12:27,735
更准确的说这里有两个隐藏的问题

154
00:12:27,735 --> 00:12:33,935
第一它可能在匹配常见词方面

155
00:12:33,935 --> 00:12:35,795
比匹配特有词做的更好

156
00:12:36,825 --> 00:12:44,300
因为在点积中
如果一个项有很高的值，而且

157
00:12:44,300 --> 00:12:50,190
这个项还都在两个文档里
那么它对最后的和贡献是非常大的

158
00:12:51,250 --> 00:12:55,710
相比于两个有很多不同词频低共有词的向量点积

159
00:12:55,710 --> 00:13:01,150
这一项很有可能让前者最后的值更高

160
00:13:01,150 --> 00:13:06,878
所以这并不是我们想要的

161
00:13:06,878 --> 00:13:09,586
当然这应用在其他方面可能很不错

162
00:13:09,586 --> 00:13:14,527
但是在我们这里
直觉上我们应该更偏好

163
00:13:14,527 --> 00:13:19,645
匹配文档中不一样的词更多的向量
这样我们才更有自信说

164
00:13:19,645 --> 00:13:24,253
这两个词确实经常出现在相似的语境里

165
00:13:24,253 --> 00:13:27,020
如果仅仅靠一个词

166
00:13:27,020 --> 00:13:32,465
那么最终结果是有点问题的
最终模型不一定稳定

167
00:13:34,675 --> 00:13:38,795
现在第二个问题是这个方法对每个词都一样

168
00:13:38,795 --> 00:13:42,131
所以如果我们匹配the这样的词

169
00:13:42,131 --> 00:13:47,443
它的概率可能跟eats差不多

170
00:13:47,443 --> 00:13:52,388
但是直觉上我们知道
一个句子里有the

171
00:13:52,388 --> 00:13:57,816
并不是多吃惊的事
the这个词哪里都会出现

172
00:13:57,816 --> 00:14:02,787
所以相比匹配到像eats这样并不常出现的词

173
00:14:02,787 --> 00:14:07,956
匹配到了the并不算是很有力的证据

174
00:14:07,956 --> 00:14:11,216
这就是这个方法另一个问题

175
00:14:13,426 --> 00:14:19,003
下一讲我们会谈谈如何解决这两个问题

176
00:14:19,003 --> 00:14:29,003
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community