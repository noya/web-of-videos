1
00:00:00,025 --> 00:00:05,098
 这节课是关于

2
00:00:05,098 --> 00:00:09,728
如何在社交网络语境下

3
00:00:09,728 --> 00:00:15,028
进行数据挖掘

4
00:00:15,028 --> 00:00:18,400
在这节课我们将继续讨论语境相关数据挖掘

5
00:00:18,400 --> 00:00:25,000
更精确的说，我们要讨论的是社交网络的语境

6
00:00:26,160 --> 00:00:31,670
那么首先，我们讲一讲以社交网络为语境分析文本的动机

7
00:00:32,970 --> 00:00:36,440
一篇文章的语境就可以组成一个网络

8
00:00:37,460 --> 00:00:40,470
比如说，科研论文的作者们

9
00:00:40,470 --> 00:00:42,000
可能属于同一个协作网络

10
00:00:44,140 --> 00:00:48,300
社交媒介的内容作者也可以组成社交网络

11
00:00:48,300 --> 00:00:51,910
比如在微博，Twitter上大家可以互相关注

12
00:00:51,910 --> 00:01:00,570
或者在微信，脸书上大家可以互加好友

13
00:01:00,570 --> 00:01:07,940
这种语境将不同人产生或分享的内容关联起来

14
00:01:07,940 --> 00:01:12,422
与之相似的，我们也可以收集文字相关的位置信息

15
00:01:12,422 --> 00:01:13,952
从而形成地理网络

16
00:01:13,952 --> 00:01:18,507
一般来说，对相关连文本数据的解释和补充文本

17
00:01:18,507 --> 00:01:22,809
都可以组成某种网络

18
00:01:24,550 --> 00:01:29,650
一般来说，结合社交网络语境

19
00:01:29,650 --> 00:01:34,450
进行文本分析,是有好处的

20
00:01:34,450 --> 00:01:40,590
因为我们可以利用网络去界定文本包含的主题

21
00:01:41,860 --> 00:01:45,716
比如我们可以联想到，同一协作网络中的作者

22
00:01:45,716 --> 00:01:50,560
更有可能写出主题相似的文章

23
00:01:53,760 --> 00:01:57,850
我们可以依赖这种假设去分析文章主题

24
00:01:57,850 --> 00:02:06,540
另一方面，文本也可以帮助定义各子网络的特征

25
00:02:06,540 --> 00:02:10,520
这就是说，文本和网络这两种数据

26
00:02:11,980 --> 00:02:15,370
是相辅相成的

27
00:02:16,850 --> 00:02:21,991
比如说，两个社交网络内相左的观点

28
00:02:21,991 --> 00:02:27,720
可以通过文本和网络协同分析反映出来

29
00:02:30,560 --> 00:02:38,980
下面我简单介绍一下网络监督主题模型

30
00:02:40,380 --> 00:02:43,970
在这张图里我们介绍一些基本概念

31
00:02:43,970 --> 00:02:46,880
下一张图里我们会更详细的展开

32
00:02:48,520 --> 00:02:53,930
但是其实这边我没有太多时间

33
00:02:53,930 --> 00:02:56,940
去深入讲解这些前沿话题

34
00:02:56,940 --> 00:03:01,870
但是我会给出相关论文的引用

35
00:03:01,870 --> 00:03:04,300
这样课后你可以自己深入的了解一下

36
00:03:05,560 --> 00:03:09,400
但是了解大体的概念是有帮助的

37
00:03:09,400 --> 00:03:16,190
如知道能用来作什么以及什么时候可以用

38
00:03:16,190 --> 00:03:22,140
所以下面是网络监督主题模型的总体思路

39
00:03:22,140 --> 00:03:28,320
首先我们来看普通的主题模型

40
00:03:28,320 --> 00:03:33,750
比如用PLSA或者LDA解决最优化问题

41
00:03:33,750 --> 00:03:34,810
当然如图所示

42
00:03:34,810 --> 00:03:38,170
最优化的目标函数是一个似然函数

43
00:03:38,170 --> 00:03:42,910
所以我们一般用最大似然估计的方法去取得函数的参数

44
00:03:42,910 --> 00:03:47,456
这些参数即是我们从文本数据

45
00:03:47,456 --> 00:03:50,000
中得到的有效信息

46
00:03:50,000 --> 00:03:51,760
比如说，主题信息

47
00:03:51,760 --> 00:03:56,590
在这个方程里，我们想得到在已知标记为Lambda的参数的情况下

48
00:03:56,590 --> 00:04:01,490
可以取得的文本数据(TextData)的最大概率

49
00:04:01,490 --> 00:04:06,490
在这里引入网络

50
00:04:06,490 --> 00:04:12,520
主要就是通过网络增加限制条件

51
00:04:12,520 --> 00:04:16,110
基本上就是利用网络

52
00:04:16,110 --> 00:04:20,350
限制模型参数，这里标记为Lambda

53
00:04:20,350 --> 00:04:21,010
比如

54
00:04:21,010 --> 00:04:27,340
在网络相邻节点上的文本应该是包含相同的主题

55
00:04:27,340 --> 00:04:31,360
有很多例子可以反映，他们是包含相同主题的

56
00:04:34,010 --> 00:04:38,039
我们也可以利用这个方法

57
00:04:39,170 --> 00:04:43,510
让网络节点的主题分布变得平整

58
00:04:43,510 --> 00:04:48,253
也就是让每个关联节点的主题更相似

59
00:04:48,253 --> 00:04:53,780
这样每个相连节点主题的分布基本是一样的

60
00:04:53,780 --> 00:05:00,460
或者是在覆盖相同主题的前提下，分布有一点不一样

61
00:05:02,210 --> 00:05:07,200
理论上，我们可以像这样很简单的加一些

62
00:05:07,200 --> 00:05:11,530
由网络引导的正则项在目标函数的似然函数上

63
00:05:11,530 --> 00:05:14,480
不同于之前的只优化似然函数

64
00:05:14,480 --> 00:05:18,590
我们要优化一个新的函数 f

65
00:05:19,770 --> 00:05:26,400
这个f方程同时包括似然函数和正则项函数r

66
00:05:26,400 --> 00:05:31,820
这个由参数Lambda和网络定义的正则项

67
00:05:31,820 --> 00:05:34,330
基本就是要告诉我们

68
00:05:34,330 --> 00:05:38,540
网络更加偏好界定的参数值

69
00:05:38,540 --> 00:05:41,470
你应该能察觉到这里的建模思路其实就是

70
00:05:41,470 --> 00:05:45,730
在原有模型参数上加了某种先验

71
00:05:45,730 --> 00:05:50,060
当然这里的模型不一定是概率模型

72
00:05:50,060 --> 00:05:51,140
但是道理是一样的

73
00:05:51,140 --> 00:05:55,450
我们就是要在同一个目标方程里结合这两项

74
00:05:57,770 --> 00:06:02,130
这个模型好处是非常通用

75
00:06:02,130 --> 00:06:06,270
这里的主题模型可以是任何通用的文本模型

76
00:06:07,350 --> 00:06:11,370
不一定非要是PLSA或者LDA模型，或是其他当前流行的模型

77
00:06:12,830 --> 00:06:17,080
同样的，网络也可以是任意网络

78
00:06:17,080 --> 00:06:19,280
任何连接文本对象的图都可以

79
00:06:22,531 --> 00:06:26,470
这个正则项也可以任选

80
00:06:26,470 --> 00:06:31,440
我们可以随意选择想要着重抓取的，我们认为重要的信息

81
00:06:32,590 --> 00:06:36,210
最后，函数f也是可变的

82
00:06:36,210 --> 00:06:38,840
所有这些变量有很多组合的方式

83
00:06:38,840 --> 00:06:42,490
因此这个理念其实是非常非常强大的

84
00:06:42,490 --> 00:06:47,530
它提供了一个通用的方法将不同种类的数据

85
00:06:47,530 --> 00:06:52,620
综合到一单一的优化框架上

86
00:06:52,620 --> 00:06:54,809
而且这个模型显然可以解决很多问题

87
00:06:56,900 --> 00:06:59,280
下面我们具体讨论这篇参考论文里

88
00:06:59,280 --> 00:07:05,540
一个叫NetPLSA的具体实例

89
00:07:05,540 --> 00:07:06,962
在这里，就是

90
00:07:06,962 --> 00:07:11,990
PLSA模型的一个拓展，在模型中加入网络限制

91
00:07:11,990 --> 00:07:15,730
而这里的先验条件就是

92
00:07:15,730 --> 00:07:18,070
网络的相邻内容必须有相似的主题分布

93
00:07:18,070 --> 00:07:20,500
互联文章必须用相似的方式阐述类似的主题

94
00:07:20,500 --> 00:07:22,720
简单的说起来大概就是这样

95
00:07:24,080 --> 00:07:27,470
更严谨的说，我们这有一个更改过的目标方程

96
00:07:27,470 --> 00:07:32,780
这里定义了文本集C和网络图G

97
00:07:34,070 --> 00:07:36,050
如果你观察一下这个方程

98
00:07:36,050 --> 00:07:38,489
你其实可以认出很多熟悉的地方

99
00:07:40,100 --> 00:07:45,046
现在应该很熟悉了吧

100
00:07:45,046 --> 00:07:49,182
那么你能看出哪一部分是

101
00:07:49,182 --> 00:07:51,720
主题模型生成特定文档的似然函数吗

102
00:07:52,720 --> 00:07:58,962
如果你看看它，就能看出这一部分完全就是PLSA对数似然

103
00:07:58,962 --> 00:08:04,160
我们可以只通过最大化它来估计PLSA模型参数

104
00:08:04,160 --> 00:08:10,305
但是第二个方程引入了对参数更多的限制

105
00:08:10,305 --> 00:08:15,610
特别是在这里，它是来表示

106
00:08:15,610 --> 00:08:21,300
节点u和节点v主题覆盖的差异

107
00:08:21,300 --> 00:08:25,570
这两个节点在网络里是相连的

108
00:08:25,570 --> 00:08:27,680
我们希望它们的概率分布是相似的

109
00:08:27,680 --> 00:08:31,880
所以这里我们计算出这两项差值的平方

110
00:08:31,880 --> 00:08:34,400
我们希望这个差达到最小

111
00:08:34,400 --> 00:08:40,825
注意这个总和符号，最前面的总和符号前面有个负号

112
00:08:40,825 --> 00:08:46,204
因为这个，我们可以找到

113
00:08:46,204 --> 00:08:51,385
我们需要的参数

114
00:08:51,385 --> 00:08:57,780
这个参数可以让PLSA概率似然达到最大

115
00:08:57,780 --> 00:09:01,560
也就是模型可以很好的反应已知数据的分布

116
00:09:01,560 --> 00:09:05,600
也同时可以考虑到网络的限制

117
00:09:06,700 --> 00:09:09,640
而这个我刚说的负号

118
00:09:09,640 --> 00:09:12,742
因为它使这一项为负，当我们最大化

119
00:09:12,742 --> 00:09:16,780
目标方程时，我们其实是在最小化这一项

120
00:09:19,589 --> 00:09:23,906
如果更进一步分析这里，我们能看到

121
00:09:23,906 --> 00:09:29,560
这里有一个u,v间距离的权重的乘数

122
00:09:29,560 --> 00:09:32,120
这是来自我们的网络

123
00:09:32,120 --> 00:09:34,495
如果你的权重表达出

124
00:09:34,495 --> 00:09:38,470
这两个点代表的研究人群有很强的协同关系

125
00:09:38,470 --> 00:09:45,510
或者这两个点代表两个在社交网络里联系很紧密的人

126
00:09:45,510 --> 00:09:46,740
那么这个权重应该很高

127
00:09:46,740 --> 00:09:52,590
这就表示让两点的主题覆盖类似很重要

128
00:09:52,590 --> 00:09:54,070
这就是这里想表达的意思

129
00:09:55,460 --> 00:09:57,660
最后这里的Lambda参数

130
00:09:57,660 --> 00:10:02,260
这个新参数是用来调节网络限制的影响力

131
00:10:02,260 --> 00:10:07,460
很容易看出，如果Lambda为0，我们就只有一个正统的PLSA模型

132
00:10:07,460 --> 00:10:09,470
但是如果Lambda是个较大的数值

133
00:10:09,470 --> 00:10:14,920
我们就是要让网络对估量模型的影响变大

134
00:10:14,920 --> 00:10:19,650
所以你可以看出，这样做的结果就是，我们是要建基本的PLSA模型

135
00:10:19,650 --> 00:10:24,020
但是我们也希望网络上相邻两点

136
00:10:24,020 --> 00:10:28,860
能在网络中有很强的关联，这样它们的主题覆盖就更类似

137
00:10:28,860 --> 00:10:30,930
我们需要保证它们的覆盖更相似

138
00:10:33,800 --> 00:10:37,860
所以这里抽取论文里的一些结果

139
00:10:37,860 --> 00:10:41,440
这个图告诉我们使用PLSA模型记录到的结果

140
00:10:41,440 --> 00:10:45,917
这里使用的训练集是DBLP

141
00:10:45,917 --> 00:10:48,970
是关于研究论文的书目索引训练集

142
00:10:48,970 --> 00:10:55,140
这里的这些实验用有四个应用群落

143
00:10:55,140 --> 00:10:56,800
IR是信息检索

144
00:10:56,800 --> 00:10:59,400
DM是数据挖掘

145
00:10:59,400 --> 00:11:00,860
ML是机器学习，还有互联网

146
00:11:00,860 --> 00:11:05,240
这就是4种类型的文章群落，我们希望

147
00:11:06,780 --> 00:11:14,590
主题挖掘可以帮助我们分辨出这四个群落

148
00:11:14,590 --> 00:11:19,860
但是这里，从PLSA生成的主题集你们都可以看出

149
00:11:19,860 --> 00:11:24,400
PLSA没有办法生成四种

150
00:11:24,400 --> 00:11:26,750
符合我们直觉的群落

151
00:11:26,750 --> 00:11:30,310
这是因为这些文本全部混在一起

152
00:11:30,310 --> 00:11:33,620
而这四个群落有很多共用的词汇

153
00:11:33,620 --> 00:11:37,900
所以把他们分成四个群落并不容易

154
00:11:37,900 --> 00:11:41,750
可能如果我们引入更多的主题，我们可以有相干性更高的结果

155
00:11:42,960 --> 00:11:48,420
但是有趣的是如果我们使用NetPLSA模型

156
00:11:48,420 --> 00:11:54,180
就是在这里利用论文作者所在的协作网络来增加限制条件

157
00:11:54,180 --> 00:11:57,210
这里我们依旧用这四个群落

158
00:11:57,210 --> 00:12:01,780
则NetPLSA可以给出更有意义的主题集

159
00:12:01,780 --> 00:12:07,690
这里我们能看出四个主题集很映衬四个群落

160
00:12:07,690 --> 00:12:09,340
第一个是信息检索

161
00:12:09,340 --> 00:12:11,260
第二个是数据挖掘

162
00:12:11,260 --> 00:12:12,410
第三个是机器学习

163
00:12:12,410 --> 00:12:13,970
第四个是互联网

164
00:12:13,970 --> 00:12:18,771
这个分类主要是网络的影响

165
00:12:18,771 --> 00:12:24,000
因为我们引入了协作网络信息

166
00:12:24,000 --> 00:12:28,300
组成同一个协作网络里的人大体上

167
00:12:28,300 --> 00:12:32,280
让人觉得会写关于相同主题的文章

168
00:12:32,280 --> 00:12:35,210
这也是我们会有更相关主题集的原因

169
00:12:35,210 --> 00:12:39,500
如果我们只单独考虑文本出现的频率

170
00:12:39,500 --> 00:12:42,700
是没有办法得到这么好的结果的

171
00:12:42,700 --> 00:12:45,720
虽然说，像PLSA，LDA这样的主题模型

172
00:12:45,720 --> 00:12:50,790
也会察觉到经常同时出现的词汇

173
00:12:50,790 --> 00:12:55,581
总的来说他们生成的主题集

174
00:12:55,581 --> 00:12:58,680
可以反映出一些同时出现词汇的相干性

175
00:12:58,680 --> 00:13:03,980
但是他们没有办法产生如此相关的结果

176
00:13:03,980 --> 00:13:07,700
这就证明了网络在这里非常有用

177
00:13:08,740 --> 00:13:13,143
类似的模型还可以用来区分

178
00:13:13,143 --> 00:13:16,270
协作网络中各子网络的内容

179
00:13:19,497 --> 00:13:24,585
更进一步看以网络为语境的数据挖掘

180
00:13:24,585 --> 00:13:29,870
你可以想象文本是存在于一个富含丰富信息的网络环境中

181
00:13:29,870 --> 00:13:35,700
这意味着我们可以把所有相关的数据连成一个大网络

182
00:13:35,700 --> 00:13:41,750
文本数据还可以通过不同的方式依附于网络

183
00:13:41,750 --> 00:13:46,420
比如，文本数据可以在网络的点上

184
00:13:46,420 --> 00:13:51,100
这基本就是我们在NetPLSA里讲到的情况

185
00:13:51,100 --> 00:13:56,614
但是文本数据还可以关联于时间，或者路径上，或者是子网络等

186
00:13:56,614 --> 00:14:01,010
在大语境环境下用这些方式表达文本

187
00:14:01,010 --> 00:14:04,170
是非常有效的

188
00:14:04,170 --> 00:14:09,860
因为这让我们可以把全部信息和数据联合起来分析

189
00:14:09,860 --> 00:14:16,130
所以结论是，文本分析应该和整个

190
00:14:16,130 --> 00:14:21,270
跟文本有关的网络信息结合起来

191
00:14:21,270 --> 00:14:23,350
最后这里是一篇推荐阅读的论文

192
00:14:23,350 --> 00:14:27,742
这就是关于NetPLSA的论文，你可以了解关于这个模型更多的细节

193
00:14:27,742 --> 00:14:29,300
以及如何估量这个模型

194
00:14:29,300 --> 00:14:39,300
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community