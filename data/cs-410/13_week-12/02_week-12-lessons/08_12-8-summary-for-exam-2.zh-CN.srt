1
00:00:06,910 --> 00:00:09,570
这节课是对整个课程的一个总结

2
00:00:10,810 --> 00:00:14,512
先让我们温习下本课程涵盖的主题

3
00:00:14,512 --> 00:00:18,662
我们先是谈到了自然语言处理

4
00:00:18,662 --> 00:00:21,025
和它如何丰富文本表述

5
00:00:21,025 --> 00:00:26,207
然后，我们谈到了如何从文本和数据中挖掘

6
00:00:26,207 --> 00:00:29,434
自然语言所表达的

7
00:00:29,434 --> 00:00:33,270
人们观察到的世界中的知识

8
00:00:34,320 --> 00:00:38,410
特别地，我们谈到了如何挖掘词的关联

9
00:00:38,410 --> 00:00:42,300
然后我们谈到了如何分析文本的主题

10
00:00:42,300 --> 00:00:45,130
如何发现并分析主题

11
00:00:47,580 --> 00:00:51,314
这可以看作是我们观察到的世界的知识

12
00:00:51,314 --> 00:00:55,747
然后我们谈到了如何挖掘关于观察者的知识

13
00:00:55,747 --> 00:01:00,988
尤其是分析他们的意见和情感

14
00:01:00,988 --> 00:01:06,048
最后我们讨论了基于文本的预测

15
00:01:06,048 --> 00:01:11,204
既基于文本数预测等真实世界中的其它变量

16
00:01:11,204 --> 00:01:16,270
同时我们也讨论了非文本数据的作用

17
00:01:16,270 --> 00:01:21,421
它们是预测问题中的其它因素

18
00:01:21,421 --> 00:01:25,425
并且还可以为文本数据分析提供上下文

19
00:01:25,425 --> 00:01:30,110
我们特别谈到了如何联系上下文分析主题

20
00:01:33,240 --> 00:01:39,078
以上是这门课的一个概括

21
00:01:39,078 --> 00:01:41,670
下面我会讲到这些主题

22
00:01:41,670 --> 00:01:46,400
并且指出你应该记住哪些关键内容

23
00:01:47,560 --> 00:01:50,630
首先是自然语言处理和文本表达

24
00:01:53,530 --> 00:01:56,840
要知道自然语言处理对任何文本的再创制作都是非常重要的

25
00:01:56,840 --> 00:02:01,510
因为它能丰富文本表达

26
00:02:01,510 --> 00:02:05,060
自然语言处理做得越好 我们就能更好地进行文本表达

27
00:02:05,060 --> 00:02:08,500
这也使知识发现更加准确

28
00:02:08,500 --> 00:02:11,710
得以从文本中发现更深层次的知识

29
00:02:12,950 --> 00:02:17,510
然而 现今的文本挖掘水平

30
00:02:17,510 --> 00:02:19,130
仍然不够好

31
00:02:19,130 --> 00:02:23,586
因此如今的稳健文本挖掘技术

32
00:02:23,586 --> 00:02:26,960
是基于单词的表征

33
00:02:26,960 --> 00:02:30,710
更多依赖于统计分析

34
00:02:30,710 --> 00:02:33,520
正如我们之前讨论过的

35
00:02:33,520 --> 00:02:39,700
你可能还记得我们主要用的是基于词的表征法

36
00:02:39,700 --> 00:02:42,478
我们大量依赖于统计技术

37
00:02:42,478 --> 00:02:45,202
尤其是统计学习的技术

38
00:02:47,790 --> 00:02:52,771
在词关联挖掘和分析当中
重点首先是

39
00:02:52,771 --> 00:02:56,282
我们引入了两个互补的词关联基本概念

40
00:02:56,282 --> 00:03:02,835
聚合关系和组合关系

41
00:03:02,835 --> 00:03:08,130
它们是元素序列之间通常存在的关系

42
00:03:08,130 --> 00:03:14,330
如果说到意思
指的分别就是

43
00:03:14,330 --> 00:03:18,840
序列中在相似情境中出现的元素
以及会同时出现的元素

44
00:03:18,840 --> 00:03:24,090
这些关系对于数据中其他序列可能也是有价值的

45
00:03:25,810 --> 00:03:29,810
我们还谈到了如何测量这种相似性

46
00:03:29,810 --> 00:03:34,350
然后我们提到如何去发掘聚合关系

47
00:03:34,350 --> 00:03:38,390
我们去比较词的情境
发现那些具有相同情境的词

48
00:03:38,390 --> 00:03:39,858
在这点上

49
00:03:39,858 --> 00:03:44,437
我们谈到了如何将文本数据表征为<br />一个向量空间模型

50
00:03:44,437 --> 00:03:48,638
我们讨论到了一些检索技术
像是BM25

51
00:03:48,638 --> 00:03:52,995
来衡量相似性和对语素分配权重

52
00:03:52,995 --> 00:03:55,193
及tf-idf权重等

53
00:03:55,193 --> 00:03:59,480
这部分跟文本检索是紧密相关的

54
00:03:59,480 --> 00:04:02,330
还有其他一些相关的技术

55
00:04:03,890 --> 00:04:08,650
下一点是关于文本共生分析

56
00:04:08,650 --> 00:04:12,770
我们介绍了一些信息论的概念
比如熵、

57
00:04:12,770 --> 00:04:15,170
条件熵和互信息

58
00:04:15,170 --> 00:04:18,293
他们不仅在

59
00:04:18,293 --> 00:04:23,680
衡量文本共生上十分有用

60
00:04:23,680 --> 00:04:26,940
在其他类型数据当中也能用到

61
00:04:26,940 --> 00:04:29,600
比如说文本分类中的特征选择

62
00:04:30,920 --> 00:04:34,460
那么这是另外一个重要的概念
需要了解

63
00:04:35,480 --> 00:04:38,640
然后讲到的是话题模型的挖掘与分析

64
00:04:38,640 --> 00:04:41,570
这里我们介绍了概率性话题模型

65
00:04:41,570 --> 00:04:45,960
我们花了许多时间来解释基本的话题模型

66
00:04:45,960 --> 00:04:52,930
PLSA模型
这个是理解LDA模型的基础

67
00:04:52,930 --> 00:04:56,190
理论上LDA是一个更关注意见的模型

68
00:04:56,190 --> 00:05:01,460
但是我们没有足够的时间
深入的讲解LDA模型

69
00:05:02,960 --> 00:05:06,600
但是实际当中
PLSA模型看起来和LDA模型一样有效

70
00:05:06,600 --> 00:05:09,520
更容易实施
也更为高效

71
00:05:11,520 --> 00:05:15,930
这里我们会讲一些有用的基本概念

72
00:05:15,930 --> 00:05:20,410
一个是生成模型
这是一个

73
00:05:20,410 --> 00:05:23,630
对文本及其他类型数据建模的一般方法

74
00:05:24,740 --> 00:05:30,250
我们讲到了极大似然估计
EM算法

75
00:05:30,250 --> 00:05:35,290
来计算极大似然的估计值

76
00:05:35,290 --> 00:05:38,720
这些都是些一般的方法

77
00:05:38,720 --> 00:05:39,840
它们在其他情境当中也适用

78
00:05:40,940 --> 00:05:45,020
然后我们讲到了文本聚类和分本分类

79
00:05:45,020 --> 00:05:50,450
这是所有文本挖掘应用当中重要的两块

80
00:05:50,450 --> 00:05:56,110
在文本聚类当中
我们讲到了如何通过

81
00:05:56,110 --> 00:06:02,400
利用一个稍有不同的混合模型来解决问题
而不是概率性话题模型

82
00:06:02,400 --> 00:06:07,060
我们也倾向于使用一些相似性的方法

83
00:06:07,060 --> 00:06:10,000
来检测文本聚类

84
00:06:11,340 --> 00:06:15,350
在分本分类上
我们也讲到了两类方法

85
00:06:15,350 --> 00:06:19,390
一个生成性分类器
这个方法是基于贝叶斯规则

86
00:06:20,690 --> 00:06:24,870
来推断给定文本数据的条件概率

87
00:06:24,870 --> 00:06:28,250
更深一步
我们详细介绍了朴素贝叶斯

88
00:06:29,280 --> 00:06:36,160
这是一个技术的具体应用
针对的是大量文本的分类问题

89
00:06:37,210 --> 00:06:41,010
我们也讲了一些判别类的分类器

90
00:06:41,010 --> 00:06:45,300
尤其是逻辑回归、K最邻近法和SBN算法

91
00:06:45,300 --> 00:06:49,030
它们都是很重要、很流行、很有用的方法

92
00:06:49,030 --> 00:06:50,490
在文本分类当中

93
00:06:52,370 --> 00:06:57,100
在这两个部分
我们还将讨论如何评估结果

94
00:06:57,100 --> 00:07:03,110
评估也是很重要的
如果你用的匹配结果

95
00:07:03,110 --> 00:07:07,430
没有反映出方法上的波动性
它可能会给出误导性的结果

96
00:07:07,430 --> 00:07:10,530
所以评估正确是很重要的

97
00:07:10,530 --> 00:07:15,420
我们细致讨论过了分类的评估问题

98
00:07:15,420 --> 00:07:16,550
包括许多特别的指标

99
00:07:18,530 --> 00:07:21,725
然后我们讲了情感分析及范例

100
00:07:21,725 --> 00:07:25,053
这里我们介绍了情感分类问题

101
00:07:25,053 --> 00:07:29,681
虽然说这是文本分类的一个特例

102
00:07:29,681 --> 00:07:34,932
我们讲了如何通过使用更为复杂的特征

103
00:07:34,932 --> 00:07:41,261
延展和改进文本分类方法
这些内容在情感分析当中都能用得到

104
00:07:41,261 --> 00:07:46,240
我们回顾了一些文本分析中
使用复杂特征的一般方法

105
00:07:46,240 --> 00:07:50,836
然后我们讲到了在情感分类当中
如何捕捉类别上的顺序

106
00:07:50,836 --> 00:07:55,511
尤其是

107
00:07:55,511 --> 00:08:00,822
我们引入了顺序逻辑回归
我们还讲了潜在方面评分分析

108
00:08:00,822 --> 00:08:05,104
这是一种利用生成模型

109
00:08:05,104 --> 00:08:07,280
来理解评论数据的无监督方法

110
00:08:07,280 --> 00:08:12,650
特别的是它能够让我们理解评论者

111
00:08:14,580 --> 00:08:18,490
话题不同方面的复合评分

112
00:08:18,490 --> 00:08:20,998
给定具有综合评分的文本评论

113
00:08:20,998 --> 00:08:24,503
这个方法给出了不同方面的进一步评分

114
00:08:24,503 --> 00:08:26,781
它也让我们能够推断

115
00:08:26,781 --> 00:08:30,638
评论者在方面上的权重

116
00:08:30,638 --> 00:08:35,740
或者说哪些方面对评论者更为重要
也能够得到展现

117
00:08:35,740 --> 00:08:39,140
这就带来了许多有趣的应用

118
00:08:41,330 --> 00:08:46,260
最后在预测的讨论上
我们主要说的是

119
00:08:46,260 --> 00:08:50,340
文本和非文本数据的联合挖掘
因为两者对于预测都是很重要的

120
00:08:51,960 --> 00:08:57,070
我们尤其讲到了如何通过文本数据补充非文本数据
反之亦然

121
00:08:58,100 --> 00:09:01,863
在使用非文本数据补充文本数据分析当中

122
00:09:01,863 --> 00:09:04,565
我们讲到了背景文本挖掘

123
00:09:04,565 --> 00:09:08,921
我们介绍了背景PLSA
算是一个PLSA的通用模型

124
00:09:08,921 --> 00:09:13,354
然后我们能够将变量的背景加入分析当中
像是时间和位置

125
00:09:13,354 --> 00:09:18,328
这让我们能够从文本数据当中

126
00:09:18,328 --> 00:09:20,248
发现许多有趣的话题模式

127
00:09:20,248 --> 00:09:24,750
我们还讲了网络PLSA
这里我们利用了社交网络

128
00:09:24,750 --> 00:09:30,550
或者说文本数据中广义的网络来分析话题

129
00:09:31,950 --> 00:09:36,520
最后我们讲时间序列如何可以作为背景

130
00:09:36,520 --> 00:09:40,560
加入到文本数据的因果话题挖掘当中

131
00:09:43,110 --> 00:09:46,560
其他利用文本的方法

132
00:09:47,990 --> 00:09:51,470
来协助解释非文本数据中的模式

133
00:09:51,470 --> 00:09:57,300
我们没有细讲
只是作为一个参考

134
00:09:57,300 --> 00:10:02,670
但是我应该说这是一个
我们要了解的很重要的方向

135
00:10:02,670 --> 00:10:06,700
如果你要建立一个文本挖掘体系

136
00:10:06,700 --> 00:10:10,730
因为理解和解释模式特别重要

137
00:10:13,870 --> 00:10:18,560
这就是主要内容的一个总结

138
00:10:18,560 --> 00:10:22,710
我希望这些对于你

139
00:10:22,710 --> 00:10:27,010
建立文本挖掘应用
或是了解这些算法能够有用

140
00:10:27,010 --> 00:10:31,100
这应该能够在你研究论文上提供了一个很好的基础

141
00:10:31,100 --> 00:10:33,580
来了解更多更先进的算法

142
00:10:33,580 --> 00:10:37,320
或者发明新的算法

143
00:10:40,320 --> 00:10:43,760
那么想要了解更多这方面的内容

144
00:10:43,760 --> 00:10:47,519
我建议你可以更深入地了解其他领域

145
00:10:48,550 --> 00:10:51,820
由于课程时间有限

146
00:10:51,820 --> 00:10:57,830
我们只是涉及了文本挖掘的基本概念和基本原则

147
00:10:57,830 --> 00:11:03,390
我们强调了一些实用的算法

148
00:11:03,390 --> 00:11:09,128
这也就在内容上牺牲了一些更为先进的算法

149
00:11:09,128 --> 00:11:15,062
许多情况下我们也忽视了许多先进算法的讨论

150
00:11:15,062 --> 00:11:19,240
要了解更多这个主题的内容

151
00:11:19,240 --> 00:11:22,120
你需要学习更多自然语言学习的内容

152
00:11:22,120 --> 00:11:24,200
因为这是所有文本类应用的基础

153
00:11:24,200 --> 00:11:28,790
你可以做的NLP越多
你可以得到的附加文本就越好

154
00:11:28,790 --> 00:11:32,520
你也能发现更深层的知识

155
00:11:32,520 --> 00:11:34,010
这点十分重要

156
00:11:37,010 --> 00:11:39,910
第二个要看的领域就是统计机器学习

157
00:11:41,120 --> 00:11:45,090
这些技术现在不仅是在应用文本分析

158
00:11:46,160 --> 00:11:49,970
在自然语言处理上也是基本的技术

159
00:11:49,970 --> 00:11:55,310
现在许多自然语言处理技术都是来自于有监督的机器学习

160
00:11:56,900 --> 00:12:00,790
它们十分重要
是因为

161
00:12:00,790 --> 00:12:04,570
在理解一些前沿的自然语言处理技术很关键

162
00:12:04,570 --> 00:12:08,220
自然它们也能够为文本分析提供更多工具

163
00:12:09,770 --> 00:12:13,930
现在一个特别有趣的领域

164
00:12:13,930 --> 00:12:17,640
叫做深度学习
今年吸引了许多关注

165
00:12:17,640 --> 00:12:21,110
在许多应用领域也体现出了一些希望

166
00:12:21,110 --> 00:12:26,660
尤其是在语音和视觉方面
在文本数据当中也有所应用

167
00:12:26,660 --> 00:12:30,820
比如最近有一项研究
使用深度学习进行分段分析

168
00:12:30,820 --> 00:12:34,330
得到了更好的精确度

169
00:12:34,330 --> 00:12:38,320
这是其中一个我们没有讲到

170
00:12:38,320 --> 00:12:40,050
但是也十分重要的前沿技术

171
00:12:41,390 --> 00:12:45,400
其他在统计学习当中崭露头角的是

172
00:12:45,400 --> 00:12:50,720
词汇嵌入技术
可以获得词汇向量化的状态

173
00:12:50,720 --> 00:12:55,210
这种状态就可以计算词汇间的相似度

174
00:12:55,210 --> 00:12:55,820
如你所见

175
00:12:55,820 --> 00:13:01,230
这种方法可以直接发现词汇间的聚合关系

176
00:13:01,230 --> 00:13:06,600
得到的结果让人印象深刻

177
00:13:06,600 --> 00:13:10,360
那是另外一个我们没有讲到
但有很好发展前景的领域

178
00:13:12,510 --> 00:13:16,290
但是当然

179
00:13:16,290 --> 00:13:20,970
这些新的技术是否会是实用的技术
是否会比现有技术有很大提升

180
00:13:20,970 --> 00:13:25,172
还需要检验

181
00:13:25,172 --> 00:13:28,000
现在还没有严格的评估

182
00:13:28,000 --> 00:13:32,310
比如说检验词汇嵌入的实用性

183
00:13:32,310 --> 00:13:34,990
相比其他对于词汇相似性检验的评估

184
00:13:36,710 --> 00:13:39,650
然而这些前沿的技术

185
00:13:39,650 --> 00:13:43,520
在未来肯定会在文本挖掘领域占据一席之地

186
00:13:43,520 --> 00:13:46,860
所以了解这些也会十分重要

187
00:13:46,860 --> 00:13:50,780
统计学习在预测性建模中也十分关键

188
00:13:50,780 --> 00:13:55,180
尤其是在大数据的应用当中
我们没有讲到预测性建模的内容

189
00:13:55,180 --> 00:13:59,994
但是主要是关于回归和分类技术的

190
00:13:59,994 --> 00:14:05,050
这是统计学习重要性的另外一点原因

191
00:14:07,350 --> 00:14:11,730
我们也建议你多学一点数据挖掘

192
00:14:11,730 --> 00:14:16,610
这只是因为一般的数据挖掘算法
总是可以应用到文本数据当中

193
00:14:16,610 --> 00:14:21,660
只是一般数据的一个特例

194
00:14:23,520 --> 00:14:26,030
数据挖掘技术有许多的应用

195
00:14:26,030 --> 00:14:30,510
比如说对于模式的探索

196
00:14:30,510 --> 00:14:35,860
能够生成文本分析中需要的特征

197
00:14:35,860 --> 00:14:40,940
数据挖掘技术也可以用来分析文本信息网络

198
00:14:42,360 --> 00:14:44,980
这些内容都值得了解

199
00:14:44,980 --> 00:14:49,050
为了开发出更有效率的文本分析技术

200
00:14:49,050 --> 00:14:52,860
最后我们也建议了解更多关于文本检索的内容

201
00:14:52,860 --> 00:14:55,930
还有搜索引擎上信息检索的内容

202
00:14:55,930 --> 00:15:00,403
这点对于建立使用的文本分析应用体系

203
00:15:00,403 --> 00:15:02,750
尤其重要

204
00:15:02,750 --> 00:15:05,950
搜索引擎在任何基于文本的应用当中

205
00:15:05,950 --> 00:15:08,632
都会是一个重要的体系

206
00:15:08,632 --> 00:15:13,910
这是因为文本数据是为了人们消费而产生的

207
00:15:13,910 --> 00:15:19,330
所以人类是理解文本数据最重要的群体

208
00:15:19,330 --> 00:15:24,910
所以在大型文本数据应用当中
需要把人类包含进来

209
00:15:24,910 --> 00:15:29,870
特别会从两个方面帮助到文本挖掘体系

210
00:15:29,870 --> 00:15:35,099
一是有效地将数据从很大的集合

211
00:15:35,099 --> 00:15:40,158
缩小到最相关数据的小集合上

212
00:15:40,158 --> 00:15:42,627
只与特定的解释相关

213
00:15:42,627 --> 00:15:47,901
另一点就是能够提供一种诠释模式的方式

214
00:15:47,901 --> 00:15:51,521
这点会与只是的重要性相关

215
00:15:51,521 --> 00:15:54,853
一旦我们发现了一些知识
我们要去分析

216
00:15:54,853 --> 00:15:57,370
这个发现是否可靠

217
00:15:57,370 --> 00:16:00,000
我们就要回到原文本去验证

218
00:16:00,000 --> 00:16:02,380
这就是为什么搜索引擎很重要

219
00:16:04,070 --> 00:16:08,040
另外一些信息检索的技术

220
00:16:08,040 --> 00:16:13,380
比如BM25、向量空间和语言模型在文本挖掘中也很有用

221
00:16:13,380 --> 00:16:16,400
我们只提到了其中的一部分
但是如果你知道更多关于文本检索的知识

222
00:16:16,400 --> 00:16:20,500
你会发现还有许多有用的技术

223
00:16:20,500 --> 00:16:25,030
另一个有用的技术就是索引

224
00:16:25,030 --> 00:16:28,450
它能够对用户的查询做出迅速的反应

225
00:16:28,450 --> 00:16:32,150
这在建立有效的文本挖掘体系当中也很有用

226
00:16:35,160 --> 00:16:39,830
最后我要提醒你的是

227
00:16:39,830 --> 00:16:43,900
这种处理大型文本数据的大图
我在学期开始的时候展示过

228
00:16:45,350 --> 00:16:48,970
一般来讲
建立一个大量文本的应用体系

229
00:16:48,970 --> 00:16:51,760
我们需要两种技术
文本检索和文本挖掘

230
00:16:53,380 --> 00:16:58,040
文本检索我已经解释过
是为了将大型的文本数据

231
00:16:58,040 --> 00:17:02,930
转化为一个与问题最为相关的小型数据

232
00:17:02,930 --> 00:17:07,240
这也能够得到知识的重要性
应用到模式的解释当中

233
00:17:07,240 --> 00:17:12,060
文本挖掘进一步分析相关数据

234
00:17:12,060 --> 00:17:16,460
发现可供决策的知识

235
00:17:16,460 --> 00:17:18,510
用以决策会是其他任务

236
00:17:18,510 --> 00:17:20,530
这门课是关于文本挖掘

237
00:17:20,530 --> 00:17:24,020
还有一门伴随的课程

238
00:17:24,020 --> 00:17:27,130
是文本检索与搜索引擎
是关于文本检索的内容

239
00:17:27,130 --> 00:17:32,040
如果你还没上过那门课
上那门课会很有用

240
00:17:32,040 --> 00:17:37,490
尤其是你要做文本缓存系统的话

241
00:17:37,490 --> 00:17:42,138
完成两门课能够提供给你

242
00:17:42,138 --> 00:17:43,708
建立这样系统的所有知识

243
00:17:43,708 --> 00:17:49,250
最后的最后
我想感谢你

244
00:17:49,250 --> 00:17:51,050
来上这门课程。

245
00:17:51,050 --> 00:17:57,915
我希望你获得了
文本挖掘与分析相关有用的技能与知识

246
00:17:57,915 --> 00:18:02,185
正如我们讨论当中所说
现在有许多机会

247
00:18:02,185 --> 00:18:06,235
是关于这类技术的
也有许多的空间

248
00:18:06,235 --> 00:18:10,910
我希望你们能够利用所学

249
00:18:10,910 --> 00:18:15,550
创造许多对社会有用的应用

250
00:18:15,550 --> 00:18:20,759
加入到文本挖掘与分析新技术开发的研究团队当中来

251
00:18:20,759 --> 00:18:21,259
谢谢大家

252
00:18:21,259 --> 00:18:31,259
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community