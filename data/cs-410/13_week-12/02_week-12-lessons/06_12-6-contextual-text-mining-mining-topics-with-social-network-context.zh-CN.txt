这节课是关于 如何在社交网络语境下 进行数据挖掘 在这节课我们将继续讨论语境相关数据挖掘 更精确的说，我们要讨论的是社交网络的语境 那么首先，我们讲一讲以社交网络为语境分析文本的动机 一篇文章的语境就可以组成一个网络 比如说，科研论文的作者们 可能属于同一个协作网络 社交媒介的内容作者也可以组成社交网络 比如在微博，Twitter上大家可以互相关注 或者在微信，脸书上大家可以互加好友 这种语境将不同人产生或分享的内容关联起来 与之相似的，我们也可以收集文字相关的位置信息 从而形成地理网络 一般来说，对相关连文本数据的解释和补充文本 都可以组成某种网络 一般来说，结合社交网络语境 进行文本分析,是有好处的 因为我们可以利用网络去界定文本包含的主题 比如我们可以联想到，同一协作网络中的作者 更有可能写出主题相似的文章 我们可以依赖这种假设去分析文章主题 另一方面，文本也可以帮助定义各子网络的特征 这就是说，文本和网络这两种数据 是相辅相成的 比如说，两个社交网络内相左的观点 可以通过文本和网络协同分析反映出来 下面我简单介绍一下网络监督主题模型 在这张图里我们介绍一些基本概念 下一张图里我们会更详细的展开 但是其实这边我没有太多时间 去深入讲解这些前沿话题 但是我会给出相关论文的引用 这样课后你可以自己深入的了解一下 但是了解大体的概念是有帮助的 如知道能用来作什么以及什么时候可以用 所以下面是网络监督主题模型的总体思路 首先我们来看普通的主题模型 比如用PLSA或者LDA解决最优化问题 当然如图所示 最优化的目标函数是一个似然函数 所以我们一般用最大似然估计的方法去取得函数的参数 这些参数即是我们从文本数据 中得到的有效信息 比如说，主题信息 在这个方程里，我们想得到在已知标记为Lambda的参数的情况下 可以取得的文本数据(TextData)的最大概率 在这里引入网络 主要就是通过网络增加限制条件 基本上就是利用网络 限制模型参数，这里标记为Lambda 比如 在网络相邻节点上的文本应该是包含相同的主题 有很多例子可以反映，他们是包含相同主题的 我们也可以利用这个方法 让网络节点的主题分布变得平整 也就是让每个关联节点的主题更相似 这样每个相连节点主题的分布基本是一样的 或者是在覆盖相同主题的前提下，分布有一点不一样 理论上，我们可以像这样很简单的加一些 由网络引导的正则项在目标函数的似然函数上 不同于之前的只优化似然函数 我们要优化一个新的函数 f 这个f方程同时包括似然函数和正则项函数r 这个由参数Lambda和网络定义的正则项 基本就是要告诉我们 网络更加偏好界定的参数值 你应该能察觉到这里的建模思路其实就是 在原有模型参数上加了某种先验 当然这里的模型不一定是概率模型 但是道理是一样的 我们就是要在同一个目标方程里结合这两项 这个模型好处是非常通用 这里的主题模型可以是任何通用的文本模型 不一定非要是PLSA或者LDA模型，或是其他当前流行的模型 同样的，网络也可以是任意网络 任何连接文本对象的图都可以 这个正则项也可以任选 我们可以随意选择想要着重抓取的，我们认为重要的信息 最后，函数f也是可变的 所有这些变量有很多组合的方式 因此这个理念其实是非常非常强大的 它提供了一个通用的方法将不同种类的数据 综合到一单一的优化框架上 而且这个模型显然可以解决很多问题 下面我们具体讨论这篇参考论文里 一个叫NetPLSA的具体实例 在这里，就是 PLSA模型的一个拓展，在模型中加入网络限制 而这里的先验条件就是 网络的相邻内容必须有相似的主题分布 互联文章必须用相似的方式阐述类似的主题 简单的说起来大概就是这样 更严谨的说，我们这有一个更改过的目标方程 这里定义了文本集C和网络图G 如果你观察一下这个方程 你其实可以认出很多熟悉的地方 现在应该很熟悉了吧 那么你能看出哪一部分是 主题模型生成特定文档的似然函数吗 如果你看看它，就能看出这一部分完全就是PLSA对数似然 我们可以只通过最大化它来估计PLSA模型参数 但是第二个方程引入了对参数更多的限制 特别是在这里，它是来表示 节点u和节点v主题覆盖的差异 这两个节点在网络里是相连的 我们希望它们的概率分布是相似的 所以这里我们计算出这两项差值的平方 我们希望这个差达到最小 注意这个总和符号，最前面的总和符号前面有个负号 因为这个，我们可以找到 我们需要的参数 这个参数可以让PLSA概率似然达到最大 也就是模型可以很好的反应已知数据的分布 也同时可以考虑到网络的限制 而这个我刚说的负号 因为它使这一项为负，当我们最大化 目标方程时，我们其实是在最小化这一项 如果更进一步分析这里，我们能看到 这里有一个u,v间距离的权重的乘数 这是来自我们的网络 如果你的权重表达出 这两个点代表的研究人群有很强的协同关系 或者这两个点代表两个在社交网络里联系很紧密的人 那么这个权重应该很高 这就表示让两点的主题覆盖类似很重要 这就是这里想表达的意思 最后这里的Lambda参数 这个新参数是用来调节网络限制的影响力 很容易看出，如果Lambda为0，我们就只有一个正统的PLSA模型 但是如果Lambda是个较大的数值 我们就是要让网络对估量模型的影响变大 所以你可以看出，这样做的结果就是，我们是要建基本的PLSA模型 但是我们也希望网络上相邻两点 能在网络中有很强的关联，这样它们的主题覆盖就更类似 我们需要保证它们的覆盖更相似 所以这里抽取论文里的一些结果 这个图告诉我们使用PLSA模型记录到的结果 这里使用的训练集是DBLP 是关于研究论文的书目索引训练集 这里的这些实验用有四个应用群落 IR是信息检索 DM是数据挖掘 ML是机器学习，还有互联网 这就是4种类型的文章群落，我们希望 主题挖掘可以帮助我们分辨出这四个群落 但是这里，从PLSA生成的主题集你们都可以看出 PLSA没有办法生成四种 符合我们直觉的群落 这是因为这些文本全部混在一起 而这四个群落有很多共用的词汇 所以把他们分成四个群落并不容易 可能如果我们引入更多的主题，我们可以有相干性更高的结果 但是有趣的是如果我们使用NetPLSA模型 就是在这里利用论文作者所在的协作网络来增加限制条件 这里我们依旧用这四个群落 则NetPLSA可以给出更有意义的主题集 这里我们能看出四个主题集很映衬四个群落 第一个是信息检索 第二个是数据挖掘 第三个是机器学习 第四个是互联网 这个分类主要是网络的影响 因为我们引入了协作网络信息 组成同一个协作网络里的人大体上 让人觉得会写关于相同主题的文章 这也是我们会有更相关主题集的原因 如果我们只单独考虑文本出现的频率 是没有办法得到这么好的结果的 虽然说，像PLSA，LDA这样的主题模型 也会察觉到经常同时出现的词汇 总的来说他们生成的主题集 可以反映出一些同时出现词汇的相干性 但是他们没有办法产生如此相关的结果 这就证明了网络在这里非常有用 类似的模型还可以用来区分 协作网络中各子网络的内容 更进一步看以网络为语境的数据挖掘 你可以想象文本是存在于一个富含丰富信息的网络环境中 这意味着我们可以把所有相关的数据连成一个大网络 文本数据还可以通过不同的方式依附于网络 比如，文本数据可以在网络的点上 这基本就是我们在NetPLSA里讲到的情况 但是文本数据还可以关联于时间，或者路径上，或者是子网络等 在大语境环境下用这些方式表达文本 是非常有效的 因为这让我们可以把全部信息和数据联合起来分析 所以结论是，文本分析应该和整个 跟文本有关的网络信息结合起来 最后这里是一篇推荐阅读的论文 这就是关于NetPLSA的论文，你可以了解关于这个模型更多的细节 以及如何估量这个模型 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community