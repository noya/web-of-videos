1
00:00:07,000 --> 00:00:14,567
本节课的内容是关于隐含狄利克雷分布
或者说LDA

2
00:00:14,567 --> 00:00:17,885
我们会继续讨论主题模型

3
00:00:17,885 --> 00:00:22,468
特别地,我们会讨论PLSA 
(基于概率的隐含语义分析)的一些扩展

4
00:00:22,468 --> 00:00:30,858
其中的一个就是LDA
或者说隐含狄利克雷分布

5
00:00:30,858 --> 00:00:33,765
所以本节课的计划会覆盖两个内容

6
00:00:33,765 --> 00:00:37,730
一是用先验知识扩展PLSA,

7
00:00:37,730 --> 00:00:41,795
以允许我们可以做某种意义上的用户监督PLSA

8
00:00:41,795 --> 00:00:44,329
这样它们就不再只从数据里学习

9
00:00:44,329 --> 00:00:47,360
也会倾听我们的需求

10
00:00:47,360 --> 00:00:52,220
二是将PLSA扩展为一个生成模型

11
00:00:52,220 --> 00:00:54,158
一个完整的生成模型

12
00:00:54,158 --> 00:01:00,887
这个扩展发展成了隐含狄利克雷分布
或者说LDA

13
00:01:00,887 --> 00:01:04,520
首先,我们探讨有先验知识的PLSA

14
00:01:04,520 --> 00:01:09,060
在实践中,当我们应用PLSA做文本分析时

15
00:01:09,060 --> 00:01:14,665
我们可能有一些希望用于指导分析的
额外知识

16
00:01:14,665 --> 00:01:20,935
标准的PLSA是完全基于数据
做最大似然估计

17
00:01:20,935 --> 00:01:25,690
我们会尽可能适应数据,并得到对数据的认识

18
00:01:25,690 --> 00:01:27,155
这样也是很有用的

19
00:01:27,155 --> 00:01:32,500
但有时用户可能对要分析的主题
有一定的期望

20
00:01:32,500 --> 00:01:35,723
例如,我们可能希望看到'检索模型'
作为主题出现在信息检索

21
00:01:35,723 --> 00:01:40,531
我们也可能对某些特定的概念有兴趣

22
00:01:40,531 --> 00:01:43,745
例如电池和内存

23
00:01:43,745 --> 00:01:46,500
如在看有关笔记本的意见时

24
00:01:46,500 --> 00:01:50,075
因为用户会对这些概念特别感兴趣

25
00:01:50,075 --> 00:01:55,160
用户也可能对主题的覆盖率有认识

26
00:01:55,160 --> 00:02:00,390
我们可能知道对文档来说,
某个主题是一定不包括或者包括在内的

27
00:02:00,390 --> 00:02:03,530
例如,我们已经见过这些标记

28
00:02:03,530 --> 00:02:06,520
分配给文档的主题标记

29
00:02:06,520 --> 00:02:09,800
这些标记可以被视为主题

30
00:02:09,800 --> 00:02:13,275
如果我们这样做
那么一个文档帐户将生成的主题

31
00:02:13,275 --> 00:02:17,420
就会和已经分配给文档的标记相关

32
00:02:17,420 --> 00:02:19,087
如果未为文档分配标记

33
00:02:19,087 --> 00:02:25,925
我们会说,没有办法用那个主题来生成文档

34
00:02:25,925 --> 00:02:32,291
文档必须由已分配的标记相关的主题生成

35
00:02:32,291 --> 00:02:35,835
问题是我们如何将这些知识纳入PLSA

36
00:02:35,835 --> 00:02:39,410
事实证明,有一个非常优雅的方式能做到

37
00:02:39,410 --> 00:02:44,235
把这些模型的先验知识纳入

38
00:02:44,235 --> 00:02:47,194
你可能会想起贝叶斯推断里

39
00:02:47,194 --> 00:02:49,935
我们把先验知识和数据一起用来

40
00:02:49,935 --> 00:02:54,185
估计参数,这也是我们将要做的

41
00:02:54,185 --> 00:02:55,207
所以 在这种情况下所以 在这种情况下

42
00:02:55,207 --> 00:02:57,095
我们可以用一个

43
00:02:57,095 --> 00:03:02,730
最大后验概率估计,也叫MAP,公式已给出

44
00:03:02,730 --> 00:03:06,230
基本上,这就是最大化后验分布的概率

45
00:03:06,230 --> 00:03:09,650
这个综合了数据的可能性和先验知识

46
00:03:09,650 --> 00:03:13,895
将会发生的是我们有一个预估即基于数据

47
00:03:13,895 --> 00:03:19,400
也会基于我们的先验偏好

48
00:03:19,400 --> 00:03:23,195
我们用的先验知识用P(^)表示

49
00:03:23,195 --> 00:03:28,070
对各种偏好和约束进行编码

50
00:03:28,070 --> 00:03:31,615
例如,我们用这个方式

51
00:03:31,615 --> 00:03:35,870
表示对主题需有精确的背景知识

52
00:03:35,870 --> 00:03:43,995
现在,这也可以编码为先验知识,因为可以设定

53
00:03:43,995 --> 00:03:46,565
参数的先验知识为一个非0值

54
00:03:46,565 --> 00:03:52,740
仅当参数包含一个主题
这个主题等价于背景语言模型

55
00:03:52,740 --> 00:03:56,060
换句话说,如果不是这样

56
00:03:56,060 --> 00:03:59,200
我们就可以说先验知识表明这是不可能的

57
00:03:59,200 --> 00:04:07,708
所以依据我们的先验知识,那种模型的概率会是0

58
00:04:07,708 --> 00:04:12,455
现在我们也可以,例如用先验知识

59
00:04:12,455 --> 00:04:19,310
强制以特定的概率选择出主题

60
00:04:19,310 --> 00:04:26,175
例如,我们可以强制文档D以二分之一的概率
来选择主题一

61
00:04:26,175 --> 00:04:33,395
或者也可以在生成的文档中阻止主题被使用

62
00:04:33,395 --> 00:04:36,440
我们可以说第三个主题不应该用于生成文档D

63
00:04:36,440 --> 00:04:41,510
我们会把那个主题设成概率为0

64
00:04:41,510 --> 00:04:44,720
我们还可以用先验知识来加工参数组合

65
00:04:44,720 --> 00:04:48,660
方式是给特定的词分配高的概率

66
00:04:48,660 --> 00:04:52,340
这种情况下,我们不会说它不可能,但我们强烈地

67
00:04:52,340 --> 00:04:57,590
偏好某些种类的分布,你们稍后会看到例子

68
00:04:57,590 --> 00:05:00,031
MAP(最大后验概率)可以用类似EM的算法计算

69
00:05:00,031 --> 00:05:04,345
我们已经在最大相似预估里用过

70
00:05:04,345 --> 00:05:06,177
只要做一些修改

71
00:05:06,177 --> 00:05:10,970
大部分参数会反应出先验知识的偏好

72
00:05:10,970 --> 00:05:16,306
在这种估计里,如果我们使用特殊形式的
先验知识编码,或者使用共轭先验

73
00:05:16,306 --> 00:05:20,145
先验知识的生效形式就和数据一样

74
00:05:20,145 --> 00:05:23,900
结论是,我们可以结合两点,结果就是

75
00:05:23,900 --> 00:05:28,707
你们可以基本上把对先验知识的引用

76
00:05:28,707 --> 00:05:33,184
转化为对额外的伪数据引用

77
00:05:33,184 --> 00:05:37,772
因为两种生效形式是一样的,它们可以组合

78
00:05:37,772 --> 00:05:44,395
所以就和我们有更多的数据一样,这也方便计算

79
00:05:44,395 --> 00:05:49,680
这并不意味着共轭先验是最好的定义先验知识方法

80
00:05:49,680 --> 00:05:52,780
现在让我们看具体的例子

81
00:05:52,780 --> 00:05:55,444
假设用户对笔记本电脑的电池寿命
有特别兴趣

82
00:05:55,444 --> 00:05:59,270
并且我们是在分析(笔记本电脑的)评价

83
00:05:59,270 --> 00:06:02,875
所以先验知识说分布里的其中一个应该有

84
00:06:02,875 --> 00:06:09,551
分配给batter和life高概率的分布

85
00:06:09,551 --> 00:06:14,770
我们可以说有个分布专注在电池寿命上

86
00:06:14,770 --> 00:06:20,421
而先验知识说你的其中一个分布
应该和这个分布一样

87
00:06:20,421 --> 00:06:24,794
现在假设我们使用MAP和共轭先验

88
00:06:24,794 --> 00:06:27,463
就是原始的先验知识

89
00:06:27,463 --> 00:06:32,565
原始的分布基于这种偏好

90
00:06:32,565 --> 00:06:38,991
在EM里唯一的区别是
我们重新预估了词的分布

91
00:06:38,991 --> 00:06:44,700
我们将增加额外的计数来反映我们的先验知识

92
00:06:44,700 --> 00:06:49,047
这里你可以看到伪计数

93
00:06:49,047 --> 00:06:53,826
是用先验知识中的词概率来定义的

94
00:06:53,826 --> 00:06:55,780
所以battery明显会有

95
00:06:55,780 --> 00:07:00,365
高的伪计数,类似的life也有高的伪计数

96
00:07:00,365 --> 00:07:04,140
其它词的伪计数是0,因为它们

97
00:07:04,140 --> 00:07:07,900
在先验知识里的概率为0,我们还可以看到

98
00:07:07,900 --> 00:07:15,130
这由一个参数μ控制
我们会把μ乘上先验中给的w概率

99
00:07:15,130 --> 00:07:20,260
给到先验分布关联的账号

100
00:07:20,260 --> 00:07:26,920
当我们重新估计这个词的分布时

101
00:07:26,920 --> 00:07:30,910
所以这是唯一有修改的一步,改变的地方在这

102
00:07:30,910 --> 00:07:34,570
之前,我们只是把认为已经从主题中生成的

103
00:07:34,570 --> 00:07:38,230
词计数连接起来,但现在

104
00:07:38,230 --> 00:07:42,455
我们强制这个分布给这些词更大的概率

105
00:07:42,455 --> 00:07:47,714
办法是给这些词增加伪计数

106
00:07:47,714 --> 00:07:53,180
所以实际上我们夸大了它们的概率

107
00:07:53,180 --> 00:07:55,641
为了构造这个分布

108
00:07:55,641 --> 00:08:00,190
我们也需要把同样的伪计数加到分母上

109
00:08:00,190 --> 00:08:02,995
这是我们为所有词增加的伪计数总和

110
00:08:02,995 --> 00:08:07,300
这会构成一个伽马分布

111
00:08:07,300 --> 00:08:14,142
这是一个非常直观合理的修改EM算法的方式
理论上说

112
00:08:14,142 --> 00:08:17,376
这个有效并且可以计算MAP估计

113
00:08:17,376 --> 00:08:22,884
考虑两个特定例子的μ是有效的

114
00:08:22,884 --> 00:08:24,900
现在请看图

115
00:08:24,900 --> 00:08:27,555
考虑如果把μ设成0会发生什么

116
00:08:27,555 --> 00:08:29,260
这实质上就去除了先验知识

117
00:08:29,260 --> 00:08:35,529
所有μ某种程度上反应了先验知识的强度

118
00:08:35,529 --> 00:08:38,590
如果把μ设成正无穷会发生什么呢

119
00:08:38,590 --> 00:08:40,687
这意思就是说先验知识非常强

120
00:08:40,687 --> 00:08:44,140
强到我们根本不会管数据

121
00:08:44,140 --> 00:08:46,765
最后,在例子里可以看到

122
00:08:46,765 --> 00:08:50,010
我们会把其中一个分布固化到先验知识
知道为什么吗?

123
00:08:50,010 --> 00:08:57,360
当μ为正无穷时,我们在让这项占支配地位

124
00:08:57,360 --> 00:09:02,961
实际上我们正在把这项,设置成这个分布

125
00:09:02,961 --> 00:09:06,680
在这个例子里,就是这个分布

126
00:09:06,680 --> 00:09:08,605
这就是我们为什么会说

127
00:09:08,605 --> 00:09:11,384
背景语言模型是一个加强先验知识的方式

128
00:09:11,384 --> 00:09:17,890
因为它会强制一个分布变得和
我们所给的一样

129
00:09:17,890 --> 00:09:19,898
那是背景分布

130
00:09:19,898 --> 00:09:25,980
在这个例子里,我们甚至可以强制让分布
完全聚焦在battery和life上

131
00:09:25,980 --> 00:09:29,915
当然,这么做结果不会很好
因为它不能处理好其他词

132
00:09:29,915 --> 00:09:34,240
这会影响到计算有关电池寿命的
主题的准确性

133
00:09:34,240 --> 00:09:38,770
所以在实践中,μ是介于两者之间的某个值

134
00:09:38,770 --> 00:09:41,625
这是引入先验知识的一种办法

135
00:09:41,625 --> 00:09:44,500
我们也可以引入一些其他的限制

136
00:09:44,500 --> 00:09:48,880
例如,我们可以设置某些参数为常量
也包括0

137
00:09:48,880 --> 00:09:52,515
例如,我们也许想设置其中一个π为0

138
00:09:52,515 --> 00:09:56,497
这会意味着

139
00:09:56,497 --> 00:10:01,080
我们不允许这个主题参与到文档生成中

140
00:10:01,080 --> 00:10:03,280
当然这仅在我们的先验知识

141
00:10:03,280 --> 00:10:08,180
强烈建议这么做时才合理