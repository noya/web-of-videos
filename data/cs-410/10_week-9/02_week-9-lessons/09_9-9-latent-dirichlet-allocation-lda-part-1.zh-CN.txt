本节课的内容是关于隐含狄利克雷分布
或者说LDA 我们会继续讨论主题模型 特别地,我们会讨论PLSA 
(基于概率的隐含语义分析)的一些扩展 其中的一个就是LDA
或者说隐含狄利克雷分布 所以本节课的计划会覆盖两个内容 一是用先验知识扩展PLSA, 以允许我们可以做某种意义上的用户监督PLSA 这样它们就不再只从数据里学习 也会倾听我们的需求 二是将PLSA扩展为一个生成模型 一个完整的生成模型 这个扩展发展成了隐含狄利克雷分布
或者说LDA 首先,我们探讨有先验知识的PLSA 在实践中,当我们应用PLSA做文本分析时 我们可能有一些希望用于指导分析的
额外知识 标准的PLSA是完全基于数据
做最大似然估计 我们会尽可能适应数据,并得到对数据的认识 这样也是很有用的 但有时用户可能对要分析的主题
有一定的期望 例如,我们可能希望看到'检索模型'
作为主题出现在信息检索 我们也可能对某些特定的概念有兴趣 例如电池和内存 如在看有关笔记本的意见时 因为用户会对这些概念特别感兴趣 用户也可能对主题的覆盖率有认识 我们可能知道对文档来说,
某个主题是一定不包括或者包括在内的 例如,我们已经见过这些标记 分配给文档的主题标记 这些标记可以被视为主题 如果我们这样做
那么一个文档帐户将生成的主题 就会和已经分配给文档的标记相关 如果未为文档分配标记 我们会说,没有办法用那个主题来生成文档 文档必须由已分配的标记相关的主题生成 问题是我们如何将这些知识纳入PLSA 事实证明,有一个非常优雅的方式能做到 把这些模型的先验知识纳入 你可能会想起贝叶斯推断里 我们把先验知识和数据一起用来 估计参数,这也是我们将要做的 所以 在这种情况下所以 在这种情况下 我们可以用一个 最大后验概率估计,也叫MAP,公式已给出 基本上,这就是最大化后验分布的概率 这个综合了数据的可能性和先验知识 将会发生的是我们有一个预估即基于数据 也会基于我们的先验偏好 我们用的先验知识用P(^)表示 对各种偏好和约束进行编码 例如,我们用这个方式 表示对主题需有精确的背景知识 现在,这也可以编码为先验知识,因为可以设定 参数的先验知识为一个非0值 仅当参数包含一个主题
这个主题等价于背景语言模型 换句话说,如果不是这样 我们就可以说先验知识表明这是不可能的 所以依据我们的先验知识,那种模型的概率会是0 现在我们也可以,例如用先验知识 强制以特定的概率选择出主题 例如,我们可以强制文档D以二分之一的概率
来选择主题一 或者也可以在生成的文档中阻止主题被使用 我们可以说第三个主题不应该用于生成文档D 我们会把那个主题设成概率为0 我们还可以用先验知识来加工参数组合 方式是给特定的词分配高的概率 这种情况下,我们不会说它不可能,但我们强烈地 偏好某些种类的分布,你们稍后会看到例子 MAP(最大后验概率)可以用类似EM的算法计算 我们已经在最大相似预估里用过 只要做一些修改 大部分参数会反应出先验知识的偏好 在这种估计里,如果我们使用特殊形式的
先验知识编码,或者使用共轭先验 先验知识的生效形式就和数据一样 结论是,我们可以结合两点,结果就是 你们可以基本上把对先验知识的引用 转化为对额外的伪数据引用 因为两种生效形式是一样的,它们可以组合 所以就和我们有更多的数据一样,这也方便计算 这并不意味着共轭先验是最好的定义先验知识方法 现在让我们看具体的例子 假设用户对笔记本电脑的电池寿命
有特别兴趣 并且我们是在分析(笔记本电脑的)评价 所以先验知识说分布里的其中一个应该有 分配给batter和life高概率的分布 我们可以说有个分布专注在电池寿命上 而先验知识说你的其中一个分布
应该和这个分布一样 现在假设我们使用MAP和共轭先验 就是原始的先验知识 原始的分布基于这种偏好 在EM里唯一的区别是
我们重新预估了词的分布 我们将增加额外的计数来反映我们的先验知识 这里你可以看到伪计数 是用先验知识中的词概率来定义的 所以battery明显会有 高的伪计数,类似的life也有高的伪计数 其它词的伪计数是0,因为它们 在先验知识里的概率为0,我们还可以看到 这由一个参数μ控制
我们会把μ乘上先验中给的w概率 给到先验分布关联的账号 当我们重新估计这个词的分布时 所以这是唯一有修改的一步,改变的地方在这 之前,我们只是把认为已经从主题中生成的 词计数连接起来,但现在 我们强制这个分布给这些词更大的概率 办法是给这些词增加伪计数 所以实际上我们夸大了它们的概率 为了构造这个分布 我们也需要把同样的伪计数加到分母上 这是我们为所有词增加的伪计数总和 这会构成一个伽马分布 这是一个非常直观合理的修改EM算法的方式
理论上说 这个有效并且可以计算MAP估计 考虑两个特定例子的μ是有效的 现在请看图 考虑如果把μ设成0会发生什么 这实质上就去除了先验知识 所有μ某种程度上反应了先验知识的强度 如果把μ设成正无穷会发生什么呢 这意思就是说先验知识非常强 强到我们根本不会管数据 最后,在例子里可以看到 我们会把其中一个分布固化到先验知识
知道为什么吗? 当μ为正无穷时,我们在让这项占支配地位 实际上我们正在把这项,设置成这个分布 在这个例子里,就是这个分布 这就是我们为什么会说 背景语言模型是一个加强先验知识的方式 因为它会强制一个分布变得和
我们所给的一样 那是背景分布 在这个例子里,我们甚至可以强制让分布
完全聚焦在battery和life上 当然,这么做结果不会很好
因为它不能处理好其他词 这会影响到计算有关电池寿命的
主题的准确性 所以在实践中,μ是介于两者之间的某个值 这是引入先验知识的一种办法 我们也可以引入一些其他的限制 例如,我们可以设置某些参数为常量
也包括0 例如,我们也许想设置其中一个π为0 这会意味着 我们不允许这个主题参与到文档生成中 当然这仅在我们的先验知识 强烈建议这么做时才合理