1
00:00:00,012 --> 00:00:08,031
这次讲座

2
00:00:08,031 --> 00:00:10,570
是关于混合模型估计

3
00:00:12,240 --> 00:00:16,260
在这里我们会继续讨论概率主题模型

4
00:00:16,260 --> 00:00:16,830
在 bootstrap 中

5
00:00:16,830 --> 00:00:20,380
我们会继续讲怎么估算混合模型的参数

6
00:00:23,010 --> 00:00:26,870
首先我们来看我们使用混合模型的原因

7
00:00:26,870 --> 00:00:30,026
我们希望利用上边这个高频词公式

8
00:00:30,026 --> 00:00:33,480
来筛掉背景词

9
00:00:33,480 --> 00:00:39,990
思路是假设文本数据其实包含两种词

10
00:00:39,990 --> 00:00:44,817
一种是这边的背景词

11
00:00:44,817 --> 00:00:48,800
就是the,is,we这种词

12
00:00:48,800 --> 00:00:53,820
另一种就是我们真正感兴趣的高频词分布中的词

13
00:00:56,310 --> 00:01:01,420
为了解决这个筛除背景词的问题

14
00:01:01,420 --> 00:01:05,780
我们可以设混合模型为假

15
00:01:05,780 --> 00:01:09,250
即我们将假设混合模型中

16
00:01:09,250 --> 00:01:14,010
除了以参数θd表示的文本分布

17
00:01:14,010 --> 00:01:19,110
所有其他分布的参数值都是已知的

18
00:01:20,130 --> 00:01:25,130
通过插入一些已知的，我们感兴趣的变量

19
00:01:25,130 --> 00:01:29,500
这里我们其实定制了一个概率模型

20
00:01:29,500 --> 00:01:31,265
我们将简化一些其它的事情

21
00:01:31,265 --> 00:01:34,180
我们将假设我们对模型有更充分的了解

22
00:01:34,180 --> 00:01:37,760
这是一个非常给力的

23
00:01:37,760 --> 00:01:39,500
为特别需要
定制模型方法

24
00:01:39,500 --> 00:01:40,320
现在你可以想象

25
00:01:40,320 --> 00:01:45,000
我们其实还可以假设
我们其实也不了解背景词具体是哪些

26
00:01:45,000 --> 00:01:46,230
但在这种情况下

27
00:01:46,230 --> 00:01:51,810
我们的目标非常清晰
就是筛除高概率的背景词

28
00:01:51,810 --> 00:01:55,530
所以我们可以假设背景词模型已知

29
00:01:56,680 --> 00:02:01,670
现在的问题是我们怎么调整θd

30
00:02:01,670 --> 00:02:06,270
来让已观测到的高频词概率达到最大

31
00:02:06,270 --> 00:02:07,920
我们已假设其他参数已知

32
00:02:09,470 --> 00:02:12,530
虽然为了尝试筛除背景词

33
00:02:12,530 --> 00:02:16,230
我们基于已有经验设计好了模型

34
00:02:16,230 --> 00:02:21,056
我们并不知道
如果使用最大似然估计量

35
00:02:21,056 --> 00:02:25,850
是否真的可以得出一个
像the这样的背景词

36
00:02:25,850 --> 00:02:29,940
成为小概率词的词分布

37
00:02:31,220 --> 00:02:36,880
在目前这个例子里，答案是可以

38
00:02:36,880 --> 00:02:41,070
当我们以这种方式设置概率模型

39
00:02:41,070 --> 00:02:45,720
然后使用最大似然估计量
就可以得到一个

40
00:02:46,890 --> 00:02:49,990
常用词被筛除的词分布

41
00:02:49,990 --> 00:02:52,570
就是使用了背景词分布

42
00:02:53,580 --> 00:02:56,710
为了理解为什么会这样

43
00:02:56,710 --> 00:03:00,600
观察混合模型的行为很有用

44
00:03:00,600 --> 00:03:03,910
为了理解混合模型一些有趣的行为

45
00:03:03,910 --> 00:03:08,860
我们将看一个非常非常简单的例子

46
00:03:08,860 --> 00:03:15,130
这里观察到的模式
其实可以推广到广义上的混合模型上

47
00:03:15,130 --> 00:03:17,920
但是使用这里我们看到的
这个非常简单的例子

48
00:03:17,920 --> 00:03:21,750
理解它的行为就更加容易了

49
00:03:21,750 --> 00:03:25,290
具体点说在这个例子里，假设混合模型中

50
00:03:25,290 --> 00:03:29,670
选择这两个模型的概率是一样的

51
00:03:29,670 --> 00:03:33,400
即我们抛一个真硬币来决定使用哪个模型

52
00:03:34,420 --> 00:03:36,610
在此基础，我们还要假设

53
00:03:36,610 --> 00:03:39,510
这里只有两个词，the和text

54
00:03:39,510 --> 00:03:46,120
显然这个例子比真正的文本简单太多了

55
00:03:46,120 --> 00:03:52,110
但是还是要说
这个特殊情况对审视模型的行为非常有用

56
00:03:53,690 --> 00:03:58,180
然后我们假设背景模型中

57
00:03:58,180 --> 00:04:03,059
词the的概率为0.9
词text的概率为0.1

58
00:04:03,059 --> 00:04:08,340
现在我们假设我们的数据集极其简单，文本里

59
00:04:08,340 --> 00:04:13,820
只有两个词，text和the
现在我们写出这个例子下的似然函数

60
00:04:13,820 --> 00:04:18,350
首先，text的概率是怎么表示
the的概率又怎么表示呢

61
00:04:19,550 --> 00:04:22,340
我希望这时候你已经可以自己写出来

62
00:04:23,760 --> 00:04:28,644
所以text的概率其实就是两个可能事件的和

63
00:04:28,644 --> 00:04:33,460
每一个事件即
一个词在其所属分布中的概率

64
00:04:34,480 --> 00:04:38,060
这解释了两种生成文本的可能性

65
00:04:39,490 --> 00:04:43,580
在每一个事件里，也同时包含了选择该分布模型的可能

66
00:04:43,580 --> 00:04:50,360
所以每一项就是0.5乘以词text在其所属分布中的概率

67
00:04:50,360 --> 00:04:54,980
同理，the的概率也是这样的形式

68
00:04:54,980 --> 00:04:57,480
不同的就是这个词自己具体的概率值

69
00:04:58,900 --> 00:05:03,490
自然我们的似然函数就是两者的乘积

70
00:05:03,490 --> 00:05:07,110
当你了解每个词的概率

71
00:05:08,140 --> 00:05:11,000
这个模型很简单的就得出了

72
00:05:11,000 --> 00:05:15,450
这也是为什么理解这个混合模型里

73
00:05:15,450 --> 00:05:19,870
每一个词具体概率的表示
非常重要

74
00:05:19,870 --> 00:05:25,690
现在有趣的问题是
我们如何优化似然函数

75
00:05:25,690 --> 00:05:29,420
当然你将注意到这里只有两个变量

76
00:05:29,420 --> 00:05:32,270
更准确的说就是它们就是θd表示的

77
00:05:32,270 --> 00:05:35,950
the和text这两个词的概率

78
00:05:35,950 --> 00:05:39,660
因为我们已经假设其他参数已知了

79
00:05:41,240 --> 00:05:45,460
所以现在这个问题就是一个非常简单的代数问题

80
00:05:45,460 --> 00:05:48,450
就是，我们有一个有两个变量的表达式

81
00:05:48,450 --> 00:05:53,140
并且我们希望能选出让这个函数达到最大的变量值

82
00:05:54,270 --> 00:05:58,910
这就是我们常见的代数练习题了

83
00:06:00,150 --> 00:06:04,650
注意这所求的两个概率值相加等于1
所以这里有些约束条件

84
00:06:06,340 --> 00:06:08,080
如果没有约束条件

85
00:06:08,080 --> 00:06:12,020
那么为了让函数达到最大
我们将设这两个概率为其最大值都为1

86
00:06:12,020 --> 00:06:18,000
但是我们不能这么做
因为text和the必须相加为1

87
00:06:18,000 --> 00:06:20,240
我们不能把它俩都设成1

88
00:06:21,840 --> 00:06:25,150
所以现在的问题是
我们如何分配概率值

89
00:06:25,150 --> 00:06:27,090
这两个词之间又什么数学关系

90
00:06:27,090 --> 00:06:28,400
你怎么看？必须指出的是

91
00:06:28,400 --> 00:06:32,320
现在我们仔细观察一下这个式子

92
00:06:32,320 --> 00:06:36,540
思考一下，直观的想，我们做什么

93
00:06:36,540 --> 00:06:39,940
才能让这个函数的值达到最大

94
00:06:42,420 --> 00:06:44,310
好的，如果我们想的更远些

95
00:06:44,310 --> 00:06:50,070
我们可以预见到这两个模型一些有趣的表现

96
00:06:50,070 --> 00:06:54,730
它们俩会相互合作来使当前词的概率达到最大

97
00:06:54,730 --> 00:06:57,790
这是由最大似然函数估计量决定的

98
00:06:57,790 --> 00:07:02,020
但是它们也存在竞争关系，特别是

99
00:07:02,020 --> 00:07:05,350
它们会为词相互竞争

100
00:07:05,350 --> 00:07:09,140
它们趋向于将高概率给予不同的词

101
00:07:09,140 --> 00:07:14,680
这样在某种意义上是为了避免竞争
或者说是在竞争中取得先机

102
00:07:14,680 --> 00:07:16,970
所以，再次看这个目标函数

103
00:07:16,970 --> 00:07:20,220
还有对两个概率的限制条件

104
00:07:21,360 --> 00:07:25,460
如果你凭直觉观察这个方程

105
00:07:25,460 --> 00:07:30,509
你可能觉得你希望
把text这个词的概率设的更高些

106
00:07:32,130 --> 00:07:38,160
这种直觉其实是由数学推导支撑的

107
00:07:38,160 --> 00:07:43,280
当两变量的和是一个常数时

108
00:07:43,280 --> 00:07:49,150
它们的积会在它们相等时达到最大
这是代数中已被证明的事实

109
00:07:49,150 --> 00:07:52,910
如果我们应用这个事实
我们就得把混合模型

110
00:07:52,910 --> 00:07:55,100
的两个概率项设为相等

111
00:07:56,170 --> 00:07:57,830
当我们设它们相等

112
00:07:57,830 --> 00:08:02,180
再考虑到限制条件
问题很容易就解决了

113
00:08:02,180 --> 00:08:09,310
最后的解是text概率为0.9
and概率为0.1

114
00:08:09,310 --> 00:08:14,150
你能发现真的是
现在text的概率比the要大多了

115
00:08:14,150 --> 00:08:17,200
当我们只有一个分布时可不是这样

116
00:08:17,200 --> 00:08:21,040
这很显然是背景分布起到的作用

117
00:08:21,040 --> 00:08:26,480
它给the一个很高的概率，给text一个很低的概率

118
00:08:26,480 --> 00:08:30,270
如果你看一下这个等式
你很快就能发现

119
00:08:30,270 --> 00:08:33,300
这里有一些两个分布的互动

120
00:08:35,070 --> 00:08:39,090
准确的说，你将发现为了让它相等

121
00:08:39,090 --> 00:08:46,350
分给背景模型的概率更低

122
00:08:46,350 --> 00:08:50,849
分给θd的概率就一定会更高

123
00:08:53,380 --> 00:08:56,690
审视这个等式后很自然就能发现这一点

124
00:08:56,690 --> 00:08:59,850
因为text在背景模型中较弱

125
00:08:59,850 --> 00:09:00,710
它值很小

126
00:09:00,710 --> 00:09:04,900
为了补偿这一点，我们必须让

127
00:09:04,900 --> 00:09:11,270
θd中text的概率更大
这样等式两边才能平衡

128
00:09:11,270 --> 00:09:17,280
所以这是这个混合模型很常见的表现

129
00:09:17,280 --> 00:09:21,780
就是说，如果其中一个分布分给某个词的概率更高

130
00:09:21,780 --> 00:09:25,540
另一个分布中这个词的概率就更低

131
00:09:25,540 --> 00:09:28,960
它会阻止其他分布分给这个词高概率

132
00:09:28,960 --> 00:09:34,650
这就是为了让子模型相抵
保证每一个词在主模型中享有公平的概率

133
00:09:34,650 --> 00:09:38,811
这也意味着
当我们使用一个把背景词设为高概率的

134
00:09:38,811 --> 00:09:42,341
固定参数背景模型时

135
00:09:42,341 --> 00:09:47,194
我们真的可以鼓励参数未知的主题模型

136
00:09:47,194 --> 00:09:50,035
赋予常见词更小的概率

137
00:09:50,035 --> 00:09:54,302
从而将更多的概率分给

138
00:09:54,302 --> 00:09:58,170
背景模型无法很好解释的内容相关词

139
00:09:58,170 --> 00:10:02,754
意思就是这些像text这样在背景模型里

140
00:10:02,754 --> 00:10:03,452
概率很小的词

141
00:10:03,452 --> 00:10:13,452
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community