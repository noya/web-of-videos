这次讲座 是关于混合模型估计 在这里我们会继续讨论概率主题模型 在 bootstrap 中 我们会继续讲怎么估算混合模型的参数 首先我们来看我们使用混合模型的原因 我们希望利用上边这个高频词公式 来筛掉背景词 思路是假设文本数据其实包含两种词 一种是这边的背景词 就是the,is,we这种词 另一种就是我们真正感兴趣的高频词分布中的词 为了解决这个筛除背景词的问题 我们可以设混合模型为假 即我们将假设混合模型中 除了以参数θd表示的文本分布 所有其他分布的参数值都是已知的 通过插入一些已知的，我们感兴趣的变量 这里我们其实定制了一个概率模型 我们将简化一些其它的事情 我们将假设我们对模型有更充分的了解 这是一个非常给力的 为特别需要
定制模型方法 现在你可以想象 我们其实还可以假设
我们其实也不了解背景词具体是哪些 但在这种情况下 我们的目标非常清晰
就是筛除高概率的背景词 所以我们可以假设背景词模型已知 现在的问题是我们怎么调整θd 来让已观测到的高频词概率达到最大 我们已假设其他参数已知 虽然为了尝试筛除背景词 我们基于已有经验设计好了模型 我们并不知道
如果使用最大似然估计量 是否真的可以得出一个
像the这样的背景词 成为小概率词的词分布 在目前这个例子里，答案是可以 当我们以这种方式设置概率模型 然后使用最大似然估计量
就可以得到一个 常用词被筛除的词分布 就是使用了背景词分布 为了理解为什么会这样 观察混合模型的行为很有用 为了理解混合模型一些有趣的行为 我们将看一个非常非常简单的例子 这里观察到的模式
其实可以推广到广义上的混合模型上 但是使用这里我们看到的
这个非常简单的例子 理解它的行为就更加容易了 具体点说在这个例子里，假设混合模型中 选择这两个模型的概率是一样的 即我们抛一个真硬币来决定使用哪个模型 在此基础，我们还要假设 这里只有两个词，the和text 显然这个例子比真正的文本简单太多了 但是还是要说
这个特殊情况对审视模型的行为非常有用 然后我们假设背景模型中 词the的概率为0.9
词text的概率为0.1 现在我们假设我们的数据集极其简单，文本里 只有两个词，text和the
现在我们写出这个例子下的似然函数 首先，text的概率是怎么表示
the的概率又怎么表示呢 我希望这时候你已经可以自己写出来 所以text的概率其实就是两个可能事件的和 每一个事件即
一个词在其所属分布中的概率 这解释了两种生成文本的可能性 在每一个事件里，也同时包含了选择该分布模型的可能 所以每一项就是0.5乘以词text在其所属分布中的概率 同理，the的概率也是这样的形式 不同的就是这个词自己具体的概率值 自然我们的似然函数就是两者的乘积 当你了解每个词的概率 这个模型很简单的就得出了 这也是为什么理解这个混合模型里 每一个词具体概率的表示
非常重要 现在有趣的问题是
我们如何优化似然函数 当然你将注意到这里只有两个变量 更准确的说就是它们就是θd表示的 the和text这两个词的概率 因为我们已经假设其他参数已知了 所以现在这个问题就是一个非常简单的代数问题 就是，我们有一个有两个变量的表达式 并且我们希望能选出让这个函数达到最大的变量值 这就是我们常见的代数练习题了 注意这所求的两个概率值相加等于1
所以这里有些约束条件 如果没有约束条件 那么为了让函数达到最大
我们将设这两个概率为其最大值都为1 但是我们不能这么做
因为text和the必须相加为1 我们不能把它俩都设成1 所以现在的问题是
我们如何分配概率值 这两个词之间又什么数学关系 你怎么看？必须指出的是 现在我们仔细观察一下这个式子 思考一下，直观的想，我们做什么 才能让这个函数的值达到最大 好的，如果我们想的更远些 我们可以预见到这两个模型一些有趣的表现 它们俩会相互合作来使当前词的概率达到最大 这是由最大似然函数估计量决定的 但是它们也存在竞争关系，特别是 它们会为词相互竞争 它们趋向于将高概率给予不同的词 这样在某种意义上是为了避免竞争
或者说是在竞争中取得先机 所以，再次看这个目标函数 还有对两个概率的限制条件 如果你凭直觉观察这个方程 你可能觉得你希望
把text这个词的概率设的更高些 这种直觉其实是由数学推导支撑的 当两变量的和是一个常数时 它们的积会在它们相等时达到最大
这是代数中已被证明的事实 如果我们应用这个事实
我们就得把混合模型 的两个概率项设为相等 当我们设它们相等 再考虑到限制条件
问题很容易就解决了 最后的解是text概率为0.9
and概率为0.1 你能发现真的是
现在text的概率比the要大多了 当我们只有一个分布时可不是这样 这很显然是背景分布起到的作用 它给the一个很高的概率，给text一个很低的概率 如果你看一下这个等式
你很快就能发现 这里有一些两个分布的互动 准确的说，你将发现为了让它相等 分给背景模型的概率更低 分给θd的概率就一定会更高 审视这个等式后很自然就能发现这一点 因为text在背景模型中较弱 它值很小 为了补偿这一点，我们必须让 θd中text的概率更大
这样等式两边才能平衡 所以这是这个混合模型很常见的表现 就是说，如果其中一个分布分给某个词的概率更高 另一个分布中这个词的概率就更低 它会阻止其他分布分给这个词高概率 这就是为了让子模型相抵
保证每一个词在主模型中享有公平的概率 这也意味着
当我们使用一个把背景词设为高概率的 固定参数背景模型时 我们真的可以鼓励参数未知的主题模型 赋予常见词更小的概率 从而将更多的概率分给 背景模型无法很好解释的内容相关词 意思就是这些像text这样在背景模型里 概率很小的词 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community