1
00:00:00,012 --> 00:00:08,224
那么

2
00:00:08,224 --> 00:00:12,538
这就是期望最大化算法(Expectation-Maximization)或者也叫EM

3
00:00:12,538 --> 00:00:13,310
演算法

4
00:00:14,640 --> 00:00:19,210
所以在所有的最大期望演算法里我们加入一个隐形变量

5
00:00:19,210 --> 00:00:21,970
来帮助我们更容易解决问题

6
00:00:21,970 --> 00:00:25,453
在我们的例子中隐藏变量是一个

7
00:00:25,453 --> 00:00:27,203
对于每个词发生的二元变量

8
00:00:27,203 --> 00:00:32,020
这个二元变量表明这个词是否

9
00:00:32,020 --> 00:00:35,144
来自0d或0p

10
00:00:35,144 --> 00:00:38,420
这里我们来看一些这些变量可能的值

11
00:00:38,420 --> 00:00:43,470
比如，这个背景下，z值是1

12
00:00:43,470 --> 00:00:45,105
另一方面，文本

13
00:00:45,105 --> 00:00:52,040
是来自话题，那么这时z是0

14
00:00:53,260 --> 00:00:58,915
现在，当然我们没看到这些z值，我们只是想象它们是如此

15
00:00:58,915 --> 00:01:01,875
z的值和其他词语相连

16
00:01:02,905 --> 00:01:04,975
这就是为什么我们称它为隐藏变量

17
00:01:06,135 --> 00:01:08,905
现在，我们刚刚讨论的想法

18
00:01:08,905 --> 00:01:12,930
来预测词语分布在我们获得词语的时候已经被使用

19
00:01:12,930 --> 00:01:18,840
这是一个自变量吗？这个隐藏变量的值？

20
00:01:18,840 --> 00:01:25,080
EM将会这么操作

21
00:01:25,080 --> 00:01:30,060
首先，我们给所有参数分配随机数

22
00:01:30,060 --> 00:01:34,960
在我们的例子中，参数主要是概率

23
00:01:34,960 --> 00:01:37,840
一个词语的概率，在θd的情况下

24
00:01:37,840 --> 00:01:39,680
这是初始阶段

25
00:01:39,680 --> 00:01:44,150
这些初始值令我们能使用简单的方式来猜测

26
00:01:44,150 --> 00:01:48,510
这些z的值，那么我们来猜一下

27
00:01:48,510 --> 00:01:53,580
我们不能确定文本是不是来自背景资料

28
00:01:53,580 --> 00:01:55,090
但我们可以来猜测

29
00:01:55,090 --> 00:01:57,620
由这个公式可以得到

30
00:01:57,620 --> 00:01:59,710
这被称为是E-step

31
00:01:59,710 --> 00:02:06,520
演算公式将使用E-step来猜测z值

32
00:02:06,520 --> 00:02:12,190
之后，将要借助另一个称为M-step的算式

33
00:02:12,190 --> 00:02:17,490
这一步我们只需利用推断的z值

34
00:02:17,490 --> 00:02:22,825
之后只要把相同分布的词分到一组

35
00:02:22,825 --> 00:02:26,315
从那个基础上并且包括这个

36
00:02:27,585 --> 00:02:32,865
我们之后就可以用正态计数去预测概率

37
00:02:32,865 --> 00:02:35,479
或者修正我们对参数的预测

38
00:02:36,590 --> 00:02:42,310
那么也让我来展示一下我们将那些

39
00:02:42,310 --> 00:02:46,760
认为来自0d的词，以及

40
00:02:46,760 --> 00:02:50,010
文本挖掘演算法，如聚类

41
00:02:51,760 --> 00:02:55,718
我们把它们分组来帮我们

42
00:02:55,718 --> 00:03:01,170
重新预测感兴趣的参数

43
00:03:01,170 --> 00:03:05,120
这些将帮助我们预测这些参数

44
00:03:06,170 --> 00:03:09,970
注意之前我们只是把它们的值设成随机

45
00:03:09,970 --> 00:03:15,670
但这个猜测下，我们将有一些更进步的预测

46
00:03:15,670 --> 00:03:18,740
当然不能准确地知道这是0还是1

47
00:03:18,740 --> 00:03:24,850
所以我们不会真的用一个难的方法分开它们

48
00:03:24,850 --> 00:03:26,800
却而代之的是我们将用一个更柔和方法

49
00:03:26,800 --> 00:03:27,980
就是发生在这里的行为

50
00:03:29,150 --> 00:03:34,420
我们将用概率来调整计数

51
00:03:34,420 --> 00:03:38,410
用θd求得的概率

52
00:03:39,840 --> 00:03:42,580
你可以看见这个，这从哪里来的？

53
00:03:42,580 --> 00:03:46,630
从这里来的，对吧？

54
00:03:46,630 --> 00:03:48,120
从E-STEP中

55
00:03:48,120 --> 00:03:52,472
EM演算法能迭代提高你的原始

56
00:03:52,472 --> 00:03:57,375
参数预测，通过利用E-STEP和M-STEP

57
00:03:57,375 --> 00:04:02,458
E-STEP是用附加信息来支持数据，如z

58
00:04:02,458 --> 00:04:05,910
M-STEP是利用

59
00:04:05,910 --> 00:04:08,660
附加信息来分开数据

60
00:04:08,660 --> 00:04:13,467
分隔数据账目并收集正确的数据账目

61
00:04:13,467 --> 00:04:17,870
重新预测参数

62
00:04:17,870 --> 00:04:22,400
一旦我们有新一代的参数，我们就再重复它

63
00:04:22,400 --> 00:04:25,150
重复E-STEP

64
00:04:25,150 --> 00:04:28,520
来提升我们对隐藏变量的预测

65
00:04:28,520 --> 00:04:33,630
之后它会引到另一类再次预测的参数

66
00:04:34,770 --> 00:04:37,910
至于我们感兴趣的词语分布

67
00:04:39,610 --> 00:04:44,670
好，如我所说，两者之间的桥梁

68
00:04:44,670 --> 00:04:50,380
就是变量z，隐藏变量，即告诉我们有多大可能

69
00:04:50,380 --> 00:04:55,200
这是来源于分布的顶端，θp

70
00:04:56,810 --> 00:05:00,780
这一页有很多你可能需要的内容

71
00:05:00,780 --> 00:05:03,850
暂停让读者来消化一下

72
00:05:03,850 --> 00:05:07,300
但这只是基本地概括EM演算的精髓

73
00:05:07,300 --> 00:05:12,500
从随机的初始值

74
00:05:12,500 --> 00:05:18,150
借助E-STEP和M-STEP来提高

75
00:05:18,150 --> 00:05:19,690
参数的设定

76
00:05:19,690 --> 00:05:23,340
之后我们重复这些步骤，这就是爬山算法

77
00:05:23,340 --> 00:05:27,060
能逐渐提升参数的预测值

78
00:05:27,060 --> 00:05:30,050
之后我会解释这其中有个保证

79
00:05:30,050 --> 00:05:35,340
来达到一个似然函数的本地最大值

80
00:05:35,340 --> 00:05:40,180
让我们来看一下一个例子的计算

81
00:05:40,180 --> 00:05:41,840
这些公式就是EM

82
00:05:41,840 --> 00:05:48,220
这些你们之前看到过的，你们也能看到这里的上标

83
00:05:48,220 --> 00:05:53,720
这里，比如这里，n，来表示参数的生成

84
00:05:53,720 --> 00:05:56,040
例如这里我们有n+1

85
00:05:56,040 --> 00:05:59,728
那表示我们已经提升了

86
00:05:59,728 --> 00:06:04,047
从这到这有一个提升

87
00:06:04,047 --> 00:06:08,106
这个设定里我们假设两个数字有同等的概率

88
00:06:08,106 --> 00:06:09,689
并且背景模型是空值

89
00:06:09,689 --> 00:06:11,872
那么这在统计上有什么关系呢

90
00:06:11,872 --> 00:06:13,892
其实这是词语计数问题

91
00:06:13,892 --> 00:06:18,290
假设我们有四个词，它们计数是这样

92
00:06:18,290 --> 00:06:22,680
我们的背景模型分配给常见词比如the

93
00:06:22,680 --> 00:06:23,380
一个高频出现概率

94
00:06:25,910 --> 00:06:29,860
在第一个循环里，你能画出这个过程

95
00:06:29,860 --> 00:06:32,280
那么我们来设定初始值

96
00:06:32,280 --> 00:06:37,360
这里，我们感兴趣的概率是一个正态均一

97
00:06:37,360 --> 00:06:38,890
是一个正态均一的分布

98
00:06:40,330 --> 00:06:45,940
之后E-STEP会告诉我们一个已使用的分布猜测

99
00:06:45,940 --> 00:06:48,470
来获得每个词

100
00:06:48,470 --> 00:06:51,450
我们能看到每个词有不同的概率

101
00:06:51,450 --> 00:06:52,430
为什么？

102
00:06:52,430 --> 00:06:56,840
那是因为这些词在背景中有不同的概率

103
00:06:56,840 --> 00:07:00,020
所以尽管两个分布是同等概率

104
00:07:00,020 --> 00:07:05,320
我们的初次尝试比如均匀分布，由于

105
00:07:05,320 --> 00:07:09,270
背景分布的不同，我们对概率有不同的猜测

106
00:07:09,270 --> 00:07:14,280
我们相信这些词更可能出现在话题中

107
00:07:15,820 --> 00:07:17,930
这些更不可能

108
00:07:17,930 --> 00:07:19,030
大约是源于背景

109
00:07:20,620 --> 00:07:23,040
一旦我们得到这些z值

110
00:07:23,040 --> 00:07:28,810
我们知道在M-STEP里4概率将会用来调整计数

111
00:07:28,810 --> 00:07:33,670
4必须和0.33相乘

112
00:07:33,670 --> 00:07:38,190
来获得主题的分配计数

113
00:07:39,550 --> 00:07:43,770
这由乘数得到

114
00:07:43,770 --> 00:07:49,700
注意如果我们的猜测说这是100%，即1.0

115
00:07:52,380 --> 00:07:58,010
那么我么就得到这个词语在这个话题里的所有计数

116
00:07:58,010 --> 00:08:01,200
一般来说不会是1.0

117
00:08:01,200 --> 00:08:06,760
只是这个主题的计数百分比

118
00:08:06,760 --> 00:08:09,550
之后我们取这些数的标准差

119
00:08:09,550 --> 00:08:13,170
来得到一组新的参数预测

120
00:08:13,170 --> 00:08:16,600
所以你能看到，它和旧的相比，就是这里

121
00:08:18,330 --> 00:08:23,060
这两个一比较我们能看见它们的概率不同

122
00:08:23,060 --> 00:08:25,930
不止这个，我们也能看到

123
00:08:25,930 --> 00:08:30,110
一些我们相信来自话题的会有更大的概率

124
00:08:30,110 --> 00:08:31,400
比如这个，text

125
00:08:32,530 --> 00:08:35,930
当然，这组新的参数可以令我们

126
00:08:35,930 --> 00:08:42,680
进一步调整推断的隐藏变量或隐藏的变量值

127
00:08:42,680 --> 00:08:45,742
我们现在有一组新数据

128
00:08:45,742 --> 00:08:51,115
由基于新参数的E-STEP得到

129
00:08:51,115 --> 00:08:56,343
这些z的新推测值会给我们

130
00:08:56,343 --> 00:09:03,166
另一组词语的概率预测

131
00:09:03,166 --> 00:09:07,990
这就是当我们用EM演算法计算这些概率时

132
00:09:07,990 --> 00:09:11,750
将会发生的过程

133
00:09:11,750 --> 00:09:16,745
你们可以看到最后一行我们展示了似然预测

134
00:09:16,745 --> 00:09:20,985
并且当我们迭代计算时概率会增加

135
00:09:20,985 --> 00:09:25,875
注意这些似然是负数，因为概率是

136
00:09:25,875 --> 00:09:30,070
0到1之间，那么它就会得到负数

137
00:09:30,070 --> 00:09:33,180
有趣的是，你会注意到最后一列

138
00:09:33,180 --> 00:09:36,600
这些反转了词语分断

139
00:09:36,600 --> 00:09:42,150
这些来自一个分布的词语的概率

140
00:09:42,150 --> 00:09:47,980
在这里即话题分布

141
00:09:47,980 --> 00:09:50,580
以及你也许会好奇这是否有用

142
00:09:50,580 --> 00:09:55,540
因为我们的目标是预测这些词语的分布

143
00:09:55,540 --> 00:09:57,400
所以这是我们的原始目标

144
00:09:57,400 --> 00:10:00,900
我们希望得到一个更有区分性的分布

145
00:10:00,900 --> 00:10:04,400
但最后一列也是双积

146
00:10:04,400 --> 00:10:07,170
这其实也是很有用的

147
00:10:07,170 --> 00:10:08,380
你们可以思考一下

148
00:10:08,380 --> 00:10:10,220
我们想用来

149
00:10:10,220 --> 00:10:16,080
比如预测文本覆盖背景词语到什么程度

150
00:10:16,080 --> 00:10:18,165
这里，当我们加进去

151
00:10:18,165 --> 00:10:23,304
或取平均值，我们或多或少会知道覆盖了多少

152
00:10:23,304 --> 00:10:27,823
相比那些没被背景很好地解释的

153
00:10:27,823 --> 00:10:37,823
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community