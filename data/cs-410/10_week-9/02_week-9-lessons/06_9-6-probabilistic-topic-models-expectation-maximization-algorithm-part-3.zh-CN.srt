1
00:00:07,553 --> 00:00:12,636
我刚展示了在经验上可能性将趋于汇集

2
00:00:12,636 --> 00:00:17,041
但理论上也可以证明EM演算

3
00:00:17,041 --> 00:00:19,295
能趋于一个本地最大值

4
00:00:19,295 --> 00:00:24,925
这里是关于已发生的证明和具体解释

5
00:00:24,925 --> 00:00:29,613
这需要更多相关的知识

6
00:00:29,613 --> 00:00:36,910
一些不等式我们还没有讲到

7
00:00:39,380 --> 00:00:45,040
所以这里你看到的在X轴有个C0值

8
00:00:45,040 --> 00:00:46,799
这是我们有的参数

9
00:00:46,799 --> 00:00:49,714
y轴上我们能看到似然函数

10
00:00:49,714 --> 00:00:57,171
这条曲线是原似然函数

11
00:00:57,171 --> 00:01:04,110
这是我们想要最大化的

12
00:01:04,110 --> 00:01:06,630
我们想要找到一个C0值来取最大值

13
00:01:06,630 --> 00:01:11,480
但在Mitsumoto的例子中我们无法找到一个简单的分析方法

14
00:01:11,480 --> 00:01:12,470
来解决问题

15
00:01:12,470 --> 00:01:14,698
我们必须解决数字错误

16
00:01:14,698 --> 00:01:16,457
EM演算公式就是这样一个算法

17
00:01:16,457 --> 00:01:17,850
这是一个爬坡演算法

18
00:01:17,850 --> 00:01:22,490
那意味着你从某种随机猜测开始

19
00:01:22,490 --> 00:01:26,260
比如从这里开始，这是你的起点

20
00:01:26,260 --> 00:01:32,090
接着你试着把这个挪到

21
00:01:32,090 --> 00:01:35,420
另一个能取得更大概率的点

22
00:01:35,420 --> 00:01:37,630
那是个理想的爬坡

23
00:01:37,630 --> 00:01:43,030
在EM演算中，我们实现这个目标是通过如下两点

24
00:01:43,030 --> 00:01:46,940
首先，我们将使用一个似然函数的下限

25
00:01:46,940 --> 00:01:48,628
这个就是下限

26
00:01:48,628 --> 00:01:49,128
看这里

27
00:01:51,010 --> 00:01:57,560
一旦踩到下限，我们就可以最大化它

28
00:01:57,560 --> 00:01:59,420
当然，能这样操作的原因是

29
00:01:59,420 --> 00:02:02,850
下限比较容易优化

30
00:02:02,850 --> 00:02:05,780
我们知道我们现在的猜测是这里

31
00:02:05,780 --> 00:02:11,530
最大化下限时我们将把这个点移到顶端

32
00:02:11,530 --> 00:02:12,030
到这里

33
00:02:13,300 --> 00:02:14,650
对吧?

34
00:02:14,650 --> 00:02:20,150
我们可以映射到原始似然函数上，找到这个点

35
00:02:20,150 --> 00:02:25,600
因为这是个下限，我们肯定能改进这个猜测，对吧？

36
00:02:25,600 --> 00:02:30,570
因为我们提升了下限，跟着原始概率

37
00:02:30,570 --> 00:02:35,040
曲线在这下限之上肯定也会提升

38
00:02:36,310 --> 00:02:39,090
那我们已知下限在提升

39
00:02:39,090 --> 00:02:42,440
我们的确是改进了这个原始似然函数

40
00:02:42,440 --> 00:02:47,253
即在这下限之上的

41
00:02:47,253 --> 00:02:49,770
这个例子中

42
00:02:49,770 --> 00:02:53,520
现在的猜测是已知的参数值

43
00:02:53,520 --> 00:02:57,660
下一个猜测是重新评估的参数值

44
00:02:57,660 --> 00:03:01,110
从这个说明中你可以看到下一个猜测

45
00:03:01,110 --> 00:03:03,620
总是比现在这个好

46
00:03:03,620 --> 00:03:06,930
除非已经到达最大值，就会停在这里

47
00:03:06,930 --> 00:03:08,008
两个猜测就会相等

48
00:03:08,008 --> 00:03:12,821
所以，E-STEP就是

49
00:03:12,821 --> 00:03:17,650
来计算这个下限

50
00:03:17,650 --> 00:03:22,061
我们不直接计算这个似然函数

51
00:03:22,061 --> 00:03:25,452
但我们计算变量值的长度

52
00:03:25,452 --> 00:03:28,990
这些基本上也是下限的一部分

53
00:03:28,990 --> 00:03:31,150
这帮助我们决定下限

54
00:03:31,150 --> 00:03:34,460
另一方面，M-STEP也是来最大化下限

55
00:03:34,460 --> 00:03:37,480
这使得我们能将参数移动到一个新的位置

56
00:03:37,480 --> 00:03:41,460
这就是为什么EM算法能确保汇聚于一局部的最大值

57
00:03:42,490 --> 00:03:46,720
现在你可以想象，当我们有很多局部最大值

58
00:03:46,720 --> 00:03:50,100
我们也必须去重复EM演算

59
00:03:50,100 --> 00:03:54,340
来找出哪个是实际的整体最大值

60
00:03:54,340 --> 00:03:59,070
这在数字优化上一般很困难

61
00:03:59,070 --> 00:04:02,689
比如我们从这里出发

62
00:04:02,689 --> 00:04:06,223
那么我们逐渐爬到这个顶端

63
00:04:06,223 --> 00:04:11,227
那不是最理想的，我们要一路爬到这里

64
00:04:11,227 --> 00:04:16,575
唯一的方法从这里的这儿或那儿开始

65
00:04:16,575 --> 00:04:22,767
在EM演算中，我们通常从不同的点出发

66
00:04:22,767 --> 00:04:27,880
或用其他方法来确定一个好的原始起点

67
00:04:29,840 --> 00:04:34,320
总结来说，我们介绍了EM演算法

68
00:04:34,320 --> 00:04:38,683
这是一种计算最大概率预测的算法

69
00:04:38,683 --> 00:04:42,153
对于所有模型而不是我们这里简单的模型

70
00:04:42,153 --> 00:04:46,468
这是爬坡算法，所以这只能趋近于局部最大值

71
00:04:46,468 --> 00:04:48,250
并且这取决于起点

72
00:04:49,770 --> 00:04:55,414
大致上我们有两步来改进预测

73
00:04:55,414 --> 00:05:00,270
E-STEP中，我们大体知道

74
00:05:00,270 --> 00:05:05,560
即通过预测有用的隐藏变量可用于简化估算

75
00:05:05,560 --> 00:05:10,056
我们的例子中就是用来取词的分布

76
00:05:10,056 --> 00:05:15,750
M-STEP中我们利用使得

77
00:05:15,750 --> 00:05:20,790
分布更好预测的增强版数据来改进参数的预测

78
00:05:20,790 --> 00:05:24,860
在似然函数方面，提升是必然的

79
00:05:24,860 --> 00:05:30,240
注意我们有个稳定的参数值聚集不是必要的

80
00:05:30,240 --> 00:05:35,260
尽管似然函数肯定能提高

81
00:05:35,260 --> 00:05:40,370
这里有些属性是必须满足的

82
00:05:40,370 --> 00:05:44,640
从而使参数也能趋于某个稳定值

83
00:05:47,500 --> 00:05:50,790
这里数据增强是通过可能概率来完成

84
00:05:50,790 --> 00:05:51,360
这意味着

85
00:05:51,360 --> 00:05:54,830
我们不准备单说隐藏变量是什么值

86
00:05:54,830 --> 00:05:59,390
但我们会有个概率分布

87
00:05:59,390 --> 00:06:01,140
即隐藏变量的可能取值

88
00:06:01,140 --> 00:06:05,990
所以这造成一些事件的概率上的计数拆分

89
00:06:07,430 --> 00:06:12,783
在我们的例子中我们将词语计数从两个分布中分开

90
00:06:12,783 --> 00:06:22,783
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community