我刚展示了在经验上可能性将趋于汇集 但理论上也可以证明EM演算 能趋于一个本地最大值 这里是关于已发生的证明和具体解释 这需要更多相关的知识 一些不等式我们还没有讲到 所以这里你看到的在X轴有个C0值 这是我们有的参数 y轴上我们能看到似然函数 这条曲线是原似然函数 这是我们想要最大化的 我们想要找到一个C0值来取最大值 但在Mitsumoto的例子中我们无法找到一个简单的分析方法 来解决问题 我们必须解决数字错误 EM演算公式就是这样一个算法 这是一个爬坡演算法 那意味着你从某种随机猜测开始 比如从这里开始，这是你的起点 接着你试着把这个挪到 另一个能取得更大概率的点 那是个理想的爬坡 在EM演算中，我们实现这个目标是通过如下两点 首先，我们将使用一个似然函数的下限 这个就是下限 看这里 一旦踩到下限，我们就可以最大化它 当然，能这样操作的原因是 下限比较容易优化 我们知道我们现在的猜测是这里 最大化下限时我们将把这个点移到顶端 到这里 对吧? 我们可以映射到原始似然函数上，找到这个点 因为这是个下限，我们肯定能改进这个猜测，对吧？ 因为我们提升了下限，跟着原始概率 曲线在这下限之上肯定也会提升 那我们已知下限在提升 我们的确是改进了这个原始似然函数 即在这下限之上的 这个例子中 现在的猜测是已知的参数值 下一个猜测是重新评估的参数值 从这个说明中你可以看到下一个猜测 总是比现在这个好 除非已经到达最大值，就会停在这里 两个猜测就会相等 所以，E-STEP就是 来计算这个下限 我们不直接计算这个似然函数 但我们计算变量值的长度 这些基本上也是下限的一部分 这帮助我们决定下限 另一方面，M-STEP也是来最大化下限 这使得我们能将参数移动到一个新的位置 这就是为什么EM算法能确保汇聚于一局部的最大值 现在你可以想象，当我们有很多局部最大值 我们也必须去重复EM演算 来找出哪个是实际的整体最大值 这在数字优化上一般很困难 比如我们从这里出发 那么我们逐渐爬到这个顶端 那不是最理想的，我们要一路爬到这里 唯一的方法从这里的这儿或那儿开始 在EM演算中，我们通常从不同的点出发 或用其他方法来确定一个好的原始起点 总结来说，我们介绍了EM演算法 这是一种计算最大概率预测的算法 对于所有模型而不是我们这里简单的模型 这是爬坡算法，所以这只能趋近于局部最大值 并且这取决于起点 大致上我们有两步来改进预测 E-STEP中，我们大体知道 即通过预测有用的隐藏变量可用于简化估算 我们的例子中就是用来取词的分布 M-STEP中我们利用使得 分布更好预测的增强版数据来改进参数的预测 在似然函数方面，提升是必然的 注意我们有个稳定的参数值聚集不是必要的 尽管似然函数肯定能提高 这里有些属性是必须满足的 从而使参数也能趋于某个稳定值 这里数据增强是通过可能概率来完成 这意味着 我们不准备单说隐藏变量是什么值 但我们会有个概率分布 即隐藏变量的可能取值 所以这造成一些事件的概率上的计数拆分 在我们的例子中我们将词语计数从两个分布中分开 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community