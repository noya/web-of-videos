那么 这就是期望最大化算法(Expectation-Maximization)或者也叫EM 演算法 所以在所有的最大期望演算法里我们加入一个隐形变量 来帮助我们更容易解决问题 在我们的例子中隐藏变量是一个 对于每个词发生的二元变量 这个二元变量表明这个词是否 来自0d或0p 这里我们来看一些这些变量可能的值 比如，这个背景下，z值是1 另一方面，文本 是来自话题，那么这时z是0 现在，当然我们没看到这些z值，我们只是想象它们是如此 z的值和其他词语相连 这就是为什么我们称它为隐藏变量 现在，我们刚刚讨论的想法 来预测词语分布在我们获得词语的时候已经被使用 这是一个自变量吗？这个隐藏变量的值？ EM将会这么操作 首先，我们给所有参数分配随机数 在我们的例子中，参数主要是概率 一个词语的概率，在θd的情况下 这是初始阶段 这些初始值令我们能使用简单的方式来猜测 这些z的值，那么我们来猜一下 我们不能确定文本是不是来自背景资料 但我们可以来猜测 由这个公式可以得到 这被称为是E-step 演算公式将使用E-step来猜测z值 之后，将要借助另一个称为M-step的算式 这一步我们只需利用推断的z值 之后只要把相同分布的词分到一组 从那个基础上并且包括这个 我们之后就可以用正态计数去预测概率 或者修正我们对参数的预测 那么也让我来展示一下我们将那些 认为来自0d的词，以及 文本挖掘演算法，如聚类 我们把它们分组来帮我们 重新预测感兴趣的参数 这些将帮助我们预测这些参数 注意之前我们只是把它们的值设成随机 但这个猜测下，我们将有一些更进步的预测 当然不能准确地知道这是0还是1 所以我们不会真的用一个难的方法分开它们 却而代之的是我们将用一个更柔和方法 就是发生在这里的行为 我们将用概率来调整计数 用θd求得的概率 你可以看见这个，这从哪里来的？ 从这里来的，对吧？ 从E-STEP中 EM演算法能迭代提高你的原始 参数预测，通过利用E-STEP和M-STEP E-STEP是用附加信息来支持数据，如z M-STEP是利用 附加信息来分开数据 分隔数据账目并收集正确的数据账目 重新预测参数 一旦我们有新一代的参数，我们就再重复它 重复E-STEP 来提升我们对隐藏变量的预测 之后它会引到另一类再次预测的参数 至于我们感兴趣的词语分布 好，如我所说，两者之间的桥梁 就是变量z，隐藏变量，即告诉我们有多大可能 这是来源于分布的顶端，θp 这一页有很多你可能需要的内容 暂停让读者来消化一下 但这只是基本地概括EM演算的精髓 从随机的初始值 借助E-STEP和M-STEP来提高 参数的设定 之后我们重复这些步骤，这就是爬山算法 能逐渐提升参数的预测值 之后我会解释这其中有个保证 来达到一个似然函数的本地最大值 让我们来看一下一个例子的计算 这些公式就是EM 这些你们之前看到过的，你们也能看到这里的上标 这里，比如这里，n，来表示参数的生成 例如这里我们有n+1 那表示我们已经提升了 从这到这有一个提升 这个设定里我们假设两个数字有同等的概率 并且背景模型是空值 那么这在统计上有什么关系呢 其实这是词语计数问题 假设我们有四个词，它们计数是这样 我们的背景模型分配给常见词比如the 一个高频出现概率 在第一个循环里，你能画出这个过程 那么我们来设定初始值 这里，我们感兴趣的概率是一个正态均一 是一个正态均一的分布 之后E-STEP会告诉我们一个已使用的分布猜测 来获得每个词 我们能看到每个词有不同的概率 为什么？ 那是因为这些词在背景中有不同的概率 所以尽管两个分布是同等概率 我们的初次尝试比如均匀分布，由于 背景分布的不同，我们对概率有不同的猜测 我们相信这些词更可能出现在话题中 这些更不可能 大约是源于背景 一旦我们得到这些z值 我们知道在M-STEP里4概率将会用来调整计数 4必须和0.33相乘 来获得主题的分配计数 这由乘数得到 注意如果我们的猜测说这是100%，即1.0 那么我么就得到这个词语在这个话题里的所有计数 一般来说不会是1.0 只是这个主题的计数百分比 之后我们取这些数的标准差 来得到一组新的参数预测 所以你能看到，它和旧的相比，就是这里 这两个一比较我们能看见它们的概率不同 不止这个，我们也能看到 一些我们相信来自话题的会有更大的概率 比如这个，text 当然，这组新的参数可以令我们 进一步调整推断的隐藏变量或隐藏的变量值 我们现在有一组新数据 由基于新参数的E-STEP得到 这些z的新推测值会给我们 另一组词语的概率预测 这就是当我们用EM演算法计算这些概率时 将会发生的过程 你们可以看到最后一行我们展示了似然预测 并且当我们迭代计算时概率会增加 注意这些似然是负数，因为概率是 0到1之间，那么它就会得到负数 有趣的是，你会注意到最后一列 这些反转了词语分断 这些来自一个分布的词语的概率 在这里即话题分布 以及你也许会好奇这是否有用 因为我们的目标是预测这些词语的分布 所以这是我们的原始目标 我们希望得到一个更有区分性的分布 但最后一列也是双积 这其实也是很有用的 你们可以思考一下 我们想用来 比如预测文本覆盖背景词语到什么程度 这里，当我们加进去 或取平均值，我们或多或少会知道覆盖了多少 相比那些没被背景很好地解释的 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community