[背景音乐]本节课接上一节 继续讨论概率主题模型 在这节课中 我们将要继续讨论概率模型 我们将讨论一个非常简单的情形 其中 我们关注于只从一个文档中挖掘一个主题 在这个简单的设置中 我们感兴趣的是 分析一篇文档并从中挖掘一个主题 这是主题模型最简单的例子 现在的输入主题数不再是k 因为我们已经知道只有一篇文档和一个主题 对于输出 我们也不再需要覆盖率 因为我们已经假设这篇文档对于这个主题的覆盖率是100% 所以主要任务就是挖掘这个单一主题的词语分布 如图所示 一如往常 当我们考虑使用生成模型解决类似问题时 首先考虑的是我们将为怎样的数据建模 或者说 我们将以什么样的角度
来为这些数据或数据表示建模 然后我们才会从我们的角度 为这些数据生成设计特定的模型 我们的角度意思是
我们想从某一个特定的角度来看待这些数据 所以模型才会拥有适当的参数 以发掘我们想要的知识 接着我们会考虑似然函数 或者写下某些似然函数以正式地获得 这个模型中某个数据点的概率 似然函数包含参数 接下来我们讨论对这些参数的估计  比如说 通过将概率最大化我们得到的是极大似然估计值 这些参数估计值将成为挖掘算法的输出 即我们将这些参数估计值 作为我们从文本中挖掘到的知识 让我们来看看这个简单例子中的这些步骤 稍后我们将看到这些步骤在更复杂的例子中的应用 在这个例子中 我们的数据 即这篇文档
仅仅是一个词序列 这里的每一个词表示为Xi 我们的模型是一元语法模型 我们的目标是
一个我们希望用来表示一个主题的词语分布 所以参数的数量将和单词表中的单词数一致
在本例中用M表示 方便起见 我们用Ɵi 表示单词Wi的概率 显然这些Ɵi的和为1 现在我们的似然函数是什么样子？ 它正是给出的这个模型 生成整个文档的概率 因为我们假设单词生成的独立性 所以文档的概率就是每个单词的概率的积 既然某些单词重复出现 我们可以将这个积写成另一种形式 在这一行 我们将公式重写成
单词表中所有不同单词的积 从W1到Wm 现在它与前面那一行有了区别 它覆盖了所有在文档中不同位置的单词 做这个变换的时候 我们需要 介绍一下这个计数函数 它表示某一个单词在文中出现的次数 同样 这个计数函数表示Wn在文中出现的次数 --因为这些单词有可能重复出现 你可以看到 如果一个单词在文中没有出现 它的计数为0 则其相应的项在公式中也不会出现 所以这是一种书写似然函数的非常有用的形式 我们后面会经常用到 所以我想要你对此予以重视 熟悉这个形式 这是所有单词表中不同单词的积的另一种表示 最后 当然 我们用Ɵi来表达这个似然函数 如图所示 接下来我们要寻求最大化似然函数值的 Ɵ值或者说这些单词的概率 现在让我们进一步了解极大似然估计问题 这一行是从上一页ppt拷贝过来的 也就是我们的似然函数 我们的目标是最大化这个似然函数 我们将发现通常 局部最大化比全局最大化更容易实现 这个形式只是为了计算方便 因为对数转换使乘法变成了加法 而且我们有关于这些概率的约束 和更容易求导 其经常用于 求解优化问题 所以请再看一下这个和 这里 这是你后面会经常看到的函数形式 --在讲述更通用的主题模型的时候 这是包含了单词表中所有单词的一个和式 每一项都是单词的计数 与概率对数的乘积 让我们看看怎样来解决这个问题 此时这个问题就成了一个单纯的数学问题 因为我们只是解一个约束最大化问题 目标函数是一个似然函数 约束是所有概率的和为1 一种求解的方法是拉格朗日乘数法 具体内容已超出了本书的范围 但是因为拉格朗日乘数法应用非常广泛 我在这里为感兴趣的同学做一个基本的介绍 根据这个方法 我们将构建一个拉格朗日函数 这里 这个函数结合了我们的目标函数 和包含了我们的约束的另一项 我们在此引入拉格朗日乘子 λ 所以这是一个额外加入的参数 现在 这个方法的思想是将约束优化问题转化为 某种意义上的非约束优化问题 现在我们只要关注拉格朗日函数的优化问题了 你可能记起微积分的知识 极值点（optimal point） 出现在导数为0的时候 这是一个必要条件 虽非充分条件 所以如果我们这样做 你可以看到Ɵi的偏导 等于这个 这一部分来自于拉格朗日函数的偏导 λ则只是从这里搬过来 我们将其赋值为0 我们容易看出Ɵi与λ的关系是这样的 既然我们已知所有Ɵi的和为1 我们将其代入这个约束  这里 这样我们就解得λ 这只是所有计数的和取负 这使我们进一步解出这个优化问题 最终求得Ɵi的最优解 如果你看下这个公式就会发现它很直观 因为它就是用文本长度正则化后的计数值 文本长度同时也是文本中所有单词的计数和 经过所有这些计算 最终我们得到非常直观的结果 同我们的直觉一样我们想要 通过赋予所有已观察到的单词
尽可能大的概率 以获得最大的数值 同时我们也可能注意到 这是极大似然估计法的一般结果 通常来说 估计法将使计数正则化 而有时计数必须以某种特别的方式进行
你之后会看到 所以这只是我们的优化问题的解析解 但是通常情况下 当解析函数太复杂时 我们将不会使用闭合公式解优化问题 而是会采用一些数值算法 我们后面会遇到这样的例子 想像一下 如果我们用这样的极大似然估计法 来估计单一文本d中单个主题的估计
我们能得到什么？ 假设这个文本是一篇数据挖据的论文 你可以想象 我们看到的将会是这样 在最上面的是概率最高的单词 他们通常是非常常见的词
在英语中经常是功能词 接下来是一些能真正表征这个主题的实义词 比如  text  mining 等 最后你也会看到一些小概率的 并非与主题有实际联系的词 他们也在文档中延伸地提及 作为一个主题表示 你会看出它并不理想 是不是？ 这是因为高概率的词是功能词 他们并不能真正描述主题特征 所以我的问题是 怎样去掉这些常见词？ 这是我们下一节的主题 我们将讨论如何利用概率模型以某种方式 消除这些常用词 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community