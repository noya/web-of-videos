[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community 现在 让我们对这个问题 
--尤其是两中不同的参数估计方法 进行详细讨论 一种是我们已经介绍过的极大似然估计 另一种是贝叶斯估计 在极大似然估计中 我们将最优定义为 数据似然值达到最大 其正式的表述如ppt所示 我们将估计定义为
给定Ɵ下的X的条件概率的arg max 这里的arg max仅仅表示这个函数将以 使这个函数取得最大值的参数的值最为返回值 即arg max的值不是这个概率函数的值 而是使得函数值达到最大时的参数值 所以在这个公式中arg max返回值为Ɵ 正是Ɵ决定了X的条件概率
--某个特定的Ɵ使其达到最大值 因此这个估计方法符合我们的直觉
通常也很有用 它寻求能够最好地解释数据的参数 但是当数据太小时它也会产生问题 -因为如果数据点太小 则我们只有很少的数据点 在样本集合很小的情况下 
如果我们完全依赖于已有数据 并且试图使估计拟合这些数据
则将导致偏差 以文本数据为例，假设我们观察到100个单词 其中并未包含另一个与文本挖掘有关的单词 我们的极大似然估计将为
这个单词的概率赋值为0 因为 如果赋予它非零值 某些已观察到的单词的概率值将被减小 这显然不是被观察数据 由极大似然估计法得到的最优 但是这种未观察到的单词的0概率 有时候可能并不合理 尤其当我们想要用这个分布来
描述文本挖掘主题的特征时 解决这个问题的一种方式是使用贝叶斯（Bayesian）估计 它能使我们兼顾数据 与参数的先验知识 我们假设我们对于参数有前置信念（prior belief） 当然在这个例子中 我们不仅要关注数据 还要考虑先验 这里的先验由P(Ɵ)定义 即 我们对某些Ɵ值施加倾向 通过使用贝叶斯定理 如图所示 我们可以组合出似然函数 将先验考虑进去 我们得到这样的参数后验概率 完整的贝叶斯定理的解释及相关证明 超出了课程的范围 不过我要在这里做一个简单的介绍 因为这是很有用的常识 贝叶斯定理的基本定义如ppt所示 它使我们能够用给定X条件下的Y的概率 表述给定Y时X的条件概率 你可以看到这2个概率 在2个变量的顺序上是不同的 但是这个定理通常用于推导变量 让我们再来看看 我们假设p(X)表述的是我们对于X的先验信念 即在观察到任何其他数据之前
我们已经对X有了信念(belief) --我们相信X取某些值的概率比其他值更高 然后 这里是给定Y条件下的X的概率 这是一个条件概率
也是我们关于X的后验信念 因为这是我们观察到Y之后对于X值分布的信念 在已经观察到了Y之后
现在我们对X的信念是什么？ 我们仍然相信取某些值的概率比其他值率高么？ 现在这2个概率通过这个概率联系到一起 这个概率可以看成 对于特定的X 观察到的证据Y的概率 你可以将X看成我们的假设 对于选择哪个假设我们事先有一些想法 观察到Y之后 我们将修正我们的信念 修正的公式基于 先验和X确实为真的条件下观察到的Y的可能性
的组合 关于贝叶斯定理的解释先到这里 本课程中 我们感兴趣的是推导Ɵ 我们的先验在这里
它包括参数相关的先验知识 然后我们还有数据似然值  这里 它能告诉我们
怎样的参数值能恰当地解释数据 而后验概率结合两者 所以它体现了这两种倾向的折衷 在这种情况下 我们可以将后验概率最大化 以寻求将后验概率最大化的Ɵ 这种估计方法叫做最大后验估计
简称MAP估计 这种估计方法 比极大似然估计更通用 因为如果我们定义的先验不包含任何信息 即意味着所有Ɵ值的均匀分布 没有偏向 则我们实际上就回到了极大似然估计 这种情况下 它将主要取决于似然值 与这里一样 但是如果我们有包含信息的先验 有对于不同值的偏向
则MAP估计能使我们利用这些信息 当然 这里的问题是如何定义先验 没有免费的午餐
如果我们想要利用更多的知识来解决问题 我们必须先有这些知识 而且在理想情况下
这些知识应当是可靠的 否则你的估计不一定 比极大似然估计的结果更精确 现在让我们来详细看看贝叶斯估计 我将Ɵ值以一维数值来表示 这当然经过了简化 我们感兴趣的是哪个Ɵ值最优 首先 我们有先验 先验告诉我们 我们相信某些值比其他值具有更大可能性 比如 这些值就比这里的值更有可能 或者这里 或者其他地方 这是我们的先验 然后我们还有Ɵ分布概率 这条曲线中的数据也会告诉我们
哪些Ɵ值可能性更大 即那些能最好地解释我们的数据的Ɵ值 那么当我们结合两者 就得到了后验分布 这就是两者的某种折衷 如图所示 它的值在两者之间 现在让我们来看下一些有趣的Ɵ估计值点 这个点表示先验众数
即在观察到任何数据之前 根据我们的先验取得的最有可能的Ɵ值 这个点表示极大似然估计值 它表示获得最大概率时的Ɵ值 现在 这个点很有趣 这是后验众数 这是根据后验分布得到的最有可能的Ɵ取值 它代表了先验众数和极大似然估计的 一种好的折衷 总的来说 在贝叶斯推断中 
我们感兴趣的是 你见到的所有这些参数值的分布 如果已经有了一个Ɵ值的分布 这里  P(Ɵ|X) 则贝叶斯推断的问题是 推导这个后验分布 这个区间 以及其他依赖于Ɵ的有趣的量 所以 在这里我给出了f(Ɵ) 作为我们想要计算的一个有趣的变量 但是为了计算这些值 我们需要知道Ɵ的值 在贝叶斯推断中  我们将Ɵ视为不确定变量 所以我们要考虑Ɵ的所有可能取值 因此我们把
根据给定观察证据X条件下 Ɵ的后验分布 得到的 f的期望值作为 f的估计值 作为一个特例 我们假设f(Ɵ) = Ɵ 则我们得到的是Ɵ的期望值 实际上就是Ɵ的后验均值 这个值也对应一个Ɵ点 这个点有时与后验众数（mode）一致 
但并非总是如此 所以 它也给出了参数估计的另一种方法 以上就是贝叶斯估计及其相关内容 之后 你会看到它在 主题挖掘当中的应用
因为我们想要在其中注入关于主题的先验知识 总结： 我们使用了语言模型 即文本中的概率分布 又被称为文本数据的生成模型 最简单的语言模型是一元语法模型 即单词分布 我们介绍了似然函数的概念 即给定模型下的数据的概率 这个函数非常重要 给定特定参数值 这个函数能告诉我们哪个X值 哪个数据点有更高的似然值 更高的概率 给定X的样本 我们能用这个函数确定 哪些参数值能使观察数据概率最大化 这是极大似然估计的情形 我们也讨论了贝叶斯估计或贝叶斯推断 其中 我们必须定义P(Ɵ)参数的先验 接着我们关注参数的后验分布的计算 其结果与先验和似然值成正比 这样的分布又能使我们推导任意从Ɵ导出的值 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community