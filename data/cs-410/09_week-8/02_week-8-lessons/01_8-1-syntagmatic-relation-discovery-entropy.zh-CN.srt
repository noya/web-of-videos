1
00:00:00,250 --> 00:00:06,380
[声音]

2
00:00:06,380 --> 00:00:13,220
本节课是关于组合关系发现和熵。

3
00:00:13,220 --> 00:00:17,760
这节课我们将继续讨论文字联想挖掘。

4
00:00:17,760 --> 00:00:22,420
尤其我们将会讨论如何发现组合关系。

5
00:00:22,420 --> 00:00:25,770
同时我们将开始介绍熵。

6
00:00:25,770 --> 00:00:29,860
它是发现这些关系的基础

7
00:00:32,480 --> 00:00:33,110
根据传统宗教等等的定义即我们控制了库存水平

8
00:00:33,110 --> 00:00:39,890
存在于文字间的组合关系有着相互关系的共生。

9
00:00:39,890 --> 00:00:44,190
那意味着，当我们在一段文字中看见一个词时，

10
00:00:44,190 --> 00:00:47,350
我们往往能看见其他词的出现。

11
00:00:48,370 --> 00:00:53,560
那么来看个更具体的例子。

12
00:00:53,560 --> 00:00:55,470
我们可以问，

13
00:00:55,470 --> 00:00:59,750
当出现“吃”的时候，哪些词可能也会出现？

14
00:01:01,140 --> 00:01:06,000
在左边的句子里我们可以看到这些词也可能出现

15
00:01:06,000 --> 00:01:11,030
和“吃”一起的有猫，狗或鱼都是对的。

16
00:01:11,030 --> 00:01:15,870
但如果把他们拿出来看右边的部分

17
00:01:15,870 --> 00:01:21,550
只显示“吃”和其他词语，问题就变成

18
00:01:21,550 --> 00:01:27,050
你能猜到左右会出现什么词吗？

19
00:01:28,315 --> 00:01:31,040
对所以这会迫使我们去思考

20
00:01:31,040 --> 00:01:33,630
有哪些其他词语和吃相关。

21
00:01:33,630 --> 00:01:37,610
如果它们可以和“吃”相联系，它们很可能也会出现在“吃”的上下文中。

22
00:01:38,625 --> 00:01:43,060
更明确一点说，我们的预测问题就是去

23
00:01:43,060 --> 00:01:47,072
搜寻关于文字的部分，它可以是一个句子，一个段落或者一篇文章。

24
00:01:47,072 --> 00:01:51,340
然后提问 是否有一个特定的词在片段中

25
00:01:51,340 --> 00:01:52,640
出现或不出现

26
00:01:54,550 --> 00:01:57,400
现在我们提问的这个词为W

27
00:01:57,400 --> 00:02:00,160
W在片段中出现还是不出现

28
00:02:02,400 --> 00:02:05,100
有趣的地方在于

29
00:02:05,100 --> 00:02:08,230
某些词比别的词更容易预测

30
00:02:10,150 --> 00:02:14,570
请看这里显示的三个词 '肉' '那个' '独角兽'

31
00:02:14,570 --> 00:02:17,970
你认为哪一个词更容易预测

32
00:02:20,630 --> 00:02:23,530
如果你稍微想一些可能有结论

33
00:02:24,530 --> 00:02:27,910
'那个'(the)更容易预测因为它到处都是

34
00:02:27,910 --> 00:02:30,770
所以我可以直接说那个词(the)会在句子里出现

35
00:02:31,940 --> 00:02:37,946
独角兽也相对简单 因为独角兽少见 非常少见

36
00:02:37,946 --> 00:02:41,470
所以我可以打赌它不在句子里出现

37
00:02:42,780 --> 00:02:46,080
但肉在出现频率上就在两者之间了

38
00:02:46,080 --> 00:02:50,580
这使得预测它在句子中是否出现变得难了

39
00:02:50,580 --> 00:02:52,520
或者准确说 在片段中

40
00:02:53,842 --> 00:02:58,820
但它也可能不在句子中出现

41
00:02:58,820 --> 00:03:01,500
现在我们来正式地研究这个问题

42
00:03:02,680 --> 00:03:06,090
问题可以被形式化定义为

43
00:03:06,090 --> 00:03:10,030
预测一个二元随机变量的值

44
00:03:10,030 --> 00:03:14,080
这里我们记为X(w) w代表一个词

45
00:03:14,080 --> 00:03:17,340
所以这个随机变量精确地联系了一个词

46
00:03:18,380 --> 00:03:23,020
当该随机变量值为1 意味着这个词出现

47
00:03:23,020 --> 00:03:26,110
当它是0 意味着词不出现

48
00:03:26,110 --> 00:03:31,010
自然地 0和1的概率求和应该为1

49
00:03:31,010 --> 00:03:34,187
因为片段中一个词要么出现要么不出现

50
00:03:35,240 --> 00:03:36,070
没有其他选择

51
00:03:38,290 --> 00:03:43,610
所以前面这个直觉上的概念可以形式化表示如下

52
00:03:43,610 --> 00:03:48,280
这个随机变量的随机程度越强 预测的难度越难

53
00:03:49,710 --> 00:03:53,600
现在的问题是如何量化测量随机程度

54
00:03:53,600 --> 00:03:55,590
对类似X(w)这样的随机变量

55
00:03:56,940 --> 00:04:01,850
如何在一般意义上 量化一个变量的随机程度

56
00:04:01,850 --> 00:04:04,690
因此我们需要一个叫熵的度量

57
00:04:04,690 --> 00:04:10,560
这个度量从信息论引入 用于度量X的随机程度

58
00:04:10,560 --> 00:04:13,790
这与信息论也有某种关联

59
00:04:13,790 --> 00:04:15,620
但超出了本节课范围

60
00:04:17,460 --> 00:04:20,750
出于我们的目的只把熵函数

61
00:04:20,750 --> 00:04:22,910
看做定义在随机变量上的一个函数

62
00:04:22,910 --> 00:04:27,000
在这个例子里 是一个二元随机变量

63
00:04:27,000 --> 00:04:30,930
虽然定义可以容易推广多个值的随机变量

64
00:04:32,070 --> 00:04:34,950
函数形式是这个样子的

65
00:04:34,950 --> 00:04:39,410
对所有随机变量的取值求和

66
00:04:39,410 --> 00:04:44,030
在求和函数内我们有概率的乘积

67
00:04:45,210 --> 00:04:52,060
是对随机变量的值等于这个值的概率的对数

68
00:04:53,380 --> 00:04:55,250
注意这里还有个负号

69
00:04:56,270 --> 00:04:59,900
熵一般情况下是非负的

70
00:04:59,900 --> 00:05:01,480
可以从数学上证明

71
00:05:02,620 --> 00:05:10,320
如果我们把求和展开 我们会看到第二个公式的样子

72
00:05:10,320 --> 00:05:14,130
这里我明确地写出了两个值 0 和 1

73
00:05:14,130 --> 00:05:18,370
有时我们会遇到对0求对数

74
00:05:18,370 --> 00:05:25,960
我们一般会把那个定义为0 因为0的对数值未定义

75
00:05:28,480 --> 00:05:30,330
这就是熵函数

76
00:05:30,330 --> 00:05:33,020
这个函数有不同的值

77
00:05:33,020 --> 00:05:35,520
对应到这个随机变量的不同分布

78
00:05:37,260 --> 00:05:40,650
它很明显依赖于

79
00:05:40,650 --> 00:05:43,850
随机变量取0或1的概率值

80
00:05:43,850 --> 00:05:49,780
如果我们画出这个函数

81
00:05:49,780 --> 00:05:55,114
对照随机变量取1的概率

82
00:05:56,990 --> 00:05:59,080
函数是这样子的

83
00:06:01,310 --> 00:06:06,820
在两个末端 意味着X为1的概率

84
00:06:07,950 --> 00:06:13,698
非常小或非常大 熵函数有很小的值

85
00:06:13,698 --> 00:06:18,280
当(概率)为0.5时(熵)取到最大值

86
00:06:20,180 --> 00:06:24,150
如果我们把函数对照上

87
00:06:25,950 --> 00:06:31,090
X取0的情况

88
00:06:31,090 --> 00:06:37,810
会有一摸一样的曲线 你可以想象出原因

89
00:06:37,810 --> 00:06:40,620
因此

90
00:06:42,340 --> 00:06:46,730
两个概率是对称的 完全对称

91
00:06:48,740 --> 00:06:52,850
因此可以想到一个有趣的问题是

92
00:06:52,850 --> 00:06:59,390
什么样子的X使得熵取最大或者最小值

93
00:06:59,390 --> 00:07:02,960
我们可以考虑一些特殊情况

94
00:07:02,960 --> 00:07:07,700
例如 考虑我们有一个随机变量

95
00:07:08,840 --> 00:07:10,600
永远取值为1

96
00:07:10,600 --> 00:07:14,304
概率值为1

97
00:07:16,390 --> 00:07:18,650
或者有一个随机变量

98
00:07:19,890 --> 00:07:24,320
以相同的概率等于0或1

99
00:07:24,320 --> 00:07:28,750
这是X等于1的概率为0.5

100
00:07:30,700 --> 00:07:32,250
哪种情况的熵更高

101
00:07:34,650 --> 00:07:38,530
为了简化思考问题可以用一个简单的例子

102
00:07:40,800 --> 00:07:42,380
抛硬币

103
00:07:43,420 --> 00:07:47,660
所以考虑抛一个硬币的随机实验

104
00:07:48,770 --> 00:07:55,740
这就给了我们一个可以代表结果的随机变量

105
00:07:55,740 --> 00:07:57,860
可以是正(head)或者反(tail)

106
00:07:57,860 --> 00:08:03,040
所以我们可以定义一个随机变量X(coin)

107
00:08:03,040 --> 00:08:08,470
硬币正面朝上时为1 反面朝上时为0

108
00:08:09,800 --> 00:08:15,390
现在我们可以计算这个随机变量的熵了

109
00:08:15,390 --> 00:08:20,050
这个熵显示了预测抛硬币结果的

110
00:08:22,050 --> 00:08:22,890
困难程度

111
00:08:25,440 --> 00:08:27,530
可以考虑两种情况

112
00:08:27,530 --> 00:08:29,590
一是均匀的硬币 完美的均匀

113
00:08:29,590 --> 00:08:34,160
硬币正面和反面朝上的次数很可能相等

114
00:08:34,160 --> 00:08:39,160
所以两个概率应该是对半

115
00:08:39,160 --> 00:08:42,890
对不对 所以都等于0.5

116
00:08:44,680 --> 00:08:47,620
另一个极端例子是完全偏斜的硬币

117
00:08:47,620 --> 00:08:50,420
例如硬币永远正面朝上

118
00:08:50,420 --> 00:08:52,760
这时它是完全偏斜的

119
00:08:54,670 --> 00:08:57,910
考虑两种情况里的熵

120
00:08:57,910 --> 00:09:04,850
把值代入后你可以看到熵值如下

121
00:09:04,850 --> 00:09:09,524
均匀硬币可以看到熵达到最大值1

122
00:09:11,270 --> 00:09:14,460
完全偏斜的硬币 可以看到为0

123
00:09:14,460 --> 00:09:17,360
这个很符合直觉

124
00:09:17,360 --> 00:09:20,490
因为完美硬币是最难预测的

125
00:09:22,080 --> 00:09:24,950
对应完全偏斜的硬币是很容易预测的

126
00:09:24,950 --> 00:09:26,860
我们总是可以说 这是正面

127
00:09:26,860 --> 00:09:29,190
因为它所有时间都是正面

128
00:09:29,190 --> 00:09:34,400
所以在曲线上显示如下

129
00:09:34,400 --> 00:09:40,300
完美硬币对应到中间点 它是非常不确定的

130
00:09:40,300 --> 00:09:45,410
完全偏斜的的硬币对应到末端

131
00:09:45,410 --> 00:09:48,058
那里我们有概率为1.0并且熵为0

132
00:09:48,058 --> 00:09:54,870
现在看我们如何利用熵来做词预测

133
00:09:54,870 --> 00:09:59,670
我们的问题是预测W在片段中

134
00:09:59,670 --> 00:10:01,650
出现或不出现

135
00:10:01,650 --> 00:10:05,300
再一次用(前面的)三个词 考虑它们的熵

136
00:10:06,540 --> 00:10:10,130
现在我们可以假设熵更高的词更难预测

137
00:10:11,910 --> 00:10:18,790
因此我们现在有一个定量的方式来告诉我们哪个词更难预测

138
00:10:20,890 --> 00:10:25,810
现在再看三个词 肉 那个 独角兽

139
00:10:25,810 --> 00:10:33,310
我们显然会期望肉比独角兽有更高的熵

140
00:10:33,310 --> 00:10:39,180
事实上 如果你看'那个'的熵 它接近于零

141
00:10:39,180 --> 00:10:41,570
因为它到处都出现

142
00:10:41,570 --> 00:10:43,390
它就像一个完全偏斜的硬币

143
00:10:44,610 --> 00:10:46,380
因此熵为0

144
00:10:48,710 --> 00:10:58,710
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community