1
00:00:00,025 --> 00:00:07,457
[声音]

2
00:00:07,457 --> 00:00:11,800
这一课是关于组合关系探索和交互信息

3
00:00:13,400 --> 00:00:18,196
在这一课我们会继续讨论组合关系探索

4
00:00:18,196 --> 00:00:20,850
我们还会讨论信息学系列中的另一个重要的概念

5
00:00:20,850 --> 00:00:24,880
我们称之为交互信息

6
00:00:24,880 --> 00:00:28,760
还有如何使用交互信息来探索组合关系

7
00:00:28,760 --> 00:00:32,880
在我们讨论条件熵的问题之前

8
00:00:32,880 --> 00:00:38,014
我们需要知道在文字的不同配对都有计算到条件熵

9
00:00:38,014 --> 00:00:42,600
并不都是相似的， 使得从语料库里得到

10
00:00:42,600 --> 00:00:48,360
这种全局性的隐藏的强组合关系更难

11
00:00:48,360 --> 00:00:53,050
所以现在我们会介绍交互信息

12
00:00:53,050 --> 00:00:57,370
这是信息学系列中的另一种概念，可以让我们

13
00:00:57,370 --> 00:01:03,460
有时候来标准化这种条件熵，以便条件熵可以和其他不同的配对来进行对比

14
00:01:04,930 --> 00:01:10,090
尤其是交互信息，为了发现 I(X:Y)满足熵减少的等式

15
00:01:10,090 --> 00:01:17,380
在知道Y的条件下就可以得到X

16
00:01:17,380 --> 00:01:22,270
更准确的来说，我们这里更感兴趣的问题是

17
00:01:22,270 --> 00:01:25,463
在知道Y的情况下我们能得到什么样的X熵系数

18
00:01:27,220 --> 00:01:31,940
所以数学意义上我们可以定义原来的X熵系数

19
00:01:31,940 --> 00:01:36,670
以及给定X的条件Y之间的不同

20
00:01:37,970 --> 00:01:42,730
你可以看到这里也能够定义

21
00:01:42,730 --> 00:01:47,790
在知道X情况下Y熵系数的减少

22
00:01:48,930 --> 00:01:54,070
现在正常情况下给定Y的条件接口H的X

23
00:01:54,070 --> 00:01:58,240
和给定X下的Y的熵不相同，但有意思的是

24
00:01:58,240 --> 00:02:05,476
知道任何一个条件下的熵减少是相等的

25
00:02:05,476 --> 00:02:12,805
所以这里我同意这个数值被称为交互信息

26
00:02:12,805 --> 00:02:17,085
这个函数有一些有趣的属性，首先它也是非负的

27
00:02:17,085 --> 00:02:21,415
这很容易理解，因为原始熵永远

28
00:02:22,782 --> 00:02:29,132
不会比减少的条件熵可能性低

29
00:02:29,132 --> 00:02:33,512
换句话说，条件熵永远不会超过原始熵

30
00:02:33,512 --> 00:02:37,784
知道这些潜在信息可以帮助我们

31
00:02:37,784 --> 00:02:40,282
来预测X，而不是误导我们

32
00:02:41,510 --> 00:02:46,375
信号属性是对称的，像额外熵

33
00:02:46,375 --> 00:02:51,142
不是对称的，交互信息是对称的

34
00:02:51,142 --> 00:02:56,394
第三个属性是当且仅当这两个随机变量完全独立时

35
00:02:56,394 --> 00:03:01,580
它达到其最小值，零

36
00:03:01,580 --> 00:03:07,949
这意味着说已知其中一个不能告诉我们另一个的任何信息

37
00:03:07,949 --> 00:03:14,626
看上面这个等式就可以帮我们确认最后一个属性

38
00:03:14,626 --> 00:03:19,144
仅在X的条件熵

39
00:03:19,144 --> 00:03:24,102
Y与X的原始熵完全一致时它的值为0

40
00:03:24,102 --> 00:03:28,344
所以这意味着知道为什么并没有任何帮助

41
00:03:28,344 --> 00:03:30,520
只有当X和Y完全独立才成立

42
00:03:32,120 --> 00:03:37,880
现在当我们用条件熵给定X一个不同于X的等级

43
00:03:37,880 --> 00:03:44,180
由于等级是基于交互信息我们会得到同样的序列

44
00:03:44,180 --> 00:03:49,940
因为在这里函数H(X)是固定值，因为X是

45
00:03:49,940 --> 00:03:53,820
所以基于交互熵的等级和基于给定Y的条件熵X等级

46
00:03:53,820 --> 00:03:57,600
是完全相同的

47
00:03:57,600 --> 00:04:03,058
但是交互信息允许我们对不同配对的X和Y进行比较

48
00:04:03,058 --> 00:04:07,990
这也是为什么交互信息更普遍，而且一般来说更有用

49
00:04:10,688 --> 00:04:14,420
让我们检查一下使用交互信息的直觉性

50
00:04:14,420 --> 00:04:15,880
对于横组合关系挖掘

51
00:04:17,150 --> 00:04:20,430
现在迫使我们进行关系挖掘的问题是

52
00:04:20,430 --> 00:04:24,300
在“吃(eats)”发生的时候,其他什么样的词也趋向发生?

53
00:04:25,610 --> 00:04:30,710
所以这个问题可以被构造为一个交互信息问题

54
00:04:30,710 --> 00:04:33,055
即哪些词与吃(eats)有很高的交互信息

55
00:04:33,055 --> 00:04:37,700
计算出吃(eats)和其他词之间丢失的信息

56
00:04:39,050 --> 00:04:44,520
这样做的话基本也是一个有条件的基准

57
00:04:44,520 --> 00:04:48,990
我们会看到与吃(eats)强相关的词

58
00:04:48,990 --> 00:04:50,960
有一个最高点

59
00:04:50,960 --> 00:04:55,200
而不相关的词会有较低的交互信息

60
00:04:55,200 --> 00:04:58,530
关于这一点我会给出一些例子

61
00:04:58,530 --> 00:05:01,220
“吃(eats)”和“肉(meats)”之间的交互信息

62
00:05:01,220 --> 00:05:05,650
相等于“肉(meats)”和“吃(eats)”之间的交互信息

63
00:05:05,650 --> 00:05:10,960
因为信息是对称的，并且这个信息含量比“肉(meats)”和“这(the)”之间的交互信息更高

64
00:05:10,960 --> 00:05:14,638
因为知道“这(the)”并不能帮助像我们这样的预测者

65
00:05:14,638 --> 00:05:17,998
相似的，知道“吃(eats)”也不能帮助我们预测

66
00:05:17,998 --> 00:05:22,280
“这(the)”

67
00:05:22,280 --> 00:05:26,970
你也可以很容易发现

68
00:05:26,970 --> 00:05:32,030
一个词与它自己之间的交互信息是最高的

69
00:05:32,030 --> 00:05:37,890
等于这个词的熵

70
00:05:37,890 --> 00:05:42,740
因为在这种情况下减少的是最大值

71
00:05:42,740 --> 00:05:48,530
在知道其中一个的情况下允许我们去完全预测另外一个词

72
00:05:48,530 --> 00:05:50,570
所以条件熵是零

73
00:05:50,570 --> 00:05:54,472
因此交互信息达到其最大值

74
00:05:54,472 --> 00:06:02,520
 它会不断增大然后达到“吃(eats)”和其他词之间的机器容积

75
00:06:02,520 --> 00:06:05,420
换句话说选择任何其他词

76
00:06:05,420 --> 00:06:08,588
让计算机选择"吃(eats)"和任意的另一个词

77
00:06:08,588 --> 00:06:13,511
你将不会得到比计算“吃(eats)”和它自己之间更大的信息量

78
00:06:16,386 --> 00:06:21,390
现在让我们来看看如何计算交互信息

79
00:06:21,390 --> 00:06:23,490
为了计算我们经常

80
00:06:25,110 --> 00:06:29,100
使用一种不同的交互信息形式

81
00:06:29,100 --> 00:06:34,190
在数学上我们可以把交互信息重写成幻灯片上这样的形式

82
00:06:34,190 --> 00:06:38,655
基本上我们可以看到一个方程式用于计算

83
00:06:38,655 --> 00:06:43,075
所谓的KL发散

84
00:06:43,075 --> 00:06:45,615
这是信息理论中的另一个术语

85
00:06:45,615 --> 00:06:48,865
它度量了两个分布之间的发散程度

86
00:06:50,615 --> 00:06:54,645
现在如果你看一下这个方程式，它也是这两个随机变量

87
00:06:54,645 --> 00:06:58,190
不同值的许多结合的加和

88
00:06:58,190 --> 00:07:04,110
但是在这个总和中我们主要为两个联合分布之间做对比

89
00:07:04,110 --> 00:07:06,690
分子中含有节点

90
00:07:06,690 --> 00:07:11,110
实际观察到的这两个随机变量的联合分布

91
00:07:12,690 --> 00:07:15,720
下半部分或者说分母可以被

92
00:07:15,720 --> 00:07:20,695
解释为理想的两个随机变量的联合分布

93
00:07:20,695 --> 00:07:26,782
如果他们是独立的,因为当这两个随机变量是独立时

94
00:07:26,782 --> 00:07:32,810
它们的联合分布等于这两个可能性的乘积

95
00:07:35,300 --> 00:07:39,800
所以这个比值会告诉我们这两个变量是否确实是独立的

96
00:07:39,800 --> 00:07:43,170
如果它们是实际独立的那么我们可以期望这两个变量是一样的

97
00:07:44,390 --> 00:07:49,470
但是如果分子与分母是不同的

98
00:07:49,470 --> 00:07:54,530
那么这说明两个变量不是独立的并且可以帮助度量关联

99
00:07:56,120 --> 00:08:00,110
这个加和是把所有这两个随机变量的

100
00:08:00,110 --> 00:08:04,180
可能值的组合都考虑进去得到的

101
00:08:04,180 --> 00:08:08,750
在我们的例子中, 每个随机变量能够取这两个值中的一个

102
00:08:08,750 --> 00:08:13,950
零或一, 所以我们这里有四个组合

103
00:08:13,950 --> 00:08:17,330
我们看到这个形式的交互信息, 它显示了

104
00:08:17,330 --> 00:08:21,230
交互信息匹配这种真实的联合分布的发散

105
00:08:21,230 --> 00:08:25,800
在独立性假设情况下的期望分布

106
00:08:25,800 --> 00:08:30,144
发散越大,交互信息的值越高

107
00:08:33,507 --> 00:08:37,091
现在让我我们进一步看一下这个

108
00:08:37,091 --> 00:08:39,840
交互信息方程中涉及到的概率

109
00:08:41,300 --> 00:08:45,080
还有这里, 是所有涉及到的概率

110
00:08:45,080 --> 00:08:46,500
你可以很容易确认

111
00:08:46,500 --> 00:08:51,610
进本上这是我们第一次讨论

112
00:08:51,610 --> 00:08:56,380
对应每个词的出现或者不出现的概率

113
00:08:56,380 --> 00:08:59,610
对于w1我们有两个概率

114
00:09:02,600 --> 00:09:07,995
它们加起来应该等于一, 因为一个词只有出现或不出现两个状态

115
00:09:07,995 --> 00:09:13,260
这个分段中对于第二个词也是类似的

116
00:09:13,260 --> 00:09:18,230
我们也有两个概率代表了这个词的出现或者不出现

117
00:09:18,230 --> 00:09:20,920
也有一个加和等于y

118
00:09:21,920 --> 00:09:26,162
最后我们有许多联合概率

119
00:09:26,162 --> 00:09:31,100
来表示两个词同时出现的情况,显示在这里

120
00:09:34,513 --> 00:09:39,107
它们的加和为一,因为这两个词仅有

121
00:09:39,107 --> 00:09:41,420
四种可能的情形

122
00:09:41,420 --> 00:09:43,730
任何一种同时出现的情况

123
00:09:43,730 --> 00:09:49,500
所以在这种情况下两个变量会有单个词出现的概率

124
00:09:49,500 --> 00:09:50,579
这里有两种情况

125
00:09:51,660 --> 00:09:55,910
在这两种情况下其中一个随机变量将会等于一

126
00:09:55,910 --> 00:10:03,560
另一个会等于零,最后我们还有一种情况是两个词都没有出现

127
00:10:03,560 --> 00:10:06,420
这个时候两个变量都得到零这个值

128
00:10:07,620 --> 00:10:12,855
这就是在计算交互信息时可能涉及到的

129
00:10:12,855 --> 00:10:13,600
这里的这一项

130
00:10:16,007 --> 00:10:18,416
一旦我们知道如何计算这些概率

131
00:10:18,416 --> 00:10:20,670
我们可以很容易计算交互信息

132
00:10:24,063 --> 00:10:28,231
很有趣的是当我们知道这些概率之间的联系和约束

133
00:10:28,231 --> 00:10:32,960
我们已经看到了其中的两个对吗

134
00:10:32,960 --> 00:10:36,400
在之前的幻灯片里

135
00:10:36,400 --> 00:10:41,830
你看到了这些小概率的加和为一

136
00:10:41,830 --> 00:10:46,114
我们也看到了这个约束条件

137
00:10:46,114 --> 00:10:53,190
它说这两个词有这四种同时发生的情况

138
00:10:53,190 --> 00:10:57,370
但我们也有一些额外的约束条件显示在下面

139
00:10:58,600 --> 00:11:03,670
比如，这个是说如果我们把我们观察到两个词同时出现的概率

140
00:11:03,670 --> 00:11:07,890
和第一个词出现但是第二个词不出现的概率

141
00:11:07,890 --> 00:11:12,500
相加起来

142
00:11:12,500 --> 00:11:16,860
我们就得出可以看到第一个词的概率

143
00:11:16,860 --> 00:11:20,040
换句话说就是可以看到第一个词的概率

144
00:11:20,040 --> 00:11:22,210
当我们看到第一个词时

145
00:11:22,210 --> 00:11:27,640
根据是否看到第二个词只有两种情况

146
00:11:27,640 --> 00:11:31,750
所以这个概率捕获了第一种情形即第二个词

147
00:11:31,750 --> 00:11:33,860
也出现

148
00:11:33,860 --> 00:11:38,130
而这个则捕获了第二种情形即第二个词不出现

149
00:11:38,130 --> 00:11:40,145
我们只看到第一个词

150
00:11:40,145 --> 00:11:45,410
其实也很容易看到其他的等式遵循了同样的推理

151
00:11:46,980 --> 00:11:50,980
这些等式允许我们根据其他概率来计算

152
00:11:50,980 --> 00:11:54,610
未知概率,简化了整个计算

153
00:11:55,750 --> 00:12:01,010
更具体地来说如果我们知道了

154
00:12:01,010 --> 00:12:06,490
一个词出现的概率, 就像在这个例子中

155
00:12:06,490 --> 00:12:12,630
如果我们知道了这个并且知道第二个词出现的概率

156
00:12:12,630 --> 00:12:17,002
那么我们可以很容易计算它不出现的概率,对吗?

157
00:12:17,002 --> 00:12:22,770
很容易用这个等式来计算

158
00:12:22,770 --> 00:12:27,820
在计算每个词出现或不出现的这些概率时

159
00:12:27,820 --> 00:12:29,950
要十分小心

160
00:12:29,950 --> 00:12:33,146
现在让我们来看一下它们的联合分布

161
00:12:33,146 --> 00:12:36,460
假设我们知道

162
00:12:36,460 --> 00:12:39,548
它们同时发生的可能性即概率

163
00:12:39,548 --> 00:12:44,220
容易知道我们根据这些条件

164
00:12:44,220 --> 00:12:45,829
可以计算剩下的概率

165
00:12:46,870 --> 00:12:51,170
具体来说比如用这个等式我们可以计算

166
00:12:51,170 --> 00:12:56,260
第一个词出现但是第二个词不出现的概率

167
00:12:56,260 --> 00:13:02,020
因为我们知道这些框标记内的概率

168
00:13:02,020 --> 00:13:05,364
同样的用这个等式我们可以计算出我们只看到第二个词的

169
00:13:05,364 --> 00:13:06,000
概率

170
00:13:06,000 --> 00:13:10,421
最后的话,这个概率可以用

171
00:13:10,421 --> 00:13:14,745
这个等式来计算时因为这里的值已知

172
00:13:14,745 --> 00:13:19,282
这里的值已知,还有这里也是已知,对吗?

173
00:13:19,282 --> 00:13:23,120
所以这里很容易计算出来

174
00:13:23,120 --> 00:13:24,430
可以计算

175
00:13:26,080 --> 00:13:30,989
这篇幻灯片说明了我们只需要知道如何计算

176
00:13:30,989 --> 00:13:35,800
这个方框内的三个概率

177
00:13:35,800 --> 00:13:43,092
指定每个词的出现以及两个词在一个分段中的同时发生

178
00:13:43,092 --> 00:13:53,092
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community