1
00:00:00,025 --> 00:00:07,147
[音乐]

2
00:00:07,147 --> 00:00:09,992
本节课程关于统计语言模型的概述

3
00:00:09,992 --> 00:00:12,001
其中包含作为特例的概率模型

4
00:00:12,001 --> 00:00:15,906
在这一讲中 我们将介绍

5
00:00:15,906 --> 00:00:21,320
统计语言模型的概况

6
00:00:21,320 --> 00:00:24,320
这些模型都是通用模型

7
00:00:24,320 --> 00:00:28,120
概率主题模型作为特例也包含在其中

8
00:00:28,120 --> 00:00:30,530
作为开始 什么是统计语言模型？

9
00:00:31,780 --> 00:00:36,070
统计语言模型从根本上来说

10
00:00:36,070 --> 00:00:37,870
是指单词序列的概率分布

11
00:00:37,870 --> 00:00:41,520
比如 我们有一个分布 

12
00:00:41,520 --> 00:00:44,480
其中序列  today is Wednesday 的概率是0.001

13
00:00:44,480 --> 00:00:49,040
而序列  today Wednesday is（这不是一个符合语法的序列）

14
00:00:49,040 --> 00:00:53,560
的概率则非常非常小 如图所示

15
00:00:54,580 --> 00:00:56,170
类似地 另外一个语句

16
00:00:56,170 --> 00:01:01,430
the eigenvalue is positive（特征值是正数）的概率是0.00001

17
00:01:01,430 --> 00:01:06,200
你可以看到 这样的分布明显地具有
上下文依赖的性质

18
00:01:06,200 --> 00:01:09,830
它依赖于讨论的上下文

19
00:01:09,830 --> 00:01:15,370
某些单词序列相比别的单词序列具有更高的概率

20
00:01:15,370 --> 00:01:19,060
但是相同的单词序列在不同的上下文中可能会有不同的概率

21
00:01:20,490 --> 00:01:24,870
意味着这样的分布可以实际用于主题归类

22
00:01:26,960 --> 00:01:31,440
这样的模型同时也可以看作

23
00:01:31,440 --> 00:01:32,520
文本生成的概率机制（装置）

24
00:01:33,880 --> 00:01:42,370
即 我们可以将文本数据看从这个模型得到的观察数据

25
00:01:42,370 --> 00:01:49,225
基于上述原因 我们将这种模型称为生成模型

26
00:01:49,225 --> 00:01:54,310
现在 给定一个模型我们就能组合出各种单词序列

27
00:01:54,310 --> 00:01:59,790
如 基于本页幻灯片给出的分布

28
00:01:59,790 --> 00:02:04,240
我们可能生成一个序列 比如说 today is Wednesday

29
00:02:04,240 --> 00:02:07,130
因为这个序列具有较高的概率

30
00:02:07,130 --> 00:02:10,100
我们可能经常能得到这个序列

31
00:02:10,100 --> 00:02:14,470
我们也有可能以较小的概率观察到序列
 eigenvalue is positive 

32
00:02:14,470 --> 00:02:19,120
只有在非常非常偶然的情况下我们能观察到

33
00:02:19,120 --> 00:02:22,940
today Wednesday is 这样的序列
因为它的概率是如此的小

34
00:02:24,650 --> 00:02:28,960
总的来说 为了能将某一个分布归类

35
00:02:28,960 --> 00:02:33,940
我们必须给出所有这些不同单词序列的概率

36
00:02:33,940 --> 00:02:37,827
这显然是不可能的

37
00:02:37,827 --> 00:02:42,540
因为我们不能穷举所有单词序列

38
00:02:42,540 --> 00:02:49,300
因此在实践中 我们将对这个模型进行一定的简化

39
00:02:49,300 --> 00:02:52,710
最简单的语言模型为 一元语法模型

40
00:02:52,710 --> 00:02:57,270
我们简单的假设

41
00:02:57,270 --> 00:03:01,830
这种模型独立地生成构成文本的每一个单词

42
00:03:02,980 --> 00:03:06,660
当然 这些单词实际上可能并非独立

43
00:03:06,660 --> 00:03:11,020
但是这个假设可以有效地简化语言模型

44
00:03:12,230 --> 00:03:16,700
现在一个由w1到wn表述的序列的概率

45
00:03:16,700 --> 00:03:21,500
等于每个单词的概率的积

46
00:03:24,850 --> 00:03:26,210
这种模型的参数的个数

47
00:03:26,210 --> 00:03:30,470
等于我们的单词表中的单词的个数

48
00:03:30,470 --> 00:03:35,260
现在假设我们有n个单词 即n个概率值

49
00:03:35,260 --> 00:03:36,590
分别对应每一个单词

50
00:03:36,590 --> 00:03:38,700
并且和为1

51
00:03:38,700 --> 00:03:43,010
现在 假设我们的文本是

52
00:03:43,010 --> 00:03:46,220
从这个单词分布中泵出的一个样本

53
00:03:46,220 --> 00:03:50,870
即 我们以每次抽出一个单词的方式

54
00:03:50,870 --> 00:03:52,360
得到我们的最终文本

55
00:03:53,690 --> 00:03:56,163
比如 

56
00:03:56,163 --> 00:04:02,050
 我们尝试从某个分布中抽出单词组合

57
00:04:02,050 --> 00:04:05,110
我们可能经常看到 Wednesday 或者 today

58
00:04:06,610 --> 00:04:11,910
而在小概率下才能看到如 eigenvalue 等一些其他的单词

59
00:04:11,910 --> 00:04:19,370
不过这种方式能让我们计算每一个单词序列的概率

60
00:04:19,370 --> 00:04:25,980
即使我们的模型只给出每一个单词的概率

61
00:04:25,980 --> 00:04:27,780
这种可行性基于“独立”

62
00:04:27,780 --> 00:04:32,970
具体来说 我们现在可以计算  today is Wednesday 的概率了

63
00:04:34,010 --> 00:04:37,740
因为它就是  today 的概率和 is 的概率

64
00:04:37,740 --> 00:04:42,000
以及Wednesday的概率 的乘积

65
00:04:42,000 --> 00:04:45,380
举例来说  在此我给出一些编造的数据

66
00:04:45,380 --> 00:04:49,650
当你对这些数字做乘法 
你就得到了  today is Wednesday 的概率

67
00:04:49,650 --> 00:04:55,900
你可以看到 给定N个单词中每个单词对应的概率

68
00:04:55,900 --> 00:05:02,670
实际上可以得到所有单词序列的概率分布

69
00:05:02,670 --> 00:05:06,100
因此 这是一个很简单的模型

70
00:05:06,100 --> 00:05:07,890
它忽略了单词的顺序

71
00:05:07,890 --> 00:05:12,290
所以 实际上 它可能并不适用于某些问题 
比如在语音识别领域

72
00:05:12,290 --> 00:05:15,410
你可能会关心单词的顺序

73
00:05:15,410 --> 00:05:18,310
但是它实际上已足够处理

74
00:05:18,310 --> 00:05:20,950
包括主题分析在内的许多课题

75
00:05:20,950 --> 00:05:24,590
这也是我们的兴趣所在

76
00:05:24,590 --> 00:05:31,000
当有了一个模型 我们经常会考虑两个问题

77
00:05:31,000 --> 00:05:38,520
其一 给定一个模型 
我们观察到某一类数据点的概率如何

78
00:05:38,520 --> 00:05:41,890
即 我们对抽样过程感兴趣

79
00:05:41,890 --> 00:05:44,400
其二 估计过程

80
00:05:44,400 --> 00:05:49,940
即 基于给定模型和某些观测值的参数估计

81
00:05:49,940 --> 00:05:53,510
我们将在稍后讨论这个问题

82
00:05:53,510 --> 00:05:56,110
首先来讨论抽样

83
00:05:56,110 --> 00:06:02,480
在此给出2个单词分布或者说一元语法模型的例子

84
00:06:02,480 --> 00:06:04,760
第一个例子中 类似于 text mining association之类的单词

85
00:06:04,760 --> 00:06:08,530
有更高的概率

86
00:06:10,120 --> 00:06:16,030
这标志着与文本挖掘有关的主题
因为 当我们从这个分布中取单词组合时

87
00:06:16,030 --> 00:06:21,970
我们倾向于看到经常出现在文本挖掘
上下文中的单词

88
00:06:23,710 --> 00:06:27,460
针对这个例子 如果我们想知道

89
00:06:27,460 --> 00:06:30,560
生成某篇特定文章的概率

90
00:06:30,560 --> 00:06:36,610
那么我们更有可能看到看起来像
文本挖掘相关论文的文本

91
00:06:36,610 --> 00:06:42,110
当然 以从这个分布中抽取单词的方式

92
00:06:42,110 --> 00:06:45,150
生成的文档不大可能连贯

93
00:06:45,150 --> 00:06:49,079
虽然生成的文本挖掘的论文

94
00:06:49,079 --> 00:06:53,535
能在顶级会议上发表的概率并不为零

95
00:06:53,535 --> 00:06:59,090
-- 在所有字在这个分布中的概率
都非零的假设前提下

96
00:06:59,090 --> 00:07:02,590
这意味着 本质上我们可以生成
所有类型的文本文档

97
00:07:02,590 --> 00:07:06,560
包括有意义的文本文档

98
00:07:07,830 --> 00:07:09,660
在图下方的第二个分布显示

99
00:07:09,660 --> 00:07:14,310
概率较高的是另外一些单词

100
00:07:14,310 --> 00:07:17,940
food nutrition healthy等这样的单词

101
00:07:17,940 --> 00:07:20,380
很显然它代表另一个主题

102
00:07:20,380 --> 00:07:23,190
在此可能是health

103
00:07:23,190 --> 00:07:26,460
所以 如果我们从这个分布中取样

104
00:07:26,460 --> 00:07:31,390
观察到一篇文本挖掘相关论文
的概率就非常非常小

105
00:07:32,830 --> 00:07:37,020
另一方面 观察到一篇关于食物营养的论文的概率

106
00:07:37,020 --> 00:07:40,400
相对来说就比较高

107
00:07:41,510 --> 00:07:48,113
这说明 给定特定的分布
不同的文本将对应不同的概率

108
00:07:48,113 --> 00:07:51,830
现在 让我们来看看估计的问题

109
00:07:51,830 --> 00:07:54,910
在此 我们假设我们已有了观测数据

110
00:07:54,910 --> 00:07:57,410
即 我已经知道生成的文本的具体内容

111
00:07:57,410 --> 00:07:59,715
在这个例子中 假设我们有一篇文本挖掘的论文

112
00:07:59,715 --> 00:08:06,980
实际上 这是某篇论文摘要
一共100个单词

113
00:08:06,980 --> 00:08:10,960
我已经给出某些单词的个数

114
00:08:12,550 --> 00:08:16,880
现在 如果我们问

115
00:08:17,950 --> 00:08:22,440
用于生成这个文本数据的语言模型
最有可能是什么样的？

116
00:08:22,440 --> 00:08:26,400
假设这个文本是基于某个语言模型的观察结果

117
00:08:26,400 --> 00:08:28,920
关于这个语言模型的最优的猜测是什么？

118
00:08:30,740 --> 00:08:35,510
Ok 现在的问题是对这些单词进行概率估计

119
00:08:35,510 --> 00:08:36,490
就像我在这里显示的这样

120
00:08:37,560 --> 00:08:38,370
那么你怎么认为？

121
00:08:38,370 --> 00:08:39,610
你的猜测是什麽？

122
00:08:40,680 --> 00:08:45,590
你倾向于小概率的文本还是

123
00:08:45,590 --> 00:08:47,180
概率相对较大的文本？

124
00:08:48,360 --> 00:08:50,310
query（这个词）的概率如何？

125
00:08:50,310 --> 00:08:53,200
你的猜测将依赖于

126
00:08:53,200 --> 00:08:56,516
我们在文本数据中观察到的
这个词的次数 对不对？

127
00:08:56,516 --> 00:09:00,550
现在思考一下

128
00:09:00,550 --> 00:09:04,960
如果你和许多人一样
那么你的猜测将会是

129
00:09:04,960 --> 00:09:10,140
text（这个词）的概率是100分之10 因为

130
00:09:10,140 --> 00:09:15,040
在这个100个单词的文本中
观察到 text 的次数是 10 次

131
00:09:15,040 --> 00:09:19,640
同样 mining是100分之5

132
00:09:19,640 --> 00:09:25,180
而 query的概率相对小些 只有一次

133
00:09:25,180 --> 00:09:27,130
所以是100分之1

134
00:09:27,130 --> 00:09:32,220
因此 直观上来说 这是一个合理的猜测

135
00:09:32,220 --> 00:09:36,440
但是问题是 
这是我们关于参数的最优估计吗？

136
00:09:37,840 --> 00:09:40,000
当然 为了回答这个问题

137
00:09:40,000 --> 00:09:45,070
我们要定义什么是最优

138
00:09:45,070 --> 00:09:50,540
这个例子中 我们的猜测在某种意义上的确是最优的

139
00:09:50,540 --> 00:09:54,680
这种估计叫做极大似然估计

140
00:09:54,680 --> 00:10:00,789
称它是最优的是因为它能赋予
我们的被观测数据最大概率

141
00:10:01,960 --> 00:10:05,740
意味着 如果我们改变估计值 哪怕是稍微的

142
00:10:05,740 --> 00:10:10,760
被观测的文本数据的概率都会变小

143
00:10:10,760 --> 00:10:13,952
这就是极大似然估计

144
00:10:13,952 --> 00:10:23,952
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community