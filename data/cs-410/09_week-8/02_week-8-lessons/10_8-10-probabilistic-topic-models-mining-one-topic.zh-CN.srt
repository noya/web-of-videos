1
00:00:00,025 --> 00:00:05,683
[背景音乐]本节课接上一节

2
00:00:05,683 --> 00:00:13,370
继续讨论概率主题模型

3
00:00:13,370 --> 00:00:19,990
在这节课中 我们将要继续讨论概率模型

4
00:00:19,990 --> 00:00:24,970
我们将讨论一个非常简单的情形 其中

5
00:00:24,970 --> 00:00:28,300
我们关注于只从一个文档中挖掘一个主题

6
00:00:30,880 --> 00:00:35,910
在这个简单的设置中 我们感兴趣的是

7
00:00:35,910 --> 00:00:41,060
分析一篇文档并从中挖掘一个主题

8
00:00:41,060 --> 00:00:44,810
这是主题模型最简单的例子

9
00:00:44,810 --> 00:00:49,921
现在的输入主题数不再是k

10
00:00:49,921 --> 00:00:55,670
因为我们已经知道只有一篇文档和一个主题

11
00:00:55,670 --> 00:01:00,738
对于输出 我们也不再需要覆盖率

12
00:01:00,738 --> 00:01:06,150
因为我们已经假设这篇文档对于这个主题的覆盖率是100%

13
00:01:06,150 --> 00:01:10,532
所以主要任务就是挖掘这个单一主题的词语分布

14
00:01:10,532 --> 00:01:12,930
如图所示

15
00:01:14,770 --> 00:01:19,275
一如往常 当我们考虑使用生成模型解决类似问题时

16
00:01:19,275 --> 00:01:24,280
首先考虑的是我们将为怎样的数据建模 或者说

17
00:01:24,280 --> 00:01:28,880
我们将以什么样的角度
来为这些数据或数据表示建模

18
00:01:28,880 --> 00:01:32,268
然后我们才会从我们的角度

19
00:01:32,268 --> 00:01:36,520
为这些数据生成设计特定的模型

20
00:01:36,520 --> 00:01:41,310
我们的角度意思是
我们想从某一个特定的角度来看待这些数据 

21
00:01:41,310 --> 00:01:45,700
所以模型才会拥有适当的参数

22
00:01:45,700 --> 00:01:48,770
以发掘我们想要的知识

23
00:01:48,770 --> 00:01:54,210
接着我们会考虑似然函数

24
00:01:54,210 --> 00:02:00,480
或者写下某些似然函数以正式地获得

25
00:02:00,480 --> 00:02:04,860
这个模型中某个数据点的概率

26
00:02:05,900 --> 00:02:10,370
似然函数包含参数

27
00:02:10,370 --> 00:02:15,780
接下来我们讨论对这些参数的估计  比如说

28
00:02:15,780 --> 00:02:21,680
通过将概率最大化我们得到的是极大似然估计值

29
00:02:21,680 --> 00:02:26,710
这些参数估计值将成为挖掘算法的输出

30
00:02:26,710 --> 00:02:31,640
即我们将这些参数估计值

31
00:02:31,640 --> 00:02:35,320
作为我们从文本中挖掘到的知识

32
00:02:35,320 --> 00:02:39,690
让我们来看看这个简单例子中的这些步骤

33
00:02:39,690 --> 00:02:45,970
稍后我们将看到这些步骤在更复杂的例子中的应用

34
00:02:45,970 --> 00:02:50,170
在这个例子中 我们的数据 即这篇文档
仅仅是一个词序列

35
00:02:50,170 --> 00:02:52,520
这里的每一个词表示为Xi

36
00:02:52,520 --> 00:02:56,800
我们的模型是一元语法模型

37
00:02:56,800 --> 00:03:03,420
我们的目标是
一个我们希望用来表示一个主题的词语分布

38
00:03:03,420 --> 00:03:08,950
所以参数的数量将和单词表中的单词数一致
在本例中用M表示

39
00:03:09,950 --> 00:03:14,580
方便起见 我们用Ɵi

40
00:03:14,580 --> 00:03:18,270
表示单词Wi的概率

41
00:03:20,450 --> 00:03:23,384
显然这些Ɵi的和为1

42
00:03:24,480 --> 00:03:27,110
现在我们的似然函数是什么样子？

43
00:03:27,110 --> 00:03:30,970
它正是给出的这个模型

44
00:03:30,970 --> 00:03:31,948
生成整个文档的概率

45
00:03:31,948 --> 00:03:36,920
因为我们假设单词生成的独立性

46
00:03:36,920 --> 00:03:41,010
所以文档的概率就是每个单词的概率的积

47
00:03:42,790 --> 00:03:46,900
既然某些单词重复出现

48
00:03:46,900 --> 00:03:51,070
我们可以将这个积写成另一种形式

49
00:03:52,580 --> 00:03:58,550
在这一行 我们将公式重写成
单词表中所有不同单词的积

50
00:03:58,550 --> 00:04:05,360
 从W1到Wm

51
00:04:05,360 --> 00:04:09,170
现在它与前面那一行有了区别

52
00:04:09,170 --> 00:04:13,990
它覆盖了所有在文档中不同位置的单词

53
00:04:15,040 --> 00:04:19,694
做这个变换的时候 我们需要

54
00:04:19,694 --> 00:04:24,120
介绍一下这个计数函数

55
00:04:24,120 --> 00:04:29,395
它表示某一个单词在文中出现的次数

56
00:04:29,395 --> 00:04:33,390
同样 这个计数函数表示Wn在文中出现的次数

57
00:04:33,390 --> 00:04:37,890
--因为这些单词有可能重复出现

58
00:04:37,890 --> 00:04:40,459
你可以看到 如果一个单词在文中没有出现

59
00:04:41,810 --> 00:04:46,790
它的计数为0 则其相应的项在公式中也不会出现

60
00:04:46,790 --> 00:04:50,410
所以这是一种书写似然函数的非常有用的形式

61
00:04:50,410 --> 00:04:55,060
我们后面会经常用到

62
00:04:55,060 --> 00:05:01,230
所以我想要你对此予以重视 熟悉这个形式

63
00:05:01,230 --> 00:05:07,120
这是所有单词表中不同单词的积的另一种表示

64
00:05:07,120 --> 00:05:12,013
最后 当然 我们用Ɵi来表达这个似然函数

65
00:05:12,013 --> 00:05:14,512
如图所示

66
00:05:14,512 --> 00:05:19,468
接下来我们要寻求最大化似然函数值的

67
00:05:19,468 --> 00:05:24,530
Ɵ值或者说这些单词的概率

68
00:05:24,530 --> 00:05:30,539
现在让我们进一步了解极大似然估计问题

69
00:05:32,520 --> 00:05:35,870
这一行是从上一页ppt拷贝过来的

70
00:05:35,870 --> 00:05:37,340
也就是我们的似然函数

71
00:05:38,590 --> 00:05:43,950
我们的目标是最大化这个似然函数

72
00:05:43,950 --> 00:05:46,210
我们将发现通常

73
00:05:47,310 --> 00:05:51,110
局部最大化比全局最大化更容易实现

74
00:05:51,110 --> 00:05:56,531
这个形式只是为了计算方便

75
00:05:56,531 --> 00:06:03,698
因为对数转换使乘法变成了加法

76
00:06:03,698 --> 00:06:10,704
而且我们有关于这些概率的约束

77
00:06:10,704 --> 00:06:16,743
和更容易求导 其经常用于

78
00:06:16,743 --> 00:06:21,022
求解优化问题

79
00:06:21,022 --> 00:06:27,349
所以请再看一下这个和 这里

80
00:06:27,349 --> 00:06:32,434
这是你后面会经常看到的函数形式

81
00:06:32,434 --> 00:06:38,430
--在讲述更通用的主题模型的时候

82
00:06:38,430 --> 00:06:42,340
这是包含了单词表中所有单词的一个和式

83
00:06:42,340 --> 00:06:48,105
每一项都是单词的计数

84
00:06:48,105 --> 00:06:54,980
与概率对数的乘积

85
00:06:55,990 --> 00:06:57,920
让我们看看怎样来解决这个问题

86
00:06:58,920 --> 00:07:04,030
此时这个问题就成了一个单纯的数学问题

87
00:07:04,030 --> 00:07:11,360
因为我们只是解一个约束最大化问题

88
00:07:11,360 --> 00:07:14,694
目标函数是一个似然函数

89
00:07:14,694 --> 00:07:18,621
约束是所有概率的和为1

90
00:07:18,621 --> 00:07:23,234
一种求解的方法是拉格朗日乘数法

91
00:07:24,520 --> 00:07:29,040
具体内容已超出了本书的范围

92
00:07:29,040 --> 00:07:33,670
但是因为拉格朗日乘数法应用非常广泛

93
00:07:33,670 --> 00:07:37,940
我在这里为感兴趣的同学做一个基本的介绍

94
00:07:39,720 --> 00:07:43,857
根据这个方法 我们将构建一个拉格朗日函数 这里

95
00:07:43,857 --> 00:07:49,887
这个函数结合了我们的目标函数

96
00:07:49,887 --> 00:07:55,392
和包含了我们的约束的另一项

97
00:07:55,392 --> 00:07:59,980
我们在此引入拉格朗日乘子

98
00:07:59,980 --> 00:08:04,978
λ 所以这是一个额外加入的参数

99
00:08:04,978 --> 00:08:10,432
现在 这个方法的思想是将约束优化问题转化为

100
00:08:10,432 --> 00:08:14,800
某种意义上的非约束优化问题

101
00:08:14,800 --> 00:08:18,318
现在我们只要关注拉格朗日函数的优化问题了

102
00:08:19,460 --> 00:08:24,022
你可能记起微积分的知识 极值点（optimal point）

103
00:08:24,022 --> 00:08:29,910
出现在导数为0的时候

104
00:08:29,910 --> 00:08:31,673
这是一个必要条件

105
00:08:31,673 --> 00:08:33,182
虽非充分条件

106
00:08:33,182 --> 00:08:38,205
所以如果我们这样做 你可以看到Ɵi的偏导

107
00:08:38,205 --> 00:08:42,785
等于这个

108
00:08:42,785 --> 00:08:50,815
这一部分来自于拉格朗日函数的偏导

109
00:08:50,815 --> 00:08:55,390
λ则只是从这里搬过来

110
00:08:55,390 --> 00:09:00,178
我们将其赋值为0

111
00:09:00,178 --> 00:09:05,610
我们容易看出Ɵi与λ的关系是这样的

112
00:09:06,820 --> 00:09:09,900
既然我们已知所有Ɵi的和为1

113
00:09:09,900 --> 00:09:12,423
我们将其代入这个约束  这里

114
00:09:12,423 --> 00:09:15,600
这样我们就解得λ

115
00:09:16,630 --> 00:09:20,840
这只是所有计数的和取负

116
00:09:20,840 --> 00:09:27,350
这使我们进一步解出这个优化问题

117
00:09:27,350 --> 00:09:31,380
最终求得Ɵi的最优解

118
00:09:31,380 --> 00:09:37,280
如果你看下这个公式就会发现它很直观

119
00:09:37,280 --> 00:09:43,089
因为它就是用文本长度正则化后的计数值

120
00:09:43,089 --> 00:09:47,751
文本长度同时也是文本中所有单词的计数和

121
00:09:47,751 --> 00:09:52,157
经过所有这些计算

122
00:09:52,157 --> 00:09:59,044
最终我们得到非常直观的结果

123
00:09:59,044 --> 00:10:04,415
同我们的直觉一样我们想要

124
00:10:04,415 --> 00:10:10,338
通过赋予所有已观察到的单词
尽可能大的概率

125
00:10:10,338 --> 00:10:16,419
以获得最大的数值

126
00:10:16,419 --> 00:10:21,408
同时我们也可能注意到

127
00:10:21,408 --> 00:10:23,450
这是极大似然估计法的一般结果

128
00:10:23,450 --> 00:10:29,333
通常来说 估计法将使计数正则化

129
00:10:29,333 --> 00:10:35,050
而有时计数必须以某种特别的方式进行
你之后会看到

130
00:10:35,050 --> 00:10:41,730
所以这只是我们的优化问题的解析解

131
00:10:41,730 --> 00:10:46,303
但是通常情况下 当解析函数太复杂时

132
00:10:46,303 --> 00:10:50,919
我们将不会使用闭合公式解优化问题

133
00:10:50,919 --> 00:10:55,134
而是会采用一些数值算法

134
00:10:55,134 --> 00:10:58,787
我们后面会遇到这样的例子

135
00:10:58,787 --> 00:11:02,385
想像一下 如果我们用这样的极大似然估计法

136
00:11:02,385 --> 00:11:07,146
来估计单一文本d中单个主题的估计
我们能得到什么？

137
00:11:07,146 --> 00:11:09,903
假设这个文本是一篇数据挖据的论文

138
00:11:09,903 --> 00:11:16,277
你可以想象 我们看到的将会是这样

139
00:11:16,277 --> 00:11:20,555
在最上面的是概率最高的单词

140
00:11:20,555 --> 00:11:23,710
他们通常是非常常见的词
在英语中经常是功能词

141
00:11:23,710 --> 00:11:27,742
接下来是一些能真正表征这个主题的实义词

142
00:11:27,742 --> 00:11:31,622
比如  text  mining 等

143
00:11:31,622 --> 00:11:36,275
最后你也会看到一些小概率的

144
00:11:36,275 --> 00:11:40,017
并非与主题有实际联系的词

145
00:11:40,017 --> 00:11:44,320
他们也在文档中延伸地提及

146
00:11:44,320 --> 00:11:49,590
作为一个主题表示 你会看出它并不理想 是不是？

147
00:11:49,590 --> 00:11:52,452
这是因为高概率的词是功能词

148
00:11:52,452 --> 00:11:55,310
他们并不能真正描述主题特征

149
00:11:55,310 --> 00:11:58,280
所以我的问题是 怎样去掉这些常见词？

150
00:11:59,720 --> 00:12:02,680
这是我们下一节的主题

151
00:12:02,680 --> 00:12:06,913
我们将讨论如何利用概率模型以某种方式

152
00:12:06,913 --> 00:12:08,077
消除这些常用词

153
00:12:08,077 --> 00:12:18,077
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community