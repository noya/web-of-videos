1
00:00:00,000 --> 00:00:04,714
[声音]

2
00:00:06,455 --> 00:00:09,677
一般来说 我们用一个事件在可以观测到的数据里

3
00:00:09,677 --> 00:00:15,340
所发生的次数来计算其概率

4
00:00:15,340 --> 00:00:19,080
最常见的方法是最大似然估计

5
00:00:19,080 --> 00:00:22,600
也就是简单的对事件发生的次数做归一化

6
00:00:22,600 --> 00:00:30,330
所以当我们这样做时 我们就可以按照如下计算它们的概率

7
00:00:30,330 --> 00:00:36,811
想要计算一个词出现在段里的概率

8
00:00:36,811 --> 00:00:42,773
我们只需要数一数这个词出现在了多少个段里 并对这个值做归一化

9
00:00:42,773 --> 00:00:47,278
让我们首先来看看数据

10
00:00:47,278 --> 00:00:52,970
在幻灯片右侧 我们可以看到列出了一些假设的数据

11
00:00:52,970 --> 00:00:55,010
这些是假设的段

12
00:00:55,010 --> 00:00:59,860
在有些段我们可以看到两个词都出现了 

13
00:00:59,860 --> 00:01:01,630
这两个词的纵列都标记了1 来表示出现

14
00:01:01,630 --> 00:01:05,830
在另一些段中 只有一个词出现了 出现的词标记为1

15
00:01:05,830 --> 00:01:07,590
而没有出现的词标记为0

16
00:01:07,590 --> 00:01:11,130
当然 另一种可能出现的情况是两个词都没有出现

17
00:01:11,130 --> 00:01:13,930
所以两个词都被标记为0

18
00:01:13,930 --> 00:01:19,310
为了计算这个概率 我们只需要数三次数

19
00:01:20,340 --> 00:01:23,560
第一个要数的 是第一个词W1出现的次数

20
00:01:23,560 --> 00:01:27,337
也就是说在多少个段中出现了W1这个词

21
00:01:27,337 --> 00:01:30,950
也就是在词W1这一列中出现的1的次数

22
00:01:30,950 --> 00:01:34,470
我们可以数一数在这列出现了多少次1

23
00:01:34,470 --> 00:01:40,460
第二个要数的 是第二个词W2出现的次数 也就是在第二列里出现了多少次1

24
00:01:40,460 --> 00:01:45,425
也就是在多少个段中出现了W2这个词

25
00:01:45,425 --> 00:01:49,650
第三个要数的 是W1和W2这两个词同时出现的次数

26
00:01:49,650 --> 00:01:55,370
所以这次 我们要数的是在多少个段中 两列都标记为1

27
00:01:56,580 --> 00:02:00,060
这样我们就能得到在多少个段中

28
00:02:00,060 --> 00:02:03,510
W1和W2这两个词都出现了

29
00:02:03,510 --> 00:02:08,112
一旦三个数都数出来了

30
00:02:08,112 --> 00:02:11,019
我们就用段的总数N来做归一化

31
00:02:11,019 --> 00:02:16,706
这样我们就可以计算出它们的概率 并用来计算它们的互信息

32
00:02:16,706 --> 00:02:22,301
有一个要注意的小问题 有时候数出来的数是0次

33
00:02:22,301 --> 00:02:27,458
我们不希望计算出的概率为0 因为我们的数据可能只是一个很小的样本

34
00:02:27,458 --> 00:02:33,365
而一般来说 我们相信某个词出现在任何地方

35
00:02:33,365 --> 00:02:35,806
都是有一定可能的

36
00:02:35,806 --> 00:02:39,630
为了解决这个问题 我们可以用平滑化这个方法

37
00:02:39,630 --> 00:02:43,780
基本上也就是在计数时加上一个很小的常量

38
00:02:43,780 --> 00:02:48,410
这样在任何情况下都不会出现概率为0了

39
00:02:48,410 --> 00:02:54,250
理解平滑化 最好的办法就是想象我们其实观测到了更多的数据

40
00:02:54,250 --> 00:03:00,310
也就是我们假设我们观测到了一些虚拟的段

41
00:03:00,310 --> 00:03:04,650
我在幻灯片的右侧列出了这些虚拟段

42
00:03:04,650 --> 00:03:10,095
这些虚拟段会额外增加某个词的出现次数

43
00:03:10,095 --> 00:03:15,047
那么 任何一个词出现的概率就不可能是0了

44
00:03:15,047 --> 00:03:18,169
我们特别引入了四个虚拟段

45
00:03:18,169 --> 00:03:20,990
每一个虚拟段相当于0.25个实际出现的段

46
00:03:20,990 --> 00:03:25,930
这四个虚拟段分别代表了W1和W2这两个词出现或不出现的各种可能情况

47
00:03:25,930 --> 00:03:30,490
这么一来 每一个可能产生的情况

48
00:03:30,490 --> 00:03:35,390
要不然在实际数据中至少发生过一次 要不然再虚拟数据中发生过

49
00:03:35,390 --> 00:03:39,380
那么 在我们可以观测到的实际的段中

50
00:03:39,380 --> 00:03:44,231
即使没有观测到任何可能出现的情况 也没有关系

51
00:03:44,231 --> 00:03:49,671
更准确的来说
这里的0.5来自两个假设片段里的1

52
00:03:49,671 --> 00:03:55,560
因为每一个的权重是四分之一

53
00:03:55,560 --> 00:03:59,315
把它们相加就是0.5

54
00:03:59,315 --> 00:04:03,319
同样的
这个0.25来自一个假设片段

55
00:04:03,319 --> 00:04:08,240
这个1表示两个单词同时出现

56
00:04:09,450 --> 00:04:14,000
在分母位置
我们加上了假设片段的总和

57
00:04:14,000 --> 00:04:17,520
我们加了4个假设片段

58
00:04:17,520 --> 00:04:21,780
每个的权重是四分之一
所以和是1

59
00:04:21,780 --> 00:04:24,110
这就是为什么你在分母看到加了1

60
00:04:25,990 --> 00:04:31,460
所以这大致概括了怎么计算交互信息

61
00:04:31,460 --> 00:04:33,920
来进行语义关联挖掘

62
00:04:36,090 --> 00:04:42,050
综上
语义关联可以通过

63
00:04:42,050 --> 00:04:46,240
通过描述两个单词出现的相关性来发掘

64
00:04:46,240 --> 00:04:49,580
我们引入了3个信息理论的概念

65
00:04:49,580 --> 00:04:53,230
熵衡量一个变量的不确定性

66
00:04:53,230 --> 00:04:59,060
条件性熵描述的是
当Y已知时变量X的熵

67
00:04:59,060 --> 00:05:04,530
X和Y的交互信息
表示的是由于Y已知带来变量X的熵损失

68
00:05:04,530 --> 00:05:11,240
或者X已知时变量Y的熵损失

69
00:05:11,240 --> 00:05:12,660
两个情况是一样的

70
00:05:12,660 --> 00:05:17,111
这三个概念在其他应用中也非常有用

71
00:05:17,111 --> 00:05:20,340
这也是我们为什么要花时间细致地解释它们

72
00:05:20,340 --> 00:05:23,150
特别地它们对

73
00:05:23,150 --> 00:05:25,960
发掘语义关联非常有用

74
00:05:25,960 --> 00:05:30,142
特别地来讲
交互信息是

75
00:05:30,142 --> 00:05:32,370
研究这种联系的主要方法

76
00:05:32,370 --> 00:05:37,241
它允许我们为不同单词对取值

77
00:05:37,241 --> 00:05:42,211
让我们可以为这些单词对排序

78
00:05:42,211 --> 00:05:48,208
并从文档集合当中发掘最突出的语义关联

79
00:05:48,208 --> 00:05:53,700
注意语义关联发掘和

80
00:05:53,700 --> 00:05:55,910
范式关系关联发掘存在着关联

81
00:05:55,910 --> 00:06:01,835
我们已经讨论了用BM25来衡量背景中术语的权重

82
00:06:01,835 --> 00:06:06,683
这也就揭示了

83
00:06:06,683 --> 00:06:11,187
与候选单词有语义关联的候选单词

84
00:06:11,187 --> 00:06:17,958
但是这里
一旦我们用交互信息来发掘语义关联

85
00:06:17,958 --> 00:06:24,436
我们可以把这个交互信息
作为权重来表征背景

86
00:06:24,436 --> 00:06:29,567
这样就给了我们另一种描述

87
00:06:29,567 --> 00:06:33,490
一个单词上下文的方式
比如说一种类别

88
00:06:33,490 --> 00:06:37,394
如果我们对所有的单词做同样的处理
我们就可以对单词进行聚类

89
00:06:37,394 --> 00:06:42,320
通过比较它们背景上的相似性
来比较这些单词的相似性

90
00:06:42,320 --> 00:06:45,850
这就给我们提供了另一种确定权重的方法

91
00:06:45,850 --> 00:06:48,800
建立一个词聚合关系语境模型的简便方法

92
00:06:48,800 --> 00:06:55,770
总结一下
整个部分是关于单词关联关系的挖掘

93
00:06:55,770 --> 00:06:59,190
我们介绍了两个基本的关联
范式关联发掘

94
00:06:59,190 --> 00:07:01,000
和语义关联发掘

95
00:07:01,000 --> 00:07:05,710
这些都是非常宽泛的
可以被应用到任何语言的任何地方

96
00:07:05,710 --> 00:07:10,009
基本不必是单词
也可以是词组或者实体

97
00:07:11,120 --> 00:07:16,235
我们介绍了多种统计学方法来研究它们

98
00:07:16,235 --> 00:07:20,762
纯统计学方法是可行的

99
00:07:20,762 --> 00:07:24,840
对两种关系的发掘也都是有效的

100
00:07:24,840 --> 00:07:28,800
它们也可以合在一起进行联合分析

101
00:07:28,800 --> 00:07:35,040
这些方法在不用人力的情况下
可以应用到任何文本当中

102
00:07:35,040 --> 00:07:39,940
因为它们是基于对单词的统计

103
00:07:39,940 --> 00:07:42,690
可以发掘单词间有趣的关系

104
00:07:44,360 --> 00:07:47,880
我们可以用不同的方法来定义上下文和片段

105
00:07:47,880 --> 00:07:51,360
这将带来一些应用上的多样性

106
00:07:51,360 --> 00:07:56,190
例如上下文可以非常短小
比如一个单词周围

107
00:07:56,190 --> 00:08:00,760
或者一个句子、一个段落周围的几个单词
使用不同的上下文

108
00:08:00,760 --> 00:08:05,330
允许我们发现范式关联不同的分析结果

109
00:08:05,330 --> 00:08:09,362
同样地
统计单词同时出现的频数

110
00:08:09,362 --> 00:08:13,380
利用视觉信息来发现语义关联

111
00:08:13,380 --> 00:08:19,110
我们也需要定义片段
而这个片段也可以

112
00:08:19,110 --> 00:08:22,560
被定义为很短小的文本框或一个很长的文章

113
00:08:22,560 --> 00:08:26,508
这可以返回不同的关联结果

114
00:08:26,508 --> 00:08:32,677
这些关联发掘方法也能对其他很多应用进行支持

115
00:08:32,677 --> 00:08:37,701
包括信息提取和文本数据挖掘

116
00:08:37,701 --> 00:08:44,100
如果你想要更多地了解这个话题
这里是一些建议阅读材料

117
00:08:44,100 --> 00:08:46,880
第一本书其中一章讲的是相互定位

118
00:08:46,880 --> 00:08:50,810
是和本次课程的话题相关的

119
00:08:50,810 --> 00:08:55,120
第二个是一篇讲述用不同统计学方法

120
00:08:55,120 --> 00:08:58,160
来研究词条的文章

121
00:08:58,160 --> 00:09:03,764
就是那些非组合型的短语

122
00:09:03,764 --> 00:09:07,560
例如热狗(hot dog)并不是很热的狗(dog)

123
00:09:08,610 --> 00:09:11,550
蓝筹股(blue chip)并不是蓝色的片(chip)

124
00:09:11,550 --> 00:09:16,180
这篇文章就是讨论如何处理这类短语的

125
00:09:17,400 --> 00:09:23,227
第三个是一篇新发表的文章
是关于研究范式关联和语义关联的一种统一的方法

126
00:09:23,227 --> 00:09:29,441
用的是基于词图的随机游走理论

127
00:09:29,441 --> 00:09:39,441
[声音]