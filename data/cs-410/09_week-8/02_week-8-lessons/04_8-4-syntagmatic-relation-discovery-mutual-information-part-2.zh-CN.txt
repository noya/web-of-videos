[声音] 一般来说 我们用一个事件在可以观测到的数据里 所发生的次数来计算其概率 最常见的方法是最大似然估计 也就是简单的对事件发生的次数做归一化 所以当我们这样做时 我们就可以按照如下计算它们的概率 想要计算一个词出现在段里的概率 我们只需要数一数这个词出现在了多少个段里 并对这个值做归一化 让我们首先来看看数据 在幻灯片右侧 我们可以看到列出了一些假设的数据 这些是假设的段 在有些段我们可以看到两个词都出现了 这两个词的纵列都标记了1 来表示出现 在另一些段中 只有一个词出现了 出现的词标记为1 而没有出现的词标记为0 当然 另一种可能出现的情况是两个词都没有出现 所以两个词都被标记为0 为了计算这个概率 我们只需要数三次数 第一个要数的 是第一个词W1出现的次数 也就是说在多少个段中出现了W1这个词 也就是在词W1这一列中出现的1的次数 我们可以数一数在这列出现了多少次1 第二个要数的 是第二个词W2出现的次数 也就是在第二列里出现了多少次1 也就是在多少个段中出现了W2这个词 第三个要数的 是W1和W2这两个词同时出现的次数 所以这次 我们要数的是在多少个段中 两列都标记为1 这样我们就能得到在多少个段中 W1和W2这两个词都出现了 一旦三个数都数出来了 我们就用段的总数N来做归一化 这样我们就可以计算出它们的概率 并用来计算它们的互信息 有一个要注意的小问题 有时候数出来的数是0次 我们不希望计算出的概率为0 因为我们的数据可能只是一个很小的样本 而一般来说 我们相信某个词出现在任何地方 都是有一定可能的 为了解决这个问题 我们可以用平滑化这个方法 基本上也就是在计数时加上一个很小的常量 这样在任何情况下都不会出现概率为0了 理解平滑化 最好的办法就是想象我们其实观测到了更多的数据 也就是我们假设我们观测到了一些虚拟的段 我在幻灯片的右侧列出了这些虚拟段 这些虚拟段会额外增加某个词的出现次数 那么 任何一个词出现的概率就不可能是0了 我们特别引入了四个虚拟段 每一个虚拟段相当于0.25个实际出现的段 这四个虚拟段分别代表了W1和W2这两个词出现或不出现的各种可能情况 这么一来 每一个可能产生的情况 要不然在实际数据中至少发生过一次 要不然再虚拟数据中发生过 那么 在我们可以观测到的实际的段中 即使没有观测到任何可能出现的情况 也没有关系 更准确的来说
这里的0.5来自两个假设片段里的1 因为每一个的权重是四分之一 把它们相加就是0.5 同样的
这个0.25来自一个假设片段 这个1表示两个单词同时出现 在分母位置
我们加上了假设片段的总和 我们加了4个假设片段 每个的权重是四分之一
所以和是1 这就是为什么你在分母看到加了1 所以这大致概括了怎么计算交互信息 来进行语义关联挖掘 综上
语义关联可以通过 通过描述两个单词出现的相关性来发掘 我们引入了3个信息理论的概念 熵衡量一个变量的不确定性 条件性熵描述的是
当Y已知时变量X的熵 X和Y的交互信息
表示的是由于Y已知带来变量X的熵损失 或者X已知时变量Y的熵损失 两个情况是一样的 这三个概念在其他应用中也非常有用 这也是我们为什么要花时间细致地解释它们 特别地它们对 发掘语义关联非常有用 特别地来讲
交互信息是 研究这种联系的主要方法 它允许我们为不同单词对取值 让我们可以为这些单词对排序 并从文档集合当中发掘最突出的语义关联 注意语义关联发掘和 范式关系关联发掘存在着关联 我们已经讨论了用BM25来衡量背景中术语的权重 这也就揭示了 与候选单词有语义关联的候选单词 但是这里
一旦我们用交互信息来发掘语义关联 我们可以把这个交互信息
作为权重来表征背景 这样就给了我们另一种描述 一个单词上下文的方式
比如说一种类别 如果我们对所有的单词做同样的处理
我们就可以对单词进行聚类 通过比较它们背景上的相似性
来比较这些单词的相似性 这就给我们提供了另一种确定权重的方法 建立一个词聚合关系语境模型的简便方法 总结一下
整个部分是关于单词关联关系的挖掘 我们介绍了两个基本的关联
范式关联发掘 和语义关联发掘 这些都是非常宽泛的
可以被应用到任何语言的任何地方 基本不必是单词
也可以是词组或者实体 我们介绍了多种统计学方法来研究它们 纯统计学方法是可行的 对两种关系的发掘也都是有效的 它们也可以合在一起进行联合分析 这些方法在不用人力的情况下
可以应用到任何文本当中 因为它们是基于对单词的统计 可以发掘单词间有趣的关系 我们可以用不同的方法来定义上下文和片段 这将带来一些应用上的多样性 例如上下文可以非常短小
比如一个单词周围 或者一个句子、一个段落周围的几个单词
使用不同的上下文 允许我们发现范式关联不同的分析结果 同样地
统计单词同时出现的频数 利用视觉信息来发现语义关联 我们也需要定义片段
而这个片段也可以 被定义为很短小的文本框或一个很长的文章 这可以返回不同的关联结果 这些关联发掘方法也能对其他很多应用进行支持 包括信息提取和文本数据挖掘 如果你想要更多地了解这个话题
这里是一些建议阅读材料 第一本书其中一章讲的是相互定位 是和本次课程的话题相关的 第二个是一篇讲述用不同统计学方法 来研究词条的文章 就是那些非组合型的短语 例如热狗(hot dog)并不是很热的狗(dog) 蓝筹股(blue chip)并不是蓝色的片(chip) 这篇文章就是讨论如何处理这类短语的 第三个是一篇新发表的文章
是关于研究范式关联和语义关联的一种统一的方法 用的是基于词图的随机游走理论 [声音]