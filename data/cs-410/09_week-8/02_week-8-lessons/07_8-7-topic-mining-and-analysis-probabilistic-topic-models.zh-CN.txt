这节课是关于概率主题模型的主题挖掘和分析。 在本次课中， 我们将要继续讨论主题挖掘与分析 我们将介绍概率主题模型 这张是你早先看到过的幻灯片 就是我们讨论使用术语做主题的问题 所以为了直观的解决这些问题 我们需要更多的词汇来描述主题 从而解决缺少表现力的问题 当拥有更多的词汇时 我们就可以描述复杂主题了 为了解决第二个问题我们需要量化词语 用来区别主题间的细微差别 和模糊地介绍语义相连的词 最终为了解决语义模糊的问题 咱们得把 多重语义的词汇分开 从而辨识主题 所有这些都可以用一个概率主题模型表达 这就是为什么我们将会在很多课程中讨论它 所以在这里的基本概念是 推进主题作为一个分布的可移植性 你现在所能看到的是旧移植 植入每个主题 只是一个单词 一个术语 或者短语 但是现在我们将要用词汇分布来描述主题 可以看到在体育类别中 我们将用词汇分布取代 理论上地表述所有已知单词 举个例子 这里出现的高概率词汇是体育 游戏 篮球 足够 娱乐 明星等等 这些是体育术语 当然了它对于其他词会给出一个非0概率 就像 麻烦 这个可能与体育相关 而非主题相关的词 笼统来说我们可以想像出所有词汇有非0概率 许多词并不会读到且出现几率非常非常小 那么这些词出现的概率总和为1 所以它构成了一个所有单词的分布 直观来说 这个分布代表了一个主题 一个如果我们集合词汇 将会看到准备遗弃的单词的主题 可以看出来 作为一个特例 如果概率质量 集中在一个词上 就是体育 它基本退化成了一个主题的符号 只有一个单词 但是作为一个分布 这个主题可以 掺和进许多次来描述主题 同时还能在主题语义中发现很多不同 类似地可以用旅游和科学的分布来建模 旅游的分布中可以看到高频词比如景点 行程 航班等 相对地 科学中可以看到科学家 飞船 望远镜 或者 基因组学 你懂的 科学术语 这并不意味着体育术语 在自然科学中概率是0 我们可以想像所有这些单词概率为0 只不过某个特别主题中的某些单词有非常 非常小的概率 现在你会发现这些主题共用一些词 共用的意思就是甚至共用概率可能性阀值 还可以看到一个词出现在很多主题中 这些词我用了黑色来标识 所以你能看到 比如旅行这个词 在三个主题红都出现了 但是概率不同 在旅游主题中概率最高 有0.05 但是在体育和科学主题中概率稍小 类似的可以看到明星出现在体育里 也同样以高概率出现在科学主题 因为这同一个词和两个主题都有关系 所以它解决了开始提到的三个问题 第一 它现在使用很多词描述一个主题 使得我们可以描述一个相对复杂的主题 第二 它能量化术语 现在我们可以模型化语义差异 于是可以在模式化一个主题时引入相关词汇 第三 因为我们可以用概率指代不同主题中的同一词汇 从而分离语感 在文本中解码隐藏主题 来解决这三个问题 是一种新的表达主题的方法 所以现在我们的问题要稍微重新定义一下 非常类似于以前所见 除了 在对主题更加精雕细琢 现在每个主题都是词语分布 而且我们所知的每个词语分布 中所有的单词概率和应为一 如你所见 这里出现了一个限制 我们在主题上有另一个限制 叫π（Pi） 所以 同一个文件的所有π（Pi）下角标ij总和必须是1 那么我们怎么解决这个问题呢 将这个问题作为计算问题来看待 所以我们清楚地强调这是输入和 输出 并在这边作出说明 课程的输入就是我们的文本数据 C是集合但一般假设主题数目已知 为k 或者假设一个数字 然后绑定k个主题 即使不知道集合中具体主题数目 V是词汇表 它有一组词来决定 哪个单位可以作为基本单位来分析 大多数情况下可以用词汇作分析的基础 意味着每个词都是不同的 输出首先包含了一组用Ɵ来标示的主题 每一个Ɵ就是一个词汇分布 我们也希望在每个文档中考虑主题的覆盖 就是说 之前看到过的同样的π（Pi） 已知一组文档数据我们就会想计算所有的分布 和所有的在这组课件中看到的覆盖 当然了现在有许多不同方法来解决这个问题 理论上 你可以写一段［听不见］的程序来解决这一问题 但是我们准备介绍 一种普遍的解决方法 叫生成模型 这事实上是一种更常见的点子 一种用统计模型来解决文本挖掘的原理 我把你们以前见过的图片暗化了 为了展示生成的过程 所以这种方法的原理实际上是先去为数据设计一个模型 所以我们设计了概率模型来还原数据的生成 当然了 一切都是基于假设 实际数据并不一定是这么生成的 只是提出一个数据的概率分布 就是在这里看到的 已知一个特别的模型和参数 用λ 表示 这个模版实际上包含了 所有我们关注的参数 这些参数一般是能操控 概率风险模型的表现 意味着如果你用不同的数值代入这些参数 会得到比其它更高的数据概率 现在这种情况 对于我们的文本挖掘问题 或其它更精确的主题挖掘问题 我们有如下计划 首先 有Ѳ也就是一个词汇分布 然后我们还有每个文档的一组π 因为有n个文档 所以我们有n组π 每组π到最大 π的值总和为1 就是说首先应该装作已知 词汇分布和覆盖极限数值 然后就用这些分布来生成数据 如何用这种方法生成数据模型 假设数据是实际的符号 来自于一个基于这些参数的模型 这里有一个有趣的问题 就是我们到底总共需要多少参数呢 显然已知n乘以K个参数 对于π 已知K个Θ 但是每个Θ实际上是一组概率值 对吧 这是一组词汇分布呗 我要留下这个当练习 希望你能找出到底这里有多少个参数 这样一旦建好模型就可以套在数据上 就是说我们可以预测估计参数 或者 基于数据来推理参数 换句话说 想要调整参数值 除非找到数据组的最大概率 像我刚刚说的 根据参数值 一些数据点会比其它数据有更高的概率 我们所感兴趣的是 哪个参数之能提供最高概率 我也会用图片来说明你们看到的问题 X轴上用λ 作一维向量 虽然过于简洁 但是足够说明原理 Y轴显示了观察到的数据概率 这个概率明显基于λ的设定 这就是为什么变动λ值会导致变化 我们感兴趣的是找到λ＊ 这样可以最大化观测数据的概率 也就是我们估计的参数 并且这些参数 正是我们希望从文本数据中找到的参数 需要当这些参数是真实的结果 或者 数据挖掘算法的结果 这就是常见的使用 文本挖掘的生成模型的方法 首先我们设计了一个模型并用参数值去 尽可能的匹配 匹配数据后 我们会还原一些参数值 将使用特定参数值及 这种算法的结果 要当它们是从文本挖掘中找到的方法 模型不同 发现的知识和方法不同 总的来说 我们介绍了一种新表达主题的方法 就是词汇分布 以及使用 多个词汇来描述复杂主题的优点 我们还可以 量化词汇从而得到多种多样的语义 我们讨论了主题挖掘的任务与答案 当定义一个主题为分布 那么文章文字 主题数目和词汇表组的冲突就是一个输出 得到的结果就是一组主题 每一个是一个词汇分布 同时也覆盖每个文档中所有主题 这些可以用Θ和π来指代 在这些参数上还有两个限制 第一个是词汇分布的限制 每个词汇分布中所有词的概率 总和必须是1 是说词汇表中所有词 第二个限制是关于每个文档的主题覆盖 一个文档不允许在发现的主题组 之外还原 所以一个文档中k个主题的每一个主题的覆盖总和为1 我们也介绍了一种常见的文本挖掘生成模型的使用 方法就是先在生成的数据中建立一个模型 简单的假设它们就是这么生成的 模型中嵌入一些一直关注的参数 用λ表示 之后推出参数像是λ＊ 假设已知一组数据 可以用λ＊作为从文本中发现的 就是问题的文本 我们可以调整模型设计 和参数从而在文字中发现多种多样的知识 就像你在以后其他课件中能看到的一样 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community