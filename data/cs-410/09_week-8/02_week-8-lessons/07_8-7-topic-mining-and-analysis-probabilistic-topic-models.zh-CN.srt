1
00:00:06,750 --> 00:00:12,040
这节课是关于概率主题模型的主题挖掘和分析。

2
00:00:13,350 --> 00:00:14,110
在本次课中，

3
00:00:14,110 --> 00:00:16,909
我们将要继续讨论主题挖掘与分析

4
00:00:18,190 --> 00:00:20,490
我们将介绍概率主题模型

5
00:00:22,410 --> 00:00:26,140
这张是你早先看到过的幻灯片

6
00:00:26,140 --> 00:00:30,640
就是我们讨论使用术语做主题的问题

7
00:00:30,640 --> 00:00:35,370
所以为了直观的解决这些问题

8
00:00:35,370 --> 00:00:37,950
我们需要更多的词汇来描述主题

9
00:00:37,950 --> 00:00:43,110
从而解决缺少表现力的问题

10
00:00:43,110 --> 00:00:45,040
当拥有更多的词汇时

11
00:00:45,040 --> 00:00:49,880
我们就可以描述复杂主题了

12
00:00:49,880 --> 00:00:54,030
为了解决第二个问题我们需要量化词语

13
00:00:54,030 --> 00:00:59,140
用来区别主题间的细微差别

14
00:00:59,140 --> 00:01:04,600
和模糊地介绍语义相连的词

15
00:01:04,600 --> 00:01:09,240
最终为了解决语义模糊的问题 咱们得把

16
00:01:09,240 --> 00:01:14,700
多重语义的词汇分开 从而辨识主题

17
00:01:15,720 --> 00:01:21,060
所有这些都可以用一个概率主题模型表达

18
00:01:21,060 --> 00:01:25,520
这就是为什么我们将会在很多课程中讨论它

19
00:01:25,520 --> 00:01:28,130
所以在这里的基本概念是

20
00:01:28,130 --> 00:01:32,600
推进主题作为一个分布的可移植性

21
00:01:32,600 --> 00:01:35,650
你现在所能看到的是旧移植

22
00:01:35,650 --> 00:01:40,730
植入每个主题 只是一个单词 一个术语 或者短语

23
00:01:40,730 --> 00:01:45,240
但是现在我们将要用词汇分布来描述主题

24
00:01:45,240 --> 00:01:47,110
可以看到在体育类别中

25
00:01:47,110 --> 00:01:50,220
我们将用词汇分布取代

26
00:01:50,220 --> 00:01:53,160
理论上地表述所有已知单词

27
00:01:54,650 --> 00:01:59,150
举个例子 这里出现的高概率词汇是体育

28
00:01:59,150 --> 00:02:03,880
游戏 篮球 足够 娱乐 明星等等

29
00:02:03,880 --> 00:02:06,100
这些是体育术语

30
00:02:06,100 --> 00:02:10,150
当然了它对于其他词会给出一个非0概率

31
00:02:10,150 --> 00:02:15,430
就像 麻烦 这个可能与体育相关

32
00:02:15,430 --> 00:02:17,420
而非主题相关的词

33
00:02:18,900 --> 00:02:23,030
笼统来说我们可以想像出所有词汇有非0概率

34
00:02:23,030 --> 00:02:27,890
许多词并不会读到且出现几率非常非常小

35
00:02:27,890 --> 00:02:29,830
那么这些词出现的概率总和为1

36
00:02:31,780 --> 00:02:34,500
所以它构成了一个所有单词的分布

37
00:02:36,650 --> 00:02:41,440
直观来说 这个分布代表了一个主题

38
00:02:41,440 --> 00:02:46,780
一个如果我们集合词汇 将会看到准备遗弃的单词的主题

39
00:02:48,470 --> 00:02:53,236
可以看出来 作为一个特例 如果概率质量

40
00:02:53,236 --> 00:02:57,387
集中在一个词上 就是体育

41
00:02:57,387 --> 00:03:01,670
它基本退化成了一个主题的符号

42
00:03:01,670 --> 00:03:03,270
只有一个单词

43
00:03:04,640 --> 00:03:10,420
但是作为一个分布 这个主题可以

44
00:03:10,420 --> 00:03:13,980
掺和进许多次来描述主题

45
00:03:13,980 --> 00:03:17,970
同时还能在主题语义中发现很多不同

46
00:03:17,970 --> 00:03:24,500
类似地可以用旅游和科学的分布来建模

47
00:03:24,500 --> 00:03:30,120
旅游的分布中可以看到高频词比如景点 行程 航班等

48
00:03:31,670 --> 00:03:36,110
相对地 科学中可以看到科学家 飞船 望远镜 或者

49
00:03:36,110 --> 00:03:39,820
基因组学 你懂的 科学术语

50
00:03:39,820 --> 00:03:43,260
这并不意味着体育术语

51
00:03:43,260 --> 00:03:46,330
在自然科学中概率是0

52
00:03:46,330 --> 00:03:51,860
我们可以想像所有这些单词概率为0

53
00:03:51,860 --> 00:03:55,250
只不过某个特别主题中的某些单词有非常

54
00:03:55,250 --> 00:03:56,620
非常小的概率

55
00:03:58,200 --> 00:04:02,770
现在你会发现这些主题共用一些词

56
00:04:02,770 --> 00:04:07,600
共用的意思就是甚至共用概率可能性阀值

57
00:04:07,600 --> 00:04:10,990
还可以看到一个词出现在很多主题中

58
00:04:10,990 --> 00:04:13,140
这些词我用了黑色来标识

59
00:04:13,140 --> 00:04:17,110
所以你能看到 比如旅行这个词 在三个主题红都出现了

60
00:04:17,110 --> 00:04:19,420
但是概率不同

61
00:04:19,420 --> 00:04:23,237
在旅游主题中概率最高 有0.05

62
00:04:23,237 --> 00:04:29,050
但是在体育和科学主题中概率稍小

63
00:04:29,050 --> 00:04:32,450
类似的可以看到明星出现在体育里

64
00:04:32,450 --> 00:04:35,420
也同样以高概率出现在科学主题

65
00:04:35,420 --> 00:04:39,690
因为这同一个词和两个主题都有关系

66
00:04:39,690 --> 00:04:43,420
所以它解决了开始提到的三个问题

67
00:04:43,420 --> 00:04:46,750
第一 它现在使用很多词描述一个主题

68
00:04:46,750 --> 00:04:50,700
使得我们可以描述一个相对复杂的主题

69
00:04:50,700 --> 00:04:53,400
第二 它能量化术语

70
00:04:53,400 --> 00:04:57,060
现在我们可以模型化语义差异

71
00:04:57,060 --> 00:05:02,390
于是可以在模式化一个主题时引入相关词汇

72
00:05:02,390 --> 00:05:07,930
第三 因为我们可以用概率指代不同主题中的同一词汇

73
00:05:07,930 --> 00:05:12,210
从而分离语感

74
00:05:12,210 --> 00:05:16,930
在文本中解码隐藏主题

75
00:05:16,930 --> 00:05:22,480
来解决这三个问题 是一种新的表达主题的方法

76
00:05:22,480 --> 00:05:27,650
所以现在我们的问题要稍微重新定义一下

77
00:05:27,650 --> 00:05:32,090
非常类似于以前所见 除了

78
00:05:32,090 --> 00:05:34,920
在对主题更加精雕细琢

79
00:05:34,920 --> 00:05:41,180
现在每个主题都是词语分布 而且我们所知的每个词语分布

80
00:05:41,180 --> 00:05:45,460
中所有的单词概率和应为一

81
00:05:45,460 --> 00:05:47,640
如你所见 这里出现了一个限制

82
00:05:47,640 --> 00:05:53,060
我们在主题上有另一个限制 叫π（Pi）

83
00:05:53,060 --> 00:05:58,180
所以 同一个文件的所有π（Pi）下角标ij总和必须是1

84
00:05:59,620 --> 00:06:01,250
那么我们怎么解决这个问题呢

85
00:06:01,250 --> 00:06:05,470
将这个问题作为计算问题来看待

86
00:06:05,470 --> 00:06:07,560
所以我们清楚地强调这是输入和

87
00:06:07,560 --> 00:06:11,190
输出 并在这边作出说明

88
00:06:11,190 --> 00:06:12,920
课程的输入就是我们的文本数据

89
00:06:12,920 --> 00:06:18,620
C是集合但一般假设主题数目已知 为k

90
00:06:18,620 --> 00:06:22,940
或者假设一个数字 然后绑定k个主题

91
00:06:22,940 --> 00:06:27,820
即使不知道集合中具体主题数目

92
00:06:27,820 --> 00:06:32,960
V是词汇表 它有一组词来决定

93
00:06:32,960 --> 00:06:38,880
哪个单位可以作为基本单位来分析

94
00:06:38,880 --> 00:06:44,780
大多数情况下可以用词汇作分析的基础

95
00:06:44,780 --> 00:06:46,429
意味着每个词都是不同的

96
00:06:47,610 --> 00:06:53,560
输出首先包含了一组用Ɵ来标示的主题

97
00:06:53,560 --> 00:06:55,280
每一个Ɵ就是一个词汇分布

98
00:06:56,430 --> 00:07:02,860
我们也希望在每个文档中考虑主题的覆盖

99
00:07:02,860 --> 00:07:03,520
就是说

100
00:07:03,520 --> 00:07:06,250
之前看到过的同样的π（Pi）

101
00:07:07,470 --> 00:07:13,460
已知一组文档数据我们就会想计算所有的分布

102
00:07:13,460 --> 00:07:16,980
和所有的在这组课件中看到的覆盖

103
00:07:18,130 --> 00:07:21,520
当然了现在有许多不同方法来解决这个问题

104
00:07:21,520 --> 00:07:24,670
理论上 你可以写一段［听不见］的程序来解决这一问题

105
00:07:24,670 --> 00:07:27,050
但是我们准备介绍

106
00:07:27,050 --> 00:07:32,200
一种普遍的解决方法 叫生成模型

107
00:07:32,200 --> 00:07:35,770
这事实上是一种更常见的点子

108
00:07:35,770 --> 00:07:41,390
一种用统计模型来解决文本挖掘的原理

109
00:07:41,390 --> 00:07:46,190
我把你们以前见过的图片暗化了

110
00:07:46,190 --> 00:07:49,470
为了展示生成的过程

111
00:07:49,470 --> 00:07:55,960
所以这种方法的原理实际上是先去为数据设计一个模型

112
00:07:55,960 --> 00:08:02,070
所以我们设计了概率模型来还原数据的生成

113
00:08:02,070 --> 00:08:04,180
当然了 一切都是基于假设

114
00:08:04,180 --> 00:08:08,060
实际数据并不一定是这么生成的

115
00:08:08,060 --> 00:08:11,930
只是提出一个数据的概率分布

116
00:08:11,930 --> 00:08:13,980
就是在这里看到的

117
00:08:13,980 --> 00:08:18,840
已知一个特别的模型和参数 用λ 表示

118
00:08:18,840 --> 00:08:22,040
这个模版实际上包含了

119
00:08:22,040 --> 00:08:24,380
所有我们关注的参数

120
00:08:24,380 --> 00:08:27,780
这些参数一般是能操控

121
00:08:27,780 --> 00:08:29,370
概率风险模型的表现

122
00:08:29,370 --> 00:08:32,530
意味着如果你用不同的数值代入这些参数

123
00:08:32,530 --> 00:08:36,820
会得到比其它更高的数据概率

124
00:08:36,820 --> 00:08:39,910
现在这种情况 对于我们的文本挖掘问题

125
00:08:39,910 --> 00:08:44,100
或其它更精确的主题挖掘问题 我们有如下计划

126
00:08:44,100 --> 00:08:49,450
首先 有Ѳ也就是一个词汇分布

127
00:08:49,450 --> 00:08:52,070
然后我们还有每个文档的一组π

128
00:08:52,070 --> 00:08:58,980
因为有n个文档 所以我们有n组π 每组π到最大

129
00:08:58,980 --> 00:09:01,430
π的值总和为1

130
00:09:01,430 --> 00:09:06,370
就是说首先应该装作已知

131
00:09:06,370 --> 00:09:10,640
词汇分布和覆盖极限数值

132
00:09:10,640 --> 00:09:18,010
然后就用这些分布来生成数据

133
00:09:18,010 --> 00:09:21,950
如何用这种方法生成数据模型

134
00:09:21,950 --> 00:09:25,280
假设数据是实际的符号

135
00:09:25,280 --> 00:09:29,530
来自于一个基于这些参数的模型

136
00:09:29,530 --> 00:09:31,290
这里有一个有趣的问题

137
00:09:32,320 --> 00:09:35,080
就是我们到底总共需要多少参数呢

138
00:09:35,080 --> 00:09:41,360
显然已知n乘以K个参数

139
00:09:41,360 --> 00:09:42,140
对于π

140
00:09:42,140 --> 00:09:44,530
已知K个Θ

141
00:09:44,530 --> 00:09:49,110
但是每个Θ实际上是一组概率值 对吧

142
00:09:49,110 --> 00:09:51,580
这是一组词汇分布呗

143
00:09:51,580 --> 00:09:54,000
我要留下这个当练习

144
00:09:54,000 --> 00:09:59,980
希望你能找出到底这里有多少个参数

145
00:09:59,980 --> 00:10:04,690
这样一旦建好模型就可以套在数据上

146
00:10:04,690 --> 00:10:07,900
就是说我们可以预测估计参数 或者

147
00:10:07,900 --> 00:10:11,010
基于数据来推理参数

148
00:10:11,010 --> 00:10:14,930
换句话说 想要调整参数值

149
00:10:14,930 --> 00:10:20,330
除非找到数据组的最大概率

150
00:10:20,330 --> 00:10:22,880
像我刚刚说的 根据参数值

151
00:10:22,880 --> 00:10:27,090
一些数据点会比其它数据有更高的概率

152
00:10:27,090 --> 00:10:28,620
我们所感兴趣的是

153
00:10:28,620 --> 00:10:33,420
哪个参数之能提供最高概率

154
00:10:33,420 --> 00:10:37,620
我也会用图片来说明你们看到的问题

155
00:10:37,620 --> 00:10:41,720
X轴上用λ

156
00:10:41,720 --> 00:10:44,260
作一维向量

157
00:10:44,260 --> 00:10:49,360
虽然过于简洁 但是足够说明原理

158
00:10:49,360 --> 00:10:53,370
Y轴显示了观察到的数据概率

159
00:10:53,370 --> 00:10:57,780
这个概率明显基于λ的设定

160
00:10:57,780 --> 00:11:01,480
这就是为什么变动λ值会导致变化

161
00:11:01,480 --> 00:11:04,830
我们感兴趣的是找到λ＊

162
00:11:05,880 --> 00:11:09,259
这样可以最大化观测数据的概率

163
00:11:10,440 --> 00:11:15,470
也就是我们估计的参数

164
00:11:15,470 --> 00:11:17,040
并且这些参数

165
00:11:17,040 --> 00:11:21,720
正是我们希望从文本数据中找到的参数

166
00:11:21,720 --> 00:11:25,405
需要当这些参数是真实的结果 或者

167
00:11:25,405 --> 00:11:28,046
数据挖掘算法的结果

168
00:11:28,046 --> 00:11:32,966
这就是常见的使用

169
00:11:32,966 --> 00:11:38,231
文本挖掘的生成模型的方法

170
00:11:38,231 --> 00:11:42,762
首先我们设计了一个模型并用参数值去

171
00:11:42,762 --> 00:11:44,804
尽可能的匹配

172
00:11:44,804 --> 00:11:47,207
匹配数据后 我们会还原一些参数值

173
00:11:47,207 --> 00:11:48,827
将使用特定参数值及

174
00:11:48,827 --> 00:11:50,910
这种算法的结果

175
00:11:50,910 --> 00:11:55,880
要当它们是从文本挖掘中找到的方法

176
00:11:55,880 --> 00:11:59,460
模型不同 发现的知识和方法不同

177
00:11:59,460 --> 00:12:03,840
总的来说 我们介绍了一种新表达主题的方法

178
00:12:03,840 --> 00:12:09,020
就是词汇分布 以及使用

179
00:12:09,020 --> 00:12:14,039
多个词汇来描述复杂主题的优点 我们还可以

180
00:12:14,039 --> 00:12:19,390
量化词汇从而得到多种多样的语义

181
00:12:19,390 --> 00:12:23,390
我们讨论了主题挖掘的任务与答案

182
00:12:23,390 --> 00:12:26,430
当定义一个主题为分布

183
00:12:26,430 --> 00:12:30,140
那么文章文字 主题数目和词汇表组的冲突就是一个输出

184
00:12:30,140 --> 00:12:33,000
得到的结果就是一组主题

185
00:12:33,000 --> 00:12:35,470
每一个是一个词汇分布

186
00:12:35,470 --> 00:12:38,730
同时也覆盖每个文档中所有主题

187
00:12:38,730 --> 00:12:43,870
这些可以用Θ和π来指代

188
00:12:43,870 --> 00:12:48,710
在这些参数上还有两个限制

189
00:12:48,710 --> 00:12:53,320
第一个是词汇分布的限制

190
00:12:53,320 --> 00:12:56,820
每个词汇分布中所有词的概率

191
00:12:56,820 --> 00:12:59,400
总和必须是1 是说词汇表中所有词

192
00:12:59,400 --> 00:13:03,960
第二个限制是关于每个文档的主题覆盖

193
00:13:03,960 --> 00:13:08,600
一个文档不允许在发现的主题组

194
00:13:08,600 --> 00:13:10,200
之外还原

195
00:13:10,200 --> 00:13:17,220
所以一个文档中k个主题的每一个主题的覆盖总和为1

196
00:13:17,220 --> 00:13:21,580
我们也介绍了一种常见的文本挖掘生成模型的使用

197
00:13:21,580 --> 00:13:27,920
方法就是先在生成的数据中建立一个模型

198
00:13:27,920 --> 00:13:30,780
简单的假设它们就是这么生成的

199
00:13:30,780 --> 00:13:34,730
模型中嵌入一些一直关注的参数

200
00:13:34,730 --> 00:13:35,650
用λ表示

201
00:13:36,770 --> 00:13:40,605
之后推出参数像是λ＊

202
00:13:40,605 --> 00:13:41,935
假设已知一组数据

203
00:13:43,095 --> 00:13:48,975
可以用λ＊作为从文本中发现的

204
00:13:48,975 --> 00:13:49,495
就是问题的文本

205
00:13:50,555 --> 00:13:53,115
我们可以调整模型设计

206
00:13:53,115 --> 00:13:58,855
和参数从而在文字中发现多种多样的知识

207
00:13:58,855 --> 00:14:04,999
就像你在以后其他课件中能看到的一样

208
00:14:04,999 --> 00:14:14,999
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community