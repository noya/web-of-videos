[声音] 这一课是关于组合关系探索和交互信息 在这一课我们会继续讨论组合关系探索 我们还会讨论信息学系列中的另一个重要的概念 我们称之为交互信息 还有如何使用交互信息来探索组合关系 在我们讨论条件熵的问题之前 我们需要知道在文字的不同配对都有计算到条件熵 并不都是相似的， 使得从语料库里得到 这种全局性的隐藏的强组合关系更难 所以现在我们会介绍交互信息 这是信息学系列中的另一种概念，可以让我们 有时候来标准化这种条件熵，以便条件熵可以和其他不同的配对来进行对比 尤其是交互信息，为了发现 I(X:Y)满足熵减少的等式 在知道Y的条件下就可以得到X 更准确的来说，我们这里更感兴趣的问题是 在知道Y的情况下我们能得到什么样的X熵系数 所以数学意义上我们可以定义原来的X熵系数 以及给定X的条件Y之间的不同 你可以看到这里也能够定义 在知道X情况下Y熵系数的减少 现在正常情况下给定Y的条件接口H的X 和给定X下的Y的熵不相同，但有意思的是 知道任何一个条件下的熵减少是相等的 所以这里我同意这个数值被称为交互信息 这个函数有一些有趣的属性，首先它也是非负的 这很容易理解，因为原始熵永远 不会比减少的条件熵可能性低 换句话说，条件熵永远不会超过原始熵 知道这些潜在信息可以帮助我们 来预测X，而不是误导我们 信号属性是对称的，像额外熵 不是对称的，交互信息是对称的 第三个属性是当且仅当这两个随机变量完全独立时 它达到其最小值，零 这意味着说已知其中一个不能告诉我们另一个的任何信息 看上面这个等式就可以帮我们确认最后一个属性 仅在X的条件熵 Y与X的原始熵完全一致时它的值为0 所以这意味着知道为什么并没有任何帮助 只有当X和Y完全独立才成立 现在当我们用条件熵给定X一个不同于X的等级 由于等级是基于交互信息我们会得到同样的序列 因为在这里函数H(X)是固定值，因为X是 所以基于交互熵的等级和基于给定Y的条件熵X等级 是完全相同的 但是交互信息允许我们对不同配对的X和Y进行比较 这也是为什么交互信息更普遍，而且一般来说更有用 让我们检查一下使用交互信息的直觉性 对于横组合关系挖掘 现在迫使我们进行关系挖掘的问题是 在“吃(eats)”发生的时候,其他什么样的词也趋向发生? 所以这个问题可以被构造为一个交互信息问题 即哪些词与吃(eats)有很高的交互信息 计算出吃(eats)和其他词之间丢失的信息 这样做的话基本也是一个有条件的基准 我们会看到与吃(eats)强相关的词 有一个最高点 而不相关的词会有较低的交互信息 关于这一点我会给出一些例子 “吃(eats)”和“肉(meats)”之间的交互信息 相等于“肉(meats)”和“吃(eats)”之间的交互信息 因为信息是对称的，并且这个信息含量比“肉(meats)”和“这(the)”之间的交互信息更高 因为知道“这(the)”并不能帮助像我们这样的预测者 相似的，知道“吃(eats)”也不能帮助我们预测 “这(the)” 你也可以很容易发现 一个词与它自己之间的交互信息是最高的 等于这个词的熵 因为在这种情况下减少的是最大值 在知道其中一个的情况下允许我们去完全预测另外一个词 所以条件熵是零 因此交互信息达到其最大值 它会不断增大然后达到“吃(eats)”和其他词之间的机器容积 换句话说选择任何其他词 让计算机选择"吃(eats)"和任意的另一个词 你将不会得到比计算“吃(eats)”和它自己之间更大的信息量 现在让我们来看看如何计算交互信息 为了计算我们经常 使用一种不同的交互信息形式 在数学上我们可以把交互信息重写成幻灯片上这样的形式 基本上我们可以看到一个方程式用于计算 所谓的KL发散 这是信息理论中的另一个术语 它度量了两个分布之间的发散程度 现在如果你看一下这个方程式，它也是这两个随机变量 不同值的许多结合的加和 但是在这个总和中我们主要为两个联合分布之间做对比 分子中含有节点 实际观察到的这两个随机变量的联合分布 下半部分或者说分母可以被 解释为理想的两个随机变量的联合分布 如果他们是独立的,因为当这两个随机变量是独立时 它们的联合分布等于这两个可能性的乘积 所以这个比值会告诉我们这两个变量是否确实是独立的 如果它们是实际独立的那么我们可以期望这两个变量是一样的 但是如果分子与分母是不同的 那么这说明两个变量不是独立的并且可以帮助度量关联 这个加和是把所有这两个随机变量的 可能值的组合都考虑进去得到的 在我们的例子中, 每个随机变量能够取这两个值中的一个 零或一, 所以我们这里有四个组合 我们看到这个形式的交互信息, 它显示了 交互信息匹配这种真实的联合分布的发散 在独立性假设情况下的期望分布 发散越大,交互信息的值越高 现在让我我们进一步看一下这个 交互信息方程中涉及到的概率 还有这里, 是所有涉及到的概率 你可以很容易确认 进本上这是我们第一次讨论 对应每个词的出现或者不出现的概率 对于w1我们有两个概率 它们加起来应该等于一, 因为一个词只有出现或不出现两个状态 这个分段中对于第二个词也是类似的 我们也有两个概率代表了这个词的出现或者不出现 也有一个加和等于y 最后我们有许多联合概率 来表示两个词同时出现的情况,显示在这里 它们的加和为一,因为这两个词仅有 四种可能的情形 任何一种同时出现的情况 所以在这种情况下两个变量会有单个词出现的概率 这里有两种情况 在这两种情况下其中一个随机变量将会等于一 另一个会等于零,最后我们还有一种情况是两个词都没有出现 这个时候两个变量都得到零这个值 这就是在计算交互信息时可能涉及到的 这里的这一项 一旦我们知道如何计算这些概率 我们可以很容易计算交互信息 很有趣的是当我们知道这些概率之间的联系和约束 我们已经看到了其中的两个对吗 在之前的幻灯片里 你看到了这些小概率的加和为一 我们也看到了这个约束条件 它说这两个词有这四种同时发生的情况 但我们也有一些额外的约束条件显示在下面 比如，这个是说如果我们把我们观察到两个词同时出现的概率 和第一个词出现但是第二个词不出现的概率 相加起来 我们就得出可以看到第一个词的概率 换句话说就是可以看到第一个词的概率 当我们看到第一个词时 根据是否看到第二个词只有两种情况 所以这个概率捕获了第一种情形即第二个词 也出现 而这个则捕获了第二种情形即第二个词不出现 我们只看到第一个词 其实也很容易看到其他的等式遵循了同样的推理 这些等式允许我们根据其他概率来计算 未知概率,简化了整个计算 更具体地来说如果我们知道了 一个词出现的概率, 就像在这个例子中 如果我们知道了这个并且知道第二个词出现的概率 那么我们可以很容易计算它不出现的概率,对吗? 很容易用这个等式来计算 在计算每个词出现或不出现的这些概率时 要十分小心 现在让我们来看一下它们的联合分布 假设我们知道 它们同时发生的可能性即概率 容易知道我们根据这些条件 可以计算剩下的概率 具体来说比如用这个等式我们可以计算 第一个词出现但是第二个词不出现的概率 因为我们知道这些框标记内的概率 同样的用这个等式我们可以计算出我们只看到第二个词的 概率 最后的话,这个概率可以用 这个等式来计算时因为这里的值已知 这里的值已知,还有这里也是已知,对吗? 所以这里很容易计算出来 可以计算 这篇幻灯片说明了我们只需要知道如何计算 这个方框内的三个概率 指定每个词的出现以及两个词在一个分段中的同时发生 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community