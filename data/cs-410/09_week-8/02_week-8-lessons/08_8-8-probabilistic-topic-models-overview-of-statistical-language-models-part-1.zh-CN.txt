[音乐] 本节课程关于统计语言模型的概述 其中包含作为特例的概率模型 在这一讲中 我们将介绍 统计语言模型的概况 这些模型都是通用模型 概率主题模型作为特例也包含在其中 作为开始 什么是统计语言模型？ 统计语言模型从根本上来说 是指单词序列的概率分布 比如 我们有一个分布 其中序列  today is Wednesday 的概率是0.001 而序列  today Wednesday is（这不是一个符合语法的序列） 的概率则非常非常小 如图所示 类似地 另外一个语句 the eigenvalue is positive（特征值是正数）的概率是0.00001 你可以看到 这样的分布明显地具有
上下文依赖的性质 它依赖于讨论的上下文 某些单词序列相比别的单词序列具有更高的概率 但是相同的单词序列在不同的上下文中可能会有不同的概率 意味着这样的分布可以实际用于主题归类 这样的模型同时也可以看作 文本生成的概率机制（装置） 即 我们可以将文本数据看从这个模型得到的观察数据 基于上述原因 我们将这种模型称为生成模型 现在 给定一个模型我们就能组合出各种单词序列 如 基于本页幻灯片给出的分布 我们可能生成一个序列 比如说 today is Wednesday 因为这个序列具有较高的概率 我们可能经常能得到这个序列 我们也有可能以较小的概率观察到序列
 eigenvalue is positive 只有在非常非常偶然的情况下我们能观察到 today Wednesday is 这样的序列
因为它的概率是如此的小 总的来说 为了能将某一个分布归类 我们必须给出所有这些不同单词序列的概率 这显然是不可能的 因为我们不能穷举所有单词序列 因此在实践中 我们将对这个模型进行一定的简化 最简单的语言模型为 一元语法模型 我们简单的假设 这种模型独立地生成构成文本的每一个单词 当然 这些单词实际上可能并非独立 但是这个假设可以有效地简化语言模型 现在一个由w1到wn表述的序列的概率 等于每个单词的概率的积 这种模型的参数的个数 等于我们的单词表中的单词的个数 现在假设我们有n个单词 即n个概率值 分别对应每一个单词 并且和为1 现在 假设我们的文本是 从这个单词分布中泵出的一个样本 即 我们以每次抽出一个单词的方式 得到我们的最终文本 比如 我们尝试从某个分布中抽出单词组合 我们可能经常看到 Wednesday 或者 today 而在小概率下才能看到如 eigenvalue 等一些其他的单词 不过这种方式能让我们计算每一个单词序列的概率 即使我们的模型只给出每一个单词的概率 这种可行性基于“独立” 具体来说 我们现在可以计算  today is Wednesday 的概率了 因为它就是  today 的概率和 is 的概率 以及Wednesday的概率 的乘积 举例来说  在此我给出一些编造的数据 当你对这些数字做乘法 
你就得到了  today is Wednesday 的概率 你可以看到 给定N个单词中每个单词对应的概率 实际上可以得到所有单词序列的概率分布 因此 这是一个很简单的模型 它忽略了单词的顺序 所以 实际上 它可能并不适用于某些问题 
比如在语音识别领域 你可能会关心单词的顺序 但是它实际上已足够处理 包括主题分析在内的许多课题 这也是我们的兴趣所在 当有了一个模型 我们经常会考虑两个问题 其一 给定一个模型 
我们观察到某一类数据点的概率如何 即 我们对抽样过程感兴趣 其二 估计过程 即 基于给定模型和某些观测值的参数估计 我们将在稍后讨论这个问题 首先来讨论抽样 在此给出2个单词分布或者说一元语法模型的例子 第一个例子中 类似于 text mining association之类的单词 有更高的概率 这标志着与文本挖掘有关的主题
因为 当我们从这个分布中取单词组合时 我们倾向于看到经常出现在文本挖掘
上下文中的单词 针对这个例子 如果我们想知道 生成某篇特定文章的概率 那么我们更有可能看到看起来像
文本挖掘相关论文的文本 当然 以从这个分布中抽取单词的方式 生成的文档不大可能连贯 虽然生成的文本挖掘的论文 能在顶级会议上发表的概率并不为零 -- 在所有字在这个分布中的概率
都非零的假设前提下 这意味着 本质上我们可以生成
所有类型的文本文档 包括有意义的文本文档 在图下方的第二个分布显示 概率较高的是另外一些单词 food nutrition healthy等这样的单词 很显然它代表另一个主题 在此可能是health 所以 如果我们从这个分布中取样 观察到一篇文本挖掘相关论文
的概率就非常非常小 另一方面 观察到一篇关于食物营养的论文的概率 相对来说就比较高 这说明 给定特定的分布
不同的文本将对应不同的概率 现在 让我们来看看估计的问题 在此 我们假设我们已有了观测数据 即 我已经知道生成的文本的具体内容 在这个例子中 假设我们有一篇文本挖掘的论文 实际上 这是某篇论文摘要
一共100个单词 我已经给出某些单词的个数 现在 如果我们问 用于生成这个文本数据的语言模型
最有可能是什么样的？ 假设这个文本是基于某个语言模型的观察结果 关于这个语言模型的最优的猜测是什么？ Ok 现在的问题是对这些单词进行概率估计 就像我在这里显示的这样 那么你怎么认为？ 你的猜测是什麽？ 你倾向于小概率的文本还是 概率相对较大的文本？ query（这个词）的概率如何？ 你的猜测将依赖于 我们在文本数据中观察到的
这个词的次数 对不对？ 现在思考一下 如果你和许多人一样
那么你的猜测将会是 text（这个词）的概率是100分之10 因为 在这个100个单词的文本中
观察到 text 的次数是 10 次 同样 mining是100分之5 而 query的概率相对小些 只有一次 所以是100分之1 因此 直观上来说 这是一个合理的猜测 但是问题是 
这是我们关于参数的最优估计吗？ 当然 为了回答这个问题 我们要定义什么是最优 这个例子中 我们的猜测在某种意义上的确是最优的 这种估计叫做极大似然估计 称它是最优的是因为它能赋予
我们的被观测数据最大概率 意味着 如果我们改变估计值 哪怕是稍微的 被观测的文本数据的概率都会变小 这就是极大似然估计 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community