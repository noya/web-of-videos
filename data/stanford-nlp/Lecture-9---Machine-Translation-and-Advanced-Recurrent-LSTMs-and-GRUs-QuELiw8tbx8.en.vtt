WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.797
[MUSIC]

00:00:04.797 --> 00:00:05.844
Stanford University.

00:00:05.844 --> 00:00:09.458
&gt;&gt; All right, hello, everybody.

00:00:09.458 --> 00:00:11.770
And welcome to lecture number nine.

00:00:11.770 --> 00:00:14.738
Today, we'll do a brief recap,
some organizational stuff, and

00:00:14.738 --> 00:00:18.554
then we'll talk about, I love when we call
them fancy, recurrent neural networks.

00:00:18.554 --> 00:00:23.491
But those are the most important
deep learning models of the day,

00:00:23.491 --> 00:00:26.290
LSTMs and GRU type models.

00:00:26.290 --> 00:00:31.140
They're very exciting and really form
the base model for pretty much every

00:00:31.140 --> 00:00:34.840
deep learning paper or almost all the deep
learning papers you see out there.

00:00:34.840 --> 00:00:40.720
So after today, you'll really have in your
hands the kind of tool that is the default

00:00:40.720 --> 00:00:45.040
tool for a lot of different deep learning
final P applications, so super exciting.

00:00:45.040 --> 00:00:49.031
And the best part is, you kind of know
most of the important math already of it,

00:00:49.031 --> 00:00:50.673
so we can just define the model.

00:00:50.673 --> 00:00:54.157
And everything else will kind of
follow through with these basic, and

00:00:54.157 --> 00:00:58.320
sometimes painful building blocks
that we went through before.

00:00:58.320 --> 00:01:00.640
All right, before we jump in,
some organizational stuff.

00:01:00.640 --> 00:01:02.770
We have a new office hour schedule and
places.

00:01:02.770 --> 00:01:07.042
Today, we're continuously trying to
optimize the whole process based on

00:01:07.042 --> 00:01:08.460
your feedback.

00:01:08.460 --> 00:01:09.760
Thanks for that.

00:01:09.760 --> 00:01:14.229
So I'll have office hours every day,
multiple times.

00:01:14.229 --> 00:01:19.214
I hope that will allow us to kind of
distribute the load a little bit,

00:01:19.214 --> 00:01:23.849
cuz I know sometimes lots of people
come to one office hour, and

00:01:23.849 --> 00:01:26.552
then there's a long wait there.

00:01:26.552 --> 00:01:28.470
Also, it's important that you register for

00:01:28.470 --> 00:01:32.740
your GPU teams by the end of today,
or ideally before.

00:01:32.740 --> 00:01:36.390
So that we can make
sure you all get a GPU.

00:01:36.390 --> 00:01:42.565
Ideally, we also encourage people
to have pairs for Problem set 3 and

00:01:42.565 --> 00:01:47.692
4 the project, at least pairs,
cuz we only have 300 or

00:01:47.692 --> 00:01:52.000
so GPUs and
almost 700 students in the class.

00:01:52.000 --> 00:01:55.735
So try to form teams, but do make sure
that you don't just have your partner or

00:01:55.735 --> 00:01:56.252
work on and

00:01:56.252 --> 00:02:00.110
implement all the GPU stuff, and you do
all the other parts of the problem set.

00:02:00.110 --> 00:02:04.763
Cuz then you really miss out on
a very important valuable skill for

00:02:04.763 --> 00:02:10.276
both research and applied deep learning,
if you don't know how to use a GPU.

00:02:10.276 --> 00:02:14.250
And sadly,
I have to get back to some work event.

00:02:14.250 --> 00:02:16.660
So I'll have a pretty
short office hour today.

00:02:16.660 --> 00:02:21.340
But then I know we have the deadline for
project proposals on Thursdays.

00:02:21.340 --> 00:02:25.022
So on Thursday,
I'm gonna have an unlimited office hour.

00:02:25.022 --> 00:02:28.690
I'm gonna start after class, and
will end when queue status is empty.

00:02:28.690 --> 00:02:33.460
So if you come half an hour late,
prepare to talk to me three hours later.

00:02:33.460 --> 00:02:38.461
So [LAUGH] the project is the coolest
part, so I don't wanna discourage

00:02:38.461 --> 00:02:42.887
people from doing the project
because we don't have enough.

00:02:42.887 --> 00:02:45.330
So it's gonna be great.

00:02:45.330 --> 00:02:48.016
I'll bring food.
You should bring food too,

00:02:48.016 --> 00:02:48.585
and-
&gt;&gt; [LAUGH]

00:02:48.585 --> 00:02:49.089
&gt;&gt; [LAUGH]

00:02:49.089 --> 00:02:51.160
&gt;&gt; It's kind of good fun, all right.

00:02:52.190 --> 00:02:57.049
If by any chance even after midnight,
people, we still have the queue status

00:02:57.049 --> 00:03:00.932
is still full, which I doubt at
that point, I think, I hope,

00:03:00.932 --> 00:03:05.819
then we can push the proposals out for
those, or you can submit the proposal.

00:03:05.819 --> 00:03:09.650
And then we'll figure out the mentor
situation, very soon thereafter.

00:03:09.650 --> 00:03:12.740
So all right, any questions
around any class organization?

00:03:21.032 --> 00:03:23.470
All right, then lets dive right in.

00:03:23.470 --> 00:03:26.927
So basically today,
we'll have a a very advanced,

00:03:26.927 --> 00:03:32.745
cutting edge blast from the past, because
while pedagogically, it'll make sense for

00:03:32.745 --> 00:03:37.563
us to first talk about a model from 2014,
from just three years ago.

00:03:37.563 --> 00:03:41.616
The main model we'll end up with,
the long-short-term-memories is actually

00:03:41.616 --> 00:03:45.092
a very old model, from 97, and
has kind of been dormant for a while.

00:03:45.092 --> 00:03:48.469
As very powerful model,
you need a lot of training data for it,

00:03:48.469 --> 00:03:50.460
you need fast machines for it.

00:03:50.460 --> 00:03:54.110
But now, that we have those two things,
this is a very powerful model for NLP.

00:03:54.110 --> 00:03:57.010
And if you ask one of the inventors,

00:03:57.010 --> 00:04:00.320
the second model is really just
a special case of the LSTM.

00:04:00.320 --> 00:04:03.760
But I think, pedagogically,
it makes sense to sort of first talk about

00:04:03.760 --> 00:04:06.842
the so-called Gated Recurrent Unit,
which is slightly simpler version.

00:04:06.842 --> 00:04:11.669
&gt;&gt; And we'll use machine translation which
is one of the sort of most useful tasks

00:04:11.669 --> 00:04:15.280
you might argue of NLP,
sort of a real life task.

00:04:15.280 --> 00:04:18.971
Something that actual people
outside academia, outside research,

00:04:18.971 --> 00:04:21.167
outside linguistics really care about.

00:04:21.167 --> 00:04:27.052
And by the end, you'll actually have the
skills to build one of the best machine

00:04:27.052 --> 00:04:32.640
translation models out there, modulo
a lot of time and some extra effort.

00:04:32.640 --> 00:04:36.365
But the biggest parts of 90% of
the top MT systems out there,

00:04:36.365 --> 00:04:40.161
you'll be able to understand at least and
probably build also,

00:04:40.161 --> 00:04:43.480
if you have the GPU
skills after this class.

00:04:43.480 --> 00:04:46.930
All right, so I'm not gonna go through
too many of the details, but in just in

00:04:46.930 --> 00:04:51.560
preparation to mentally make you also
think about the midterm that's coming up.

00:04:51.560 --> 00:04:53.610
Next lecture, we'll have midterm review.

00:04:53.610 --> 00:04:57.227
But ideally, these kinds of
equations that I'm throwing up here,

00:04:57.227 --> 00:04:58.945
you're pretty familiar with.

00:04:58.945 --> 00:05:00.728
At this point, you're like yeah,

00:05:00.728 --> 00:05:03.420
I just do some negative
sampling here from my Word2Vec.

00:05:04.450 --> 00:05:06.981
And I have my inside and
my outside vectors in the window.

00:05:06.981 --> 00:05:12.450
And similarly for glove, I have two
sets of vectors, you optimize this.

00:05:12.450 --> 00:05:14.441
You have a function here, dead limits,

00:05:14.441 --> 00:05:17.791
how important very frequent pairs
are in your concurrence matrix.

00:05:17.791 --> 00:05:22.130
You understand the max-margin
objective function.

00:05:22.130 --> 00:05:25.688
You have scores of good windows
from the large training corpus and

00:05:25.688 --> 00:05:26.857
corrupted windows.

00:05:26.857 --> 00:05:28.924
So all of these should be familiar.

00:05:28.924 --> 00:05:33.764
And if not, then you really should also
start thinking about sort of studying

00:05:33.764 --> 00:05:35.257
again for the midterm.

00:05:36.310 --> 00:05:39.076
The most basic definition of neural net,

00:05:39.076 --> 00:05:42.798
where we just have some score
at the end or some soft max.

00:05:42.798 --> 00:05:46.706
And really being comfortable
with these two final equations,

00:05:46.706 --> 00:05:51.423
that if you understand those, all
the rest of the models will basically be,

00:05:51.423 --> 00:05:56.770
in many cases, sort of fancy versions or
adapted versions of these two equations.

00:05:56.770 --> 00:05:59.424
So that's important.

00:05:59.424 --> 00:06:03.676
And then we'll have our standard recurrent
neural network that we already went

00:06:03.676 --> 00:06:07.493
through, and we kind of assume you
should know for the midterm as well.

00:06:07.493 --> 00:06:12.392
And our grade of Cross Entropy Error,
as one of the main loss or

00:06:12.392 --> 00:06:16.440
objective functions that we optimize.

00:06:16.440 --> 00:06:20.177
And when we optimize,
we usually use the Mini-batched SGD.

00:06:20.177 --> 00:06:21.080
We don't go through single example.

00:06:21.080 --> 00:06:25.136
We don't go through the entire
batch of our trained data, but

00:06:25.136 --> 00:06:28.496
we take 100 or so
of examples each time we train.

00:06:28.496 --> 00:06:33.590
So all those concepts, you should
feel reasonably comfortable now.

00:06:33.590 --> 00:06:37.154
And if not, then definitely come
back to the office hours, and

00:06:37.154 --> 00:06:39.450
start sort of studying for the midterm.

00:06:39.450 --> 00:06:44.151
All right, and we'll go over more
midterm details in the next lecture.

00:06:44.151 --> 00:06:48.500
All right, now, onto the main topic
of today, machine translation.

00:06:48.500 --> 00:06:53.273
So you might think for some NLP tasks
that you can get away with thinking of

00:06:53.273 --> 00:06:57.118
all the rules that, for
instance, sentiment analysis.

00:06:57.118 --> 00:07:00.138
A sentence might come up positive or
negative, right?

00:07:00.138 --> 00:07:03.540
You say, I have a list of all the positive
words, most of all the negative words.

00:07:03.540 --> 00:07:06.936
And I can think of the ways you can negate
positive words and things like that.

00:07:06.936 --> 00:07:10.996
And you could maybe conceive of
creating a sentiment analysis system

00:07:10.996 --> 00:07:15.260
of just all your intuitions
about linguistics and sentiment.

00:07:15.260 --> 00:07:18.860
That kind of approach is completely
ridiculous for machine translation.

00:07:18.860 --> 00:07:20.250
There's no way you would ever,

00:07:20.250 --> 00:07:24.010
nobody will ever be able to think of all
the different rules and exceptions for

00:07:24.010 --> 00:07:28.410
translating all possible sentences
of one language to another.

00:07:28.410 --> 00:07:32.920
So basically, the baseline that's
pretty well established is that

00:07:32.920 --> 00:07:36.790
all machine translation systems
are somewhat statistical in nature.

00:07:36.790 --> 00:07:38.670
We will always try to
take a very large corpus.

00:07:38.670 --> 00:07:41.137
In fact, we'll have so
called parallel copra,

00:07:41.137 --> 00:07:44.520
where we have a lot of the sentences or
paragraphs in one language.

00:07:44.520 --> 00:07:48.845
And we know that this paragraph in this
language translates to that paragraph

00:07:48.845 --> 00:07:50.119
in another language.

00:07:50.119 --> 00:07:55.040
One of the popular parallel copra
of All of, for a long time,

00:07:55.040 --> 00:07:57.060
for the last couple thousand
years is the Bible, for instance.

00:07:57.060 --> 00:07:58.451
You'll have Bible translated.

00:07:58.451 --> 00:07:59.789
It has nice paragraphs.

00:07:59.789 --> 00:08:02.243
And each paragraph is translated
in different languages.

00:08:02.243 --> 00:08:05.614
That would be one of
the first parallel corpora.

00:08:05.614 --> 00:08:07.629
The very first is actually
the Rosetta Stone.

00:08:07.629 --> 00:08:12.547
Which allowed people to have
at least some understanding

00:08:12.547 --> 00:08:15.626
of ancient Egyptian hieroglyphs.

00:08:15.626 --> 00:08:22.872
And it's pretty exciting if you're
into historical linguistics.

00:08:22.872 --> 00:08:27.742
And it allows basically to translate
those to the Demotic script and

00:08:27.742 --> 00:08:31.081
the ancient Greek also,
which we still know.

00:08:31.081 --> 00:08:35.490
And so we can gain some intuition about
what's going on in the other two.

00:08:35.490 --> 00:08:37.743
Now, in the next couple of slides,

00:08:37.743 --> 00:08:42.330
I will basically try to bring across
to you that traditional statistical

00:08:42.330 --> 00:08:46.956
machine translation systems are very,
very complex beasts.

00:08:46.956 --> 00:08:50.140
And it wouldn't have been impossible for
me to say at the end of the lecture,

00:08:50.140 --> 00:08:52.770
all right, now you could implement
this whole thing yourself,

00:08:52.770 --> 00:08:57.380
after just one lecture, going over MT cuz
there are a lot of different moving parts.

00:08:57.380 --> 00:08:59.910
So let's walk through this.

00:08:59.910 --> 00:09:04.390
You won't have to actually implement
traditional statistical MT system in

00:09:04.390 --> 00:09:05.570
this class.

00:09:05.570 --> 00:09:07.978
But I want you to appreciate
a little bit the history.

00:09:07.978 --> 00:09:13.495
And why deep learning is so impactful and
amazing for machine translation.

00:09:13.495 --> 00:09:18.210
Cuz it's replacing a lot of different
submodules in these very complex models.

00:09:20.570 --> 00:09:24.060
And sometimes it uses still
ideas from this, but not very.

00:09:24.060 --> 00:09:27.974
Most of them we don't need any more for
neural machine translation systems.

00:09:27.974 --> 00:09:30.342
All right, so let's set the stage.

00:09:30.342 --> 00:09:32.810
We have generally a source language.

00:09:32.810 --> 00:09:35.340
Let's call that f, such as French.

00:09:35.340 --> 00:09:38.599
And we have a target language, e,
in our case, let's say it's English.

00:09:38.599 --> 00:09:44.031
So we wanna translate from the source
French to the target language of English.

00:09:44.031 --> 00:09:49.205
And we'll usually describe
this here with a simple Bayes

00:09:49.205 --> 00:09:56.070
rule where we basically try to find the,
Target sentence,

00:09:56.070 --> 00:10:00.720
usually e here we assume is the whole
sentence in the target language.

00:10:00.720 --> 00:10:04.910
That gives us the largest conditional
probability conditioned on f.

00:10:04.910 --> 00:10:07.343
So this is an abstract formulation.

00:10:07.343 --> 00:10:10.496
We'll try to fill in how to actually
compute these probabilities

00:10:10.496 --> 00:10:13.960
in traditional and then later in
neural machine translation systems.

00:10:13.960 --> 00:10:15.510
So now we can use Bayes rule.

00:10:15.510 --> 00:10:19.320
Posterior equals its prior times
likelihood divided by marginal evidence.

00:10:19.320 --> 00:10:21.740
Marginal evidence here would just be for
the source language.

00:10:21.740 --> 00:10:22.371
So that doesn't change.

00:10:22.371 --> 00:10:25.340
So we can drop that,
argmax would not change from that.

00:10:25.340 --> 00:10:31.470
So basically, we'll try to
compute these two factors here.

00:10:31.470 --> 00:10:36.210
The probability of the French
sentence given, or

00:10:36.210 --> 00:10:42.940
the source language given the target,
times the probability of just the target.

00:10:44.760 --> 00:10:47.500
And now, we'll basically call
these two elements here.

00:10:47.500 --> 00:10:49.410
One is our translation model.

00:10:49.410 --> 00:10:51.140
And the other one is our language model.

00:10:51.140 --> 00:10:54.443
Remember language modeling where we
tried to get the probability of a longer

00:10:54.443 --> 00:10:54.967
sequence.

00:10:54.967 --> 00:10:58.360
This is a great use case for it.

00:10:58.360 --> 00:11:02.075
Basically, you can think of this
as you get some French sentence.

00:11:02.075 --> 00:11:04.689
Your translation model will try to find.

00:11:04.689 --> 00:11:06.830
Maybe this phrase,
I can translate into that.

00:11:06.830 --> 00:11:09.220
And this phrase,
I can translate into this.

00:11:09.220 --> 00:11:11.550
And then you have a bunch
of pieces of English.

00:11:11.550 --> 00:11:16.620
And then your language model will
essentially in the decoder be combined

00:11:16.620 --> 00:11:21.312
to try to get a single,
smooth sentence in the target language.

00:11:21.312 --> 00:11:26.362
So it'll help us to take all these pieces
that we have from the translation model.

00:11:26.362 --> 00:11:30.112
And make it into one sentence that
actually sounds reasonable and flows and

00:11:30.112 --> 00:11:31.830
is grammatical and all that.

00:11:31.830 --> 00:11:36.150
So the language model helps us to
weight grammatical sentences better.

00:11:36.150 --> 00:11:40.767
So, for instance, I go home will
sound better than I go house, right?

00:11:40.767 --> 00:11:44.965
Because I go home will have a more
likely higher probability, so

00:11:44.965 --> 00:11:47.930
more likely English
sentence to be uttered.

00:11:47.930 --> 00:11:51.711
Now, how do we actually train
all these different pieces?

00:11:51.711 --> 00:11:53.170
And how would you go about doing this?

00:11:53.170 --> 00:11:56.820
Well, if you wanted to translate,
do this translation model here.

00:11:56.820 --> 00:12:01.067
Then the first thing you'd have to do
is you'd find so called alignments.

00:12:01.067 --> 00:12:05.249
Which is basically, the goal of the
alignment step is to know which word or

00:12:05.249 --> 00:12:08.959
phrase in the source language would
translate to the other word or

00:12:08.959 --> 00:12:10.790
phrase in the target language.

00:12:10.790 --> 00:12:13.660
And that sub problem already.

00:12:13.660 --> 00:12:18.030
And now, again,
we have these three different systems.

00:12:18.030 --> 00:12:21.420
And now we're zooming in to
the step one of that system.

00:12:21.420 --> 00:12:27.093
Now that one is already hard
because alignment is non-trivial.

00:12:27.093 --> 00:12:31.908
These are actually some cool examples from
previous incarnation from Chris's class,

00:12:31.908 --> 00:12:33.871
224, and from previous years.

00:12:33.871 --> 00:12:37.020
Here are some examples of why
alignment is already hard.

00:12:37.020 --> 00:12:43.010
And this is for a language pair
that is actually quite similar.

00:12:43.010 --> 00:12:47.130
English and French share a lot
of common history, and so on,

00:12:47.130 --> 00:12:49.240
and they're more similar.

00:12:49.240 --> 00:12:54.290
But even if we have these two sentences
here, like Japan shaken by two new quakes.

00:12:54.290 --> 00:12:58.990
Or Le Japon secoue par
deux nouveaux seismes.

00:12:58.990 --> 00:13:03.730
Then we'll basically have
here a spurious word.

00:13:03.730 --> 00:13:06.850
So Le was actually not
translated to anything.

00:13:06.850 --> 00:13:10.110
And we would skip it in our alignment.

00:13:10.110 --> 00:13:13.750
So you see here this alignment matrix.

00:13:13.750 --> 00:13:16.400
And you'll notice that Le
just wasn't translated.

00:13:16.400 --> 00:13:23.195
We don't say the Japan, or
a Japan, or something like that.

00:13:23.195 --> 00:13:24.740
So it gets trickier, though.

00:13:24.740 --> 00:13:25.880
Cuz there are also so

00:13:25.880 --> 00:13:29.190
called zero fertility words
that are not translated at all.

00:13:29.190 --> 00:13:31.317
So we start in a source and
we just drop them.

00:13:31.317 --> 00:13:36.977
And, for some reason, the translators,
or for grammatical reasons and

00:13:36.977 --> 00:13:42.647
so on, they don't actually have any
equivalent in the target language.

00:13:42.647 --> 00:13:46.160
And to make it even more complex,
we can also have one-to-many alignments.

00:13:46.160 --> 00:13:51.630
So implemented in English is actually
mis en application in French.

00:13:51.630 --> 00:13:57.000
So made into an application
of sorts is just the word and

00:13:57.000 --> 00:13:58.122
the verb implemented here.

00:13:58.122 --> 00:13:59.997
So then we'll have to try to find.

00:13:59.997 --> 00:14:02.786
And now, as you try to think through
algorithms that might do this

00:14:02.786 --> 00:14:03.656
alignment for you.

00:14:03.656 --> 00:14:08.450
You'll have to think, so this word could
go to either this one word or no word.

00:14:08.450 --> 00:14:09.551
Or these three words together.

00:14:09.551 --> 00:14:11.210
Or maybe these two words together.

00:14:11.210 --> 00:14:14.980
And you can see how that would create, if
you tried to go through all the statistics

00:14:14.980 --> 00:14:18.610
and collect all of these probabilities,
of which phrase would go to what phrase.

00:14:18.610 --> 00:14:22.667
It'll get pretty hard to
actually combine them all.

00:14:22.667 --> 00:14:27.090
And language is just incredible and
very complex.

00:14:27.090 --> 00:14:28.972
And you also have many-to-one alignments.

00:14:28.972 --> 00:14:35.618
So aboriginal people are just
autochtones in French.

00:14:35.618 --> 00:14:38.403
So similar actually in German, [FOREIGN].

00:14:38.403 --> 00:14:40.260
So you'd have two words in German.

00:14:40.260 --> 00:14:45.218
And so, you have many-to-one
alignments making the combinatorial

00:14:45.218 --> 00:14:49.341
explosion even harder if you
try to find good alignments.

00:14:49.341 --> 00:14:52.039
And lastly,
you'll also have many-to-many alignments.

00:14:52.039 --> 00:14:56.114
You have certain phrases
like don't have any money.

00:14:56.114 --> 00:14:59.340
This just goes to sont demunis in French.

00:15:00.520 --> 00:15:04.838
And so it's a very, very complex
problem that has combinatorial

00:15:04.838 --> 00:15:08.853
explosion of all potential
combinations and it's tricky.

00:15:08.853 --> 00:15:14.315
All right, so now, really,
if you were to take a traditional class,

00:15:14.315 --> 00:15:19.407
you could have several lectures,
or at least an entire lecture,

00:15:19.407 --> 00:15:26.100
just on the various ways you could
implement cleverly an alignment model.

00:15:26.100 --> 00:15:30.010
And sometimes,
people use just single words.

00:15:30.010 --> 00:15:33.327
And other times, they actually use
parses like the one you're now familiar,

00:15:33.327 --> 00:15:34.188
syntactic parses.

00:15:34.188 --> 00:15:37.691
And try to find which,
no, not just words, but

00:15:37.691 --> 00:15:41.640
phrases from a parse would
map to the other language.

00:15:43.450 --> 00:15:45.720
And then, of course, it's not just that.

00:15:45.720 --> 00:15:50.250
And not usually are sentences and
languages nicely aligned, but

00:15:50.250 --> 00:15:53.660
you can also have complete reorderings.

00:15:53.660 --> 00:15:58.700
So German sometimes, for sub Clauses
actually has the verb at the end,

00:15:58.700 --> 00:16:03.390
so you flip a lot of the words, and you
can't just have this vocality assumption

00:16:03.390 --> 00:16:07.590
that words rough in this area will
translate to roughly a similar area,

00:16:07.590 --> 00:16:10.890
in terms of the sequence of
words in the other language.

00:16:13.717 --> 00:16:17.567
So yeah, ja nicht here,
ja is technically just yes in German,

00:16:17.567 --> 00:16:19.730
also not translated at all.

00:16:19.730 --> 00:16:22.870
And then actually going over there and
going, moving also.

00:16:24.320 --> 00:16:28.480
All right, now let's say we have
all these potential alignments, and

00:16:28.480 --> 00:16:31.690
now as we start from the source
language we say, all right.

00:16:31.690 --> 00:16:37.040
Let's say the source here is this German
sentence, geht ja nicht nach hause.

00:16:37.040 --> 00:16:42.880
Now could be translated
into many different words.

00:16:42.880 --> 00:16:48.446
So German it's technically just the he
of he, she, it, as the es in German.

00:16:48.446 --> 00:16:53.400
But sometimes English as
you do your alignment

00:16:53.400 --> 00:16:57.950
when not unreasonable one is just it or
comma he or

00:16:57.950 --> 00:17:01.422
he will be, cuz those were dropped
before in the alignment and so on.

00:17:01.422 --> 00:17:05.610
So you now have lots of candidates for
each possible word and for

00:17:05.610 --> 00:17:10.420
each possible phrase that you
might want to combine now in

00:17:10.420 --> 00:17:16.310
some principled way to
the final target translation.

00:17:16.310 --> 00:17:20.140
So you have again here a combinatorial
explosion of lots of potential ways you

00:17:20.140 --> 00:17:24.230
could translate each of the words or
phrases of various lengths.

00:17:24.230 --> 00:17:27.710
And so basically what that means is
you'll have a very hard search problem

00:17:29.180 --> 00:17:33.010
that also includes having to
have a good language model.

00:17:33.010 --> 00:17:37.880
So that as you put all these pieces
together, you essentially try to keep

00:17:37.880 --> 00:17:42.060
saying or combining phrases that
are grammatically plausible or

00:17:42.060 --> 00:17:44.010
sound reasonable to native speakers.

00:17:45.710 --> 00:17:48.707
And this often ends up being
so-called beam search,

00:17:48.707 --> 00:17:52.969
where you try to keep around a couple of
candidates as you go from left to right

00:17:52.969 --> 00:17:56.253
and you try to put all of these
different pieces together.

00:17:56.253 --> 00:17:59.685
Now again, this is totally not
doing traditional MT justice.

00:17:59.685 --> 00:18:04.085
Right, we just went in five minutes over
what could have been an entire lecture on

00:18:04.085 --> 00:18:07.725
statistical machine translation, or
maybe even many multiple lectures.

00:18:07.725 --> 00:18:11.110
So there are lots of important
details we skipped over.

00:18:11.110 --> 00:18:14.060
But the main gist here
is that there's a lot of

00:18:14.060 --> 00:18:18.550
human feature engineering that's required
and involved in all of these different

00:18:18.550 --> 00:18:21.550
pieces that used to require building
a machine translation system.

00:18:21.550 --> 00:18:26.020
And it also meant that there were whole
companies that you could form just for

00:18:26.020 --> 00:18:29.720
machine translation because nobody
could go through all that work and

00:18:29.720 --> 00:18:31.510
really build out a good system.

00:18:31.510 --> 00:18:36.233
Whereas now you have companies that have
worked for decades in this and they start

00:18:36.233 --> 00:18:40.900
using an open-source machine translation
system that anybody can download.

00:18:40.900 --> 00:18:43.898
And now a normal student, a PhD
student can spend a couple months and

00:18:43.898 --> 00:18:45.808
then he has like one of
the best MT systems.

00:18:45.808 --> 00:18:50.035
Which just completely would have been
completely impossible in their large

00:18:50.035 --> 00:18:54.214
groups that all work together in very
large systems before, in academia.

00:18:54.214 --> 00:18:57.120
So one of the main problems
of this kind of approach,

00:18:57.120 --> 00:19:00.291
is actually that not only is
it a very complex system, but

00:19:00.291 --> 00:19:04.348
it's also a system of independently
trained machine learning models.

00:19:04.348 --> 00:19:09.400
And, if there's one thing that I think
that I like most, when property of deep

00:19:09.400 --> 00:19:14.540
learning models, not just for MT, but
in all of NLP and maybe in all of AI.

00:19:14.540 --> 00:19:18.420
Is that we're usually in deep learning
try to have end to end trainable models

00:19:18.420 --> 00:19:22.260
where you have your final objective
function that you care about and

00:19:22.260 --> 00:19:24.390
everything is learned
jointly in one model.

00:19:25.420 --> 00:19:27.850
And this MT system is kind
of the opposite of that.

00:19:27.850 --> 00:19:30.050
You have an alignment model
you optimize for that, and

00:19:30.050 --> 00:19:35.630
then you have a reordering model maybe,
and then you have the language model.

00:19:35.630 --> 00:19:41.152
And they're all separate systems and you
couldn't jointly train all of it together.

00:19:41.152 --> 00:19:46.408
So that's kind of the very quick summary
for traditional machine transaction.

00:19:46.408 --> 00:19:47.625
Any high level questions
around traditional MT?

00:19:49.976 --> 00:19:55.267
All right, so now deep learning to

00:19:55.267 --> 00:20:00.379
the rescue, maybe, probably.

00:20:00.379 --> 00:20:07.710
So let's go through a sequence of
models and see if they would suffice.

00:20:07.710 --> 00:20:12.628
So the simplest one that we could
possibly do is kind of an encoder and

00:20:12.628 --> 00:20:16.220
decoder model that looks like this.

00:20:16.220 --> 00:20:20.200
Where we literally just have
a single recurrent neural network,

00:20:20.200 --> 00:20:22.910
where we have our word vectors so
let's say here

00:20:22.910 --> 00:20:28.390
we translate from German to English
Echt Kiste is awesome sauce in English.

00:20:28.390 --> 00:20:32.690
And we now have our word vectors
here we learned them in German, and

00:20:32.690 --> 00:20:35.160
we have our soft max classifier here.

00:20:35.160 --> 00:20:38.820
And we just have a single recurrent neural
network and once it sees the end of German

00:20:38.820 --> 00:20:44.681
sentence and there's no input left we'll
just try to output the translation.

00:20:46.923 --> 00:20:50.290
Not totally unreasonable,
it's an end-to-end trainable model.

00:20:50.290 --> 00:20:53.626
We'll have our standard cross entry
pair here that tries to just predict

00:20:53.626 --> 00:20:54.342
the next word.

00:20:54.342 --> 00:20:59.336
But the next word actually has
to be in a different language.

00:20:59.336 --> 00:21:02.898
Now, basically this last vector here,
if this was our main model,

00:21:02.898 --> 00:21:06.410
this last vector would have to
capture the entirety of the phrase.

00:21:06.410 --> 00:21:10.511
And sadly, I've already told you
that usually five or six words or so

00:21:10.511 --> 00:21:13.552
can be captured and
after that, we don't really,

00:21:13.552 --> 00:21:17.320
we can't memorize the entire
context of the sentence before.

00:21:17.320 --> 00:21:23.820
So this might work for like,
very short sentenced but maybe not.

00:21:23.820 --> 00:21:27.720
But let's define what this model
would be in its most basic form,

00:21:27.720 --> 00:21:30.900
cuz we'll work on top of this afterwards.

00:21:30.900 --> 00:21:34.480
So we have here our standard recurrent
neural network from the last lecture.

00:21:34.480 --> 00:21:39.000
Where we have our next hidden state,
it's just basically a linear

00:21:40.080 --> 00:21:43.330
network here followed by
non-element wise linearities.

00:21:43.330 --> 00:21:46.970
And we sum here the matrix
vector product with the vector,

00:21:46.970 --> 00:21:49.710
the previous hidden state in
our current word vector xt.

00:21:49.710 --> 00:21:55.470
And that's our encoder and
then in our decoder in the simplest form,

00:21:55.470 --> 00:21:58.130
again not the final model,
in the simplest form we could just

00:21:58.130 --> 00:22:01.550
drop this cuz the decoder doesn't
have an input at that time.

00:22:01.550 --> 00:22:05.320
Right, it's just we wanna now
translate and just generate an output.

00:22:05.320 --> 00:22:09.400
So during the decoder we drop
this matrix vector product and

00:22:09.400 --> 00:22:10.780
we just go each time step.

00:22:10.780 --> 00:22:15.810
It's just basically moving along based
on the previous hidden time step.

00:22:15.810 --> 00:22:20.470
And we'll have our final softmax output
here at each time step of the decoder.

00:22:22.750 --> 00:22:26.780
Now I also introduced this phi notation
here, and basically whenever you have,

00:22:26.780 --> 00:22:29.280
we'll see this only in
the next couple of slides.

00:22:29.280 --> 00:22:32.180
But whenever I write phi of two vectors,

00:22:32.180 --> 00:22:37.420
that means we'll have two separate W
matrices for each of these vectors.

00:22:37.420 --> 00:22:42.210
This is the little, shorter notation, and
then the default here would be well, just

00:22:42.210 --> 00:22:46.350
like I said, minimize the cross entropy
error for all the target words conditioned

00:22:46.350 --> 00:22:49.740
on all the source words that we hoped
would be captured in that hidden state.

00:22:51.130 --> 00:22:57.140
All right, any questions, concerns,
thoughts about how this model would do?

00:23:14.673 --> 00:23:19.250
So, the comment or question is that
neither are the traditional model or

00:23:19.250 --> 00:23:21.940
this model account for grammar.

00:23:21.940 --> 00:23:23.048
And in some ways, that's not true.

00:23:23.048 --> 00:23:28.035
So there are actually a lot of traditional
models that work on top of syntactic

00:23:28.035 --> 00:23:30.120
grammatical tree structures.

00:23:30.120 --> 00:23:34.373
And they do this alignment based
on the syntactic structure of

00:23:34.373 --> 00:23:37.290
prefer potentially the alignment step.

00:23:37.290 --> 00:23:39.400
But also for the generation and
the encoding step and

00:23:39.400 --> 00:23:40.128
all these different steps.

00:23:40.128 --> 00:23:45.860
So there are several ways you can infuse
grammar and chromatical sort of priors

00:23:45.860 --> 00:23:52.010
into neuro machine translation systems or
so syntactic machine translation systems.

00:23:52.010 --> 00:23:54.480
It turns out it's questionable
if that actually helps.

00:23:54.480 --> 00:23:58.440
In many cases for machine translation,
you have such a broad range of sentences.

00:23:58.440 --> 00:24:01.575
You actually might have un-grammatical
sentences sometimes, and

00:24:01.575 --> 00:24:03.870
you still want them to be translated.

00:24:03.870 --> 00:24:05.390
You have very short,

00:24:05.390 --> 00:24:08.505
complex ambiguous kinds of
sentences like headlines and so on.

00:24:08.505 --> 00:24:12.452
So it's tricky, the jury was sort of out.

00:24:12.452 --> 00:24:16.068
And some tactic models were battling
it out with non-tactic models until

00:24:16.068 --> 00:24:17.825
neural machine translation came.

00:24:17.825 --> 00:24:21.019
And now, it's not as important
of a question anymore.

00:24:21.019 --> 00:24:26.029
Now, for neural systems, we would assume
and hope that our hidden state actually

00:24:26.029 --> 00:24:31.340
captures some grammatical structures and
some grammatical intuitions that we have.

00:24:31.340 --> 00:24:34.880
But we don't explicitly give
that to the algorithm anymore.

00:24:34.880 --> 00:24:40.503
Which some people who are very good
at giving those kinds of features,

00:24:40.503 --> 00:24:43.473
your algorithms might think is sad.

00:24:43.473 --> 00:24:46.040
But at the same time,
it's good if we don't have to, right?

00:24:46.040 --> 00:24:49.880
It's less work for us, putting more
artificial back into artificial

00:24:49.880 --> 00:24:55.400
intelligence, less human
intelligence on designing grammars.

00:24:55.400 --> 00:24:57.511
Anyways, so any other questions?

00:24:57.511 --> 00:24:58.401
Yeah.

00:25:08.690 --> 00:25:12.579
Good question, so sometimes, the number of
input words is different to the numbers

00:25:12.579 --> 00:25:15.000
of output words, and that's very true.

00:25:15.000 --> 00:25:19.364
So one modification we would have to
make to this kind of model for sure,

00:25:19.364 --> 00:25:27.090
is actually say, have the last output word
here, BA, stop out putting up words work.

00:25:27.090 --> 00:25:29.860
Like a special token that says, I'm done.

00:25:29.860 --> 00:25:35.250
And one, you add that to your softmax
classifier sort of the last row.

00:25:35.250 --> 00:25:38.480
And then you hope that when it
predicts that token, it just stops.

00:25:40.000 --> 00:25:45.226
And that is good enough and
not uncommon actually for

00:25:45.226 --> 00:25:49.406
all these neural machine translations.

00:25:49.406 --> 00:25:52.592
The superscript S is just again,

00:25:52.592 --> 00:25:57.950
to distinguish the different
W matrices that we have for

00:25:57.950 --> 00:26:04.222
hidden connections, visible or
hidden inputs, and softmax W.

00:26:04.222 --> 00:26:07.206
All right, now sadly,
while neural MT is pretty cool, and

00:26:07.206 --> 00:26:11.400
it is simpler than traditional systems,
it's not quite that simple.

00:26:11.400 --> 00:26:14.340
So we'll have to be a little more clever.

00:26:14.340 --> 00:26:19.640
And so let's go through a series of
extensions to this model where in the end,

00:26:19.640 --> 00:26:23.830
we'll have a very big
powerful LSTM type model.

00:26:23.830 --> 00:26:27.211
So step one, is we'll actually have
different recurrent neural network weights

00:26:27.211 --> 00:26:28.396
for encoding and decoding.

00:26:28.396 --> 00:26:33.265
So instead of having the same W here,
we actually should have a different set

00:26:33.265 --> 00:26:37.320
of parameters, a different W for
the decoding step.

00:26:37.320 --> 00:26:40.533
That's still relatively similar.

00:26:40.533 --> 00:26:45.167
All right, so again, remember this
notation here of fi where every

00:26:45.167 --> 00:26:48.356
input has its own matrix
W associated with it.

00:26:48.356 --> 00:26:53.388
The second modification is
that the previous hidden state

00:26:53.388 --> 00:26:59.149
is kind of the standard that you
have as input for during decoding.

00:26:59.149 --> 00:27:02.298
But instead of just having
the previous hidden state,

00:27:02.298 --> 00:27:06.880
we'll actually also add the last
hidden vector of the encoding.

00:27:06.880 --> 00:27:10.360
So we call this c here,
but it's essentially ht.

00:27:10.360 --> 00:27:16.127
So at this input here, we don't just
have the previous hidden state,

00:27:16.127 --> 00:27:21.428
but we always take the last hidden
state from the encoding step.

00:27:21.428 --> 00:27:25.158
And we have, again,
a separate matrix for that.

00:27:25.158 --> 00:27:28.620
And then on top of that, we will also add,
and that's actually, if you think about

00:27:28.620 --> 00:27:32.000
it, it's a lot of parameters, we'll add
the previous predicted output word.

00:27:33.190 --> 00:27:35.280
So as we translate,

00:27:35.280 --> 00:27:40.670
we have three inputs for each hidden
state during the decoding step.

00:27:40.670 --> 00:27:43.453
We'll have the previous hidden state
as a standard recurrent neural network.

00:27:43.453 --> 00:27:45.800
We have the last hidden
state of the encoder.

00:27:46.800 --> 00:27:50.660
And we have the actual output word
we predicted just before that.

00:27:50.660 --> 00:27:55.413
And this will essentially help the model
to know that it just output a word, and

00:27:55.413 --> 00:27:58.596
it'll prevent it from
outputting that word again.

00:27:58.596 --> 00:28:02.071
Cuz it'll learn to
transform the hidden state,

00:28:02.071 --> 00:28:05.809
based on having just upload
a specific word before.

00:28:05.809 --> 00:28:06.495
Yeah?

00:28:14.213 --> 00:28:15.637
That's right, that's right, yeah.

00:28:15.637 --> 00:28:19.543
So whenever you have fi of xyz here,

00:28:19.543 --> 00:28:23.980
it'll just f of w times
x + u of y + v of z.

00:28:25.980 --> 00:28:28.730
So you just,
I don't wanna define all the matrices.

00:28:39.760 --> 00:28:40.760
That's a great question.

00:28:40.760 --> 00:28:45.344
So why do we need to make y,
t minus one a parameter,

00:28:45.344 --> 00:28:51.331
if we actually had computed yt
minus one from ht minus one, right?

00:28:51.331 --> 00:28:56.363
So two answers, one, it will allow
us to have the softmax weights

00:28:56.363 --> 00:29:02.250
also modify a little bit how that
hidden state behaves at test time.

00:29:02.250 --> 00:29:05.295
And two,
we actually can choose usually yt, and

00:29:05.295 --> 00:29:07.905
there are different ways you can do this.

00:29:07.905 --> 00:29:13.380
You could take the actual probability, the
multinomial distribution from the softmax.

00:29:13.380 --> 00:29:15.620
But here,
we'll actually make a hard choice, and

00:29:15.620 --> 00:29:19.120
we'll actually tell the model
we chose exactly this one.

00:29:19.120 --> 00:29:21.780
So instead of having the distribution,
we'll make a hard choice.

00:29:21.780 --> 00:29:25.108
And we say, this is the one word, the
highest probability that had the highest

00:29:25.108 --> 00:29:28.800
probability, we predicted that one,
and that's the one we give us input.

00:29:28.800 --> 00:29:30.150
So it turns out in practice,

00:29:30.150 --> 00:29:35.520
that helps to prevent the model
from repeating words many times.

00:29:35.520 --> 00:29:40.197
And again, it incorporates the softmax
weights in that computation indirectly.

00:29:40.197 --> 00:29:40.930
Yeah.

00:29:53.361 --> 00:29:55.210
That is a good catch.

00:29:55.210 --> 00:29:57.050
That is not how we define the model.

00:29:57.050 --> 00:29:58.601
Ignore those errors.

00:29:58.601 --> 00:29:59.730
Yeah, well done.

00:30:00.810 --> 00:30:05.410
In theory, again, so I didn't define
it but you can also, you can do

00:30:05.410 --> 00:30:09.360
the same thing with the softmax, and
this is what the picture actually shows.

00:30:09.360 --> 00:30:14.035
So instead of having a softmax of just W,

00:30:14.035 --> 00:30:17.480
ht for the probability of yt.

00:30:17.480 --> 00:30:22.410
You can also concatenate here your c,
and that's what the picture said.

00:30:22.410 --> 00:30:25.291
But I wanted to skip over the details so
you caught it, well done.

00:30:40.302 --> 00:30:41.555
So this model usually,

00:30:41.555 --> 00:30:44.820
so the question is, do we have
kind of a look ahead type thing?

00:30:44.820 --> 00:30:46.679
Or does the model output blanks?

00:30:46.679 --> 00:30:52.019
And the model basically has to
output the words in the right order.

00:30:52.019 --> 00:30:57.415
And it doesn't not have the ability
to do this whole reordering step or

00:30:57.415 --> 00:30:59.431
look ahead kind of thing.

00:30:59.431 --> 00:31:03.748
Or there's no sort of post processing
of reordering at the end, so

00:31:03.748 --> 00:31:08.028
this model isn't able to output
the verb at the right time stamp.

00:31:08.028 --> 00:31:11.672
It's over, okay, here we go.

00:31:11.672 --> 00:31:12.476
Now, of course,

00:31:12.476 --> 00:31:16.071
once it works well, everybody will try to
see if they can kind of improve it, and

00:31:16.071 --> 00:31:19.091
eventually you can do beam searches
too for these kinds of models.

00:31:19.091 --> 00:31:23.610
But surprisingly, in many cases, you
don't have to get a reasonable MT system.

00:31:28.489 --> 00:31:31.869
All right, now, I want you to
become more and more familiar,

00:31:31.869 --> 00:31:33.760
to be able to read the literature.

00:31:33.760 --> 00:31:36.290
So the same picture that we had here and

00:31:36.290 --> 00:31:40.090
the same equations we defined,
here's another way off looking at this.

00:31:42.390 --> 00:31:46.268
So with the exception that this
one doesn't have the c connection

00:31:46.268 --> 00:31:47.336
that you caught.

00:31:47.336 --> 00:31:48.963
So, Yeah, it's similar.

00:31:48.963 --> 00:31:53.031
It's the same exact model,
just a different way to look at it, and

00:31:53.031 --> 00:31:54.890
it's kind of good to see.

00:31:54.890 --> 00:31:56.960
Sometimes people explicitly write

00:31:58.200 --> 00:32:03.160
that you start out with a discreet
one of k and coding of the words.

00:32:03.160 --> 00:32:05.770
It's just like you want one-hot
vectors that we defined, and

00:32:05.770 --> 00:32:09.030
then you embed it into
continuous word vector space.

00:32:09.030 --> 00:32:13.714
You give those as input, you compute
your recurrent neural network, ht steps.

00:32:13.714 --> 00:32:17.762
And now,
you give those as input to the decoder.

00:32:17.762 --> 00:32:22.454
And that each time stamp of decoder, you
get the one word sample that you actually

00:32:22.454 --> 00:32:28.070
took as input, the previous hidden state
and to see vector, we defined before.

00:32:28.070 --> 00:32:30.860
So all these three already
are the inputs for

00:32:30.860 --> 00:32:33.733
each node in this
recurrent neural network.

00:32:33.733 --> 00:32:37.547
So just a different picture for

00:32:37.547 --> 00:32:41.800
the same model we just defined, so

00:32:41.800 --> 00:32:49.310
you learn picture in variances first,
model semantics.

00:32:49.310 --> 00:32:50.095
Now, it gets more powerful.

00:32:50.095 --> 00:32:53.594
It needs to get more powerful cuz
even with those two assumptions here,

00:32:53.594 --> 00:32:56.918
we have a very simple recurrent
neural network with just one layer,

00:32:56.918 --> 00:32:58.770
that's not going to cut it.

00:32:58.770 --> 00:33:03.490
So we'll use some of the extensions
we discussed in the last lecture,

00:33:03.490 --> 00:33:07.740
we'll actually have stacked
deep recurrent neural networks

00:33:07.740 --> 00:33:09.630
where we have multiple layers.

00:33:09.630 --> 00:33:15.140
And then we'll also have,
in some cases, this is not as common,

00:33:15.140 --> 00:33:19.652
but sometimes it's used,
we have a bidirectional encoder.

00:33:19.652 --> 00:33:23.920
Where you go from left to right,
and then we give both of,

00:33:23.920 --> 00:33:31.674
last hidden states of both directions
as input to every step of the decoder.

00:33:31.674 --> 00:33:34.181
And then this is kind
of almost an XOR here.

00:33:34.181 --> 00:33:38.981
If you don't do this, than another way
to improve your system slightly is by

00:33:38.981 --> 00:33:41.981
training the input
sequence in reverse order,

00:33:41.981 --> 00:33:45.510
because then you have a simpler
optimization problem.

00:33:45.510 --> 00:33:51.230
So especially for languages that align
reasonably well like English and French.

00:33:51.230 --> 00:33:55.485
You might instead of saying A,
B, C, the other word's A,

00:33:55.485 --> 00:34:00.264
the word B, or C goes to in the different
language the words X and Y.

00:34:00.264 --> 00:34:04.272
You'll say, C B A goes to X Y,
because as they align,

00:34:04.272 --> 00:34:08.563
A is more likely to translate to X,
and B is more like to Y.

00:34:08.563 --> 00:34:10.208
And as you have longer sequences,

00:34:10.208 --> 00:34:14.250
you basically bring the words that are
actually being translated closer together.

00:34:15.520 --> 00:34:20.560
And hence, you have less of a vanishing
gradient problems and so on, because where

00:34:20.560 --> 00:34:25.532
you want the work to be predicted, it's
closer to where it came in to the encoder.

00:34:25.532 --> 00:34:26.032
Yeah?

00:34:31.362 --> 00:34:36.055
That's right, but yeah,
it's still an average force.

00:34:44.907 --> 00:34:46.582
So how does reversing not mess it up?

00:34:46.582 --> 00:34:48.990
Cuz this sentence doesn't
make grammatical sense.

00:34:48.990 --> 00:34:54.600
So we never gave this model
an explicit grammar for

00:34:54.600 --> 00:34:56.330
the source language, or
the target language, right?

00:34:56.330 --> 00:35:01.549
It's essentially trying, in some really
deep, clever, continuous function,

00:35:01.549 --> 00:35:07.220
general function approximation kind of
way, just correlation, basically, right?

00:35:07.220 --> 00:35:12.211
And it doesn't have to know the grammar,
but as long as you're consistent and

00:35:12.211 --> 00:35:15.323
you just reverse every sequence,
the same way.

00:35:15.323 --> 00:35:17.361
It's still grammatical if you
read it from the other side.

00:35:17.361 --> 00:35:21.760
And the model reads it from
potentially both sides, and so on.

00:35:21.760 --> 00:35:24.750
So it doesn't really matter
to these learning models,

00:35:24.750 --> 00:35:29.580
as long as your transformation of the
input is consistent across training and

00:35:29.580 --> 00:35:30.390
testing times, and so on.

00:35:42.130 --> 00:35:44.986
So the question is,
he understands the argument, but

00:35:44.986 --> 00:35:47.330
it could still change the meaning.

00:35:47.330 --> 00:35:51.620
And it doesn't change the meaning if
you assume the model will always go

00:35:51.620 --> 00:35:53.060
from one direction to the other.

00:35:53.060 --> 00:35:55.390
If you start to sometimes do it and
sometimes not,

00:35:55.390 --> 00:35:57.250
then it will totally mess up the system.

00:35:57.250 --> 00:36:00.060
But as long as it's
a consistent transformation,

00:36:00.060 --> 00:36:02.420
it is still the same order and
so you're good.

00:36:08.889 --> 00:36:11.760
So why is reversing the order
a simpler optimization problem?

00:36:11.760 --> 00:36:14.100
Imagine, you had a very
long sequence here.

00:36:14.100 --> 00:36:18.800
And again, this is only the case
if the languages align well.

00:36:18.800 --> 00:36:21.410
As in usually,
the first capital words in one

00:36:21.410 --> 00:36:24.810
of the source language translated to first
capital words in the target language.

00:36:24.810 --> 00:36:29.892
Now, If you have a long sequence and
you try to translate

00:36:29.892 --> 00:36:35.856
it to another long sequence, and
say there are a lot of them here.

00:36:35.856 --> 00:36:41.149
Now, what that would mean is that this
word here is very far away from that word,

00:36:41.149 --> 00:36:45.410
cuz it has to go through
this entire transformation.

00:36:45.410 --> 00:36:47.960
And likewise,
these words are also very far away.

00:36:47.960 --> 00:36:54.019
So everything is far away from
everything in terms of the number

00:36:54.019 --> 00:37:01.320
of non-linear function applications
before you get to the actual output.

00:37:01.320 --> 00:37:06.295
Now, if you just reverse this one,
then this word, so

00:37:06.295 --> 00:37:09.768
let's call this a, b, c, d, e, f.

00:37:09.768 --> 00:37:14.684
Now, this is now f,

00:37:14.684 --> 00:37:19.030
e, d, c, b, a.

00:37:19.030 --> 00:37:21.405
Now, this word, it's here now.

00:37:21.405 --> 00:37:24.570
And now, this word translates
directly to that word, right?

00:37:24.570 --> 00:37:25.680
So in your decoder.

00:37:25.680 --> 00:37:29.090
So now, these two are very,
very close to one another.

00:37:29.090 --> 00:37:33.323
And so as you do back propagation and we
learn about the vanishing creating problem

00:37:33.323 --> 00:37:37.020
in the last lecture you have much
less of a vanishing creating problem.

00:37:37.020 --> 00:37:40.420
So at least in the beginning,
it'll be much better at translating those.

00:37:51.243 --> 00:37:56.270
So, how does this check work for
languages with different morphology?

00:37:56.270 --> 00:38:00.040
It doesn't actually matter, but
the sad truth is also that very few

00:38:00.040 --> 00:38:04.470
MT researchers work on languages
with super complex morphology.

00:38:04.470 --> 00:38:08.990
So like Finnish doesn't have
very large parallel corpora

00:38:08.990 --> 00:38:10.600
of tons of other languages.

00:38:10.600 --> 00:38:13.618
And so you don't sadly see
as many people work on that.

00:38:13.618 --> 00:38:14.657
German does work.

00:38:14.657 --> 00:38:17.380
And for German actually,
a lot of other tricks that we'll get to.

00:38:17.380 --> 00:38:22.860
And really these tricks are not as
important as the one as trick number six.

00:38:22.860 --> 00:38:24.539
But before that,
we'll have a research highlight.

00:38:24.539 --> 00:38:30.140
&gt;&gt; [LAUGH]
&gt;&gt; Give you a bit of a break, all right.

00:38:30.140 --> 00:38:33.876
Allen, take it away.

00:38:33.876 --> 00:38:34.744
&gt;&gt; This?
&gt;&gt; Yes.

00:38:34.744 --> 00:38:35.941
&gt;&gt; Okay.
Hi, everyone.

00:38:35.941 --> 00:38:36.751
My name is Allen.

00:38:36.751 --> 00:38:41.950
So I'm gonna talk about Building Towards
a Better Language Modeling.

00:38:41.950 --> 00:38:43.449
So as we've learned last week,

00:38:43.449 --> 00:38:46.288
language modeling is one of
the most canonical task in NLP.

00:38:46.288 --> 00:38:49.520
And there are three different ways
we can make it a little bit better.

00:38:49.520 --> 00:38:51.294
We can have better input representation.

00:38:51.294 --> 00:38:54.590
We can have better regularization or
preprocessing.

00:38:54.590 --> 00:38:57.760
And eventually,
we can have a better model.

00:38:57.760 --> 00:39:01.162
So for input, I know you guys
have all played with Glove, and

00:39:01.162 --> 00:39:03.530
that's a word level representation.

00:39:03.530 --> 00:39:05.070
And I heard morphemes.

00:39:05.070 --> 00:39:06.310
From you guys who are down there.

00:39:06.310 --> 00:39:09.790
So in fact,
you can code the word at a subword level.

00:39:09.790 --> 00:39:11.702
You can do morpheme encoding.

00:39:11.702 --> 00:39:13.111
You can do BPE.

00:39:13.111 --> 00:39:14.540
You can eventually do
character level embedding.

00:39:14.540 --> 00:39:18.110
What it does is that it drastically
reduce the size of your vocabulary,

00:39:18.110 --> 00:39:22.010
make the model prediction much easier.

00:39:22.010 --> 00:39:26.220
So as you can see, Tomas Mikolov in 2012,
and Yoon Kim in 2015,

00:39:26.220 --> 00:39:32.350
explored this route and got better results
compared to just plain word-based models.

00:39:33.880 --> 00:39:38.270
So another way to improve your model
is that one of the bigger problems for

00:39:38.270 --> 00:39:40.220
language modelling is over-fitting.

00:39:40.220 --> 00:39:44.050
And we know that we need to apply
regularization techniques when the model

00:39:44.050 --> 00:39:45.111
is over-fitting.

00:39:45.111 --> 00:39:46.752
So there are a bunch of them, but today,

00:39:46.752 --> 00:39:50.378
I'm gonna focus on preprocessing
because it's a little bit newer.

00:39:50.378 --> 00:39:52.840
What preprocessing does is
that we know that we're

00:39:54.950 --> 00:39:57.390
never gonna have unlimited training data.

00:39:57.390 --> 00:40:02.180
So in order to have our corpus look
more like the true distribution

00:40:02.180 --> 00:40:07.710
of the English language, what we can do is
quite similar to computer vision we can

00:40:07.710 --> 00:40:12.150
do this type of data augmentation
technique where we try to replace

00:40:12.150 --> 00:40:15.410
some words in our corpus
with some other words.

00:40:15.410 --> 00:40:16.180
So for example,

00:40:16.180 --> 00:40:19.740
your model during the first pass
you can see a word called New York,

00:40:19.740 --> 00:40:23.890
the next pass you can see New Zealand,
the next pass you can see New England.

00:40:23.890 --> 00:40:28.380
So by doing that, you're basically
generating this data by yourself and

00:40:28.380 --> 00:40:32.090
eventually you achieve
a smoothed out distribution.

00:40:32.090 --> 00:40:35.250
The reason this happens is
that more frequent word by

00:40:35.250 --> 00:40:37.300
replacing by dropping them.

00:40:37.300 --> 00:40:41.340
They appear less often and
rarer words by making them appear.

00:40:41.340 --> 00:40:43.010
They appear more often.

00:40:43.010 --> 00:40:47.810
So a smooth distribution allow us to
learn a better language model and

00:40:47.810 --> 00:40:51.880
the result is on the, I think is on
the right hand side of you guys.

00:40:51.880 --> 00:40:56.900
And the left hand side is what happen when
we apply better regularization techniques.

00:40:58.160 --> 00:41:03.630
So at last we can, wait,
that's it okay, awesome thank you guys.

00:41:09.101 --> 00:41:14.487
&gt;&gt; All right, now what you'll also see
in these tables is that the default for

00:41:14.487 --> 00:41:21.160
all these models is an LSTM and that's
exactly what we'll end up very soon with.

00:41:21.160 --> 00:41:24.800
Which is basically a better
type of recurrent unit.

00:41:25.850 --> 00:41:30.980
And so, we'll start with gated
recurrent units that were introduced

00:41:30.980 --> 00:41:33.460
by Cho just three years ago.

00:41:33.460 --> 00:41:37.430
And the main idea is that,
we wanna basically keep around

00:41:37.430 --> 00:41:40.470
memories that capture long
distance dependencies and

00:41:40.470 --> 00:41:44.110
you wanna have the model learn when and
how to do that.

00:41:44.110 --> 00:41:48.030
And with that,
you also allow your error messages to flow

00:41:48.030 --> 00:41:50.250
differently at different strengths,
depending on the input.

00:41:51.260 --> 00:41:52.900
So, how does this work?

00:41:52.900 --> 00:41:56.330
What is a GRU as our step to the LSDM?

00:41:56.330 --> 00:41:58.300
And sometimes you don't need
to go all the way to the LSDM.

00:41:58.300 --> 00:42:00.640
The GRU is a really good model by itself.

00:42:00.640 --> 00:42:03.030
In many cases already in its simpler.

00:42:03.030 --> 00:42:06.560
So let's start with our standard
recurrent neural network,

00:42:06.560 --> 00:42:11.780
which basically computes our hidden
layer at the next time step directly.

00:42:11.780 --> 00:42:16.990
So we just have again previous hidden
state recurring to our vector that's it.

00:42:16.990 --> 00:42:19.180
Now instead what we'll do for

00:42:19.180 --> 00:42:24.300
gated recurring units or GRUs,
is we'll compute to gates first.

00:42:24.300 --> 00:42:30.340
These gates are also just like ht,
continuous vectors of the same

00:42:30.340 --> 00:42:36.040
length as the hidden state, and
they are computed exactly the same way.

00:42:36.040 --> 00:42:40.930
And here, it's important to note that
the superscripts that's just basically

00:42:40.930 --> 00:42:44.210
are lined with the kind of
gate that you're computing.

00:42:44.210 --> 00:42:48.400
So we'll compute a so
called update gate and a reset gate.

00:42:49.830 --> 00:42:52.450
Now the inside here is
the exact same thing but

00:42:52.450 --> 00:42:55.780
is important to note that we
have here a sigmoid function.

00:42:55.780 --> 00:42:59.860
So we'll have elements of this vector
are exactly between zero and one.

00:42:59.860 --> 00:43:03.010
And we could interpret them as
probabilities if we want to.

00:43:04.610 --> 00:43:07.060
And it's also important to note that
the super scripts here are different.

00:43:07.060 --> 00:43:09.150
So the update gate of course,

00:43:09.150 --> 00:43:13.220
uses a different set of
weights to the reset gate.

00:43:13.220 --> 00:43:16.230
Now why are they called update and
reset gates, and how do we use them?

00:43:16.230 --> 00:43:18.630
It's relatively straight forward.

00:43:18.630 --> 00:43:25.250
We just introduced one new function
here just the element wise product.

00:43:25.250 --> 00:43:27.280
We've remember it from back propagation.

00:43:27.280 --> 00:43:29.020
We also call it the Hadamard
product sometimes.

00:43:29.020 --> 00:43:33.930
Where we just element wise multiply
this vector here from the reset

00:43:33.930 --> 00:43:39.056
gate with this,
which would be our new memory content.

00:43:39.056 --> 00:43:43.290
We call it ht,
this is our intermediate memory content,

00:43:43.290 --> 00:43:46.700
it has the standard tanh that
we also know as a [INAUDIBLE].

00:43:46.700 --> 00:43:51.250
This part here is exactly the same,
we just have to input our word vector and

00:43:51.250 --> 00:43:54.040
then transformed with a W.

00:43:54.040 --> 00:43:56.347
But what's going on in here?

00:43:56.347 --> 00:44:01.500
So intuitively right, this is just a long
vector of numbers between zero and one.

00:44:02.530 --> 00:44:07.920
Now intuitively,
if this reset gate at a certain unit,

00:44:07.920 --> 00:44:12.950
is around zero,
then we essentially ignore all the past.

00:44:12.950 --> 00:44:17.430
We ignore that entire computation
of the past, and we're just going

00:44:17.430 --> 00:44:22.540
to define that element where our zero,
with the current word vector.

00:44:22.540 --> 00:44:23.790
Now why would we want to do that?

00:44:23.790 --> 00:44:25.270
What's the intuition here?

00:44:25.270 --> 00:44:28.960
Let's take the task of sentiment analysis
cuz it's very simple and intuitive.

00:44:30.040 --> 00:44:35.750
If you were to say, you're talking
about a plot of a movie review.

00:44:35.750 --> 00:44:38.750
And you talk about the plot and
you know some girl falls in love for

00:44:38.750 --> 00:44:41.850
some guy who falls in love with her but
then they can't meet, blah, blah, blah.

00:44:41.850 --> 00:44:46.205
That's a long plot and in the end you say,
but the movie was really boring.

00:44:47.900 --> 00:44:51.060
Then really doesn't matter that
you keep around that whole plot.

00:44:51.060 --> 00:44:55.870
You wanna say boring as a really
negative strong word for sentiments, and

00:44:55.870 --> 00:45:02.322
you wanna basically be able to allow the
model to ignore the previous plot summary.

00:45:02.322 --> 00:45:06.930
Cuz for the task of sentiments
analysis it's irrelevant.

00:45:06.930 --> 00:45:10.090
Now this is essentially what
the reset gate will let you do, but

00:45:10.090 --> 00:45:11.700
of course not in this global fashion,

00:45:11.700 --> 00:45:17.290
where you update the entire hidden state,
but in a more subtle way, where you learn

00:45:17.290 --> 00:45:21.610
which of the units you actually will reset
and which ones you will keep around.

00:45:22.860 --> 00:45:25.020
So this will allow some
of the units to say,

00:45:25.020 --> 00:45:28.410
well maybe I want to be a plot unit and
I will keep around the plot.

00:45:28.410 --> 00:45:33.330
But other units learn, well if I see one
of the sentiment words, I will definitely

00:45:33.330 --> 00:45:37.892
set that reset gate to zero and I will
now make sure that I don't wash out,

00:45:37.892 --> 00:45:44.310
the content with previous stuff
by summing these two, right?

00:45:44.310 --> 00:45:47.070
You're sort of like, not quite
averaging but you're summing the two.

00:45:47.070 --> 00:45:50.380
So you wash out the content
from this word and

00:45:50.380 --> 00:45:54.410
instead it will set that to zero and take
only the content from that current word.

00:45:57.820 --> 00:46:03.592
Now the final memory it will compute,
we'll combine this with the update gate.

00:46:03.592 --> 00:46:06.860
And the update gate now,
there's something similar but

00:46:08.090 --> 00:46:13.060
basically allows us to keep around
only the past and not the future.

00:46:13.060 --> 00:46:14.400
Or not the current time steps.

00:46:14.400 --> 00:46:20.698
So intuitively here when you look at Z,
if Z is a vector of all ones,

00:46:20.698 --> 00:46:25.424
then what we would do is
essentially do ht = ht-1

00:46:25.424 --> 00:46:30.800
+ 1-1 is 0, so this term just falls away.

00:46:30.800 --> 00:46:37.170
Basically if zt was all ones we could
just copy over our previous time step.

00:46:37.170 --> 00:46:40.060
Super powerful,
if you copied over the previous time step

00:46:40.060 --> 00:46:42.530
you have no vanishing gradient problem,
right.

00:46:42.530 --> 00:46:45.100
Your vector just gets a bunch of ones.

00:46:45.100 --> 00:46:47.720
Nothing changes in your
gradient computation.

00:46:47.720 --> 00:46:52.590
So that's very powerful and intuitively
you can use that same sentiment example.

00:46:52.590 --> 00:46:55.410
But you say in the beginning man,
I love this movie so much,

00:46:55.410 --> 00:46:57.960
here's this beautiful love story.

00:46:57.960 --> 00:47:01.330
And now you go through the love story,
and really what's important for

00:47:01.330 --> 00:47:05.190
sentiment is not about the love story,
but it's about the person saying,

00:47:05.190 --> 00:47:06.630
I love this movie a lot.

00:47:06.630 --> 00:47:09.780
And you wanna make sure you
don't lose that information.

00:47:09.780 --> 00:47:11.580
And with the standard
recurring neural network,

00:47:11.580 --> 00:47:14.740
we update our hidden state,
every time, every word.

00:47:14.740 --> 00:47:18.350
No matter how unimportant a word is,
we're gonna sum up those two vectors,

00:47:18.350 --> 00:47:21.890
washing out the content as we
move further and further along.

00:47:21.890 --> 00:47:25.430
Here we can decide, and what's even
more amazing, you don't have to decide.

00:47:25.430 --> 00:47:28.870
You can say, this word is positive, so
I'm gonna set my reset gate manually.

00:47:28.870 --> 00:47:32.190
No, the model will learn when to reset and
when to update.

00:47:33.650 --> 00:47:38.786
So this is a very simple kind of
modification but extremely powerful.

00:47:41.920 --> 00:47:44.980
Now, we're gonna go through it and
explain it a couple more times.

00:47:44.980 --> 00:47:46.510
And we'll try to.

00:47:46.510 --> 00:47:49.320
Have an attempt here at
a clean illustration.

00:47:49.320 --> 00:47:52.930
Honestly, personally, I feel the equations
here are still straight forward, and

00:47:52.930 --> 00:47:55.900
very intuitive, that I don't know
if these illustrations always help,

00:47:55.900 --> 00:47:59.950
but some people like
them more than others.

00:47:59.950 --> 00:48:05.300
So intuitively here, you basically
see that only the final memory,

00:48:05.300 --> 00:48:08.880
that you computed is the one that's
actually used as input to the next step.

00:48:08.880 --> 00:48:14.360
So all of these are only
modifying through the final state.

00:48:14.360 --> 00:48:18.270
And now this one gets as input to
our reset gate or update gate,

00:48:18.270 --> 00:48:22.330
the intermediate state and
the final state of the memory.

00:48:22.330 --> 00:48:27.199
And so does our x vector the word vector
here also gets its input through the reset

00:48:27.199 --> 00:48:30.989
gate, the update gate, and
our intermediate memory state.

00:48:30.989 --> 00:48:35.674
And then, I tried to use this,
so the dotted line here,

00:48:35.674 --> 00:48:40.380
as basically gates that modify
how these two interact.

00:48:43.300 --> 00:48:48.297
All right, so I've said, I think,
most of these things already, but

00:48:48.297 --> 00:48:51.490
again, reset gate here is close to 0.

00:48:51.490 --> 00:48:53.757
We ignore our previous state.

00:48:53.757 --> 00:48:57.562
And that again, allows the model,
in general, to drop information that is

00:48:57.562 --> 00:49:01.460
irrelevant for the future
predictions that it wants to make.

00:49:01.460 --> 00:49:04.670
And if we update the gate z controls,

00:49:04.670 --> 00:49:07.730
how much of the past state should
matter at the current time stamp?

00:49:09.240 --> 00:49:12.040
And again, this is a huge improvement for
the vanishing gradient problem,

00:49:12.040 --> 00:49:15.850
which allows us to actually train these
models on nontrivial, long sequences.

00:49:19.524 --> 00:49:21.198
Any questions around the GRU?

00:49:21.198 --> 00:49:21.698
Yep?

00:49:25.309 --> 00:49:27.300
Does it matter if you reset first or
update first?

00:49:27.300 --> 00:49:31.830
Well, so you can't compute
h until you have h tilled.

00:49:31.830 --> 00:49:34.210
So the order of these two doesn't matter.

00:49:34.210 --> 00:49:39.050
You can compute that in peril, but
you first have to compute h tilled

00:49:39.050 --> 00:49:40.880
with the reset gate before
you can compute that one.

00:49:54.749 --> 00:49:57.679
So the question is,
does it matter to switch and

00:49:57.679 --> 00:50:01.727
use an equation like this first,
and then an equation like that?

00:50:01.727 --> 00:50:06.040
I guess it's just a different model.

00:50:06.040 --> 00:50:09.470
It's not one that I know
of people having tried.

00:50:10.790 --> 00:50:15.420
It's not super unreasonable,
I don't see a sort of reason why

00:50:15.420 --> 00:50:20.350
it would be illogical to ever to that,
but yeah, just not the GRU model.

00:50:22.560 --> 00:50:24.026
You will actually see,

00:50:24.026 --> 00:50:28.647
in [INAUDIBLE] she has a paper on a Search
Space Odyssey type paper where there

00:50:28.647 --> 00:50:33.070
are a thousand modifications you can
make to the next model, the LSTM.

00:50:33.070 --> 00:50:36.315
And people have tried a lot of them,
and it's not trivial.

00:50:36.315 --> 00:50:37.974
There are a lot of modifications.

00:50:37.974 --> 00:50:41.256
And a lot of times they
seem kind of intuitive, but

00:50:41.256 --> 00:50:46.660
don't actually change performance that
much across a bunch of different tasks.

00:50:46.660 --> 00:50:50.988
But sometimes, one modification improves
things a tiny bit on one of the tasks.

00:50:50.988 --> 00:50:54.798
It turns out the final model of GRU here
and the LSTM, are actually incredibly

00:50:54.798 --> 00:50:58.270
stable, they give good performance
across a lot of different tasks.

00:51:00.080 --> 00:51:04.110
But it can't ever hurt to, if you have
some intuition of why you want to have,

00:51:04.110 --> 00:51:06.300
make something different,
it can't hurt to try.

00:51:20.126 --> 00:51:24.005
So the question is,
is it important of how they're computed?

00:51:24.005 --> 00:51:26.878
I think there are some people who have
tried once to have a two layer neural

00:51:26.878 --> 00:51:28.125
network to compute.

00:51:28.125 --> 00:51:30.185
These a z and update, z and r.

00:51:30.185 --> 00:51:34.049
In general, it matters of course
a lot of how they're computed, but

00:51:34.049 --> 00:51:37.860
not in the sense that you have to
modify them manually or something.

00:51:37.860 --> 00:51:41.070
It just the model learns when to
update and when not to update.

00:52:01.876 --> 00:52:03.125
That's a good question.

00:52:03.125 --> 00:52:05.395
So what do I mean when I say unit.

00:52:05.395 --> 00:52:10.800
So in general, what you'll observe in
a slide that's coming up very soon is

00:52:10.800 --> 00:52:16.470
that we will kind of abstract away from
the details of what these equations are.

00:52:16.470 --> 00:52:22.243
And we're going to write that just ht

00:52:22.243 --> 00:52:27.280
equals GRU of xt and ht minus 1.

00:52:27.280 --> 00:52:32.530
And then we'll just say that GRU
abbreviation means all these other things,

00:52:32.530 --> 00:52:35.620
all these equations, and
we're going to abstract away from that.

00:52:35.620 --> 00:52:40.210
And that's something that you'll see even
more in subsequent lectures where you

00:52:40.210 --> 00:52:44.980
just say a whole recurrent
network with a five layer GRU and

00:52:44.980 --> 00:52:48.520
combine lots of different
ways is just one block.

00:52:48.520 --> 00:52:51.683
We often see this in computer vision too
where CNNs are now just like the CNN

00:52:51.683 --> 00:52:54.555
block, and you assume you've got
a feature vector out at the end.

00:52:54.555 --> 00:52:58.100
And people will start abstracting
away more and more from that.

00:52:58.100 --> 00:53:00.520
But yeah,
you'll always have to remember that, yes,

00:53:00.520 --> 00:53:04.230
there's a lot of complexity
inside that unit.

00:53:04.230 --> 00:53:09.240
Here's another attempt at an illustration
which I'm even less of a fan of,

00:53:09.240 --> 00:53:11.090
then the one I tried to come up with.

00:53:12.160 --> 00:53:17.020
Basically, how you have your z gate
that kind of can jump back and forth.

00:53:17.020 --> 00:53:19.650
But of course,
it's usually a continuous type thing.

00:53:19.650 --> 00:53:27.036
It's not a zero one type thing, so I'm not
a big fan of this kind of illustration.

00:53:27.036 --> 00:53:28.877
And so in terms of derivatives,

00:53:28.877 --> 00:53:33.250
we couldn't theory asks you to
derive all the details of the GRU.

00:53:33.250 --> 00:53:38.120
And the only change here is that we now
have the derivative of these element

00:53:38.120 --> 00:53:43.060
wise multiplications,
both of which I have parameters or inside.

00:53:43.060 --> 00:53:47.357
And we all should know what
derivative of this is, and

00:53:47.357 --> 00:53:51.190
the rest is again,
the same kind of chain rule.

00:53:51.190 --> 00:53:55.965
But again, now you're sort of realizing
why we wanna modularized this more and

00:53:55.965 --> 00:54:00.600
more, and abstract a way from actually
manually taking these instead having

00:54:00.600 --> 00:54:03.270
error messages and deltas sent around.

00:54:03.270 --> 00:54:03.770
Yeah?

00:54:08.872 --> 00:54:12.130
Explain why we have both update and reset.

00:54:12.130 --> 00:54:17.570
So basically, it helps the model
to have different mechanisms for

00:54:17.570 --> 00:54:22.650
when to memorize something and
keep it around, versus when to update it.

00:54:22.650 --> 00:54:27.660
You're right, in theory, you could try to
put both of those into one thing, right?

00:54:27.660 --> 00:54:35.921
In theory, you'd say, well,
if this was just my previous ht here,

00:54:35.921 --> 00:54:40.730
then this could say, well, I wanna keep
it around, or I wanna update it here.

00:54:40.730 --> 00:54:42.500
But now, this update here,

00:54:42.500 --> 00:54:46.610
if you just had an equation like this it
would be still be a sum of two things.

00:54:46.610 --> 00:54:52.030
So that means that xt here
does not have complete control

00:54:52.030 --> 00:54:57.040
over modifying the current
hidden state in its entirety.

00:54:57.040 --> 00:54:59.200
It would still be summed
up with something else,

00:54:59.200 --> 00:55:01.270
and that happens at
every single time stamp.

00:55:01.270 --> 00:55:04.165
So its only once you have
this reset gates are here.

00:55:04.165 --> 00:55:09.850
These reset gates are here,
that you would allow h

00:55:09.850 --> 00:55:14.020
to be completely dominated by the current
word vector, if the model so chooses.

00:55:21.850 --> 00:55:29.857
If the reset gates are all,
Okay, so if these are all ones,

00:55:29.857 --> 00:55:36.220
then you have here basically a standard
recurrent neural network type equation.

00:55:36.220 --> 00:55:39.220
And then if you just have zs, all 0s,

00:55:39.220 --> 00:55:41.780
then you take that exact equation and
you're right.

00:55:41.780 --> 00:55:43.610
Then you just have a standard RNN.

00:55:43.610 --> 00:55:46.120
It's also beautiful,
it's always nice to say my model

00:55:46.120 --> 00:55:48.903
Is a more general form of your model or-
&gt;&gt; [LAUGH]

00:55:48.903 --> 00:55:49.438
&gt;&gt; An opposite,

00:55:49.438 --> 00:55:51.139
you're model's a special case of my model.

00:55:51.139 --> 00:55:56.865
It was actually a couple years ago
that you could by and say that.

00:55:56.865 --> 00:55:59.670
&gt;&gt; [LAUGH]
&gt;&gt; It's good machine learning banter.

00:56:01.490 --> 00:56:02.590
So yeah, it's always good.

00:56:02.590 --> 00:56:08.860
And likewise, the inventor of this model
made exactly that statement about the GRU.

00:56:08.860 --> 00:56:12.040
Not knowing why anybody
had to publish a new paper

00:56:12.040 --> 00:56:17.300
about this instead of just referring to
this and the special cases of the LSTM.

00:56:17.300 --> 00:56:19.860
So if we have one more
question about the GRU, yeah?

00:56:19.860 --> 00:56:21.741
&gt;&gt; Is there a reason.

00:56:25.464 --> 00:56:26.960
&gt;&gt; Good question.

00:56:26.960 --> 00:56:27.622
Why tanh and sigmoid?

00:56:27.622 --> 00:56:32.353
So in theory, you could say the tan h
here could be a rectified linear unit or

00:56:32.353 --> 00:56:33.740
other kind of unit.

00:56:33.740 --> 00:56:39.459
In practice, you do want sigmoids here
because you have this plus 1 minus that.

00:56:39.459 --> 00:56:43.663
And so if they're all over the place then
everything will kind of be modified and

00:56:43.663 --> 00:56:47.802
it's less intuitive that you kind of have
a hard reset in sort of a hard sort of,

00:56:47.802 --> 00:56:49.710
yeah, hard reset or a hard update.

00:56:50.910 --> 00:56:53.121
And if this wasn't 10h and

00:56:53.121 --> 00:56:58.834
was rectified linear unit then these
two might be all over the place too and

00:56:58.834 --> 00:57:05.025
it might be kind of easy to potentially
have the sum also the not very synthecal.

00:57:05.025 --> 00:57:06.375
But at the same time,

00:57:06.375 --> 00:57:11.050
it's not unreasonable to try having
a rectified learning unit here.

00:57:11.050 --> 00:57:13.980
And maybe, if you combine it with
proper regularization and so on,

00:57:13.980 --> 00:57:17.645
you could get away with other kinds
of other kinds of linearities.

00:57:17.645 --> 00:57:20.093
That's unlike probabilistic
graphical models for

00:57:20.093 --> 00:57:21.766
certain things just make no sense.

00:57:21.766 --> 00:57:25.996
And you can't do them, deep learning
you can often try some things and

00:57:25.996 --> 00:57:29.450
sometimes even nonsensical
things surprisingly work.

00:57:29.450 --> 00:57:34.343
And then other people try to analyse why
that was the case in the first place.

00:57:34.343 --> 00:57:37.871
But yeah, there's no mathematical reasons
why you couldn't at all have a rectified

00:57:37.871 --> 00:57:38.660
linear unit here.

00:57:42.117 --> 00:57:48.638
All right, now on to a even more
complex sort of overall recurrent unit.

00:57:48.638 --> 00:57:53.022
Namely the long-short-term-memories or
LSTMs.

00:57:53.022 --> 00:57:56.922
So now this is the hippest
model of the day, and

00:57:56.922 --> 00:58:00.710
it's pretty important to know it well.

00:58:00.710 --> 00:58:03.766
Fortunately, it's again very similar
to the kinds of basic building blocks.

00:58:03.766 --> 00:58:08.481
But now we allow each of
the different steps to have again,

00:58:08.481 --> 00:58:11.840
we separate them out even more.

00:58:11.840 --> 00:58:13.130
So how do we separate them out?

00:58:13.130 --> 00:58:15.925
Basically, this is what's
going on at each time step.

00:58:15.925 --> 00:58:20.095
We will have an input gate,
forget gate, output gate, memory cell,

00:58:20.095 --> 00:58:22.583
final memory, and a final hidden state.

00:58:22.583 --> 00:58:24.814
Now let's gain a little
bit of intuition and

00:58:24.814 --> 00:58:27.358
there is good intuition of
why we want any of them.

00:58:27.358 --> 00:58:31.621
So the input gate will
basically determine how much we

00:58:31.621 --> 00:58:35.043
will care about the current vector at all.

00:58:35.043 --> 00:58:39.726
So how much does the current cell or
the current input word vector matter?

00:58:39.726 --> 00:58:44.194
The forget gate is a separate mechanism
that just says maybe I should forget,

00:58:44.194 --> 00:58:45.166
maybe I don't.

00:58:45.166 --> 00:58:48.218
In this case here, just kind of
counterintuitive sometimes and

00:58:48.218 --> 00:58:51.020
they're actually different
models in the literatures.

00:58:51.020 --> 00:58:53.543
Some have the one minus there and
others don't.

00:58:53.543 --> 00:58:55.980
But in general here,
we'll define our forget gate.

00:58:55.980 --> 00:58:58.220
If it's 0 then we're forgetting the past.

00:59:00.010 --> 00:59:04.560
Then we have an output gate,
basically when you have this output gate,

00:59:04.560 --> 00:59:09.775
you will separate out what
matters to a certain prediction

00:59:09.775 --> 00:59:16.030
versus what matters to being kept around
over the current recurrent time steps.

00:59:16.030 --> 00:59:19.054
So you might say at
this current time step,

00:59:19.054 --> 00:59:24.515
this particular cell is not important,
but it will become important later.

00:59:24.515 --> 00:59:28.580
And so I'm not going to output it,
to my final softmax for instance, but

00:59:28.580 --> 00:59:30.460
I'm still gonna keep it around.

00:59:31.470 --> 00:59:35.020
So it's yet another separate
mechanism to learn when to do that.

00:59:36.250 --> 00:59:41.241
And then we have our new memory cell here,
which is similar to what we had before.

00:59:41.241 --> 00:59:45.323
So in fact all these four here
have the same equation inside and

00:59:45.323 --> 00:59:49.420
just three sigmoid non linearity and
one tan h non linearity.

00:59:51.050 --> 00:59:54.770
So these are all just four
single layer neural nets.

00:59:56.710 --> 01:00:01.660
Now we'll put all of these gates together
when we compute the memory cell and

01:00:01.660 --> 01:00:02.620
the final hidden state.

01:00:02.620 --> 01:00:06.260
So the final memory cell now
basically separated out the input and

01:00:06.260 --> 01:00:07.680
the forget gate.

01:00:07.680 --> 01:00:10.920
Instead of just c and 1 minus c,
we have two separate mechanisms

01:00:10.920 --> 01:00:14.110
that can be trained and
learn slightly different things.

01:00:14.110 --> 01:00:18.817
And actually become also in some ways
counter intuitive like you say, I don't

01:00:18.817 --> 01:00:23.546
wanna forget but you do wanna forget,
but you also input something right now.

01:00:23.546 --> 01:00:28.771
But the model turns out to work very well.

01:00:28.771 --> 01:00:33.571
So basically here we have final hidden
state is just to forget gate how to

01:00:33.571 --> 01:00:38.464
mark product with the previous hidden
states final memory cell ct-1.

01:00:38.464 --> 01:00:42.433
So this again will determine,
how much do you wanna keep this around or

01:00:42.433 --> 01:00:45.600
how much do we wanna forget from the past?

01:00:45.600 --> 01:00:50.250
And then the new memory cell here,
this has a standard recurrent neural net.

01:00:50.250 --> 01:00:55.720
If i is all 1s,
then we really keep the input around.

01:00:55.720 --> 01:00:59.211
And if the input gate says no,
this one doesn't matter,

01:00:59.211 --> 01:01:02.946
then you just basically ignore
the current word back there.

01:01:06.475 --> 01:01:09.500
So in that sense,
this equation is quite intuitive, right?

01:01:09.500 --> 01:01:16.694
Forget the past or not, take the input or
not, that's basically it, yeah?

01:01:19.751 --> 01:01:20.469
So the secret question,

01:01:20.469 --> 01:01:23.996
once you forget the past does it mean
you forget grammar or something else?

01:01:23.996 --> 01:01:30.020
And the truth is we can think of these
forget gates as sort of absolutes.

01:01:30.020 --> 01:01:31.843
They're all vectors, and

01:01:31.843 --> 01:01:36.208
they will all forget only certain
elements of a long hidden unit.

01:01:36.208 --> 01:01:42.930
And so really, I can eventually show
you what these hidden states look like.

01:01:42.930 --> 01:01:46.030
And sometimes they're actually
more intuitive than others.

01:01:46.030 --> 01:01:50.280
But it's rare that you would find this
particular unit when it was turned off or

01:01:50.280 --> 01:01:53.410
on actually had like this
perfect interpretation that

01:01:53.410 --> 01:01:56.542
we as humans find intuitive and
think of as grammar.

01:01:56.542 --> 01:02:00.760
And also of course grammar is
a very complex kind of beast.

01:02:00.760 --> 01:02:05.327
And so it's hard to say any single unit
would capture any particular like entirety

01:02:05.327 --> 01:02:08.320
of a grammar,
it might only capture certain things.

01:02:08.320 --> 01:02:12.530
So it's not implausible to think
of these three cells together

01:02:12.530 --> 01:02:16.240
suggest that the next noun should be
a plural noun or something like that.

01:02:16.240 --> 01:02:18.422
But that's the most we could hope for
in many cases.

01:02:21.624 --> 01:02:24.662
All right, and then here,
the final hidden state again,

01:02:24.662 --> 01:02:26.589
we can keep these cs around, right?

01:02:26.589 --> 01:02:30.370
And cs will compute our
computer from other cs.

01:02:30.370 --> 01:02:34.840
But we might not want to expose
the content of this memory cell

01:02:34.840 --> 01:02:38.460
in order to compute the final
hidden state, ht minus 1.

01:02:43.175 --> 01:02:46.878
All right, now yeah,
this is it, this is the LSTM.

01:02:46.878 --> 01:02:50.240
It's a really powerful model, are there
any questions around the equations?

01:02:50.240 --> 01:02:54.150
We're gonna attempt at some illustrations,
but

01:02:54.150 --> 01:02:59.283
again I think the equations
are sometimes more intuitive.

01:03:03.853 --> 01:03:09.870
Does the LSTM and GRU completely liviate
or just help with an engine came problem?

01:03:09.870 --> 01:03:13.741
And the truth is they helped with it a
lot, but they don't completely obviate it.

01:03:13.741 --> 01:03:18.420
You do multiply here a bunch of
numbers that are often smaller than 1.

01:03:18.420 --> 01:03:23.620
And over time even if it would
have to be a perfect one,

01:03:23.620 --> 01:03:26.680
but that would mean that, that unit
is really, really strongly active.

01:03:26.680 --> 01:03:31.467
And then it's hard to sort of dies,
it's like the gradient,

01:03:31.467 --> 01:03:37.837
when you have unit that's really, really
active and looks something like this.

01:03:37.837 --> 01:03:41.314
Now the input is really large
to that unit and it's here,

01:03:41.314 --> 01:03:44.170
then grade in around here,
It's pretty much 0.

01:03:44.170 --> 01:03:45.510
So that unit's kind of dead.

01:03:45.510 --> 01:03:47.380
And then the model can't do
anything with it anymore.

01:03:47.380 --> 01:03:50.610
And so it happens, there are,
when you want to train these,

01:03:50.610 --> 01:03:53.830
you'll observe some units just sort
of die after training after awhile.

01:03:53.830 --> 01:03:58.340
And you'll just sort of keep around stuff,
or delete stuff at each time step.

01:03:58.340 --> 01:04:03.560
But in general most of the units
are somewhat small than 1, and

01:04:03.560 --> 01:04:10.280
so you still have a bit of a vanishing
creating problem but much less so.

01:04:10.280 --> 01:04:13.030
And intuitively you can
come up with final P for

01:04:13.030 --> 01:04:17.170
a lot of good ways to
think about this right?

01:04:17.170 --> 01:04:19.960
Maybe you want to predict different
things at different time steps.

01:04:19.960 --> 01:04:25.150
But you wanna keep around knowledge
through the memory cells but

01:04:25.150 --> 01:04:27.290
not expose it at a given prediction.

01:04:27.290 --> 01:04:27.790
Yeah.

01:04:32.166 --> 01:04:34.470
What is the point of the exposure gate
when it already had the forget gate?

01:04:34.470 --> 01:04:37.970
So basically, you want to,

01:04:37.970 --> 01:04:40.770
sort of forget gate will tell you whether
you keep something around or not.

01:04:41.780 --> 01:04:45.990
But exposure gate, will mean, does it
matter to this current time step or not.

01:04:47.000 --> 01:04:48.968
So you might not wanna forget something.

01:04:48.968 --> 01:04:51.783
But you also might not wanna
show it to the current output,

01:04:51.783 --> 01:04:53.920
because it's irrelevant for that output.

01:04:53.920 --> 01:04:57.453
And it would just confuse the Softmax
classifier at that output.

01:05:00.749 --> 01:05:01.485
Yeah?

01:05:11.171 --> 01:05:16.620
Does the exposure gate help you, or
do you mean the output gate here, right?

01:05:16.620 --> 01:05:19.615
So does the output gate,
does it help you to what exactly?

01:05:22.221 --> 01:05:24.404
To not have to forget everything forever.

01:05:28.493 --> 01:05:29.880
So, in some ways, yes.

01:05:29.880 --> 01:05:33.710
You can basically,
this model could decide that,

01:05:33.710 --> 01:05:38.170
while it doesn't wanna give as
output something for a long time.

01:05:38.170 --> 01:05:42.660
And hence it's basically
a temporal forgetting, right?

01:05:42.660 --> 01:05:46.240
It will only be forgotten at that time
set but actually be kept around in.

01:05:46.240 --> 01:05:49.660
I don't wanna use,
like anthropomorphize the models, but

01:05:49.660 --> 01:05:52.280
like the subconsciousness of this model or
whatever, right?

01:05:52.280 --> 01:05:55.080
Keeps it around but doesn't expose it.

01:05:55.080 --> 01:05:55.828
Don't quote me on that.

01:06:00.521 --> 01:06:02.025
All right, one last question, yeah?

01:06:07.252 --> 01:06:10.750
The initialization to all these models
matters, it matters quite significantly.

01:06:10.750 --> 01:06:13.770
So, if you initialize all your weights,
for instance such that

01:06:13.770 --> 01:06:16.760
whatever you do in the beginning,
all of the weights are super large.

01:06:16.760 --> 01:06:19.450
Then your gradients are zero and
you're stuck in the optimization.

01:06:19.450 --> 01:06:23.490
So you always have to
initialize them properly.

01:06:23.490 --> 01:06:27.330
In most cases, as long as they're
relatively small, you can't go too wrong.

01:06:27.330 --> 01:06:30.100
Eventually, it might slow down
your eventual convergence, but

01:06:30.100 --> 01:06:31.980
as long as all your parameters, W here,

01:06:31.980 --> 01:06:35.310
and your word vectors and so
on are initialized to very small numbers.

01:06:35.310 --> 01:06:37.370
It will usually eventually
do it pretty well.

01:06:40.421 --> 01:06:43.190
Yes you could use lots of different
strategies for initialization.

01:06:44.270 --> 01:06:46.390
All right, now, some visualizations.

01:06:46.390 --> 01:06:50.800
I like this one from Chris Olah on
his blog from not too long ago.

01:06:50.800 --> 01:06:53.260
But again, I don't know.

01:06:53.260 --> 01:06:55.890
I feel like the equations speak mostly for
themselves.

01:06:55.890 --> 01:06:57.000
You can think of these.

01:06:57.000 --> 01:07:00.840
I have four different neural network
layers, and then you combine them in

01:07:00.840 --> 01:07:05.890
various ways with pointwise operations,
such as multiplication or addition.

01:07:05.890 --> 01:07:08.080
And sometimes you know multiplication and
then addition,

01:07:08.080 --> 01:07:11.340
and concatenation and copies and so on.

01:07:11.340 --> 01:07:13.500
But, In the end you often observe,

01:07:13.500 --> 01:07:17.590
this kind of thing where we'll
just write LSTM in this block.

01:07:17.590 --> 01:07:19.354
And has an X and an H, and

01:07:19.354 --> 01:07:24.578
we don't really look into too many
details of what's going on there.

01:07:26.498 --> 01:07:31.699
And here's some, I think, even less
helpful [LAUGH] illustrations that,

01:07:31.699 --> 01:07:35.740
yeah, I think are mostly
confusing to a lot of people.

01:07:35.740 --> 01:07:39.180
I have the forget gates here,
output gates, input gates, and so on.

01:07:39.180 --> 01:07:45.530
But and your memory cells as
they try to modify each other.

01:07:45.530 --> 01:07:47.200
This one is a little cleaner.

01:07:47.200 --> 01:07:49.000
You know you have some inputs, your gates,

01:07:49.000 --> 01:07:54.100
you have your forget gates on top
of your memory cell and so on.

01:07:54.100 --> 01:07:57.510
But in general I think the equations
are actually quite intuitive, right?

01:07:57.510 --> 01:07:58.910
If you think of your extremes,

01:07:58.910 --> 01:08:03.320
if this is zero, one, then this
input matters more to the output.

01:08:05.890 --> 01:08:09.820
All right, now as I said,
LSTMs, currently super hip.

01:08:09.820 --> 01:08:14.450
The en vogue model are for pretty
much all sequence labeling tasks and

01:08:14.450 --> 01:08:17.430
sequence to sequence tasks
like machine translation.

01:08:17.430 --> 01:08:21.740
Super powerful in many cases, you will
actually observe that we'll stack them.

01:08:21.740 --> 01:08:26.510
So just like the other RNN architectures,
we'll have a whole LSTM block and

01:08:26.510 --> 01:08:30.660
we put another LSTM block with different
sets of parameters on top of it.

01:08:30.660 --> 01:08:33.740
And then the parameters
are shared over time, but

01:08:33.740 --> 01:08:36.240
are different as you
have a very deep model.

01:08:37.360 --> 01:08:42.250
And, of course, with all these
parameters here, we have essentially

01:08:42.250 --> 01:08:45.300
many more parameters then the standard
recurrent neural network.

01:08:45.300 --> 01:08:49.210
Where we only have two such parameters and
we update every time.

01:08:49.210 --> 01:08:53.050
You wanna have more data especially
if you stack you now have

01:08:53.050 --> 01:08:57.730
10x the parameters of standard RNN,
we wanna train this on a lot of data.

01:08:57.730 --> 01:09:01.600
And in terms of amount of training
data available machine translation is

01:09:01.600 --> 01:09:04.310
actually one of the best tasks for that.

01:09:04.310 --> 01:09:10.920
And is also the one where these
model sort of shine the most.

01:09:10.920 --> 01:09:15.647
And so in 2015, I think the first time I
gave the deep learning for NLP lecture,

01:09:15.647 --> 01:09:17.790
the jury was still a little bit out.

01:09:17.790 --> 01:09:21.330
The neural network models
came up fairly quickly.

01:09:21.330 --> 01:09:26.580
But some different, more traditional
machine translation systems

01:09:26.580 --> 01:09:30.640
were still slightly better,
like by half a BLEU point.

01:09:30.640 --> 01:09:33.490
We haven't defined BLEU scores yet.

01:09:33.490 --> 01:09:36.430
You can essentially think
of it as an engram overlap.

01:09:36.430 --> 01:09:40.840
The more your translation overlaps
in terms of unigrams and bigrams and

01:09:40.840 --> 01:09:45.870
trigrams, the better it likely is, period.

01:09:45.870 --> 01:09:49.610
So you have this reference translation,
sometimes multiple reference translations.

01:09:49.610 --> 01:09:52.730
You have your translation, you look
at engram overlap between the two.

01:09:52.730 --> 01:09:53.540
So the higher the better.

01:09:54.580 --> 01:09:58.350
And basically the neural network
models were often also just use it for

01:09:58.350 --> 01:10:02.280
rescoring traditional MT model.

01:10:02.280 --> 01:10:04.980
Now, just one year later, last year,

01:10:04.980 --> 01:10:08.990
really a couple months ago,
the story was completely different.

01:10:08.990 --> 01:10:15.008
So this is WMT, the worldwide
competition for machine translation.

01:10:15.008 --> 01:10:18.210
And you have different universities,

01:10:18.210 --> 01:10:22.550
and different companies and
so on, submit their systems.

01:10:22.550 --> 01:10:28.320
And the top three systems were all
neural machine translation systems.

01:10:28.320 --> 01:10:31.580
The jury is now basically not out anymore.

01:10:31.580 --> 01:10:35.334
It's clear neural machine
translation is the most accurate

01:10:35.334 --> 01:10:37.917
machine translation model in the world.

01:10:41.427 --> 01:10:43.064
Yeah that number two was us, yeah.

01:10:43.064 --> 01:10:46.819
&gt;&gt; [LAUGH]
&gt;&gt; James Bradbury and me worked on that.

01:10:49.795 --> 01:10:53.576
James Bradbury was actually a linguistics
undergrad while he was doing that, but

01:10:53.576 --> 01:10:54.684
now he's full-time.

01:10:55.880 --> 01:10:59.990
So, yeah, basically we haven't talked
that much about ensembling and

01:10:59.990 --> 01:11:01.610
ensembles of different models.

01:11:01.610 --> 01:11:04.610
But you can also train
five of these monsters and

01:11:04.610 --> 01:11:08.580
then average all the probabilities and
you'll usually get a little better.

01:11:08.580 --> 01:11:09.960
We just, as general thing,

01:11:09.960 --> 01:11:12.990
you'll observe for every competition
machine learning competition out there.

01:11:12.990 --> 01:11:16.800
If you go on Kaggle, other machine
learning competitions usually train

01:11:16.800 --> 01:11:18.510
even the same kind of model five times.

01:11:18.510 --> 01:11:20.690
You end up in slightly different
local optimum average,

01:11:20.690 --> 01:11:23.050
and you still do pretty well.

01:11:23.050 --> 01:11:24.660
What's cool also though,

01:11:24.660 --> 01:11:30.200
is that while we might not be able
to exactly recover grammar, or

01:11:30.200 --> 01:11:37.110
have specific units be explicitly sort
of capturing very intuitive things.

01:11:37.110 --> 01:11:39.641
As we project this down
similar to the word vectors,

01:11:39.641 --> 01:11:42.760
we actually do observe some
pretty interesting regularities.

01:11:42.760 --> 01:11:47.855
So this is a paper from Sutskever in 2014,

01:11:47.855 --> 01:11:52.175
they projected different sentences.

01:11:52.175 --> 01:11:57.079
They were trained basically with
a machine translation task and

01:11:57.079 --> 01:12:01.270
basically observe quite
interesting regularities.

01:12:01.270 --> 01:12:05.419
So John admires Mary is close
to John is in love with Mary and

01:12:05.419 --> 01:12:07.120
to John respects Mary.

01:12:07.120 --> 01:12:07.695
Now of course,

01:12:07.695 --> 01:12:10.494
we have to be a little carefull here
to not over interpret the amazingness.

01:12:10.494 --> 01:12:14.260
It's amazing, but
we also have a selection vice here, right?

01:12:14.260 --> 01:12:20.080
Maybe if we just had
John did admire Mary or

01:12:20.080 --> 01:12:21.830
something, it might also be close to it,
right?

01:12:21.830 --> 01:12:23.310
And it might be closer too, but

01:12:23.310 --> 01:12:29.030
if you just project these six particular
sentences into lower dimensional space.

01:12:29.030 --> 01:12:33.790
Then you do see very nicely that whenever
John has some positive feelings for

01:12:33.790 --> 01:12:36.790
Mary, all those sentences are in here.

01:12:36.790 --> 01:12:42.410
And all the ones that are on this area
of the first two item vectors, Mary

01:12:42.410 --> 01:12:46.810
admires John, Mary admires John, Mary is
in love with John, and Mary respects John.

01:12:48.150 --> 01:12:49.150
They're all closer together,

01:12:49.150 --> 01:12:52.670
which is kind of amazing cuz
some people are also worried.

01:12:52.670 --> 01:12:54.160
Well it's a sequence model, so

01:12:54.160 --> 01:12:58.370
how could it ever capture
that the word order changes?

01:12:58.370 --> 01:13:01.288
And so this is a particularly
cool example of that.

01:13:01.288 --> 01:13:02.361
So here we have,

01:13:02.361 --> 01:13:07.202
she was given a card by me in the garden
versus in the garden I gave her a card.

01:13:07.202 --> 01:13:09.514
And I gave her a card in the garden, and

01:13:09.514 --> 01:13:13.000
despite the word order being
actually flipped, right?

01:13:13.000 --> 01:13:16.737
In the garden is in the beginning here,
and in the end here.

01:13:16.737 --> 01:13:20.740
These are still closer together
than the different ones where,

01:13:20.740 --> 01:13:25.630
in the garden basically she gave me
a card verses I gave her a card.

01:13:25.630 --> 01:13:29.481
So that shows that the semantics here
turn out to be more important than

01:13:29.481 --> 01:13:30.405
the word order.

01:13:30.405 --> 01:13:32.801
Despite the model just
going from left to right or

01:13:32.801 --> 01:13:37.450
this one was still the trick where we
reversed the order of the input sentence.

01:13:37.450 --> 01:13:41.800
But it choses that its
incredibly invariant and

01:13:41.800 --> 01:13:43.480
variance is a pretty important concept,
right?

01:13:43.480 --> 01:13:48.100
We want this model to be
invariant to simple syntactic

01:13:48.100 --> 01:13:51.940
changes when the semantics
are actually kept the same.

01:13:51.940 --> 01:13:55.313
It's pretty incredible, that it does that.

01:13:55.313 --> 01:13:58.253
So this is also the power
I think of some of these.

01:13:58.253 --> 01:14:02.578
This is a very deep LSTM model where
you have five different LSTM stacked in

01:14:02.578 --> 01:14:04.965
the encoder and several in the decoder.

01:14:04.965 --> 01:14:07.780
And they're all connected
in multiple places too.

01:14:10.380 --> 01:14:14.109
All right, any questions around
those visualizations and LSTMs?

01:14:19.392 --> 01:14:23.911
All right, you now have knowledge under
you belt that is super powerful and

01:14:23.911 --> 01:14:25.670
very interesting.

01:14:25.670 --> 01:14:27.945
I expected to maybe have
five minutes more of time.

01:14:27.945 --> 01:14:31.746
So I'm going to talk to you about a recent
improvement, two recurrent neural networks

01:14:31.746 --> 01:14:34.467
that I think is also very
applicable to machine translation.

01:14:34.467 --> 01:14:37.625
But nobody has actually yet
applied it to machine translation.

01:14:37.625 --> 01:14:42.245
And that is a general problem
with all softmax classification

01:14:42.245 --> 01:14:44.755
that we do in all the models I've so
far described to you.

01:14:44.755 --> 01:14:46.905
And really up until two or
three months ago,

01:14:46.905 --> 01:14:50.170
that everybody in NLP
had as a major problem.

01:14:50.170 --> 01:14:55.131
And that is you can only ever predict
answers if you saw that exact word at

01:14:55.131 --> 01:14:56.295
training time.

01:14:56.295 --> 01:15:00.090
And you have your cross entropy error
saying I wanna predict this word.

01:15:00.090 --> 01:15:03.612
And if you've never predicted that word,
no matter how obvious it is for

01:15:03.612 --> 01:15:05.950
the translation system it will
not be able to do it, right?

01:15:05.950 --> 01:15:13.650
So we have some kind of translation,
and let's us say we have a new word,

01:15:13.650 --> 01:15:18.120
like a new name or something that
we've never seen at training time.

01:15:18.120 --> 01:15:23.223
And it is very obvious that this word
here should go at this location.

01:15:23.223 --> 01:15:26.782
This is like Mrs. and
then maybe the new word is like yelling or

01:15:26.782 --> 01:15:29.668
something like that,
it could be any other word.

01:15:29.668 --> 01:15:32.766
And now let's say at training time,
we've never seen the word yelling.

01:15:32.766 --> 01:15:37.046
But now, it's like vowel, German misses,

01:15:37.046 --> 01:15:41.562
miss in, yeah,
German translation for this.

01:15:41.562 --> 01:15:44.930
And now it's very obvious to
everybody that after this word,

01:15:44.930 --> 01:15:47.865
it should be the next one,
the name of the of the miss.

01:15:47.865 --> 01:15:53.050
And so these models would never
be able to do that, right?

01:15:53.050 --> 01:15:58.213
And so one way to fix that is to think
about character meant translation models,

01:15:58.213 --> 01:16:03.317
where the model's actually surprisingly
similar to what we described here.

01:16:03.317 --> 01:16:09.560
Well many times it have to go, but instead
of having words we just have characters.

01:16:09.560 --> 01:16:13.170
So that's one way, but
now we have very long sequences.

01:16:13.170 --> 01:16:18.310
And at every character you have
a lot of matrix multiplications.

01:16:18.310 --> 01:16:22.480
And these matrix multiplications
that we have in here are not

01:16:23.720 --> 01:16:27.420
50 dimensional for really powerful MT
models, they're a 1,000 dimensional.

01:16:28.490 --> 01:16:32.644
And now you have several thousand
by a thousand matrices here

01:16:32.644 --> 01:16:36.151
multiplying with thousand
dimensional vectors.

01:16:36.151 --> 01:16:38.999
And you stack them, so
you have to do it five times.

01:16:38.999 --> 01:16:42.249
Doing that for every single character
actually gets really, really expensive.

01:16:43.540 --> 01:16:47.950
So at the same time,
it's very intuitive that

01:16:47.950 --> 01:16:51.366
after we see a new word at test time
we wanna be able to predict it.

01:16:51.366 --> 01:16:55.022
And also in general when we have
the softmax, even for words that we do

01:16:55.022 --> 01:16:58.817
see once or twice, it's hard for
the model to then still predict them.

01:16:58.817 --> 01:17:02.230
It's this skewed data set
distribution problem.

01:17:02.230 --> 01:17:07.012
But you have very rare, very infrequent
classes, our words are hard to predict for

01:17:07.012 --> 01:17:07.773
the models.

01:17:07.773 --> 01:17:13.225
So this is one attempt at fixing that,
which is essentially a mixture

01:17:13.225 --> 01:17:18.880
model of using standard softmax and
what we call a pointer.

01:17:18.880 --> 01:17:21.610
So what's a pointer?
It's essentially a mechanism to

01:17:21.610 --> 01:17:26.484
say well maybe my next word is one of
the previous words in the context.

01:17:26.484 --> 01:17:30.675
You say 100 words in the past,
and every time step you say,

01:17:30.675 --> 01:17:35.370
maybe I just wanna copy a word
over from the last 100 words.

01:17:35.370 --> 01:17:40.998
And if not, then I will use my
standard softmax for the rest.

01:17:40.998 --> 01:17:43.021
So this is kind of this
sentinel idea here.

01:17:43.021 --> 01:17:48.570
This is a paper by Stephen Merity and
some other folks.

01:17:48.570 --> 01:17:52.870
And basically, we now have
a mixture model, where we combine

01:17:52.870 --> 01:17:57.520
the probabilities from the standard
vocabulary and from this pointer.

01:17:57.520 --> 01:17:58.835
And now how do we compute this pointer?

01:17:58.835 --> 01:18:03.498
It's very straightforward,
we basically have a query.

01:18:03.498 --> 01:18:09.380
This query is just a modification of
the last hidden layer that we have here.

01:18:09.380 --> 01:18:12.260
And we pipe that through a standard
single layer neural network

01:18:12.260 --> 01:18:14.910
to compute another hidden layer,
which we'll call q.

01:18:14.910 --> 01:18:18.328
And then we'll do an inter
product between this q and

01:18:18.328 --> 01:18:22.457
all the previous hidden states
of the last 100 timed steps.

01:18:22.457 --> 01:18:25.241
And that will give us,
basically, the single number for

01:18:25.241 --> 01:18:26.800
each of these interproducts.

01:18:26.800 --> 01:18:30.496
And then we'll apply
a softmax on top of that.

01:18:30.496 --> 01:18:33.582
And this gives us essentially,
a probability for

01:18:33.582 --> 01:18:37.490
how likely do we wanna point
to each of these words.

01:18:37.490 --> 01:18:41.810
Or the very last one is we
don't point to anything,

01:18:41.810 --> 01:18:43.910
we just take the standard softmax.

01:18:43.910 --> 01:18:47.035
So we keep one unit
around where we do this.

01:18:47.035 --> 01:18:51.412
And now of course in the context,
the same word might appear multiple times.

01:18:51.412 --> 01:18:55.410
And so you just sum up all
the probabilities for specific words.

01:18:55.410 --> 01:18:58.980
If they appear multiple times,
you just sum them up.

01:18:58.980 --> 01:19:05.242
With this simple modification, we now
have the ability to predict unseen words.

01:19:05.242 --> 01:19:10.112
We can predict based on the pattern
of how rare words appear much more

01:19:10.112 --> 01:19:11.490
similar things.

01:19:11.490 --> 01:19:16.408
For instance, Fed, Chair, Janet,
Yellen, raised, rates and so on, Ms.

01:19:16.408 --> 01:19:20.970
is very obvious that this is the same Ms.
that we're referring to here.

01:19:22.150 --> 01:19:25.540
And you can base or
you combine this in this mixture model.

01:19:25.540 --> 01:19:28.772
And now over many, many years for
language modeling.

01:19:28.772 --> 01:19:35.203
The perplexity that we defined before
was sort of stock actually around 80.

01:19:35.203 --> 01:19:36.514
And then in 2015,

01:19:36.514 --> 01:19:40.820
we have a bunch of modifications
to LSTMs that were very powerful.

01:19:40.820 --> 01:19:46.060
And lower this, and
now were down to the lowest 70s.

01:19:46.060 --> 01:19:49.790
And was some modifications
will cover another class,

01:19:49.790 --> 01:19:51.430
were actually down on the 60s now.

01:19:51.430 --> 01:19:53.859
So it really had to told for
several years, and

01:19:53.859 --> 01:19:56.429
now perplexity numbers
are really dropping in.

01:19:56.429 --> 01:20:00.805
And this models are getting better and
bettered capturing, more and

01:20:00.805 --> 01:20:03.831
more the semantics and
the syntax of language.

01:20:03.831 --> 01:20:05.180
All right, so let's summarize.

01:20:05.180 --> 01:20:07.190
Recurrent Neural Networks, super powerful.

01:20:07.190 --> 01:20:11.654
You now know the best ones in
that family to use in LSTMs.

01:20:11.654 --> 01:20:15.110
This is a pretty advanced lecture,
I hope you gained some of the intuition.

01:20:15.110 --> 01:20:20.659
Again, most of the math falls out from the
same basic building blocks we had before.

01:20:20.659 --> 01:20:25.582
And next week or no next Thursday,
we'll do midterm review.

01:20:25.582 --> 01:20:26.520
All right, thank you.

