WEBVTT
Kind: captions
Language: en

00:00:00.820 --> 00:00:05.120
In this section, I'm going to talk
about smoothing maxent models.

00:00:05.120 --> 00:00:09.370
Just like for other models we
build in natural image processing,

00:00:09.370 --> 00:00:13.150
we still have the issue that
these models can over fit and

00:00:13.150 --> 00:00:15.640
that we want to apply
smoothing techniques.

00:00:15.640 --> 00:00:18.430
So that the parameters we
estimate don't lead to

00:00:18.430 --> 00:00:23.560
two spiky distributions that over fit
what was observed in the training data.

00:00:23.560 --> 00:00:29.650
This topic of smoothing maximum entry
models is often also described as using

00:00:29.650 --> 00:00:34.780
a prior distribution for the parameters or
doing regularization of the models.

00:00:34.780 --> 00:00:38.410
The issue of smoothing is very
prominent in the models we build

00:00:38.410 --> 00:00:42.160
Because the models we build have lots and
lots of features.

00:00:42.160 --> 00:00:45.880
Typically when you do logistic
regression in a statistics class,

00:00:45.880 --> 00:00:50.100
your model might only have four,
eight, 12 features.

00:00:50.100 --> 00:00:53.400
And there's enough data that
you can suitably estimate

00:00:53.400 --> 00:00:55.870
the parameters of all those features.

00:00:55.870 --> 00:00:59.160
But typically, the models we'll
build in natural language processing

00:00:59.160 --> 00:01:03.590
will have hundreds of thousands, millions,
even tens of millions of features.

00:01:03.590 --> 00:01:07.290
And so, one thing to notice
right there is simply storing

00:01:07.290 --> 00:01:11.000
the array of parameter values will
have a substantial memory cost.

00:01:11.000 --> 00:01:15.100
But from a statistical estimation
point of view, more importantly,

00:01:15.100 --> 00:01:20.550
that most of the parameters of those
features will be very poorly estimated

00:01:20.550 --> 00:01:24.740
because we'll have very limited data in
which to estimate the parameter values.

00:01:24.740 --> 00:01:27.180
So there are lots of issues of sparsity.

00:01:27.180 --> 00:01:32.710
Overfitting to the training data is very
easy md we need smoothing to prevent it.

00:01:32.710 --> 00:01:37.720
And many features that we saw, happen
to see at training time some example.

00:01:37.720 --> 00:01:39.350
We might never see at all.

00:01:39.350 --> 00:01:43.280
Again and further, you set the model
at test time, so you want to not

00:01:43.280 --> 00:01:46.730
give too much weight to features
we happen to see at training time.

00:01:46.730 --> 00:01:51.260
There are other reasons why we
need to smooth our maxent models.

00:01:51.260 --> 00:01:56.080
If we don't, feature weights can be
infinite, and the iterative solvers we use

00:01:56.080 --> 00:02:00.100
to set parameter values can take a long
time to get to those infinities.

00:02:00.100 --> 00:02:04.060
So really we want to change
the model formulation, so

00:02:04.060 --> 00:02:06.570
the optimal feature weights are finite,
and

00:02:06.570 --> 00:02:11.120
therefore easily found and
findable by an optimization procedure.

00:02:11.120 --> 00:02:14.680
Let me motivate this issue of smoothing,
by looking at a really simple example.

00:02:14.680 --> 00:02:20.090
Let's assume we're tossing a coin, and
we have a distribution of heads and tails.

00:02:20.090 --> 00:02:23.010
The natural way that
that's formulated with

00:02:23.010 --> 00:02:25.540
features that the kind we've talked about.

00:02:25.540 --> 00:02:31.340
Is we have two features one for
is it a head and one for is it a tail?

00:02:31.340 --> 00:02:33.870
And then we'll have the following
model distribution.

00:02:33.870 --> 00:02:38.760
So this is the probability of heads
where we have the feature weight for

00:02:38.760 --> 00:02:42.790
it being a head, and the probability of it
being tails with the feature weight for

00:02:42.790 --> 00:02:44.240
it being tails.

00:02:44.240 --> 00:02:48.510
And then this is our normalization term,
which is the same in both cases.

00:02:48.510 --> 00:02:51.170
Now that makes it look like
there are two parameters,

00:02:51.170 --> 00:02:55.000
but there are really only
one degree of freedom here.

00:02:55.000 --> 00:02:58.930
And this piece of math here shows how this

00:02:58.930 --> 00:03:02.380
max head formulation connects
to the normal formulation.

00:03:02.380 --> 00:03:06.936
You see a two class logistic progression
in the statistics text book.

00:03:06.936 --> 00:03:10.700
So, if we instead said what really matters

00:03:10.700 --> 00:03:15.380
is the difference between the weight
of the head and tail parameter.

00:03:15.380 --> 00:03:22.310
Then we can say that the probability of
heads can instead be written like this.

00:03:22.310 --> 00:03:29.180
So what we're doing here is we've
just taken this equation up here and

00:03:29.180 --> 00:03:35.010
we've multiplied each term by,
this are the minus.

00:03:35.010 --> 00:03:37.730
And then we simplified
that math down a little,

00:03:37.730 --> 00:03:41.590
we get e to the e to
the land rover plus one.

00:03:43.260 --> 00:03:47.970
And then, the probability of tails,
doing the same kind of trickery,

00:03:47.970 --> 00:03:51.843
comes out as e to the zero
over e to the lambda + 0.

00:03:51.843 --> 00:03:59.360
And again, if we simplify that down,
that's then 1 over e to the lambda + 1.

00:03:59.360 --> 00:04:03.510
And so this is the simple
form that you often see for

00:04:03.510 --> 00:04:04.850
a logistic progression model.

00:04:06.580 --> 00:04:08.000
In the general, that's true, right?

00:04:08.000 --> 00:04:13.070
That what's important is the difference
of the weights of opposing parameters

00:04:13.070 --> 00:04:16.260
when you have a multi,
two class or multi-class model.

00:04:16.260 --> 00:04:18.740
Okay, so then, if we graph lambda.

00:04:18.740 --> 00:04:22.170
We get this classic logistic curve.

00:04:22.170 --> 00:04:26.370
So, with the weight of zero, you would
get a half each chance of heads or

00:04:26.370 --> 00:04:28.990
tails, and
as the weight goes into negative,

00:04:28.990 --> 00:04:33.430
the probability drops down initially
sharply, and then very slowly.

00:04:35.230 --> 00:04:40.740
And as the lambda becomes positive,
the probability climbs initially.

00:04:40.740 --> 00:04:42.700
Quickly and then very slowly.

00:04:44.070 --> 00:04:48.870
Well let's assume that we've seen
just a little bit of data and

00:04:48.870 --> 00:04:53.700
we estimate lambda, so as to maximize
the likelihood of the observed data.

00:04:55.790 --> 00:04:59.550
So then the probability
of the observed data

00:04:59.550 --> 00:05:03.010
is the number of heads times the log
of the probability of heads.

00:05:03.010 --> 00:05:05.250
This is the log likelihood
of the observed data.

00:05:05.250 --> 00:05:10.545
Plus the number of tails times the log
likelihood of the probability of tails.

00:05:10.545 --> 00:05:16.970
And using the form on the previous slide,
that comes out like this.

00:05:16.970 --> 00:05:21.260
Okay, suppose we toss two heads and
two tails, then not surprisingly.

00:05:21.260 --> 00:05:26.610
The optimum value of lambda
comes out as zero, and

00:05:26.610 --> 00:05:31.930
that corresponds with a probability
of heads or tails as a half.

00:05:31.930 --> 00:05:36.680
If we toss three heads and one tail

00:05:36.680 --> 00:05:41.680
then the optimum value for the lambda
parameter comes out positive because

00:05:41.680 --> 00:05:46.720
there's about a three quarters chance of
a head and it comes out just about there.

00:05:46.720 --> 00:05:52.930
The problem is suppose in our training
data we saw four heads and no tails.

00:05:52.930 --> 00:05:57.340
Well, what that means is we
have a categorical distribution

00:05:57.340 --> 00:05:58.230
in the observed data.

00:05:58.230 --> 00:06:03.570
There are no tails and
that means the bigger Lambda gets

00:06:03.570 --> 00:06:08.290
the higher the conditional log
likelihood of the observed data is.

00:06:08.290 --> 00:06:12.930
Because a lambda value of
positive infinity corresponds to

00:06:12.930 --> 00:06:19.160
a categorical distribution where
the probability of head is 1 and

00:06:19.160 --> 00:06:22.104
the probability of tails equals 0.

00:06:23.360 --> 00:06:26.350
And that's immediately problematic for

00:06:26.350 --> 00:06:28.880
both of the reasons that
were mentioned earlier.

00:06:28.880 --> 00:06:36.310
Firstly, we don't want to
estimate categorical models

00:06:36.310 --> 00:06:41.120
where the certain feature appears, we say
that there's a probability of one of some

00:06:41.120 --> 00:06:45.460
outcome, we want to have smoothing
because normally our data is sparse.

00:06:45.460 --> 00:06:49.970
But also if we're asking our
optimization procedure to optimize

00:06:49.970 --> 00:06:54.690
something were the optimum
value is an infinite value for

00:06:54.690 --> 00:06:58.560
parameter, that's going to be difficult
for the optimization procedure.

00:06:58.560 --> 00:07:02.570
So there's several ways that this
problem has been dealt with,

00:07:02.570 --> 00:07:04.370
from alimentary models.

00:07:04.370 --> 00:07:07.510
And I will mention several
of them while concentrating

00:07:07.510 --> 00:07:12.040
on the use of a prior distribution,
which is the usual used method these days.

00:07:14.240 --> 00:07:18.130
So, in a 4/0 case,
there were two problems.

00:07:18.130 --> 00:07:23.130
So the first problem was the optimal value
of land, that was infinity, which is

00:07:23.130 --> 00:07:28.680
a long trip for an optimization procedure
to find the optimal value of a parameter.

00:07:28.680 --> 00:07:33.020
And indeed it kind of can't, it can just
move it out and out to a large number.

00:07:33.020 --> 00:07:36.040
The second problem is that
we've got no smoothing,

00:07:36.040 --> 00:07:40.390
our loan distribution is going to be
just a spite as the empirical one.

00:07:41.490 --> 00:07:45.060
And again, this is going to commonly
happen without N or P features,

00:07:45.060 --> 00:07:48.463
because we're going to throw
a million features into the model.

00:07:48.463 --> 00:07:53.335
And it's just going to turn out that
we're going to say words beginning

00:07:53.335 --> 00:07:57.871
with GHO well, we only saw two of
them in the training day there,

00:07:57.871 --> 00:08:01.237
and in both cases the word
was a person's name.

00:08:01.237 --> 00:08:06.063
Therefore, we're going to say any time
we see something that starts with GHO it

00:08:06.063 --> 00:08:07.719
has to be a person name, but

00:08:07.719 --> 00:08:12.060
it might turn out that in future data
that's not necessarily the case.

00:08:12.060 --> 00:08:17.370
It might be an organization name, a place
name, or maybe even a word that just

00:08:17.370 --> 00:08:21.200
starts with GHO, like ghoul,
which isn't a proper name at all.

00:08:22.240 --> 00:08:24.080
So how can we solve this problem?

00:08:24.080 --> 00:08:28.560
One crude way to solve this problem is
just to stop the optimization early.

00:08:28.560 --> 00:08:31.080
So we can run an intricate solver and

00:08:31.080 --> 00:08:36.190
say let's just run it between
iterations and stop wherever we are.

00:08:36.190 --> 00:08:40.870
So if we do that the value of
lamba will definitely be finite or

00:08:40.870 --> 00:08:42.800
have grown fairly large.

00:08:42.800 --> 00:08:46.050
And the optimization won't
run an infinite loop.

00:08:46.050 --> 00:08:49.880
And so this was commonly
used in early maxent work.

00:08:49.880 --> 00:08:54.390
So the idea is we, over here,
stop the optimization procedure.

00:08:54.390 --> 00:08:57.960
We're tracking the likelihood up,
making lambda bigger.

00:08:57.960 --> 00:09:01.380
But we simply stop after a few iterations.

00:09:01.380 --> 00:09:06.580
And so, for example, we might stop
when the value of lambda is 5.

00:09:06.580 --> 00:09:10.180
And the end result of that is that
the probability distribution,

00:09:10.180 --> 00:09:15.360
the lambda equals five might be something
like a 99% chance of a head and

00:09:15.360 --> 00:09:17.500
a 1% chance of a tail, and

00:09:17.500 --> 00:09:21.650
we have satisfied this goal of
smoothing the distribution slightly.

00:09:21.650 --> 00:09:23.970
So it's no longer categorical and

00:09:23.970 --> 00:09:28.550
having a optimization procedure
run in a finite amount of time.

00:09:28.550 --> 00:09:33.380
But here's a better way of achieving
that goal and that's to do what's

00:09:33.380 --> 00:09:38.920
referred to as using a prior
distribution and so when we do this

00:09:38.920 --> 00:09:46.030
we talk about using MAP estimation which
stands for maxima a posteriori estimation.

00:09:46.030 --> 00:09:51.080
And the idea of that is we suppose
we have a prior expectation

00:09:51.080 --> 00:09:53.000
that parameter values
shouldn't be very large.

00:09:53.000 --> 00:09:57.990
More strongly,
we can say that our prior expectation

00:09:57.990 --> 00:10:01.670
is that most features are relevant
to the classification and so

00:10:01.670 --> 00:10:05.730
our prior expectations that
feature waits are zero.

00:10:05.730 --> 00:10:10.197
And then, if we see evidence that they're
useful features we'll gradually increase

00:10:10.197 --> 00:10:12.130
the weights of the parameters.

00:10:12.130 --> 00:10:17.620
And so we can imbalance the evidence from
the observed data without prior beliefs.

00:10:17.620 --> 00:10:22.670
The evidence will never totally
defeat the prior and so

00:10:22.670 --> 00:10:25.640
parameters will be smoothed and
kept finite.

00:10:25.640 --> 00:10:29.600
And so we can do this by explicitly
changing the optimization objective,

00:10:29.600 --> 00:10:34.400
to maximum posteriro likelihood, and
so that's what we have down here.

00:10:34.400 --> 00:10:39.580
So now we're going to have a penalized
log likeli hood of condition,

00:10:39.580 --> 00:10:44.830
a penalized conditional log likelihood
of the observed data, which is going to

00:10:44.830 --> 00:10:50.420
be the sum of the prior probability
of the parameter weights, plus

00:10:50.420 --> 00:10:56.380
the previous conditional log likelihood of
the observed data, which is the evidence.

00:10:56.380 --> 00:11:01.370
The most common way to do this
in practice is to the Gaussian,

00:11:01.370 --> 00:11:05.480
which are also known as quadratic or
L 2 priors.

00:11:05.480 --> 00:11:09.400
So our formalization here is to say that,

00:11:09.400 --> 00:11:14.440
let's assume our prior belief is that
each parameter will be distributed

00:11:14.440 --> 00:11:19.610
according to a Gaussian with mean mu and
variant sigma squared.

00:11:19.610 --> 00:11:24.770
And so then the probability of
the parameter having different values

00:11:24.770 --> 00:11:30.050
is being given by this equation for
the normal curve.

00:11:30.050 --> 00:11:35.520
And the reason why we will soon see why
this is also referred to as quadratic or

00:11:35.520 --> 00:11:40.000
L2 priors is this term ends
up not really mattering and

00:11:40.000 --> 00:11:43.070
the important part is this bit in here.

00:11:43.070 --> 00:11:47.880
Where we're getting the square distance
of the parameter value from the mean.

00:11:47.880 --> 00:11:52.770
So parameters are penalized for
their value drifting far from the mean.

00:11:52.770 --> 00:11:55.930
And in practice usually we
take this mean to be zero

00:11:55.930 --> 00:11:59.950
which is saying the feature is
irrelevant to the classification.

00:11:59.950 --> 00:12:04.080
Zero weight features have no
influence on the classification.

00:12:04.080 --> 00:12:06.310
There's this extra parameter down here,

00:12:06.310 --> 00:12:10.600
hyperparameter, as to
how big is the variance.

00:12:10.600 --> 00:12:16.430
And this is going to control easy it is
for the parameter to move away from zero.

00:12:16.430 --> 00:12:18.580
If the variance is very small,

00:12:18.580 --> 00:12:23.010
the optimization will keep the parameter
values cluster close to zero.

00:12:23.010 --> 00:12:27.260
If the variance is very weak they'll
be allowed to walk farther away.

00:12:27.260 --> 00:12:29.650
As a starting off point
of building models,

00:12:29.650 --> 00:12:34.670
just taking 2 sigma squared,
to equal 1, commonly works rather well.

00:12:34.670 --> 00:12:36.030
You can play with this value.

00:12:36.030 --> 00:12:41.950
Commonly what you'll find, for the kind of
sparsely evidence many feature models that

00:12:41.950 --> 00:12:47.790
we build, that making sigma much smaller
than 1, quickly becomes problematic.

00:12:47.790 --> 00:12:53.130
But you can make sigma,
two sigma squared quite a bit larger and

00:12:53.130 --> 00:12:57.850
commonly you can have it sort of
somewhere in the range between half and

00:12:57.850 --> 00:13:00.440
a thousand and
the models will work fairly well.

00:13:01.570 --> 00:13:07.160
And so here's a graph over here that
sort of shows you the optimization

00:13:07.160 --> 00:13:11.370
happens once you've got
a regularization term.

00:13:11.370 --> 00:13:16.850
So we have exactly the same four
to zero distribution of heads and

00:13:16.850 --> 00:13:21.340
tails and then we set
the regularization differently.

00:13:21.340 --> 00:13:23.620
So if you take two sigma
squared equal to one,

00:13:23.620 --> 00:13:26.770
which is fairly strong regularization.

00:13:26.770 --> 00:13:28.650
The maximum is positive so

00:13:28.650 --> 00:13:32.890
it's going to say you're more
likely to get a head than a tail.

00:13:32.890 --> 00:13:38.110
But it's still going to be something
like two thirds head one thirds tail.

00:13:38.110 --> 00:13:43.450
If you take 2 sigma squared equal to 10,
then the regularization is weaker,

00:13:43.450 --> 00:13:48.460
and so the value of lambda is
going to come out at about two.

00:13:48.460 --> 00:13:55.780
And so that's going to say about 95%
chance of a head and 5% chance of a tail.

00:13:55.780 --> 00:14:01.340
Making the two sigma squared
infinite is equivalent to having

00:14:01.340 --> 00:14:06.750
no regularization at all for
this leads us back to our previous model,

00:14:06.750 --> 00:14:10.460
and then the optimal value of
the parameter is infinite.

00:14:10.460 --> 00:14:14.449
So if we use Gaussian priors,
we're putting in a trade off between

00:14:14.449 --> 00:14:19.045
expectation-matching versus
smaller parameter values.

00:14:19.045 --> 00:14:22.655
What this means in practice is
that when multiple features can be

00:14:22.655 --> 00:14:26.055
recruited to explain
a certain observation,

00:14:26.055 --> 00:14:29.925
it's the more common ones that
will receive the most weight.

00:14:29.925 --> 00:14:34.805
So the way to think about this is to
think that in the model formulation,

00:14:34.805 --> 00:14:39.240
that you pay the penalty for
having a non zero parameter weight just

00:14:39.240 --> 00:14:43.970
once whereas you can gain from having
a non-zero parameter weight for

00:14:43.970 --> 00:14:49.800
every data item for which that feature and
its parameter weight will be useful.

00:14:49.800 --> 00:14:54.610
And so therefore a feature that can be
useful a lot of times in explaining data

00:14:54.610 --> 00:15:00.000
items will be allowed to have a high
weight because the prior is basically

00:15:00.000 --> 00:15:04.510
overwhelmed, whereas a feature that can
only help in explaining a very small

00:15:04.510 --> 00:15:09.850
number of observations will have its
weight greatly constrained by the prior.

00:15:11.500 --> 00:15:15.910
And the other good thing to know
about having Gaussian priors is

00:15:15.910 --> 00:15:21.300
putting them in will improve the accuracy
of your classifier, as we'll see.

00:15:21.300 --> 00:15:26.370
Let me just now go very concretely
through what happens in

00:15:26.370 --> 00:15:29.370
the equations that you have
to use when you have a prior.

00:15:29.370 --> 00:15:33.830
So we're now going to use this
penalized conditional log likelihood.

00:15:33.830 --> 00:15:39.620
And so as well as this term from before,
we're adding in this term for the prior.

00:15:40.870 --> 00:15:43.940
And so what is that term for the prior?

00:15:43.940 --> 00:15:49.577
It's the log of the normal distribution
over the parameter weights.

00:15:49.577 --> 00:15:54.179
So if we then go back and
look at this equation here,

00:15:54.179 --> 00:15:57.389
we then have to take the log of this,

00:15:57.389 --> 00:16:01.903
and so
we're going to have effectively out here.

00:16:01.903 --> 00:16:07.348
This is just a constant, and then we're
going to be taking the log of an X and

00:16:07.348 --> 00:16:10.850
that will go away and
be an identity function.

00:16:12.960 --> 00:16:18.310
So what we end up with here
is that we're subtracting

00:16:18.310 --> 00:16:22.860
this term here, where the K over
here is the log of the constant.

00:16:22.860 --> 00:16:29.600
Which we can ignore for the maximization
of the penalized conditional likelihood.

00:16:29.600 --> 00:16:34.580
And so, what we're actually working
out here is this squared term

00:16:34.580 --> 00:16:39.270
that is how far are the parameter values
away from the mean of the distribution.

00:16:39.270 --> 00:16:44.200
And that term is then being
scaled by this 2 sigma squared so

00:16:44.200 --> 00:16:47.990
you should be able to see that
how big you make 2 sigma squared

00:16:47.990 --> 00:16:51.780
sort of balances the trade
off between these two turns.

00:16:53.650 --> 00:16:56.720
And then for
the derivative of the conditional

00:16:56.720 --> 00:17:01.325
penalized conditional log likelihood
we're taking the derivative of this.

00:17:01.325 --> 00:17:07.440
Which is the pop that we saw before,
which is

00:17:07.440 --> 00:17:12.520
the different between that actual weight
and the predicted weight of the feature.

00:17:12.520 --> 00:17:15.380
And then we're subtracting
off the derivative of this.

00:17:15.380 --> 00:17:18.670
And so
that's just an easy quadratic form, so

00:17:18.670 --> 00:17:21.570
the derivative of that is just over here.

00:17:21.570 --> 00:17:28.246
And so what we're then going to say is to
make this zero that our predicted weight.

00:17:28.246 --> 00:17:35.540
The predicted expectation of some feature
being true is going to be a little bit

00:17:35.540 --> 00:17:40.720
less than its actual expectation because
it's being penalized by this amount here.

00:17:42.300 --> 00:17:47.170
To simplify things down one step further,
in practice we almost

00:17:47.170 --> 00:17:51.910
always take the mean out our
prior distribution to be zero

00:17:51.910 --> 00:17:56.780
since the position of least committment
is to ensure the features are.

00:17:56.780 --> 00:18:01.460
So then those equations simplify
down one time further and

00:18:01.460 --> 00:18:08.026
we just get the results shown here
where we just have the lambdas.

00:18:08.026 --> 00:18:11.630
And so,
it's just how big the lambdas are and

00:18:11.630 --> 00:18:14.990
their distance away from 0 that's
determining what we're using.

00:18:16.180 --> 00:18:20.190
Let me go back to the example
of making a maximum entropy

00:18:20.190 --> 00:18:23.340
named entity recognition model
that I've shown you before.

00:18:23.340 --> 00:18:26.150
Actually, when I was
showing the sides before,

00:18:26.150 --> 00:18:29.350
I was leaving out a detail
because the parameter weights.

00:18:29.350 --> 00:18:36.130
This model we're actually estimated in
a model that used calcium prior smoothing.

00:18:36.130 --> 00:18:38.770
And now that we understand
a bit more about that,

00:18:38.770 --> 00:18:42.520
we can see the effect of that in the
estimation of the parameters of the model.

00:18:44.030 --> 00:18:51.790
So here we have two features and
this feature is a conjunction feature.

00:18:51.790 --> 00:18:57.850
The present tag is proper noun and
the previous tag is preposition.

00:18:57.850 --> 00:19:03.130
So this feature is necessarily going to be
much less evidence in the training data

00:19:03.130 --> 00:19:08.110
than this feature which is just that the
current part of speech is a proper noun.

00:19:08.110 --> 00:19:13.110
So the general expectation is that
higher weight should be given

00:19:13.110 --> 00:19:17.790
to the more common features if they
carry as much predictive information.

00:19:17.790 --> 00:19:20.970
And that's what we see here, that you have

00:19:23.215 --> 00:19:26.925
the high weight given to
the more general feature and

00:19:26.925 --> 00:19:30.835
much smaller weight is given
to the more specific feature.

00:19:30.835 --> 00:19:35.210
That is because even though there is some
further evidence from this conjunction

00:19:35.210 --> 00:19:36.450
feature here.

00:19:36.450 --> 00:19:39.660
The fact of the matter is you're
quite likely to have a named entity

00:19:39.660 --> 00:19:42.040
when the part of speech is a proper noun.

00:19:42.040 --> 00:19:46.780
And so most of the weight is going
to the more general features here.

00:19:46.780 --> 00:19:49.740
And we also see the same
effect in some of the other

00:19:49.740 --> 00:19:52.310
feature classes that we
talked about before.

00:19:52.310 --> 00:19:58.100
So, for the two above it, the current
word being grace is a fairly specific and

00:19:58.100 --> 00:20:02.060
rarely evident feature,
whereas the fact that the word begins.

00:20:02.060 --> 00:20:05.750
Where the capital letter G is
a much more general feature.

00:20:05.750 --> 00:20:10.090
And so, that we're seeing most of
the weight for this being good evidence

00:20:10.090 --> 00:20:15.450
that it's a person rather than a location
going to this much more general feature.

00:20:15.450 --> 00:20:17.200
And if we look down further below,

00:20:18.270 --> 00:20:22.290
you can maybe see other cases
of the same thing happening.

00:20:22.290 --> 00:20:25.110
But you should also realize that

00:20:25.110 --> 00:20:30.060
when there is a lot of extra
information from a feature conjunction,

00:20:30.060 --> 00:20:33.520
then that feature conjunction
will get a lot of weight still.

00:20:33.520 --> 00:20:37.890
And we saw an example of that
with these two cases here.

00:20:39.010 --> 00:20:44.900
So that we discussed how, since most
tokens should be classified as not

00:20:44.900 --> 00:20:50.420
an entity that if you just simply know
that the previous word was not an entity.

00:20:50.420 --> 00:20:53.500
Most likely the next node is
not an entity as well and so

00:20:53.500 --> 00:20:58.260
you get this highly negative weight for
both entity classes.

00:20:58.260 --> 00:21:02.990
But if you know that the previous
word was not an entity, but

00:21:02.990 --> 00:21:05.810
you also know that the current
word is capitalized.

00:21:05.810 --> 00:21:10.060
Then in that case there's strong
evidence that you should classify

00:21:10.060 --> 00:21:14.510
things differently, and so this
feature does get a significant weight.

00:21:14.510 --> 00:21:20.050
Even though its conjunction feature that
involves this other previous feature.

00:21:20.050 --> 00:21:24.340
You just get a kind of a lot of evidence
from knowing that conjunction is true.

00:21:25.410 --> 00:21:31.300
Okay here's an example showing the effects
of using a Gaussian smoothing for

00:21:31.300 --> 00:21:32.870
estimating a maximum entropy model.

00:21:32.870 --> 00:21:36.110
And this is an example from
learning a part of speech tagger

00:21:36.110 --> 00:21:40.970
from the work of Christina Toutanova and
me and others at Stanford.

00:21:40.970 --> 00:21:45.060
Okay, and
this example shows the typical effects of

00:21:45.060 --> 00:21:48.870
what happens with smooth
versus unsmooth models.

00:21:50.550 --> 00:21:55.050
So what we find is that, so we've trained

00:21:55.050 --> 00:21:59.880
both models for
up to 360 iterations up here.

00:22:01.320 --> 00:22:04.160
One thing we find that's
just good to know and

00:22:04.160 --> 00:22:10.170
this is looking at
accuracy on test data that

00:22:10.170 --> 00:22:15.060
the model with smoothing performs a lot
better than the model without smoothing.

00:22:15.060 --> 00:22:18.600
Okay, that's about 0.6
of a percent better, but

00:22:18.600 --> 00:22:21.780
part of speech tagging
generally is highly accurate.

00:22:21.780 --> 00:22:26.680
And so it's better off thinking
of this as error reduction, so

00:22:26.680 --> 00:22:32.281
if you're reducing the error
rate from 3.5% down to 2.9%,

00:22:32.281 --> 00:22:35.650
that's actually quite
a large error reduction.

00:22:37.160 --> 00:22:40.550
Okay, but
we can see a bit more than just that.

00:22:40.550 --> 00:22:46.570
So you should see here that for the model
with no smoothing, it shows this very

00:22:46.570 --> 00:22:50.760
typical pattern that you used to observe
for the training of max int models.

00:22:50.760 --> 00:22:55.130
That as you train the model,
initially the accuracy

00:22:55.130 --> 00:22:59.700
improves strongly up to a peak And
then it starts to decline.

00:22:59.700 --> 00:23:04.000
So this is where you see the over fitting
of the model to the train of data

00:23:04.000 --> 00:23:07.290
in a way that actually means
it does worse at test time.

00:23:07.290 --> 00:23:11.040
And so
common practice was to stop training of

00:23:11.040 --> 00:23:15.660
the max-end models at
around iteration 100.

00:23:15.660 --> 00:23:19.350
This is sort of being a little bit
unfair to what normally happened

00:23:19.350 --> 00:23:20.370
without smoothing.

00:23:20.370 --> 00:23:24.560
If you did early stopping that you'd
be getting inaccuracy of something

00:23:25.820 --> 00:23:28.410
like 96.87.

00:23:28.410 --> 00:23:34.780
Nevertheless, that's still well less
than the accuracy of the smooth model.

00:23:35.990 --> 00:23:41.000
On the other hand, if you have a smooth
model, that the smooth model's likelihood

00:23:41.000 --> 00:23:47.890
just nicely increases until it converges,
and optimization stops.

00:23:47.890 --> 00:23:49.663
Giving us this performance here.

00:23:49.663 --> 00:23:54.603
There's actually a second effect
that's very interesting as if we focus

00:23:54.603 --> 00:23:59.075
in on particular on the performance
of the model on unknown words.

00:23:59.075 --> 00:24:03.790
Now unknown words The part-of-speech
tag distribution is

00:24:03.790 --> 00:24:08.783
being estimated by using special
features that generalize over

00:24:08.783 --> 00:24:13.740
words such as what letters do
they begin with and end with.

00:24:13.740 --> 00:24:18.610
But nevertheless a lot of those
features are themselves pretty sparse.

00:24:18.610 --> 00:24:22.705
And so, what we find is that
the model without smoothing

00:24:22.705 --> 00:24:27.640
especially over fits and
tends to do badly on the unknown

00:24:27.640 --> 00:24:32.770
words where the model with smoothing
is 23% better here on the unknown word.

00:24:32.770 --> 00:24:35.240
So that's a very significant increase.

00:24:35.240 --> 00:24:37.075
And very helpful to applications.

00:24:37.075 --> 00:24:41.645
because performance on words
that the model wasn't trained on

00:24:41.645 --> 00:24:44.855
is especially important to the good
performance of applications.

00:24:46.085 --> 00:24:48.705
So smoothing is just good.

00:24:48.705 --> 00:24:54.490
It softens distributions, it pushes the
weight onto more explanatory features, it

00:24:54.490 --> 00:24:58.900
allows you to dump more and more features
into the model without a lot of problems.

00:24:58.900 --> 00:25:02.580
And at least if you don't
do at least stopping,

00:25:02.580 --> 00:25:06.110
it speeds up the convergence
of your models.

00:25:06.110 --> 00:25:08.630
Let me just give one terminology note.

00:25:08.630 --> 00:25:10.232
I'm talking of priors and

00:25:10.232 --> 00:25:15.430
maximal posterior estimation is
language from Bayesian statistics.

00:25:15.430 --> 00:25:20.727
In frequentist statistics, people will
instead talk about using regularization of

00:25:20.727 --> 00:25:25.765
XMD models, in particular Gaussian
priors called L2 regularization.

00:25:25.765 --> 00:25:29.675
I'm not really going to get into that, all
you really need to know is that the math

00:25:29.675 --> 00:25:32.809
comes out the same it doesn't
matter what name you choose.

00:25:33.990 --> 00:25:38.130
Let me just then quickly mention
two other ways of doing smoothing.

00:25:38.130 --> 00:25:43.960
Another way of thinking about smoothing
is to smooth the data not the parameters.

00:25:43.960 --> 00:25:47.210
And we saw that earlier when
talking about language models.

00:25:47.210 --> 00:25:52.280
So, if the distribution we actually
saw was four heads and zero tails.

00:25:52.280 --> 00:25:55.780
We can then smooth that by
doing add-one smoothing say.

00:25:55.780 --> 00:25:59.050
And say we got five heads and one tail.

00:25:59.050 --> 00:26:00.680
And well then we've solved our problem.

00:26:00.680 --> 00:26:04.850
Because if we then do maximum
conditional likelihood estimation

00:26:04.850 --> 00:26:09.540
that we're setting the value of
the parameter to something like 1.2.

00:26:09.540 --> 00:26:14.130
And it's got a finite value and
we don't have a problem.

00:26:15.740 --> 00:26:20.610
That works fine for
a very simple example like this.

00:26:20.610 --> 00:26:25.500
The reason that that's not practical for
models with millions of parameters

00:26:25.500 --> 00:26:30.570
is it becomes almost impossible to
know how to create artificial data.

00:26:30.570 --> 00:26:36.060
In such a way, that every,
parameter, hasn't gone zero value,

00:26:36.060 --> 00:26:41.120
without, creating,
a normal amount of artificial data,

00:26:41.120 --> 00:26:44.890
so the amount of artificial data
overwhelm the amount of real data.

00:26:46.210 --> 00:26:49.180
A final thing that's commonly done in

00:26:49.180 --> 00:26:52.890
NLP models is to use count
cutoffs with features.

00:26:52.890 --> 00:26:59.480
So the idea there is to calculate features
for each data item in your model.

00:26:59.480 --> 00:27:01.190
And then you look at the features and

00:27:01.190 --> 00:27:04.730
what their empirical support
in the training data is.

00:27:04.730 --> 00:27:09.360
Then you simply say something like, okay
for any feature that was observed three

00:27:09.360 --> 00:27:13.440
times or less in the training data, I'm
just going to dump it from my model, and

00:27:13.440 --> 00:27:15.660
estimate the model over
the remaining features.

00:27:17.180 --> 00:27:22.860
In the discussion of smoothing, this is
a weak and indirect smoothing method.

00:27:22.860 --> 00:27:25.210
So effectively what it's doing,

00:27:25.210 --> 00:27:29.690
is saying we're going to estimate
the weight of all rare features as zero.

00:27:30.900 --> 00:27:36.760
Which you can also think of a signing
their Gaussian prior with zero variance,

00:27:36.760 --> 00:27:41.250
and mean zero, so that their weights
can never move away from zero.

00:27:41.250 --> 00:27:45.080
So dropping low counts does remove
the features which are most in need of

00:27:45.080 --> 00:27:46.390
smoothing.

00:27:46.390 --> 00:27:52.080
And it does speed up estimation of
the model by reducing the model size.

00:27:52.080 --> 00:27:55.970
But it's a very crude method
of doing smoothing, and

00:27:55.970 --> 00:28:01.000
so the message I'd like to give is count

00:28:01.000 --> 00:28:05.790
cutoffs generally hurt accuracy in
the presence of proper smoothing.

00:28:05.790 --> 00:28:09.020
A lot of people got into
the habit of using count cutoffs

00:28:09.020 --> 00:28:13.650
in the days before regularized models
because in those cases, it would usually

00:28:13.650 --> 00:28:18.140
helped to use count cutoffs because you
got less overfitting of your model.

00:28:18.140 --> 00:28:22.880
But with proper smoothing, you shouldn't
need to use count cutoffs to get the best

00:28:22.880 --> 00:28:27.650
possible model, that doesn't mean that
there's no reason to use count cutoffs.

00:28:27.650 --> 00:28:30.700
The most common reason to use
count cutoffs in practice

00:28:30.700 --> 00:28:33.640
is because you want to shrink
the size of your model.

00:28:33.640 --> 00:28:37.330
That ten million parameters take
a lot of memory to store and

00:28:37.330 --> 00:28:41.900
you might prefer to build a model
that only has 300,000 parameters and

00:28:41.900 --> 00:28:45.980
then obviously what you want to do
is keep the most useful features.

00:28:45.980 --> 00:28:50.320
Which will be normally basically
the ones that have significant

00:28:50.320 --> 00:28:52.660
frequency of occurrence in the data.

00:28:52.660 --> 00:28:59.520
Okay, so that's the end of this session
on smoothing or priors for maxent models.

00:28:59.520 --> 00:29:05.550
In this section we've only talked
about use of Gaussian or L2 priors.

00:29:05.550 --> 00:29:09.060
And in recent work there has been quite
a bit of discussion of using other priors,

00:29:09.060 --> 00:29:12.530
in particular there's
common use of L1 priors,

00:29:12.530 --> 00:29:16.950
which is a different way of cutting down
the number of features in your model.

00:29:16.950 --> 00:29:20.770
You may have seen that if you've seen
other things like machine learning.

00:29:20.770 --> 00:29:23.610
But I'm not going to discuss
that in these classes.

