WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.658
[MUSIC]

00:00:04.658 --> 00:00:09.011
Stanford University.

00:00:09.011 --> 00:00:13.705
&gt;&gt; All right, hello everybody and welcome
to what I personally think is one of

00:00:13.705 --> 00:00:18.099
the most fun lectures now we can
really just assume we have all these

00:00:18.099 --> 00:00:21.549
basic lego blocks and
we can build some awesome stuff.

00:00:21.549 --> 00:00:25.470
Namely, dynamic neural networks for
question answering.

00:00:25.470 --> 00:00:29.030
In terms of organization,
there's not too much going on today.

00:00:29.030 --> 00:00:34.570
Except, hopefully, all of you are working
hard on PA4 and on your projects.

00:00:34.570 --> 00:00:37.890
I'll have my project advice office
hour today right after class.

00:00:37.890 --> 00:00:39.120
QueueStatus 90.

00:00:39.120 --> 00:00:41.370
I'll be on hangout and everything.

00:00:43.050 --> 00:00:44.610
Any questions around the projects?

00:00:46.370 --> 00:00:50.834
Where you want to be?

00:00:53.770 --> 00:00:57.980
Nope?
All right, so let's get started.

00:00:57.980 --> 00:01:04.850
Now why is PA4 so open ended and why do we
think it's so exciting as a problem set?

00:01:04.850 --> 00:01:09.215
Basically, the question
that I wanna ask today, and

00:01:09.215 --> 00:01:13.774
maybe you can also ask yourself
during your projects and

00:01:13.774 --> 00:01:18.333
your problem sets is whether
we may be able to cast all NLP

00:01:18.333 --> 00:01:22.420
tasks actually as a question
answering problem.

00:01:23.430 --> 00:01:28.785
And this question is what led us to
invent the dynamic memory network,

00:01:28.785 --> 00:01:31.250
which we'll talk about today.

00:01:31.250 --> 00:01:34.050
So what do I mean by this?

00:01:34.050 --> 00:01:39.280
Let's go through a couple of examples of
what question answering might look like.

00:01:39.280 --> 00:01:43.921
The first one here is sort of the standard
question answering type of problem,

00:01:43.921 --> 00:01:46.942
where we have some inputs,
we have a question, and

00:01:46.942 --> 00:01:51.590
we need to do some logical reasoning
maybe to answer that question.

00:01:51.590 --> 00:01:54.580
So for instance, here the inputs
are Mary walked to the bathroom,

00:01:54.580 --> 00:01:57.318
Sandra went to the garden,
Daniel went back to the garden.

00:01:57.318 --> 00:02:01.158
Sandra took the milk there and
now we ask where is the milk.

00:02:01.158 --> 00:02:05.643
Now, in order to do this, we'd have to
actually do a little bit of reasoning

00:02:05.643 --> 00:02:08.586
because if you just try to
retrieve the sentence or

00:02:08.586 --> 00:02:10.918
the last sentence that mentions milk.

00:02:10.918 --> 00:02:14.451
Well then, it will tell you
sender talked milk "there" and

00:02:14.451 --> 00:02:18.450
you don't know where Sandra is and
hence what "there" refers to.

00:02:18.450 --> 00:02:21.040
So now I have to do
some effort resolution.

00:02:22.630 --> 00:02:28.527
If we were to try to build sort of a hand
tuned old school machine learning NLP

00:02:28.527 --> 00:02:34.546
system where we put our human knowledge
into the task to answer that question.

00:02:34.546 --> 00:02:39.234
And if we were to do that then we'd
realize, all right let's try to find out

00:02:39.234 --> 00:02:43.532
where Sandra is and then we look at
this sentence and see that she is

00:02:43.532 --> 00:02:48.016
in the garden last and then we could
answer the question correctly.

00:02:48.016 --> 00:02:52.240
Now, by the end of this class, you will
know a model that you don't have to give

00:02:52.240 --> 00:02:54.170
any of that kind of information to.

00:02:54.170 --> 00:03:00.030
It will actually just learn all of
this from examples of this kind.

00:03:01.560 --> 00:03:03.897
Now that's a standard Q&amp;A problem, but

00:03:03.897 --> 00:03:07.522
you can also look at sentiment
where we might have an input here.

00:03:07.522 --> 00:03:08.920
Everybody is happy.

00:03:08.920 --> 00:03:10.960
And again we have the question
with the sentiment at the end, so

00:03:10.960 --> 00:03:13.880
it's just the label.

00:03:13.880 --> 00:03:16.914
Essentially the word pertaining
to that label, namely positive.

00:03:19.878 --> 00:03:21.890
Now we can go further.

00:03:21.890 --> 00:03:26.691
We had the task of sequence tagging,
nicknamed entity recognition and

00:03:26.691 --> 00:03:28.378
part of speech tagging.

00:03:28.378 --> 00:03:33.251
And we can also ask what are the named
entities and then we might want to

00:03:33.251 --> 00:03:39.120
obtain either a list that could include
a lot of Os for not the named entity.

00:03:39.120 --> 00:03:42.456
Or just a list of the actual
words that are named entities.

00:03:42.456 --> 00:03:46.837
And again, input question-answer,
triplet, that we would need for

00:03:46.837 --> 00:03:48.740
training, that kinda model.

00:03:50.140 --> 00:03:52.310
Same with what are the part
of speech tags.

00:03:52.310 --> 00:03:54.670
Every word has a part of speech tag, so,

00:03:54.670 --> 00:03:59.440
that's a sequence of the same
length as the input.

00:03:59.440 --> 00:04:01.450
And we can even go as far as and

00:04:01.450 --> 00:04:06.220
this is starting to be question
of how useful it is but

00:04:06.220 --> 00:04:10.720
where we can cast also machine translation
as a question answering problem, right?

00:04:10.720 --> 00:04:15.128
In the end most every NLP problem
has some input, some questions and

00:04:15.128 --> 00:04:18.157
answer about it and
some output some answers.

00:04:18.157 --> 00:04:23.033
So now with that in mind,
wouldn't it be amazing if we were able

00:04:23.033 --> 00:04:27.632
to build a single joint model for
general question answering.

00:04:28.650 --> 00:04:32.810
Can basically learn from any input
question answer triplet dataset.

00:04:35.514 --> 00:04:37.956
Now towards a single joint model for

00:04:37.956 --> 00:04:42.230
discompetence of QA there're
two major obstacles.

00:04:42.230 --> 00:04:44.400
So lets talk about what they are.

00:04:44.400 --> 00:04:49.406
The first one is that we don't even have a
single architecture, or up until last year

00:04:49.406 --> 00:04:54.202
when we published a paper that solves all
these, there was no single architecture

00:04:54.202 --> 00:04:58.382
that consistently got state-of-the-art
across these different tasks.

00:04:58.382 --> 00:05:02.510
So for question answering we have
strongly supervised memory networks.

00:05:02.510 --> 00:05:03.480
Sentiment analysis.

00:05:03.480 --> 00:05:06.064
We actually used tree structured LSTM so

00:05:06.064 --> 00:05:11.462
similar to the recursive neural networks
that Chris talked about two lectures ago.

00:05:11.462 --> 00:05:17.132
But using the ideas of having various
gates as you combine words and

00:05:17.132 --> 00:05:21.970
phrases into phrase
vectors of longer length.

00:05:21.970 --> 00:05:27.170
And for part of speech tagging it used to
be a bidirectional LSTM conditional random

00:05:27.170 --> 00:05:31.890
field, another type model family that we
didn't go into any details in this class.

00:05:31.890 --> 00:05:34.902
And sort of in the graphical models world.

00:05:34.902 --> 00:05:39.473
What you do notice is that all the state
of the art models have some kind of neural

00:05:39.473 --> 00:05:41.950
network in them somewhere these days.

00:05:41.950 --> 00:05:45.560
That's one of the reasons we
merged from 224D into 224N.

00:05:45.560 --> 00:05:50.210
Felt like you really need to know
these basic building blocks.

00:05:50.210 --> 00:05:52.950
But there are still different kinds
of models that make different kinds

00:05:52.950 --> 00:05:53.470
of assumptions.

00:05:53.470 --> 00:05:56.460
And we would call these different
kinds of architectures all right.

00:05:56.460 --> 00:05:59.528
Like so the architectures
that we've talked about so

00:05:59.528 --> 00:06:02.500
far are sort of bag of words,
window-based.

00:06:02.500 --> 00:06:06.812
Convolutional neural network, recurrent
neural networks, LSTMs and so on.

00:06:06.812 --> 00:06:08.930
So that's the first major obstacle.

00:06:08.930 --> 00:06:10.540
Now the second one,

00:06:10.540 --> 00:06:14.950
is that fully joined multitask
learning is actually incredibly hard.

00:06:16.020 --> 00:06:20.800
And what I mean by saying that, is that
we don't just wanna share some parts

00:06:20.800 --> 00:06:25.380
of the model like word vectors, which
we're already pretty good at sharing.

00:06:25.380 --> 00:06:28.700
But we wanna have the exact
same decoder or classifier.

00:06:28.700 --> 00:06:33.077
And we don't wanna just transfer between
a single source and one target task but

00:06:33.077 --> 00:06:35.572
we wanna ideally train
both of them jointly.

00:06:35.572 --> 00:06:40.736
So in computer vision I'll encourage you
all to take 231 next quarter I think

00:06:40.736 --> 00:06:45.952
when it's getting offered on convolutional
neural networks for computer vision.

00:06:45.952 --> 00:06:49.882
Computer vision's actually
better at able to share more of

00:06:49.882 --> 00:06:52.094
the layers as you go up the stack.

00:06:52.094 --> 00:06:58.008
Whereas an NLP, so far, when you try to
do multitask learning and share weights,

00:06:58.008 --> 00:07:03.000
what we've been mostly able to do so
far is to share the word vectors.

00:07:03.000 --> 00:07:04.370
And then we train the word vectors and

00:07:04.370 --> 00:07:08.600
then we initialize some other model
with those pre-trained word vectors.

00:07:08.600 --> 00:07:14.070
Nobody yet consistently got improvements,
though it's an active area of research,

00:07:14.070 --> 00:07:17.565
to share deeper layers of the LSTM,
for instance.

00:07:17.565 --> 00:07:20.060
So we trained the system
on machine translation, and

00:07:20.060 --> 00:07:24.218
then just changed the last layer of the
soft max email send machine can analysis,

00:07:24.218 --> 00:07:25.391
or something like that.

00:07:25.391 --> 00:07:31.472
It hasn't been any paper yet
on showing improvements for that.

00:07:31.472 --> 00:07:35.544
And even worse,
it's hard to publish negative results and

00:07:35.544 --> 00:07:38.966
you'll only ever read
about successful transfer

00:07:38.966 --> 00:07:43.780
learning cases because the unsuccessful
ones don't get published.

00:07:43.780 --> 00:07:48.200
But when you actually do research in
this area you'll notice that as soon as

00:07:48.200 --> 00:07:51.450
the tasks are not directly related,
they actually tend to hurt each other.

00:07:51.450 --> 00:07:54.990
So if you're trying to train two
tasks together in one model,

00:07:54.990 --> 00:08:00.020
say you just have two soft maxes on
the same Hidden state of your LSTMs.

00:08:00.020 --> 00:08:04.000
It turns to actually get
worse in many cases, too.

00:08:04.000 --> 00:08:08.460
So those are some of
the two major obstacles.

00:08:08.460 --> 00:08:11.640
Now, the dynamic memory networks
that I'll talk to you today about

00:08:11.640 --> 00:08:13.680
only tackle the first obstacle.

00:08:13.680 --> 00:08:17.590
As in their an architecture which
still has hyper parameters.

00:08:17.590 --> 00:08:20.240
That might differ for different tasks, but

00:08:20.240 --> 00:08:24.430
it's at least the same general modules
that you have in that architecture.

00:08:26.163 --> 00:08:31.425
And basically it can tackle all these
different tasks that I described to you,

00:08:31.425 --> 00:08:33.270
at least in some capacity.

00:08:33.270 --> 00:08:36.320
And several of them actually
at the state of the art level

00:08:36.320 --> 00:08:37.370
at the time of publication.

00:08:39.680 --> 00:08:43.332
Any questions around these obstacles and
sharing and

00:08:43.332 --> 00:08:45.610
the idea of multitask learning?

00:08:47.852 --> 00:08:52.803
So basically, we're thinking of
multitask learning through the lens

00:08:52.803 --> 00:08:57.540
of seeing everything as a question
over some kind of input.

00:08:57.540 --> 00:08:58.530
That has some kind of answer.

00:09:06.351 --> 00:09:08.769
So let's look at the high level idea for

00:09:08.769 --> 00:09:12.560
answering these tougher
kinds of questions.

00:09:12.560 --> 00:09:17.505
So imagine you had this story each of
the facts is relatively simple and

00:09:17.505 --> 00:09:18.400
straightforward.

00:09:18.400 --> 00:09:20.620
John went to the hallway and
things like that.

00:09:20.620 --> 00:09:26.164
But imagine I now asked you,
after you read it, where's Sandra?

00:09:26.164 --> 00:09:33.233
And you might have to actually try to
retrieve the episode in which that fact,

00:09:33.233 --> 00:09:39.350
or the answer to that question
was actually mentioned.

00:09:39.350 --> 00:09:44.070
And so, in some cases, as we saw with
the football, you may actually have to go

00:09:44.070 --> 00:09:50.710
multiple times over the input to
answer that question correctly.

00:09:50.710 --> 00:09:55.950
This is kind of what led us to this
idea of Dynamic Memory Network.

00:09:55.950 --> 00:09:59.490
This is the kind of architecture you
would see a lot in papers, and so

00:09:59.490 --> 00:10:02.160
we're going to walk through it
first on a high level intuitively,

00:10:02.160 --> 00:10:07.690
and then we'll zoom into the different
areas to gain a better understanding.

00:10:07.690 --> 00:10:11.230
Now, the first thing you will notice is,
we have different modules.

00:10:11.230 --> 00:10:14.270
This is what I call sort
of model components.

00:10:14.270 --> 00:10:19.940
And the reason we're basically
separating them out is that, I think,

00:10:19.940 --> 00:10:25.380
eventually, you have to do deep learning
research and also general engineering.

00:10:26.400 --> 00:10:30.970
Through, or with the help of good
software engineering practices.

00:10:30.970 --> 00:10:34.430
Where you have different modules,
they define interfaces and

00:10:34.430 --> 00:10:39.290
you might be able to switch out one module
with another one, but that doesn't mean

00:10:39.290 --> 00:10:43.020
you have to change all the other
modules in your larger architecture.

00:10:43.020 --> 00:10:48.750
So that's generally a good
sort of modeling framework.

00:10:48.750 --> 00:10:52.540
Now, what does this
dynamic memory network do?

00:10:52.540 --> 00:10:57.010
So it starts with having simple word
vectors like glove or word2vec.

00:10:57.010 --> 00:11:01.883
And basically we'll have a recurring
neural sequence model such as

00:11:01.883 --> 00:11:04.104
GRU that goes over the input and

00:11:04.104 --> 00:11:08.990
just computes the hidden state at
every word and at every sentence.

00:11:10.770 --> 00:11:15.350
So a standard GRU that we have
defined very carefully before.

00:11:16.460 --> 00:11:19.410
Now, that's just independent
of the question.

00:11:19.410 --> 00:11:24.300
It will just basically compute
a hidden state for every word.

00:11:24.300 --> 00:11:29.160
Now, it will have also a GRU for
the question in the question module.

00:11:29.160 --> 00:11:34.480
In fact, sometimes, you can share also
the waits between the question and

00:11:34.480 --> 00:11:37.620
the input, here, these GRUs.

00:11:37.620 --> 00:11:40.760
And we'll basically use a GRU
to compute a question vector.

00:11:40.760 --> 00:11:44.840
That question vector Q is just
going to be the last hidden state

00:11:44.840 --> 00:11:47.810
of the GRU after it went for
every word of the question.

00:11:49.731 --> 00:11:53.940
Now, and this is where
the interesting stuff starts.

00:11:53.940 --> 00:11:56.110
We'll use these attention mechanisms,

00:11:56.110 --> 00:11:58.672
that you've learned about
before in a very specific way.

00:11:58.672 --> 00:12:01.850
We will define it soon, but we'll
have this attention mechanism that is

00:12:01.850 --> 00:12:06.600
essentially triggered by
the question to go over the inputs.

00:12:06.600 --> 00:12:09.300
So here the question is,
where's the football?

00:12:09.300 --> 00:12:14.170
And now basically we would assume that
the fact that football was mentioned

00:12:14.170 --> 00:12:20.000
in this question is stored somewhere
inside the hidden state of.

00:12:20.000 --> 00:12:23.894
That last time step GRU and
we call that Q.

00:12:23.894 --> 00:12:28.552
And we use Q to essentially trigger an
attention mechanism over all the potential

00:12:28.552 --> 00:12:29.720
inputs.

00:12:29.720 --> 00:12:35.037
And whenever there is a strong attention
being paid to a specific sentence,

00:12:35.037 --> 00:12:39.452
we're going to give that sentence
as input to yet another GRU.

00:12:39.452 --> 00:12:42.110
And that is in the episodic memory module.

00:12:42.110 --> 00:12:44.920
Now, whenever we have
a line like this here,

00:12:44.920 --> 00:12:49.800
we basically assume that is a some kind of
recurrent neural network sequence model.

00:12:49.800 --> 00:12:53.290
So basically, a question triggers
an attention mechanism that

00:12:53.290 --> 00:12:56.590
goes over all the hidden
states of all my inputs, and

00:12:56.590 --> 00:13:02.520
now basically says this fact seems
relevant for this question at hand.

00:13:02.520 --> 00:13:04.450
So for instance, where is the football?

00:13:04.450 --> 00:13:08.420
Well, the last sentence that
mentions football seems relevant.

00:13:08.420 --> 00:13:11.430
And the hidden state of
this GRU captures that

00:13:11.430 --> 00:13:13.220
there is something mentioned
about a football and

00:13:13.220 --> 00:13:16.810
the hidden state of this GRU captures
that there's something about football.

00:13:16.810 --> 00:13:20.620
So we restore this and
now this GRU agglomerates

00:13:20.620 --> 00:13:24.150
only the facts that are pertinent or
relevant to the question at hand.

00:13:25.720 --> 00:13:28.970
So it's essentially a filtering

00:13:28.970 --> 00:13:33.880
GRU that tries to only keep track of
what's relevant for the current question.

00:13:33.880 --> 00:13:38.570
And now, we'll define this memory state
here as the last hidden state of that GRU.

00:13:41.330 --> 00:13:45.340
Now, for the next iteration, because
again here, John put down the football.

00:13:45.340 --> 00:13:50.800
We don't know where John is, so
we now have stored in this vector m,

00:13:50.800 --> 00:13:53.690
that in order to answer the current
question it seems pertinent, and

00:13:53.690 --> 00:13:55.970
again the model learns all of
these things of course by itself.

00:13:55.970 --> 00:14:00.494
We don't manually tell it these are
objects, these are people, or colors, or

00:14:00.494 --> 00:14:04.814
anything like that, but we basically now
store in this vector M that John and

00:14:04.814 --> 00:14:09.563
football seem relevant to the question
at hand, namely, where's the football?

00:14:09.563 --> 00:14:13.228
And so, as we go over the input again,
we'll take M and

00:14:13.228 --> 00:14:17.215
Q into consideration in order
to answer this question, and

00:14:17.215 --> 00:14:23.950
basically pay attention to now every fact
that mentions both John, or the football.

00:14:23.950 --> 00:14:27.960
And so, in this case here and
these are realistic numbers

00:14:27.960 --> 00:14:31.230
you basically pay a lot of attention
to John move to the bedroom and

00:14:31.230 --> 00:14:36.480
John went to the hallway and that is
the last sentence that seems relevant.

00:14:36.480 --> 00:14:38.710
And so, the attention scores for

00:14:38.710 --> 00:14:42.420
a subsequent and
previous sentences here are very low.

00:14:42.420 --> 00:14:46.848
Restore this, and
now M is giving us input, and so

00:14:46.848 --> 00:14:52.015
the zero time step to give to another G or
U that then just has

00:14:52.015 --> 00:14:58.882
our standard soft max classifier at
every hidden state to produce an answer.

00:14:58.882 --> 00:15:04.390
So that was a lot to digest,
it was probably the most complex model.

00:15:04.390 --> 00:15:08.680
We've looked at so far, but
all the components of this model

00:15:08.680 --> 00:15:12.470
we've already discussed, but this is
kind of where our research is right now.

00:15:12.470 --> 00:15:18.680
There are lot of folks who are trying
to find new kinds of architectures for

00:15:18.680 --> 00:15:20.110
different kinds of problems.

00:15:20.110 --> 00:15:23.920
In this particular case, we're trying to
find a general architecture that should be

00:15:23.920 --> 00:15:29.930
possible or should be usable across
a whole host of different kinds of tasks.

00:15:29.930 --> 00:15:34.000
So we'll zoom in in a second into
these different modules, but are there

00:15:34.000 --> 00:15:38.486
any questions about the general idea
of the model, the general architecture?

00:15:45.986 --> 00:15:47.580
Great.
So what are the two different tracks of

00:15:47.580 --> 00:15:48.630
the episodic memory module?

00:15:48.630 --> 00:15:54.790
Essentially, these tracks are mirrored
perfectly with the input.

00:15:54.790 --> 00:15:58.420
So there are always as many time steps in
the episodic memory module here as there

00:15:58.420 --> 00:15:59.670
are time steps in the input.

00:16:01.270 --> 00:16:05.500
But the model either can
decide with a classifier or

00:16:05.500 --> 00:16:08.170
just goes over the input
of fix number of times.

00:16:08.170 --> 00:16:09.960
Somewhere between two to five.

00:16:09.960 --> 00:16:14.296
And every time it goes over it,
it tries to basically pay

00:16:14.296 --> 00:16:18.738
attention to different kinds
of sentences in the input.

00:16:18.738 --> 00:16:23.193
Depending on what the question is and what
it has so far agglomerated in terms of

00:16:23.193 --> 00:16:28.370
relevant facts from the previous time
step, or previous episode in this case.

00:16:28.370 --> 00:16:32.200
So here, again, we go over it,
the input, the first time.

00:16:32.200 --> 00:16:35.810
Store things and
facts about John and the football.

00:16:35.810 --> 00:16:41.477
And then, the second time, we'll now
pay attention to John facts, too.

00:16:41.477 --> 00:16:47.360
And so, intuitively again, the first pass,
I ask where's the football?

00:16:47.360 --> 00:16:50.660
And at this state here, John moves to
the bedroom just doesn't seem relevant

00:16:50.660 --> 00:16:52.150
to ask the question,
to answer the question.

00:16:53.390 --> 00:16:56.140
Cuz John moving to the bedroom has
nothing to do with the football.

00:16:57.170 --> 00:17:02.070
So you have to, in order to do
transitive reasoning, as in A to B,

00:17:02.070 --> 00:17:03.863
as in B to C, and C and D.

00:17:03.863 --> 00:17:09.908
Now, if you wanna go from A to D, you need
to understand that steps to get there.

00:17:09.908 --> 00:17:13.230
So this kinda transitive
reasoning capability,

00:17:13.230 --> 00:17:17.510
you can only get if you have
multiple passes over your info.

00:17:33.246 --> 00:17:37.463
So the question is, if we had some
adverbs or temporal expressions and

00:17:37.463 --> 00:17:41.752
asked sort of different kinds of
questions like was John there before or

00:17:41.752 --> 00:17:44.975
various other kinds of questions,
like that.

00:17:44.975 --> 00:17:48.935
And the answer is, there's
surprisingly many things it can learn,

00:17:48.935 --> 00:17:51.153
if it has seen them in the training data.

00:17:51.153 --> 00:17:55.805
So this kinda model will not be able
to come out of completely new kinds of

00:17:55.805 --> 00:18:00.130
reasoning and types of reasoning, but if
you show it a lot of temporal reasoning,

00:18:00.130 --> 00:18:04.240
it will be able to answer
those kinds of questions.

00:18:04.240 --> 00:18:08.561
So I don't see why it couldn't,
for some theoretical reason,

00:18:08.561 --> 00:18:13.371
answer the questions of where was he
before that, or things like that.

00:18:24.602 --> 00:18:25.245
That's right.

00:18:25.245 --> 00:18:28.055
So m1, so it's m superscript 1,
that's right.

00:18:28.055 --> 00:18:32.145
So the question is, in the first
pass we mostly use the question.

00:18:32.145 --> 00:18:36.255
It turns out we'll actually copy
the question twice and in the second pass

00:18:36.255 --> 00:18:40.165
we'll replace the second copy of
q with m1 but sort of a detail.

00:18:40.165 --> 00:18:45.310
But in general, yes, we'll only use
the information of Q for the first pass.

00:18:45.310 --> 00:18:49.480
Once we have the second pass, we'll use
m1, and Q, to understand the attention.

00:18:49.480 --> 00:18:52.970
And again, that way we hope that
m is agglomerating the facts

00:18:52.970 --> 00:18:54.440
that are relevant to answer the question.

00:19:01.542 --> 00:19:02.260
Great question.

00:19:02.260 --> 00:19:05.858
So now,
let's zoom into the model in detail.

00:19:05.858 --> 00:19:08.680
In some cases, it's pretty easy.

00:19:08.680 --> 00:19:10.610
We basically just have a standard GRU, but

00:19:10.610 --> 00:19:13.610
maybe, before I define these
sort of different modules,

00:19:13.610 --> 00:19:16.900
in the end here we have, again, this
softmax, and so whenever you see softmax,

00:19:16.900 --> 00:19:20.980
you can think of cross-entropy
error as your training objective.

00:19:20.980 --> 00:19:23.640
And now because of how
we define the modules,

00:19:23.640 --> 00:19:26.610
we constrained this entire
architecture end-to-end.

00:19:26.610 --> 00:19:31.920
You just have to give it an input
sequence, a question, and an answer.

00:19:31.920 --> 00:19:35.940
And now from this answer here we basically
will make errors in the beginning and

00:19:35.940 --> 00:19:38.160
then we'll have high cross entropy error.

00:19:38.160 --> 00:19:42.110
And then, we basically backpropagate
through all these vectors here,

00:19:42.110 --> 00:19:44.320
through everything all the way
into the word vectors, and

00:19:44.320 --> 00:19:46.430
we can train this whole system end to end.

00:19:46.430 --> 00:19:49.361
And that's really where
the power of deep learning and

00:19:49.361 --> 00:19:51.472
these kind of architectures comes in.

00:20:12.679 --> 00:20:17.487
So the question is, should we have some
attention from the answer module also sort

00:20:17.487 --> 00:20:23.135
of skipping the episodic memory module's
attention and going directly to the input?

00:20:23.135 --> 00:20:29.850
And the answer is, yes, for the tasks
that we had, we did not even try that,

00:20:29.850 --> 00:20:34.360
cuz we solved them to the state of
the art level and slightly above.

00:20:34.360 --> 00:20:39.145
But what we actually have found is
that it makes sense in the second

00:20:39.145 --> 00:20:42.140
iteration on the harder data set and
namely those Stanford question answering

00:20:42.140 --> 00:20:47.685
data sets that you are all gonna work on,
to actually have a co-attention mechanism

00:20:47.685 --> 00:20:52.895
where you want to re-interpret
the question also in terms of the input.

00:20:52.895 --> 00:20:57.835
So for instance, intuitively,
if you have a question, may I cut you?

00:20:57.835 --> 00:21:01.115
Then, the interpretation is very different
if the person asking the question

00:21:01.115 --> 00:21:05.087
is holding a knife, or if somebody is
standing in line in a supermarket.

00:21:06.588 --> 00:21:10.590
And so, sometimes the interpretation
of your question is actually different

00:21:10.590 --> 00:21:11.880
depending on the input.

00:21:11.880 --> 00:21:14.390
And so,
you want to have co-attention mechanisms.

00:21:16.420 --> 00:21:19.720
Essentially, attention is kind of
a fun concept right now to do.

00:21:19.720 --> 00:21:22.907
A lot of people are sprinkling
into a lot of different places and

00:21:22.907 --> 00:21:24.724
a lot of different kinds of models.

00:21:24.724 --> 00:21:30.110
And I think this would be a very fine
way to do it, or a place to do it.

00:21:32.030 --> 00:21:35.459
So we train everything end-to-end,
cross entropy errors, standard stuff.

00:21:35.459 --> 00:21:38.486
Now, the input module is a standard GRU.

00:21:38.486 --> 00:21:42.646
And again, in this particular case here
where we have very simple sentences and

00:21:42.646 --> 00:21:46.869
facts about each sentences, we'll actually
make that last hidden state of each

00:21:46.869 --> 00:21:51.130
sentence explicitly accessible
when we answer the question.

00:21:51.130 --> 00:21:54.240
Now, one improvement that we've
made on the second iteration is

00:21:54.240 --> 00:21:58.920
instead of just having
a unidirectional GRU we actually have

00:21:58.920 --> 00:22:03.810
a bi-directional GRU on top here and
then each sentence is represented

00:22:05.180 --> 00:22:10.090
via the concatenation of both the left
to right and right to left direction.

00:22:12.190 --> 00:22:16.460
Now, the question also
just a standard GRU.

00:22:16.460 --> 00:22:19.730
We have here,
word vectors we call them v_t here.

00:22:19.730 --> 00:22:22.832
And basically we just get q_t and

00:22:22.832 --> 00:22:28.798
then we'll drop the subscript t for
the last hidden state,

00:22:28.798 --> 00:22:33.946
just to simplify notation and
subsequent slides.

00:22:33.946 --> 00:22:38.753
Now where it gets interesting is that
episodic memory part, and we've already

00:22:38.753 --> 00:22:43.650
seen some attention mechanisms, and
this one is slightly different.

00:22:43.650 --> 00:22:46.540
Basically, we'll have
the attention mechanism here,

00:22:47.610 --> 00:22:50.940
which I'll define in the next slide, of G.

00:22:50.940 --> 00:22:53.040
G is just a single scalar number.

00:22:53.040 --> 00:22:55.730
Should I pay attention to this sentence?

00:22:55.730 --> 00:22:57.340
At the ith time step or not.

00:22:58.730 --> 00:23:03.150
Now, the superscript here
refers to the iteration, or

00:23:03.150 --> 00:23:07.990
the episode, the tth time that
we went over the entire input.

00:23:07.990 --> 00:23:12.030
So we start here with t equals 1,
and then we go up to t equals 2.

00:23:12.030 --> 00:23:15.509
And if we go over the input five times,
which, of course,

00:23:15.509 --> 00:23:19.031
is also kinda slow,
we'd have here "h superscript 5." But

00:23:19.031 --> 00:23:25.140
the main idea is essentially that we have
a global gate on top of a standard GRU.

00:23:25.140 --> 00:23:29.220
So this global gate will basically say
this entire sentence does not matter or

00:23:29.220 --> 00:23:30.740
does matter a lot.

00:23:30.740 --> 00:23:35.160
And instead of having every single
hidden state have its own gate

00:23:35.160 --> 00:23:38.778
we just turn off the entire GRU
if a fact doesn't seem relevant.

00:23:38.778 --> 00:23:44.532
So if g_i, at the ith sentence,
and the tth episode and

00:23:44.532 --> 00:23:49.676
pass over the entire input is 0,
what we'll do is

00:23:49.676 --> 00:23:55.200
basically just entirely
copy that h vector forward.

00:23:55.200 --> 00:23:59.770
No computation really necessary,
no updates to any of the units whatsoever.

00:24:01.680 --> 00:24:03.640
So intuitively, that makes a lot of sense.

00:24:03.640 --> 00:24:07.115
If we have sentences like Sandra
went back to the kitchen and

00:24:07.115 --> 00:24:09.409
we're asking about the football, and

00:24:09.409 --> 00:24:14.500
maybe eventually we'll figure out
something about John, and so on.

00:24:14.500 --> 00:24:19.837
It's completely irrelevant to
have facts about other people

00:24:19.837 --> 00:24:27.138
going to other places and that's really
easy to capture with this single scalar.

00:24:27.138 --> 00:24:32.328
Now the last remaining question is well,
how do we compute that g and

00:24:32.328 --> 00:24:37.560
the main idea here is actually
fairly simple and straightforward.

00:24:37.560 --> 00:24:42.529
We essentially compute this vector
z here with a bunch of simple

00:24:42.529 --> 00:24:46.103
similarities between the sentence sector.

00:24:46.103 --> 00:24:50.330
That's again, the hidden state
of at the end of each sentence.

00:24:50.330 --> 00:24:54.704
The question back there and our memory
stayed from the previous iteration and

00:24:54.704 --> 00:24:57.808
m0 here would just be
initialized to be the question.

00:24:57.808 --> 00:25:01.470
So the very first iterations
just have these two twice.

00:25:01.470 --> 00:25:05.848
But that way, we can use the exact
same mechanism in the first,

00:25:05.848 --> 00:25:08.663
second and third iteration and higher.

00:25:08.663 --> 00:25:11.334
Now, what kinds of similarities
are we measuring here?

00:25:11.334 --> 00:25:14.829
These are all element-wise and
these are just how to mark products,

00:25:14.829 --> 00:25:17.784
so multiplicative interactions
between the sentence and

00:25:17.784 --> 00:25:20.700
question and
each sentence at the memory state.

00:25:20.700 --> 00:25:24.267
And then here, we just have elementwise
subtraction and absolute value.

00:25:24.267 --> 00:25:28.718
So just basically,
two very simple similarity

00:25:28.718 --> 00:25:33.295
metrics between the three
vectors that we have.

00:25:33.295 --> 00:25:33.795
Yeah.

00:25:37.525 --> 00:25:40.406
Is there a reason why we don't use
the inner product for similarity?

00:25:40.406 --> 00:25:41.747
We actually tried it.

00:25:41.747 --> 00:25:46.275
And in the first version of the paper,
we even had things like question

00:25:46.275 --> 00:25:50.802
transpose times W times a sentence for
instance so this way we have a and

00:25:50.802 --> 00:25:55.104
even more powerful similarity
matrix that's multiplicative but

00:25:55.104 --> 00:25:59.030
can weigh different elements
of the vectors and so on.

00:25:59.030 --> 00:26:02.875
It turned out after we've done some
replacement studies, again, something

00:26:02.875 --> 00:26:07.540
you should all do for you projects too, if
we remove that, we had the same accuracy.

00:26:07.540 --> 00:26:12.511
And whenever you can remove
something from your model

00:26:12.511 --> 00:26:16.608
you should, so
we just got to take that out.

00:26:16.608 --> 00:26:21.385
Now once we have this feature vector here,
this essentially it's a vector that

00:26:21.385 --> 00:26:25.180
has all the similarities between
these different sentences.

00:26:25.180 --> 00:26:28.414
We just plugged that into a standard
true layer neural network.

00:26:28.414 --> 00:26:32.898
Standard tanh hidden units were very
familiar at this point with these

00:26:32.898 --> 00:26:34.100
equations.

00:26:34.100 --> 00:26:38.969
We have a linear, they're here and then we
basically put a softmax on top of that,

00:26:38.969 --> 00:26:41.241
so all the attention scores sum to 1.

00:26:41.241 --> 00:26:44.560
In some ways,
this could actually be a limiting factor.

00:26:44.560 --> 00:26:48.303
Cuz that means that I can
only pay attention to so

00:26:48.303 --> 00:26:53.542
many things very, very strongly each
time I go over the data set and

00:26:53.542 --> 00:26:56.371
we might not always want to do that.

00:26:56.371 --> 00:27:01.690
Maybe we want to pay 100%
attention to five different facts.

00:27:01.690 --> 00:27:04.556
Turns out to work reasonably well for
some data sets but

00:27:04.556 --> 00:27:08.793
sometimes you might also instead of having
a single softmax have just sigmoids,

00:27:08.793 --> 00:27:12.118
so you can pay a lot more attention
to a lot of different things.

00:27:15.810 --> 00:27:21.186
And then at the very end here,
these two lines turn out to also be a GRU,

00:27:21.186 --> 00:27:24.759
but one that won't have
very many time steps.

00:27:24.759 --> 00:27:29.529
It's basically a GRU that goes from each
of the memory states to the next memory

00:27:29.529 --> 00:27:34.310
state and agglomerates the facts that
have been agglomerated over time here.

00:27:36.690 --> 00:27:40.171
Turns out that is actually
not an important parameter,

00:27:40.171 --> 00:27:43.283
eventually replaced that
GRU standard rectified

00:27:43.283 --> 00:27:47.875
linear unit type of two layer neural
network and that worked well too, but

00:27:47.875 --> 00:27:52.345
first iteration of the model had a GRU
between these two states as well.

00:27:56.240 --> 00:27:59.982
Any questions about
the episodic memory module and

00:27:59.982 --> 00:28:04.255
how we agglomerate facts,
how we compute the attention?

00:28:11.440 --> 00:28:12.087
Cool.

00:28:12.087 --> 00:28:14.934
Could you raise your hand if you
feel like all these modules and

00:28:14.934 --> 00:28:16.920
how we put them together
make sense to you?

00:28:19.290 --> 00:28:23.070
To the people who haven't raised their
hand, can you formulate any kind of

00:28:23.070 --> 00:28:26.496
question around, why it doesn't
make sense or what confuses you?

00:28:29.830 --> 00:28:30.330
Yeah.

00:28:46.604 --> 00:28:47.495
That's exactly right.

00:28:47.495 --> 00:28:53.340
So the question is on a very high-level,
we're going over the input multiple times.

00:28:53.340 --> 00:28:55.659
Because every time we go over the input,

00:28:55.659 --> 00:28:59.288
we can learn to pay attention
to different kinds of sentences.

00:29:16.366 --> 00:29:19.546
That's right.
So basically, intuitively here,

00:29:19.546 --> 00:29:24.808
I'll just rephrase your question
in the answer which is basically,

00:29:24.808 --> 00:29:28.707
when I go over the inputs
the sentences here, s_i for

00:29:28.707 --> 00:29:31.915
the first time maybe s_i here captures.

00:29:31.915 --> 00:29:35.818
Facts about John, but
my question here is about football.

00:29:35.818 --> 00:29:37.730
So in the very first iteration and

00:29:37.730 --> 00:29:41.812
m^{t-1} (m_0) is just initialized to
the question too, so it's essentially not

00:29:41.812 --> 00:29:46.795
adding anything either cuz we haven't
gone over the inputs an entire time yet.

00:29:46.795 --> 00:29:48.552
So, we really have these two factors.

00:29:48.552 --> 00:29:52.195
The sentence which basically
in your hidden state,

00:29:52.195 --> 00:29:56.664
we hope capture some fact about
John having move to the hallway or

00:29:56.664 --> 00:30:01.820
somewhere, but the question is
just asking where the football is.

00:30:01.820 --> 00:30:07.058
So the similarity between this vector and
this vector is not going to be very

00:30:07.058 --> 00:30:12.964
high and then we plug this long feature
vector into this two-layer neural network,

00:30:12.964 --> 00:30:17.888
but no matter what, the two sentences
just don't seem very related.

00:30:17.888 --> 00:30:21.936
And so, this two layer neural
network will not learn or

00:30:21.936 --> 00:30:27.230
be able to identify that the sentence
seems relevant for the question.

00:30:27.230 --> 00:30:32.818
And so this gate g here will just be very
small, but then in the second iteration

00:30:32.818 --> 00:30:37.742
with basically edit one sentence
that connect the John and hallway.

00:30:37.742 --> 00:30:44.051
So now in our hidden state m that
had gone through this GRU here, so

00:30:44.051 --> 00:30:50.706
that last hidden state, we define
here as m now captured from the very

00:30:50.706 --> 00:30:57.045
first iteration that John put
down the football seems relevant.

00:30:57.045 --> 00:31:02.870
So now m has, in its hidden state, some
facts about both John and the football.

00:31:04.010 --> 00:31:09.140
And now, those similarities can be
picked up by this attention mechanism.

00:31:10.250 --> 00:31:14.603
And basically now, in the next iteration
as we move over the sentence again

00:31:14.603 --> 00:31:18.559
give a higher attention score to
that sentence that mentions John.

00:31:22.420 --> 00:31:27.155
That's right, a sentence is just a GRU or
some averaging.

00:31:42.825 --> 00:31:46.019
That's a great question.

00:31:46.019 --> 00:31:50.220
So there are a bunch of different ,I will
get to this in a second, don't worry.

00:31:50.220 --> 00:31:54.826
There are a bunch of different kinds
of things that people have tried and

00:31:54.826 --> 00:31:59.296
checked if this works and
one of them is actually Basic Coreference.

00:31:59.296 --> 00:32:05.140
So being able like requiring to
answer a question that it is he And

00:32:05.140 --> 00:32:09.030
then asking, who does he refer to,
then finding the right person.

00:32:09.030 --> 00:32:13.400
And so, the model can actually do this
very accurately as long as it sees that.

00:32:13.400 --> 00:32:17.830
And so, the way it would do that is
just basically noticing how inside

00:32:18.930 --> 00:32:24.140
here, you mention John, or you mention he,
and then it learns to just go back and

00:32:24.140 --> 00:32:29.430
find the next previous
kind of sentence that

00:32:29.430 --> 00:32:32.550
mentions any kind of name, for instance.

00:32:32.550 --> 00:32:35.580
And it could learn more complex
patterns than that too.

00:32:35.580 --> 00:32:39.244
But, again, it would have to
have these multiple passes to be

00:32:39.244 --> 00:32:41.781
able to now go back to
something that only it

00:32:41.781 --> 00:32:46.656
didn't make sense in the first time you
went from left to right reading it, yeah.

00:33:00.832 --> 00:33:05.574
But you would hope that the sentence
vectors capture what is in the sentence.

00:33:21.956 --> 00:33:24.829
That's right, so
sorry to rephrase what you said,

00:33:24.829 --> 00:33:27.060
I guess it wasn't quite a question but.

00:33:28.260 --> 00:33:32.620
What important here, yeah, as the line
goes through, the last hidden state

00:33:32.620 --> 00:33:37.140
of s_2 of that second
sentence is the input and

00:33:37.140 --> 00:33:43.290
the h_0 of the next sentence,
so it's one continuous GRU.

00:33:43.290 --> 00:33:45.910
And you would hope that

00:33:45.910 --> 00:33:48.830
as it mentions John it keeps track
of that in one of its hidden states.

00:33:48.830 --> 00:33:52.127
Something happened about John,
and then as it reads in "he" and

00:33:52.127 --> 00:33:56.460
updates our hidden states we would hope
that it captures something about the two

00:33:56.460 --> 00:33:57.703
being connected now.

00:34:15.630 --> 00:34:20.547
That's right, so the question is has there
been a study of using this exact model

00:34:20.547 --> 00:34:23.915
on coreference resolution and
the answer is yes.

00:34:23.915 --> 00:34:25.725
We actually played around with it.

00:34:25.725 --> 00:34:28.945
And there are some data sets where
we actually did really well on, but

00:34:28.945 --> 00:34:31.675
we also had to modify the model slightly.

00:34:31.675 --> 00:34:32.965
There's some various tips and

00:34:32.965 --> 00:34:36.005
tricks that will be a little bit
outside the scope of this lecture.

00:34:36.005 --> 00:34:41.590
So, there's nothing in theory of why this
couldn't work at all for coreference.

00:34:41.590 --> 00:34:46.260
The main problem is that there's a lot
of different kinds of patterns and

00:34:46.260 --> 00:34:50.030
you need a lot of trained
data to show the model.

00:34:50.030 --> 00:34:53.560
What kind of patterns you might
wanna capture for co-reference?

00:34:53.560 --> 00:34:58.280
And then, the main problem in co-reference
is that you might want to have an answer

00:34:58.280 --> 00:35:00.450
for every pair of words.

00:35:00.450 --> 00:35:03.732
In saying could these two words
quote "refer to each other or not".

00:35:03.732 --> 00:35:06.163
And [INAUDIBLE] so
there is some issues but

00:35:06.163 --> 00:35:10.177
there is no reason of why this
model in general couldn't do coref.

00:35:10.177 --> 00:35:14.922
The main tricky bit in that why you needed
extra modifications is to squeeze coref

00:35:14.922 --> 00:35:19.060
into a question and answering problem,
which is not very intuitive.

00:35:19.060 --> 00:35:21.690
The question would then be so
for instance,

00:35:21.690 --> 00:35:24.300
let's say you have this whole sentence and
they're a bunch of couple of he's and

00:35:24.300 --> 00:35:26.240
she's in there, and
now you ask what does he refer to.

00:35:26.240 --> 00:35:30.120
And you wouldn't know which he
even mean from the question.

00:35:30.120 --> 00:35:32.220
So then, you have to say,
what does he refer to?

00:35:32.220 --> 00:35:35.850
And then we'd have to give
sort of a indicator function

00:35:35.850 --> 00:35:39.010
to which he were actually
caring about in the input.

00:35:39.010 --> 00:35:42.331
So, those are the kinds of changes
you may have to make to the model to

00:35:42.331 --> 00:35:43.099
do coreference

00:36:10.408 --> 00:36:12.255
So, that one seems very easy.

00:36:12.255 --> 00:36:15.605
So the question is, what if the input
was John moves to the bedroom, and

00:36:15.605 --> 00:36:18.410
the question is where did John move to?

00:36:18.410 --> 00:36:22.020
And yeah, in this case, it just needs to
pay attention to that one sentence and

00:36:22.020 --> 00:36:24.460
can immediately agglomerate that.

00:36:24.460 --> 00:36:27.640
And then, you just wanna make sure it
doesn't ignore it in the next couple of

00:36:27.640 --> 00:36:31.260
iterations if you have a fixed number
of passes that you have over the input.

00:36:31.260 --> 00:36:33.370
And then,
you can just output that bedroom.

00:36:34.730 --> 00:36:37.525
But maybe your answer is,
it's never seen the word bedroom before.

00:36:41.253 --> 00:36:42.190
I see, yeah.

00:36:42.190 --> 00:36:46.820
So, if they're completely
new words that describe new

00:36:46.820 --> 00:36:50.130
existing kinds of relationships,
it would also have some trouble.

00:36:50.130 --> 00:36:55.500
In this case, it probably would still
work because it doesn't really care about

00:36:55.500 --> 00:37:00.060
that many things in between, unless now
you have certain things like John slept

00:37:00.060 --> 00:37:04.110
in the bedroom versus John went to
the bedroom and now you have different

00:37:04.110 --> 00:37:09.190
kinds of questions and it needs to know
what the actual action and verb is.

00:37:09.190 --> 00:37:12.380
And then, if you don't have that in
a trained data, then it couldn't do it.

00:37:12.380 --> 00:37:16.898
But in general,
this kind of model struggles with

00:37:16.898 --> 00:37:21.220
these things that i mentioned,
which is this thing right here,

00:37:21.220 --> 00:37:24.570
in the first version of this
model is still a softmax.

00:37:24.570 --> 00:37:29.770
So, if you've never seen a certain answer
at training time, the word hallway or

00:37:29.770 --> 00:37:33.490
bedroom, maybe they've only went
to kitchens and living rooms or

00:37:33.490 --> 00:37:37.770
something that, it would never be
able to give you that kind of answer.

00:37:37.770 --> 00:37:42.660
But they're now ways to extend these kinds
of models with the pointer sentinel idea

00:37:42.660 --> 00:37:47.550
and generally pointers that learn to
point to certain parts of your input and

00:37:47.550 --> 00:37:52.130
that's one way of fixing that problem and
you'll implement pointers I think for

00:37:52.130 --> 00:37:53.070
your PA4 as well.

00:37:53.070 --> 00:37:58.580
But a lot of the other kinds of
ideas are not that unreasonable.

00:37:58.580 --> 00:38:01.432
For those kinds of models, too.

00:38:29.340 --> 00:38:32.075
So, the question is how do
we compute the m vector?

00:38:32.075 --> 00:38:37.780
So the m vector is going to be
defined as the last hidden state

00:38:37.780 --> 00:38:43.880
of this time sequence model,
which inside has a GRU,

00:38:43.880 --> 00:38:48.040
but also has this global
gate on top of it.

00:38:48.040 --> 00:38:51.560
So, for simplicity for instance,
if the fact is very relevant based on

00:38:51.560 --> 00:38:56.400
this attention score g, then h will
just be computed as a standard GRU.

00:38:57.940 --> 00:39:00.460
All right, so
this is just a single scalar.

00:39:00.460 --> 00:39:04.530
That single scalar is one
then we'll just have it GRU.

00:39:04.530 --> 00:39:08.967
Now the last hidden state of this GRU
as it goes over the inputs we'll just

00:39:08.967 --> 00:39:09.782
define as m.

00:39:13.833 --> 00:39:16.100
So that's for one pass.

00:39:16.100 --> 00:39:21.060
But then, the second pass will actually
take that last hidden state and give it

00:39:21.060 --> 00:39:26.190
as input together with the previous hidden
state and actually, never mind, yeah.

00:39:26.190 --> 00:39:28.590
Let's just assume that
that's your m state.

00:39:28.590 --> 00:39:31.560
There are lots of modifications
that you can make to this model but

00:39:31.560 --> 00:39:36.030
they're not that relevant and
they usually only change your accuracy.

00:39:36.030 --> 00:39:38.162
1 to 2 or 3%, so

00:39:38.162 --> 00:39:44.210
the simplest iteration is just going to
be going through these m's independently.

00:39:44.210 --> 00:39:47.450
And then, you can try various options of

00:39:47.450 --> 00:39:51.900
incorporating this last hidden state
m also at the very last time stamp.

00:39:51.900 --> 00:40:02.130
But you can kind of- All right, awesome.

00:40:02.130 --> 00:40:08.010
So, the answer module also adjust the GRU,
but one little trick here,

00:40:08.010 --> 00:40:12.776
which is We don't just have the previous
hidden state at, and some word

00:40:12.776 --> 00:40:17.583
vector cuz there are no word vectors,
there are no inputs for the answer module.

00:40:17.583 --> 00:40:22.717
And so, what we have instead is will
concatenate the question vector

00:40:22.717 --> 00:40:28.890
at every single input state here,
as well as the previous words output.

00:40:28.890 --> 00:40:33.704
So if we have this longer sequence
of things that we're generating,

00:40:33.704 --> 00:40:38.186
then we have here the previous
word that we generated each time.

00:40:38.186 --> 00:40:43.226
So this is similar to mission completion
for instance, where we give it

00:40:43.226 --> 00:40:48.109
as input every time the word we just
generated in the ten step before.

00:40:54.884 --> 00:40:57.810
Cool, now there's a bunch of related work.

00:40:57.810 --> 00:41:04.400
Many of these papers we've talked about,
well actually not that many, some of them.

00:41:04.400 --> 00:41:09.235
So Sequence to Sequence learning,
is one such model.

00:41:09.235 --> 00:41:13.312
We didn't really cover Neural Turing
Machines in some of these other models

00:41:13.312 --> 00:41:15.925
here, with somewhat over-promising titles,

00:41:15.925 --> 00:41:19.210
like teaching machines to read and
comprehend.

00:41:19.210 --> 00:41:22.650
I don't think any of these models
are really comprehending that much,

00:41:22.650 --> 00:41:25.055
or reading in the sense that people read,
but

00:41:25.055 --> 00:41:28.690
it's similar kinds of models
that have memory components.

00:41:29.830 --> 00:41:32.760
As they go over different kinds of text.

00:41:32.760 --> 00:41:37.010
The one that's most relevant and
was actually developed in peril

00:41:37.010 --> 00:41:40.970
to this dynamic memory network, is
the memory network from Jason Weston, and

00:41:40.970 --> 00:41:44.525
the extension to them in
end to end memory networks.

00:41:44.525 --> 00:41:48.485
And so it makes sense to look a little
bit at the differences between these.

00:41:48.485 --> 00:41:54.080
Basically, both of these kinds of
models have mechanisms to compute

00:41:54.080 --> 00:42:00.168
some representations for input,
then scoring attention and responses.

00:42:00.168 --> 00:42:04.267
The main difference is that for
the memory networks,

00:42:04.267 --> 00:42:08.410
the input representations
are bag of words.

00:42:08.410 --> 00:42:10.389
And then have some nonlinear or

00:42:10.389 --> 00:42:15.503
linear embeddings that explicitly
encode the position of each word.

00:42:15.503 --> 00:42:20.672
And then the memory networks interfere on
a variety of different kinds of functions,

00:42:20.672 --> 00:42:22.840
both for attention and responses.

00:42:22.840 --> 00:42:23.600
So essentially,

00:42:23.600 --> 00:42:29.100
each of these four components is
a very different kind of network.

00:42:29.100 --> 00:42:33.589
And it's not just a sequence model,
and so the dynamic memory

00:42:33.589 --> 00:42:38.355
network here really takes as its core,
a neural sequence model.

00:42:38.355 --> 00:42:41.727
It could be a GRU which we've
tried compared to LSTMs,

00:42:41.727 --> 00:42:46.142
turned out LSTMs got the same performance,
but have more parameters.

00:42:46.142 --> 00:42:50.105
So we use just a GRU, and
what's nice about this is that,

00:42:50.105 --> 00:42:56.100
that will naturally capture that we
have a temporality in our sequence.

00:42:56.100 --> 00:43:00.400
So if we asked did this happen before,
or if we want to do sequence tagging,

00:43:00.400 --> 00:43:03.910
things like that we can immediately
do that with this model.

00:43:03.910 --> 00:43:08.050
So we have basically much broader range of
different kind of tasks that we can solve

00:43:08.050 --> 00:43:08.951
with this model.

00:43:11.120 --> 00:43:15.875
Now, before we get to evaluation,
we'll have a research highlights.

00:43:19.731 --> 00:43:24.249
&gt;&gt; Cool, hi everyone, so first of all
happy belated International Women's Day

00:43:24.249 --> 00:43:28.088
and so for the recent highlight,
I'm going to present the paper.

00:43:28.088 --> 00:43:32.171
Learning Program Embeddings
to Propagate Feedback on

00:43:32.171 --> 00:43:34.931
Student Code by Chris Piech, et al.

00:43:34.931 --> 00:43:35.484
Great.
So,

00:43:35.484 --> 00:43:38.770
if you remember your first days of
programming and for some of you,

00:43:38.770 --> 00:43:42.599
if you've taken one of six-eight here,
you'll probably remember Karel.

00:43:42.599 --> 00:43:45.916
You'll know how important it is to
get feedback from your teachers,

00:43:45.916 --> 00:43:48.560
about what your coding to
become a better programmer.

00:43:49.630 --> 00:43:52.490
But now let's imagine that you're
teaching a class that has,

00:43:52.490 --> 00:43:55.560
let's say,
an online course with a million students.

00:43:55.560 --> 00:43:58.560
How do you actually make sure that
you give feedback to everyone.

00:43:58.560 --> 00:44:02.891
Wouldn't it be nice if you just gave
feedback or if you graded, let's say,

00:44:02.891 --> 00:44:04.585
about 100 assignments and

00:44:04.585 --> 00:44:08.733
you could propagate that feedback to
the other students in that course?

00:44:08.733 --> 00:44:14.304
So that's kind of the motivation for
creating program embeddings.

00:44:14.304 --> 00:44:18.130
Yeah, the idea's that you want to be
able to cluster on these programs and

00:44:18.130 --> 00:44:21.060
together by like similar in some ways.

00:44:21.060 --> 00:44:24.380
So, and you all know that we can
do that with flagged sentences.

00:44:24.380 --> 00:44:26.127
We've seen that a lot in this class.

00:44:26.127 --> 00:44:30.947
And the question is, can we also represent
programs with vector representations,

00:44:30.947 --> 00:44:35.711
such that these vectors capture something
about the functionality of the code,

00:44:35.711 --> 00:44:39.531
even if the code, let's say,
crushes or doesn't really compile.

00:44:43.010 --> 00:44:45.302
So you know how to encode sentences.

00:44:45.302 --> 00:44:47.930
You've seen lots of different
architectures to do that.

00:44:47.930 --> 00:44:52.550
Usually, we train them on some task,
which requires and bottling the language.

00:44:52.550 --> 00:44:57.740
And then we can use that neural network
to create embeddings and for new imports.

00:44:57.740 --> 00:44:59.974
So can we do the same thing for
computer programs?

00:44:59.974 --> 00:45:04.510
And here this is what's presented
in the paper, and this is for

00:45:04.510 --> 00:45:07.190
a very simple program you can
see the program in the middle.

00:45:07.190 --> 00:45:10.320
It's just like to put a beeper and
move, and

00:45:10.320 --> 00:45:14.750
then on the left side you can see that
there are two precondition states.

00:45:14.750 --> 00:45:18.410
So that means just define what
precondition, postcondition means.

00:45:18.410 --> 00:45:21.655
So, pre-condition is a current
state of Karel world.

00:45:21.655 --> 00:45:25.406
For example, Karel is in the first square
and there is no beeper to world, so,

00:45:25.406 --> 00:45:28.776
that could be like a state, so,
that's a pre-conditioned and then,

00:45:28.776 --> 00:45:31.380
once you execute a program
we get to a post-condition.

00:45:31.380 --> 00:45:34.930
It's like where Karel ends up
after we execute that program.

00:45:34.930 --> 00:45:36.213
So, as you can imagine, for

00:45:36.213 --> 00:45:39.422
one program we can actually have
a lot of different pre-conditions and

00:45:39.422 --> 00:45:42.990
then once you execute program,
you have different post-conditions.

00:45:42.990 --> 00:45:47.332
So here you see two samples
of P_1 as one precondition,

00:45:47.332 --> 00:45:51.180
once we execute the program,
we get to Q_1.

00:45:51.180 --> 00:45:56.057
We also have another example,
P_k here, which brings us to Q_k.

00:45:56.057 --> 00:46:01.610
And you can imagine we have lots of these
pre-condition post-condition pairings.

00:46:01.610 --> 00:46:06.730
And so yeah, so the first step in this
model is that we want to encode the state.

00:46:06.730 --> 00:46:08.980
So we encode the precondition state.

00:46:08.980 --> 00:46:12.210
We get a vector presentation for
that state, then we

00:46:12.210 --> 00:46:16.690
apply the matrix M_A which is actually
the embedding that we're trying to learn.

00:46:16.690 --> 00:46:20.760
So we apply that and
then we get and after from that,

00:46:20.760 --> 00:46:26.450
which we are then decoding into what
we predict to be the post condition.

00:46:26.450 --> 00:46:29.824
So it's in some ways similar
to the approach that we

00:46:29.824 --> 00:46:33.510
see at the beginning of
the class with word2vec where we

00:46:33.510 --> 00:46:37.920
are trying to learn this feature
matrix like M_A in this case.

00:46:37.920 --> 00:46:40.630
So the goal is that M_A
captures something about

00:46:40.630 --> 00:46:42.490
the meaning of this particular program.

00:46:43.750 --> 00:46:46.340
And you can also see that,
like the encoder and decoder,

00:46:46.340 --> 00:46:48.840
they follow very similar structures
that you've seen in class.

00:46:48.840 --> 00:46:53.210
We apply a linear function with
a nonlinearity around it, and also for

00:46:53.210 --> 00:46:57.530
decoding, it's very similar so we use
the output from our cell in the mddle and

00:46:57.530 --> 00:47:01.330
then we, again, apply a linear function
with a nonlinearity afterwards.

00:47:03.650 --> 00:47:06.790
So the last function here
mainly consists of two parts.

00:47:06.790 --> 00:47:12.130
We have a prediction loss, which measures
how well we can predict the post condition

00:47:12.130 --> 00:47:14.300
given our P prediction and the program.

00:47:14.300 --> 00:47:18.850
And we also have an autoencoding loss,
which measures how good our encoder and

00:47:18.850 --> 00:47:20.140
decoders are.

00:47:20.140 --> 00:47:23.020
So in a way like if we have,
get and code us and decode us,

00:47:23.020 --> 00:47:26.610
we should be able to also
reconstruct our precondition.

00:47:26.610 --> 00:47:28.719
And the last term is just regularization,

00:47:28.719 --> 00:47:31.118
which you also have seen
many times in this class.

00:47:32.447 --> 00:47:37.847
[COUGH] So but if we have like a complex
program, we would like to actually combine

00:47:37.847 --> 00:47:44.280
or we can't necessarily train the previous
model on all different kinds of programs.

00:47:44.280 --> 00:47:47.400
So we want to be able to use
these building blocks, and

00:47:47.400 --> 00:47:50.622
then create our representation form or
complex program.

00:47:50.622 --> 00:47:54.560
And here, so you've already seen
recursive neural nets in this class, so

00:47:54.560 --> 00:47:57.210
we can use recursive
neural nets to do that.

00:47:57.210 --> 00:48:00.850
And the cool thing about programs is,
they're already in a tree structure, so

00:48:00.850 --> 00:48:04.910
we don't even have to create a tree
structure but it's already given to you.

00:48:04.910 --> 00:48:09.220
So given that tree structure, we can
reconstruct a recursive neural network

00:48:09.220 --> 00:48:12.010
which exactly follows the same structure.

00:48:12.010 --> 00:48:15.990
And then we combine these embeddings
that we've learned from the first task

00:48:15.990 --> 00:48:19.650
together recursively
until we reach the root.

00:48:19.650 --> 00:48:23.510
And the ideas that embedding
like the activation at the root

00:48:23.510 --> 00:48:26.680
in this case is the one thing
that's blue at the top.

00:48:26.680 --> 00:48:30.710
That representation should contain
something about the meaning,

00:48:30.710 --> 00:48:32.180
the logic of the entire program.

00:48:33.190 --> 00:48:36.672
And we can also train this
recursive neural network because

00:48:36.672 --> 00:48:41.266
when we're combining these embeddings
together, we also multiply them with

00:48:41.266 --> 00:48:44.764
parameters which we can learn
by training on an objective.

00:48:44.764 --> 00:48:45.288
Okay.
So

00:48:45.288 --> 00:48:48.064
just to summarize what
this paper presented, is

00:48:48.064 --> 00:48:52.733
a neural network architecture which allows
us to encode programs as a mapping from

00:48:52.733 --> 00:48:57.980
precondition spaces to a postcondition
space, using recursive neural networks.

00:48:57.980 --> 00:49:01.070
And the advantage of using recursive
neural networks in this case,

00:49:01.070 --> 00:49:05.130
is that we are also using
the structure of the program itself.

00:49:05.130 --> 00:49:07.540
And once we have these learned
representations, we can use them for

00:49:07.540 --> 00:49:08.720
lots of different tasks.

00:49:08.720 --> 00:49:11.050
So going back to our initial motivation,

00:49:11.050 --> 00:49:14.540
we can use them to cluster students
by their similarity of programs.

00:49:14.540 --> 00:49:18.197
So let's say once you've waited around 100
students, we can use that feedback and

00:49:18.197 --> 00:49:19.663
give it to other students as well.

00:49:19.663 --> 00:49:21.920
We have similar programs.

00:49:21.920 --> 00:49:26.456
We can essentially to feedback prediction
part and another application is to

00:49:26.456 --> 00:49:30.690
actually perform knowledge tracing
over multiple code submissions.

00:49:30.690 --> 00:49:34.801
So an example of that is when you're
solving your programming challenge,

00:49:34.801 --> 00:49:37.630
you might actually submit multiple times.

00:49:37.630 --> 00:49:39.418
It'll be interesting to
actually see your trajectories.

00:49:39.418 --> 00:49:43.576
So as a teacher, you can oftentimes
see how the student's progressing and

00:49:43.576 --> 00:49:47.800
you can see if the student actually
understood what they were programming or

00:49:47.800 --> 00:49:50.450
if they just kind of
randomly guessed the code.

00:49:51.650 --> 00:49:54.390
So this is actually still ongoing research

00:49:55.550 --> 00:49:58.400
by something that I've actually
been working on with Chris.

00:49:58.400 --> 00:50:03.520
So for example, possible interventions
that you might want to predict,

00:50:03.520 --> 00:50:05.400
an online course.

00:50:05.400 --> 00:50:08.749
When should you give a hint to the
student, should you show a motivational

00:50:08.749 --> 00:50:12.770
video right now or should you maybe
choose a different next exercise?

00:50:12.770 --> 00:50:16.950
And in order to do that, one thing that
you could do is given all these program

00:50:16.950 --> 00:50:20.000
submissions by the student, you convert
them into the program embeddings,

00:50:20.000 --> 00:50:22.040
using the approach that we just saw.

00:50:22.040 --> 00:50:25.664
And then you can feed them into
another recurring neural network and

00:50:25.664 --> 00:50:28.380
then predict future student success.

00:50:28.380 --> 00:50:30.950
Yeah, so and that's it,
if you have any questions,

00:50:30.950 --> 00:50:34.270
feel free to find me after class and
I would love to chat about it, thanks.

00:50:34.270 --> 00:50:34.897
&gt;&gt; Thanks Lisa.

00:50:34.897 --> 00:50:35.526
That was great.

00:50:35.526 --> 00:50:42.600
&gt;&gt; [APPLAUSE]
&gt;&gt; All right, that was awesome.

00:50:42.600 --> 00:50:44.560
So now,

00:50:44.560 --> 00:50:49.730
let's look at the various tasks that we
can apply to dynamic memory network, too.

00:50:49.730 --> 00:50:54.119
So the first one here is the babI or
babI data set, it's actually not

00:50:54.119 --> 00:50:59.380
a great data set in some ways because
it is actually a synthetic data set.

00:50:59.380 --> 00:51:03.790
Which is in some ways a big no-no for
a lot of NLP people.

00:51:03.790 --> 00:51:07.490
Because that was what the field
had done many, many years ago.

00:51:07.490 --> 00:51:11.450
We've moved past that since, and
actually can use real language.

00:51:11.450 --> 00:51:15.640
But I still think it was
an interesting data set for a while.

00:51:15.640 --> 00:51:18.680
We've solved it to such a high degree at
this point already that it's not that

00:51:18.680 --> 00:51:19.560
interesting anymore.

00:51:19.560 --> 00:51:22.136
We've sort of solved it as
a community fairly quickly.

00:51:22.136 --> 00:51:27.209
But it's interesting in that it gives you
a lot of necessary, but not sufficient

00:51:27.209 --> 00:51:32.050
conditions for systems to be able to
answer certain kinds of questions.

00:51:32.050 --> 00:51:36.244
So simple things like counting, like X
goes into the room, Y goes into the room,

00:51:36.244 --> 00:51:37.731
how many people in the room?

00:51:37.731 --> 00:51:38.376
Right?

00:51:38.376 --> 00:51:41.908
So it's very simple and
if you give it enough examples,

00:51:41.908 --> 00:51:46.944
it will be able to predict some number
between 1 to 10 or something like that or

00:51:46.944 --> 00:51:50.950
a simple negation like John
did not go into the bathroom.

00:51:50.950 --> 00:51:51.625
Is John in the bathroom?

00:51:51.625 --> 00:51:52.266
No.

00:51:52.266 --> 00:51:53.251
Things like that.

00:51:53.251 --> 00:51:58.000
So if it sees a lot of certain
kinds of patterns, it can do this.

00:51:58.000 --> 00:52:00.490
Sometimes indefinite knowledge.

00:52:00.490 --> 00:52:08.330
Just simple sentences like John may have
gone to the hallway or the bathroom.

00:52:08.330 --> 00:52:09.510
Is he in the bathroom?

00:52:09.510 --> 00:52:10.210
Maybe.

00:52:10.210 --> 00:52:12.430
So, super simple stuff like that.

00:52:13.480 --> 00:52:15.400
Basic induction, positional reasoning.

00:52:15.400 --> 00:52:17.820
And this one is actually a little harder.

00:52:17.820 --> 00:52:22.881
You basically have a bunch
of inputs saying the castle

00:52:22.881 --> 00:52:28.179
is north of the beach and
the beach is east of the desert.

00:52:28.179 --> 00:52:32.156
Now John moves west and
north from the beach.

00:52:32.156 --> 00:52:34.750
Is he south of the desert or

00:52:34.750 --> 00:52:38.010
something like that, probably doesn't
make sense but you get the idea.

00:52:38.010 --> 00:52:42.193
So those kinds of reasoning, were a little
hard but it turns out there's another

00:52:42.193 --> 00:52:47.004
version of this data set where you can
sample 10,000 examples instead of 1,000.

00:52:47.004 --> 00:52:50.249
And then most of the models can
also solve positional reasoning,

00:52:50.249 --> 00:52:53.670
it's just a matter of how
many examples you've seen.

00:52:53.670 --> 00:52:57.290
So in many ways, this was encouraging.

00:52:57.290 --> 00:53:01.215
Briefly, and
then we've moved onto real data sets.

00:53:01.215 --> 00:53:03.853
But that use real kinds of real language.

00:53:03.853 --> 00:53:06.314
The agent motivation here
is fairly simple too,

00:53:06.314 --> 00:53:10.406
we basically showed a lot of examples of
somebody eating or drinking something.

00:53:10.406 --> 00:53:13.286
And then you asked,
why did they drink something, and

00:53:13.286 --> 00:53:15.740
it's because they were thirsty.

00:53:15.740 --> 00:53:17.990
And so
thirsty is sort of the simple answer.

00:53:17.990 --> 00:53:22.180
So again, very simple kinds of patterns.

00:53:22.180 --> 00:53:25.210
But again, interesting and
necessary conditions.

00:53:25.210 --> 00:53:29.870
If you can't even solve that with
your deep neural network model,

00:53:29.870 --> 00:53:33.790
you will never get to a real question
answering system either that can solve

00:53:33.790 --> 00:53:37.055
all kinds of more complex
types of reasoning.

00:53:38.205 --> 00:53:42.255
What's more interesting is when we
actually applied it to real sentiment.

00:53:42.255 --> 00:53:45.870
So here, the question is
essentially always the same,

00:53:45.870 --> 00:53:50.445
you could almost ignore the question
vector and just have a zero,

00:53:50.445 --> 00:53:54.095
it adds essentially just some
bias weights to the model.

00:53:54.095 --> 00:53:57.850
But what was cool is
that model actually got

00:53:57.850 --> 00:54:01.950
the state of the art on sentiment analysis
on the Stanford Sentiment Treebank.

00:54:01.950 --> 00:54:03.640
And it's the same architecture.

00:54:03.640 --> 00:54:06.180
But again, sadly,
it's not this same exact model.

00:54:07.200 --> 00:54:09.640
So the hyperparameters here are different.

00:54:09.640 --> 00:54:13.490
And one such hyperparameter is for
instance, how many times do you need to go

00:54:13.490 --> 00:54:17.360
over the input before you wanna
predict the final answer?

00:54:17.360 --> 00:54:22.420
And sadly, you get different state of
the art results depending on the tasks.

00:54:22.420 --> 00:54:27.970
So for sentiment, the best number for
fine grain sentiments so very negative,

00:54:27.970 --> 00:54:32.430
negative, neutral, positive, very positive
classifications for each sentiments.

00:54:32.430 --> 00:54:36.850
The best, and when you allow
the model to go over the input twice.

00:54:39.100 --> 00:54:41.570
But, for various of types of reasoning,

00:54:41.570 --> 00:54:45.630
such as reasoning that requires
three different kinds of facts.

00:54:45.630 --> 00:54:48.590
So, John went in the hallway,
the hallway is in the house,

00:54:48.590 --> 00:54:51.290
the house is in this area.

00:54:51.290 --> 00:54:53.120
Is John in this area, yes or no, right?

00:54:53.120 --> 00:54:56.880
You now need to know and
go over multiple facts.

00:54:56.880 --> 00:55:01.460
Or the simple examples,
like John drop the football there,

00:55:01.460 --> 00:55:04.850
where is John, so that those kinds of
transitive reasonings you can make.

00:55:04.850 --> 00:55:09.320
You can create artificially sort
of transitive reasoning chains,

00:55:09.320 --> 00:55:11.800
that will require multiple passes, and

00:55:11.800 --> 00:55:16.205
this also shows here that in theory,
you'd only need three passes.

00:55:16.205 --> 00:55:21.345
But in practice, the model hadn't been
able to perfectly pick up all the relevant

00:55:21.345 --> 00:55:26.025
facts at the exact right pass
over the input the first time and

00:55:26.025 --> 00:55:27.805
needed multiple times.

00:55:27.805 --> 00:55:29.705
Needed to go over it multiple times.

00:55:29.705 --> 00:55:33.695
And this is assuming you don't
give it the fact supervision.

00:55:33.695 --> 00:55:38.497
There's actually another data
set version of the data set,

00:55:38.497 --> 00:55:44.632
that tells you this fact is important and
the first time you go over the input.

00:55:44.632 --> 00:55:48.989
And then this fact is important the second
time you go over the input, if you do

00:55:48.989 --> 00:55:53.634
that, then you can get away with three
passes for these three-fact reasonings.

00:55:53.634 --> 00:55:56.845
But without that supervision,
just training everything end to end.

00:55:56.845 --> 00:55:57.365
Question, answer,

00:55:57.365 --> 00:56:02.705
input triplets need to have 5 passes
to get very high accuracy here.

00:56:10.131 --> 00:56:14.169
So why is this not
a task-dependent hyperparameter?

00:56:14.169 --> 00:56:17.450
It is, it is actually
a task-dependent hyperparameter.

00:56:17.450 --> 00:56:21.523
And we did here, based on
the development splits, the analysis and

00:56:21.523 --> 00:56:26.108
found that the best hyperparameter of
number of passes for sentiment is 2.

00:56:37.409 --> 00:56:38.988
That's exactly right.

00:56:38.988 --> 00:56:42.602
So, the question is, in practice,
you would just, at training time,

00:56:42.602 --> 00:56:46.755
adjust these hyperparameters and for your
trained data set and identify, based on

00:56:46.755 --> 00:56:51.500
your developments, what the best type of
parameter is, and that's exactly right.

00:56:51.500 --> 00:56:53.948
So at least you don't have to,
for a variety of different tasks,

00:56:53.948 --> 00:56:56.724
think about all the different kinds
of architectures that are out there.

00:56:56.724 --> 00:57:00.885
But you still have to run some
hyperparameter search on what the best

00:57:00.885 --> 00:57:03.809
type of parameters are for
this architecture.

00:57:05.640 --> 00:57:06.438
Yeah?

00:57:10.380 --> 00:57:13.323
So why is there no result here for
5 passes?

00:57:13.323 --> 00:57:19.630
Because compute time is costly and
it already went down after 3.

00:57:19.630 --> 00:57:24.014
And the probability, I guess our estimates
of that, it would magically go back up or

00:57:24.014 --> 00:57:27.003
very, very low, so
we just didn't run the experiment.

00:57:33.567 --> 00:57:37.326
All right, so now,
let's look at a couple of examples

00:57:37.326 --> 00:57:41.210
of this attention mechanism for
sentiment analysis.

00:57:41.210 --> 00:57:45.954
So we now, here, have a couple of
examples that even the dynamic memory

00:57:45.954 --> 00:57:50.797
network got wrong when we only allowed
it one iteration over the inputs.

00:57:50.797 --> 00:57:55.570
And what you see here is
basically a coloring scheme.

00:57:55.570 --> 00:57:57.722
And the darker the color is,

00:57:57.722 --> 00:58:03.384
the larger the attention weight that
g scalar is for that particular word.

00:58:06.548 --> 00:58:10.348
And so, these are the kinds of examples,
also, that you now need to get correct

00:58:10.348 --> 00:58:14.060
if you want to push the state of
the art in sentiment analysis.

00:58:14.060 --> 00:58:16.403
And they're kind of interesting and
fun, actually.

00:58:16.403 --> 00:58:20.410
So, in its ragged, cheap and
unassuming way, the movie works.

00:58:20.410 --> 00:58:25.045
You can see it in the first pass over the
input, it just kind of pays attention to

00:58:25.045 --> 00:58:28.911
all the things you would sensibly
pay attention to for a sentiment

00:58:28.911 --> 00:58:33.425
analysis trained neural network,
which is a bunch of adjectives, right?

00:58:33.425 --> 00:58:39.267
Ragged, cheap, unassuming,
a little bit of way, and so on.

00:58:39.267 --> 00:58:40.729
Now, on 2 passes,

00:58:40.729 --> 00:58:45.580
the model actually is not quite
certain where to pay attention to.

00:58:45.580 --> 00:58:49.484
In the very first pass,
a little bit of cheap, unassuming, way,

00:58:49.484 --> 00:58:51.453
somewhat oddly the, and works.

00:58:51.453 --> 00:58:56.557
But in the second one, it sort of now
takes into consideration the context of

00:58:56.557 --> 00:59:02.063
that whole sentence, and really increases
the attention to the movie working and

00:59:02.063 --> 00:59:08.370
being sort of unassuming, which is less
negative than, for instance, just cheap.

00:59:08.370 --> 00:59:11.005
And now correctly
classifies it as positive.

00:59:11.005 --> 00:59:13.665
Yeah?

00:59:13.665 --> 00:59:16.304
&gt;&gt; Why can't it do that in one pass?

00:59:16.304 --> 00:59:22.255
Like see all the words once and
process that?

00:59:22.255 --> 00:59:26.207
Why does it need multiple passes?

00:59:26.207 --> 00:59:28.712
&gt;&gt; So the question is,
why does the model need multiple passes?

00:59:28.712 --> 00:59:30.989
Why couldn't it just do it in one pass?

00:59:30.989 --> 00:59:36.044
I guess the trouble is that, basically,
as you go from left to right,

00:59:36.044 --> 00:59:40.327
and this is, in some ways,
what we think is the reason, and

00:59:40.327 --> 00:59:43.868
the intuitions that we
used to build this model.

00:59:43.868 --> 00:59:49.762
But I can't definitively tell you that
that is exactly why it cannot work.

00:59:49.762 --> 00:59:57.480
It works, so, it works on 50% or so
of the cases, it gets it perfectly right.

00:59:57.480 --> 00:59:58.776
Now, these are just examples.

00:59:58.776 --> 01:00:02.826
It didn't get it right on 0 or 1 pass, and

01:00:02.826 --> 01:00:06.910
basically the difference here is 0.6.

01:00:06.910 --> 01:00:10.030
So in a small subset of the sentences,

01:00:10.030 --> 01:00:13.842
it could only get it
right on multiple passes.

01:00:13.842 --> 01:00:19.490
So intuitively here, what's happening is
you actually agglomerate all the facts.

01:00:19.490 --> 01:00:23.871
And now, with that global viewpoint
of the sentence, you can now go back.

01:00:23.871 --> 01:00:25.480
And having this m vector,

01:00:25.480 --> 01:00:30.086
you can now use the m vector to pay
attention to every single word out there,

01:00:30.086 --> 01:00:36.520
and you can realize, with that, that maybe
some words are more important than others.

01:00:36.520 --> 01:00:40.044
So, that's intuitively what you can do.

01:00:40.044 --> 01:00:42.386
If you only go from left to right once,

01:00:42.386 --> 01:00:48.300
then you cannot really incorporate
the global information at every time step.

01:00:48.300 --> 01:00:50.816
You can only take the information
you've got from the left, or,

01:00:50.816 --> 01:00:52.189
if you have a bi-directional one,

01:00:52.189 --> 01:00:55.284
from the left and the right, but not sort
of the global picture of the sentence.

01:00:59.394 --> 01:01:00.893
All right, here's another fun example.

01:01:00.893 --> 01:01:01.913
The best way to hope for

01:01:01.913 --> 01:01:04.980
any chance of enjoying this film
is by lowering your expectations.

01:01:06.710 --> 01:01:08.100
In the beginning here,

01:01:08.100 --> 01:01:12.349
it basically pays a little bit of
attention to a lot of different things.

01:01:12.349 --> 01:01:16.109
But everything pales in
comparison to the second pass,

01:01:16.109 --> 01:01:19.229
where it really focuses
on the expectations,

01:01:19.229 --> 01:01:24.760
a little bit that they're lowered, and
realizes this is actually negative now.

01:01:25.790 --> 01:01:28.752
And when I say realizes,
I'm anthropomorphizing a little bit here.

01:01:28.752 --> 01:01:30.372
That classifies them correctly.

01:01:30.372 --> 01:01:31.453
Yep?

01:01:36.340 --> 01:01:41.059
The color scheme is the same inside each
plot, but not across different plots.

01:01:48.924 --> 01:01:49.796
That's a great question.

01:01:49.796 --> 01:01:53.504
So, does the attention converge,
or does it shift again?

01:01:53.504 --> 01:01:57.792
We've noticed it's converging in a lot
of cases, but we also didn't run it for

01:01:57.792 --> 01:01:59.310
ten passes or something.

01:01:59.310 --> 01:02:03.179
It might eventually explode or
do something crazy, I don't know.

01:02:03.179 --> 01:02:05.776
But in most cases it does well, but

01:02:05.776 --> 01:02:10.560
then we also notice that it
actually deteriorates sometimes.

01:02:10.560 --> 01:02:12.404
So there are some cases, clearly,

01:02:12.404 --> 01:02:15.732
where it then also deviates again
from what it should have done.

01:02:15.732 --> 01:02:17.150
I'm sort of over-, again,

01:02:17.150 --> 01:02:21.208
anthropomorphizing my model here
a little bit, but overthinking it.

01:02:29.830 --> 01:02:32.160
Are the weights all summing up to one?

01:02:32.160 --> 01:02:33.553
For this model, they do not, no.

01:02:33.553 --> 01:02:35.986
It's just sigmoids at every time step.

01:02:47.494 --> 01:02:48.574
Great question.

01:02:48.574 --> 01:02:51.850
So do we share weights of how we
compose information at each pass?

01:02:51.850 --> 01:02:57.056
So, yeah, so we have these different GRUs
that each, as we go over the inputs,

01:02:57.056 --> 01:03:01.046
and it's actually a hyperparameter
that we evaluate it on.

01:03:01.046 --> 01:03:06.686
And it works slightly better across
several tasks to have separate weights for

01:03:06.686 --> 01:03:09.540
each time you go over the input.

01:03:09.540 --> 01:03:11.827
My hunch is that is a balance between
how much training data do you have.

01:03:11.827 --> 01:03:16.240
If you have enough training data,
it's better to have more parameters there.

01:03:16.240 --> 01:03:18.683
And if you don't have
enough training data,

01:03:18.683 --> 01:03:20.945
you might wanna share each pass weights.

01:03:20.945 --> 01:03:22.651
Great question.

01:03:22.651 --> 01:03:25.427
All right, second-to-last example.

01:03:25.427 --> 01:03:29.365
The film starts out as competent but
unremarkable, and

01:03:29.365 --> 01:03:33.590
gradually grows into something
of considerable power.

01:03:33.590 --> 01:03:38.317
Again, focuses first as competent,
on competent, and

01:03:38.317 --> 01:03:45.121
a little bit out of everything, and power,
and then really hones in on the power.

01:03:45.121 --> 01:03:48.195
And now, the last one here,
I actually like a lot, which is,

01:03:48.195 --> 01:03:51.690
my response to the film is
best described as lukewarm.

01:03:51.690 --> 01:03:56.253
So every normal sentiment algorithm
would overindex, just like this one,

01:03:56.253 --> 01:03:57.976
on the first pass, on best.

01:03:57.976 --> 01:04:03.590
Cuz best, if you run a simple unigram,
base type model, Get very,

01:04:03.590 --> 01:04:09.800
very high certainty that best correlates
very much with a positive sentence.

01:04:09.800 --> 01:04:14.790
But in the second pass it actually
lowered it and our hope here is that it

01:04:14.790 --> 01:04:19.050
realized actually used here as an adverb
and just best describing something.

01:04:19.050 --> 01:04:22.470
But what it's actually
describing is lukewarm, and

01:04:22.470 --> 01:04:25.366
then correctly classifies
it as a negative.

01:04:29.405 --> 01:04:32.305
Now, last task is part-of-speech tagging.

01:04:32.305 --> 01:04:35.801
The difference here is instead of
triggering the answer module only

01:04:35.801 --> 01:04:37.946
at the end of the episodic memory module,

01:04:37.946 --> 01:04:40.550
we actually trigger it at
every single time step.

01:04:40.550 --> 01:04:42.940
So at every single timestep you classify.

01:04:42.940 --> 01:04:45.251
Part of speech tags.

01:04:45.251 --> 01:04:46.940
And when you combine a couple.

01:04:46.940 --> 01:04:50.310
So one thing we don't,
that's not mentioned here.

01:04:50.310 --> 01:04:52.910
Is that we actually have,
I think, two different models.

01:04:52.910 --> 01:04:55.775
And we ensemble to get
to the state of the art.

01:04:55.775 --> 01:05:00.063
But really, personally,
who cares about 0.06

01:05:00.063 --> 01:05:04.652
improvement on a task like
part-of-speech tagging.

01:05:04.652 --> 01:05:11.005
But it's good to see that it can very
accurately also predict sequence tasks.

01:05:11.005 --> 01:05:12.795
That's sort of the main
take away message here.

01:05:14.660 --> 01:05:20.180
Now, in the interest of time, I'll skip
the live demo, and go to another fun fact.

01:05:20.180 --> 01:05:21.680
Or a fun aspect of this model.

01:05:21.680 --> 01:05:27.224
Which is,
we had a new researcher join our group.

01:05:27.224 --> 01:05:31.490
And he had a vision background.

01:05:31.490 --> 01:05:33.750
And so, he said,
well there's this cool new VQA,

01:05:33.750 --> 01:05:35.790
visual question answering dataset.

01:05:35.790 --> 01:05:37.350
Can't we use this model?

01:05:37.350 --> 01:05:38.470
Cuz I was all like, yes!

01:05:38.470 --> 01:05:39.070
A general model.

01:05:39.070 --> 01:05:39.680
It's so great.

01:05:39.680 --> 01:05:40.850
Everything is question answering.

01:05:40.850 --> 01:05:41.661
I was super excited.

01:05:41.661 --> 01:05:46.524
And so, he basically said,
I'll replace the input module with

01:05:46.524 --> 01:05:51.131
one that will give us a sequence
over Images, image blocks.

01:05:51.131 --> 01:05:52.454
And he was also just new,

01:05:52.454 --> 01:05:55.450
we had implemented at this
time everything in Torch.

01:05:55.450 --> 01:05:57.510
We've since moved away from Torch and

01:05:57.510 --> 01:06:01.490
then now came back to it through pytorch,
but different story.

01:06:01.490 --> 01:06:05.290
And basically replaced
the input module and

01:06:05.290 --> 01:06:10.760
checked if we can actually run visual
question answering with this architecture.

01:06:10.760 --> 01:06:13.060
So what is visual question answering?

01:06:13.060 --> 01:06:14.400
Basically, same idea.

01:06:14.400 --> 01:06:18.380
Input, question,
answer as training input, but

01:06:18.380 --> 01:06:22.250
the input is now a picture,
an image instead of a sequence of words.

01:06:24.100 --> 01:06:27.410
So the kinds of questions you might
have here is what kind of tree

01:06:27.410 --> 01:06:28.360
is in the background.

01:06:28.360 --> 01:06:29.491
And the answer should be palm.

01:06:31.645 --> 01:06:33.710
And another simplification here.

01:06:33.710 --> 01:06:38.160
In this particular dataset, the answers,
you can get to way above state of

01:06:38.160 --> 01:06:42.320
the art if you only ever predict a single
word instead of a sequence of words.

01:06:42.320 --> 01:06:44.690
Because most of the answers
are just single words.

01:06:44.690 --> 01:06:48.820
So instead of running a fancy GRU for
always a single time step.

01:06:48.820 --> 01:06:50.540
You just classify a softmax right away.

01:06:50.540 --> 01:06:54.170
And you give the inputs that you
would have given to the GRU,

01:06:54.170 --> 01:06:56.429
directly to a single softmax layer.

01:06:56.429 --> 01:06:59.010
Now, how does this input module change?

01:06:59.010 --> 01:07:03.580
We won't be able to fully appreciate
this figure, to be honest,

01:07:03.580 --> 01:07:06.270
but we can get some intuition.

01:07:06.270 --> 01:07:10.000
We've no convolution networks,
but we don't know this exact

01:07:11.350 --> 01:07:14.756
convolution of networks which
has a lot of bells and whistles.

01:07:14.756 --> 01:07:19.710
On top of it essentially,
intuitively what's happening here is that,

01:07:19.710 --> 01:07:23.570
the convolutional network will give us
a feature of vector representation for

01:07:23.570 --> 01:07:25.390
every block of the image.

01:07:25.390 --> 01:07:30.920
So in the end that every image that we
get as input will chop into 14 by 14 and

01:07:30.920 --> 01:07:34.390
a grid of 14 by 14 and
we have a feature vector for

01:07:34.390 --> 01:07:36.740
every one of these 14 x 14 grids.

01:07:41.223 --> 01:07:42.220
How's that train?

01:07:42.220 --> 01:07:46.440
So you can actually backpropagate
everything jointly, as well.

01:07:46.440 --> 01:07:49.940
But as I mentioned in the beginning of
multitask learning computer vision is

01:07:49.940 --> 01:07:54.100
further ahead in that sense than
natural language processing

01:07:54.100 --> 01:07:57.990
because most people start their
convolution work from a pre trained.

01:07:59.040 --> 01:08:02.230
Convolutional network that
usually is trained on ImageNet.

01:08:13.053 --> 01:08:17.448
So is there yet a project that answers
questions based on both images and

01:08:17.448 --> 01:08:22.750
text, and the answer is there are some
small datasets, but no dataset that.

01:08:22.750 --> 01:08:26.495
I personally find it exciting enough
to,have started working on it.

01:08:26.495 --> 01:08:29.181
It's really a dataset
problem at this point,

01:08:29.181 --> 01:08:34.543
until we have a very compelling dataset
where you collect hundred of thousands,

01:08:34.543 --> 01:08:38.700
ideally questions that you really
need to have both an image and

01:08:38.700 --> 01:08:42.320
some text about the image to
answer the questions from.

01:08:42.320 --> 01:08:45.170
It's tricky,
cuz a lot of times there's some overlap.

01:08:45.170 --> 01:08:47.847
If you take the news, for
instance, sometimes news images.

01:08:47.847 --> 01:08:49.345
I thought about this for awhile, and

01:08:49.345 --> 01:08:51.300
thought about collecting
that kinda dataset.

01:08:51.300 --> 01:08:53.980
But a lot of times news images just show

01:08:53.980 --> 01:08:57.320
sort of some general thing that
is barely related to the content.

01:08:57.320 --> 01:09:00.630
And so, it's non-trivial to find a good
data set where that's really necessary.

01:09:10.802 --> 01:09:13.066
Paintings?

01:09:13.066 --> 01:09:17.070
So the question is, will you be able to do
this kind of model with paintings, too?

01:09:17.070 --> 01:09:18.230
And I don't see why not.

01:09:18.230 --> 01:09:21.740
I mean unless they're super abstract and
you have to really interpret a lot.

01:09:21.740 --> 01:09:25.604
But as long as they're realistic paintings
where you actually see objects I think

01:09:25.604 --> 01:09:29.029
that as long as they're into trained
data it should do reasonably well.

01:09:29.029 --> 01:09:35.530
So let's assume we have a feature vector
for every single region of the image.

01:09:35.530 --> 01:09:39.590
What we're going to do is we're just in
a snake like fashion lay them out and

01:09:39.590 --> 01:09:41.550
now have a sequence.

01:09:41.550 --> 01:09:47.240
And now this sequence is given to
again bidirectional GRU block and

01:09:47.240 --> 01:09:51.710
then the final feature vector that every
time step is just a concatenation of

01:09:51.710 --> 01:09:54.240
the forward and the backward or
the left to right, and

01:09:54.240 --> 01:09:57.250
right to left hidden states of
these by direction you use.

01:10:01.043 --> 01:10:03.804
So essentially we replaced word vectors,

01:10:03.804 --> 01:10:06.800
with feature vectors
from regions of images.

01:10:12.721 --> 01:10:16.655
Now, what was amazing is that literally,
on the second experiment we ran,

01:10:16.655 --> 01:10:18.840
we got to state of
the art on this dataset.

01:10:18.840 --> 01:10:24.575
And we didn't really change the episodic
memory module code, the question code.

01:10:24.575 --> 01:10:29.749
Just the input module changed, and we got
state of the art on this data set, and

01:10:29.749 --> 01:10:35.030
what's even more fun is looking now
at this attention visualizations.

01:10:35.030 --> 01:10:36.550
Again, it's unsupervised,

01:10:36.550 --> 01:10:42.280
the model is never given inputs on
to where it should pay attention to,

01:10:42.280 --> 01:10:47.890
to answer a certain kind of question,
so here we basically visualize this, so

01:10:47.890 --> 01:10:54.050
this is kind of the equivalent plot to
what I showed you about the sentiment.

01:10:54.050 --> 01:10:58.140
But the difference here is,
instead of being very dark,

01:11:02.801 --> 01:11:07.048
Instead of being very dark blue,
it's white.

01:11:07.048 --> 01:11:11.814
So the whiter it is,
the higher the attention score here is for

01:11:11.814 --> 01:11:13.883
that region of the image.

01:11:13.883 --> 01:11:17.095
And so, when we ask what is
the main color on the bus,

01:11:17.095 --> 01:11:22.220
it actually literally pays attention to
that bus, and then gives the answer blue.

01:11:25.407 --> 01:11:29.160
And so, we can ask what type of
trees are in the background.

01:11:29.160 --> 01:11:31.710
Pays attention to the background and
trees.

01:11:31.710 --> 01:11:32.837
Answer's pine.

01:11:32.837 --> 01:11:34.680
How many pink flags are there?

01:11:34.680 --> 01:11:37.820
Pays attention to the pink flags and
gives the answer, two.

01:11:37.820 --> 01:11:41.730
Now, in general, number questions
are actually not that great.

01:11:41.730 --> 01:11:48.099
So you can see here that, while this is
sort of the, close to the best model.

01:11:48.099 --> 01:11:51.927
None of the models actually do really
great on numbering because the attention

01:11:51.927 --> 01:11:56.051
mechanisms that all these different kinds
of models are using are very continuous.

01:11:56.051 --> 01:12:00.200
All we're doing is as we're
agglomerating facts into this GRU,

01:12:00.200 --> 01:12:04.520
we have high attention score and so
we're agglomerating that region.

01:12:04.520 --> 01:12:06.560
But these models don't
have a discrete sense,

01:12:06.560 --> 01:12:10.300
like this is a discrete object and
this is another discrete object.

01:12:10.300 --> 01:12:15.011
And so, also if you have 50
objects it would never give you

01:12:15.011 --> 01:12:20.384
the answer 50 because it can't count
in such a fine grained way and

01:12:20.384 --> 01:12:25.944
of course, we have the problem that,
it's a classifier in the end and

01:12:25.944 --> 01:12:30.184
so if it's never seen 39
objects at training time,

01:12:30.184 --> 01:12:34.840
it cannot produce the number or
the answer 39 as a class.

01:12:34.840 --> 01:12:39.541
So there's actually still problems with
this but in this particular data set

01:12:39.541 --> 01:12:43.320
most of the number of objects
actually are relatively small.

01:12:43.320 --> 01:12:44.384
Is this in the wild?

01:12:44.384 --> 01:12:45.790
It's kind of interesting actually.

01:12:45.790 --> 01:12:50.020
It pays attention to the man-made
structure, the house or barn or

01:12:50.020 --> 01:12:52.170
whatever it is in the background here,
and the answer is no.

01:12:53.340 --> 01:12:57.590
And so, I was still pretty skeptical in
the beginning of why it's doing so well.

01:12:57.590 --> 01:12:59.620
But through these
attention visualizations,

01:12:59.620 --> 01:13:02.320
I actually felt much
better about this model.

01:13:02.320 --> 01:13:06.510
It's really clearly learning
something about this domain.

01:13:07.580 --> 01:13:10.639
So again, sometimes,
you can overinterpret too much.

01:13:10.639 --> 01:13:12.450
So who is on both photos?

01:13:12.450 --> 01:13:16.280
It's not like the model actually gained
an understanding of that there are two

01:13:16.280 --> 01:13:17.320
photos.

01:13:17.320 --> 01:13:21.544
And then, captures,
is their face in the same person and both.

01:13:21.544 --> 01:13:26.432
The majority object that you would
answer to a who question on this is

01:13:26.432 --> 01:13:29.280
just a baby girl, so it just says girl.

01:13:30.780 --> 01:13:32.040
This one again is very fun.

01:13:32.040 --> 01:13:33.290
What is the boy holding?

01:13:33.290 --> 01:13:36.130
Actually, literally learns to
pay attention to the arm and

01:13:36.130 --> 01:13:38.130
then the surfboard and
gives answer surfboard.

01:13:38.130 --> 01:13:40.340
So at this point I felt like,

01:13:40.340 --> 01:13:46.250
this is more than just sort of learning
facts from just the language itself.

01:13:46.250 --> 01:13:48.923
It really takes into
consideration image and

01:13:48.923 --> 01:13:52.238
this one here's actually
another fun example of that.

01:13:52.238 --> 01:13:58.263
Cuz there are some baselines where people
compare on just looking at the image and

01:13:58.263 --> 01:14:02.320
answering, ignoring the actual question.

01:14:02.320 --> 01:14:04.895
It's like here's an image,
what's the answer?

01:14:04.895 --> 01:14:10.680
[LAUGH] And that also does 28% of
the time does the right thing.

01:14:10.680 --> 01:14:14.987
So it's just their certain patterns when
you ask a question about this image,

01:14:14.987 --> 01:14:18.658
what are they gonna ask and
it's capturing that sort of baseline.

01:14:18.658 --> 01:14:22.169
In some ways,
even is if you just look at the question,

01:14:22.169 --> 01:14:26.080
what is the bar holding in almost half
of the cases also gets it right not

01:14:26.080 --> 01:14:28.055
having to look at the image at all.

01:14:28.055 --> 01:14:29.679
&gt;&gt; [LAUGH]
&gt;&gt; So

01:14:29.679 --> 01:14:34.466
sometimes we glance over these tables, but
it's important in both your projects and

01:14:34.466 --> 01:14:36.870
that's what we'll do when we grade them.

01:14:36.870 --> 01:14:41.791
Actually you've really critically
questioned what's going on in those

01:14:41.791 --> 01:14:42.430
tables.

01:14:42.430 --> 01:14:47.436
So really, what this model has been able
to do when this data set first came out,

01:14:47.436 --> 01:14:50.150
we combine the two, question and images.

01:14:50.150 --> 01:14:55.916
Read just only 4% or
less better than just the question alone.

01:14:55.916 --> 01:15:01.102
But then this model does around
8% better than this model,

01:15:01.102 --> 01:15:06.000
and around 12 or so
than just looking at the question alone.

01:15:06.000 --> 01:15:09.290
And this is a good example of
where it actually took the image

01:15:09.290 --> 01:15:13.410
into consideration namely, the question
here is what color of the bananas?

01:15:13.410 --> 01:15:15.390
And if you just look at the question and

01:15:15.390 --> 01:15:21.150
the default answer that you'd probably
be pretty good estimate would be yellow.

01:15:21.150 --> 01:15:25.340
But in this particular Image the bananas
that it's paying attention to are not

01:15:25.340 --> 01:15:28.000
quite ripe yet and
it learns to give the answer green.

01:15:29.150 --> 01:15:32.560
Last one here is like what's the pattern
on the cat's fur on its tail?

01:15:32.560 --> 01:15:35.452
Actually pays attention to the tail and
says stripe.

01:15:35.452 --> 01:15:37.180
So some of them are pretty incredible.

01:15:37.180 --> 01:15:42.093
And so, we then had put together
a demo to play around with it and

01:15:42.093 --> 01:15:46.600
these were eight questions,
that we asked this demo.

01:15:46.600 --> 01:15:50.340
And I was actually kind of
surprised how good it was.

01:15:50.340 --> 01:15:51.510
So what is the girl holding?

01:15:51.510 --> 01:15:52.510
A tennis racket.

01:15:52.510 --> 01:15:53.310
What is the girl doing?

01:15:53.310 --> 01:15:53.910
Playing tennis.

01:15:53.910 --> 01:15:55.420
These are kind of simple.

01:15:55.420 --> 01:15:58.810
And of course, it has to have seen
these kinds of answers before.

01:15:58.810 --> 01:16:01.700
It has to have seen
pictures from that domain.

01:16:01.700 --> 01:16:02.380
Is the girl wearing a hat?

01:16:02.380 --> 01:16:05.130
That's actually,
a journalist had asked this.

01:16:05.130 --> 01:16:09.030
And I was already coming up with excuses,
because the hat's sort of black and

01:16:09.030 --> 01:16:10.414
it's a black background.

01:16:10.414 --> 01:16:11.534
But then I got it right.

01:16:11.534 --> 01:16:15.060
And then,
what is the girl wearing, shorts.

01:16:15.060 --> 01:16:17.780
And what's interesting also, when you ask
what it's wearing, it says shorts, but

01:16:17.780 --> 01:16:21.010
when you ask what color's her skirt,
it actually says white.

01:16:21.010 --> 01:16:22.480
So it's kind of an interesting.

01:16:23.560 --> 01:16:26.568
Robustness in some ways to the questions.

01:16:26.568 --> 01:16:30.807
And then, I ask what color's the ground
and said brown and then I was like,

01:16:30.807 --> 01:16:35.465
well the brown's the majority color and
so I asked what color is the ball.

01:16:35.465 --> 01:16:41.599
And actually got it right despite the ball
being a very small part of the image.

01:16:41.599 --> 01:16:46.718
And so, eventually the way I found it,
I broke sort of this demo,

01:16:46.718 --> 01:16:51.015
was I asked,
is the woman about to hit the ball?

01:16:51.015 --> 01:16:53.805
And it said, yes and then I asked,
did the woman just hit the ball?

01:16:53.805 --> 01:16:57.310
And it said, yes again,
and I was like, all right,

01:16:57.310 --> 01:17:01.880
that was the last one, but again it
boils down to having seen enough times,

01:17:01.880 --> 01:17:06.830
of certain angles I guess of the arm, and
then that kind of question, and so on.

01:17:06.830 --> 01:17:09.915
So I don't think it's something in theory,
it could never pick up, but

01:17:09.915 --> 01:17:11.195
it just didn't have enough training data.

01:17:13.665 --> 01:17:18.815
So in summary, I hope I could show you and
motivate you and excite you for

01:17:18.815 --> 01:17:24.205
your PA-4 and the various question
answering projects that you're working on.

01:17:24.205 --> 01:17:28.005
Cuz in the end, question answering is
really one of the most interesting tasks,

01:17:28.005 --> 01:17:29.575
I think, in natural language processing.

01:17:29.575 --> 01:17:33.500
And a lot of the tasks that other
task that we looked at in the class.

01:17:33.500 --> 01:17:38.195
you could incorporate, I encourage you
actually to think about your projects in

01:17:38.195 --> 01:17:43.040
PA-4 and extensions, and like could you
incorporate other kinds of tasks into

01:17:43.040 --> 01:17:47.750
your data set and then see what happens
when you try to train them jointly.

01:17:47.750 --> 01:17:52.000
The dynamic memory network can quite
accurately solve a whole variety of

01:17:52.000 --> 01:17:58.160
different QA tasks, but as we'll talk
about next week's lecture, in one of them,

01:17:58.160 --> 01:18:02.780
there are actually also extensions to this
where you can do dynamic generation of

01:18:02.780 --> 01:18:09.110
answers and pay co-attention of
the inputs and the question jointly.

01:18:09.110 --> 01:18:12.338
So still more work to be done on it.

01:18:12.338 --> 01:18:13.730
Thank you.

