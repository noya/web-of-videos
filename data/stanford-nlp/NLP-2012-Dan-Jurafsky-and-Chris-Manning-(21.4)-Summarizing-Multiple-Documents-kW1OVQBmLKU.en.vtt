WEBVTT
Kind: captions
Language: en

00:00:01.020 --> 00:00:05.320
Let's conclude this section by looking
at more advanced research systems

00:00:05.320 --> 00:00:07.660
that try to answer much
more complex questions.

00:00:09.350 --> 00:00:12.820
So consider the following definition
question, what is water spinach?

00:00:12.820 --> 00:00:17.240
And I've given a potential long answer
to the question what is water spinach.

00:00:17.240 --> 00:00:21.422
Could we generate answers to definition
questions like this if they, let's say

00:00:21.422 --> 00:00:25.801
they didn't occur in a good place, on the
web we wanted to generate them ourselves?

00:00:25.801 --> 00:00:27.763
Or take a medical question,

00:00:27.763 --> 00:00:32.685
what is the efficacy of a particular
therapy for a particular disease?

00:00:32.685 --> 00:00:35.263
We might want to give an answer and
this one is from

00:00:35.263 --> 00:00:39.140
a particular paper in the PubMed
database that answers this question.

00:00:39.140 --> 00:00:43.305
So difficult, hard to answer questions
that might involve summarizing one or

00:00:43.305 --> 00:00:45.040
more documents.

00:00:45.040 --> 00:00:49.460
There is in fact a competition on
answering such complex questions.

00:00:49.460 --> 00:00:52.370
And I've given you some simplified
versions of some of those questions.

00:00:52.370 --> 00:00:54.770
How is compost made and
used for gardening?

00:00:54.770 --> 00:00:56.510
What causes train wrecks?

00:00:56.510 --> 00:00:57.750
What can be done to prevent them?

00:00:58.800 --> 00:01:02.840
What's the human toll in death or
injury of tropical storms in recent years?

00:01:02.840 --> 00:01:06.805
So these are the kind of questions that
to answer them you'd want to read lot of

00:01:06.805 --> 00:01:11.441
documents automatically, pull information
from them and summarize this information,

00:01:11.441 --> 00:01:13.138
so it's a very difficult task.

00:01:13.138 --> 00:01:17.390
Answering these harder questions
is the task of query focus

00:01:17.390 --> 00:01:19.040
multiple document summarization.

00:01:19.040 --> 00:01:21.820
And there are two standard algorithms for

00:01:21.820 --> 00:01:26.610
this, we might call the bottom-up or
snippet style method.

00:01:26.610 --> 00:01:29.170
It's just like we saw for
single document summarization.

00:01:29.170 --> 00:01:31.570
We're going to find a set of
relevant documents first,

00:01:31.570 --> 00:01:34.710
now we're going to extract informative
sentences from the documents and

00:01:34.710 --> 00:01:37.050
then maybe we'll do some ordering and
modification.

00:01:37.050 --> 00:01:39.770
So we're just going to grab sentences
from documents and mix them together.

00:01:40.850 --> 00:01:43.490
The top down, or
information extraction method,

00:01:43.490 --> 00:01:46.530
we're going to build specific answers for
different question types.

00:01:46.530 --> 00:01:50.010
We'll build an answer for definition
questions and one for biography questions

00:01:50.010 --> 00:01:54.240
and so on by extracting particular
information needed for those questions.

00:01:54.240 --> 00:01:56.830
And we'll see both of these
in this little lecture.

00:01:58.400 --> 00:02:00.370
So here is the snippet based method,

00:02:00.370 --> 00:02:04.220
the bottom-up method for query
focused multi document summarization.

00:02:04.220 --> 00:02:07.530
We're going to start, as again

00:02:07.530 --> 00:02:11.920
by grabbing all the sentences from now
multiple documents, not just one document.

00:02:11.920 --> 00:02:15.290
So we have a set of sentences and
we're going to modify them.

00:02:15.290 --> 00:02:17.350
It's common to do sentence inculcation.

00:02:17.350 --> 00:02:19.296
We'll take a sentence,
I'll show you in a second and

00:02:19.296 --> 00:02:20.730
we'll simplify them in various ways.

00:02:20.730 --> 00:02:23.130
We'll have lots of different
simplified versions of sentences.

00:02:23.130 --> 00:02:27.220
So I've got lots of families
of clouds of sentences.

00:02:27.220 --> 00:02:31.180
And we're going to apply the log
likelihood ratio test and

00:02:31.180 --> 00:02:35.360
other methods of pulling good
sentences from this set of sentences.

00:02:35.360 --> 00:02:39.120
We now have a set of extracted sentences,
the little black dots I've marked here.

00:02:39.120 --> 00:02:42.110
And we're going to order those extracted
sentences, as we talked about for

00:02:42.110 --> 00:02:43.260
single document summarization,

00:02:43.260 --> 00:02:47.570
and we're going to modify them in various
ways to produce the realized sentences.

00:02:50.010 --> 00:02:54.350
We often simplify sentences to get
rid of unimportant details that

00:02:54.350 --> 00:02:56.720
would make the summary too long.

00:02:56.720 --> 00:03:00.040
And then one of the most common
ways to do this involves parsing.

00:03:00.040 --> 00:03:03.930
We parse the sentences and then we have
hand written rules based on the parse tree

00:03:03.930 --> 00:03:07.220
that suggest which modifiers
are better to prune.

00:03:08.510 --> 00:03:12.090
So for example, appositives
are the kind of thing you might prune.

00:03:12.090 --> 00:03:15.280
So which an artist who was living
at the time in Philadelphia,

00:03:15.280 --> 00:03:19.860
we can eliminate that or
attribution clause.

00:03:19.860 --> 00:03:22.330
International observers said
Tuesday blah blah blah,

00:03:22.330 --> 00:03:25.990
well we care more about what they
said that rebels agreed to talks.

00:03:25.990 --> 00:03:29.700
That's an important fact then we can
delete maybe the attribution clause in

00:03:29.700 --> 00:03:31.190
our summary.

00:03:31.190 --> 00:03:34.150
Prepositional phrases, especially those
without named entities turn out to be

00:03:34.150 --> 00:03:38.110
the kind of thing that we can delete and
keep a summary more concise.

00:03:38.110 --> 00:03:41.440
So the increased to a sustainable number,

00:03:41.440 --> 00:03:44.080
maybe a shorter summary
we just say increased.

00:03:44.080 --> 00:03:47.100
And adverbials at the beginning like for
example or

00:03:47.100 --> 00:03:51.960
on the other hand or as a matter of fact,
things that mattered in a longer document

00:03:51.960 --> 00:03:55.260
that aren't going to be appropriate in
a small abstract or a small summary.

00:03:55.260 --> 00:03:57.570
So the simplest method is taking,

00:03:57.570 --> 00:04:02.250
writing various roles of this kind and
each of these papers gives you another,

00:04:02.250 --> 00:04:06.720
some interesting sets of rules that
you can use to simplify the sentences.

00:04:06.720 --> 00:04:12.060
So once we've done this,
we have our large set of sentences,

00:04:12.060 --> 00:04:15.600
including the original sentence, and
the one with the appositive deleted, and

00:04:15.600 --> 00:04:17.240
the ones with the PPs deleted, and so on.

00:04:17.240 --> 00:04:20.190
We have a whole bunch of different
sentences, the original and

00:04:20.190 --> 00:04:22.120
various shortened versions.

00:04:22.120 --> 00:04:23.500
What do we do next?

00:04:23.500 --> 00:04:28.850
Well now we'd like to select from
these very redundant set of sentences

00:04:30.220 --> 00:04:33.360
just the ones to put in the summary,
and their redundant for two reasons,

00:04:33.360 --> 00:04:36.710
they come from multiple documents that
might talk about the same event and

00:04:36.710 --> 00:04:40.670
we've added these simplified sentences, so
there are all sorts of redundancies there.

00:04:40.670 --> 00:04:44.760
One iterative method for
content selection given these redundant

00:04:44.760 --> 00:04:49.270
sentences is called MMR,
Maximal Marginal Relevance.

00:04:49.270 --> 00:04:50.490
And the idea is as follows.

00:04:50.490 --> 00:04:53.070
We're going to iteratively or
greedily choose

00:04:54.100 --> 00:04:58.550
the best sentence from our set of
sentences to insert in our summary so far.

00:04:58.550 --> 00:05:01.690
And we want the best sentence
to satisfy two properties.

00:05:01.690 --> 00:05:05.360
We want it to be maximally
relevant to the user's query.

00:05:05.360 --> 00:05:07.990
So we might do something simple like just

00:05:07.990 --> 00:05:11.340
ensure that is has high cosine
similarity to the query.

00:05:11.340 --> 00:05:13.060
And we want it to be novel,

00:05:13.060 --> 00:05:16.110
we want it to be minimally
redundant with the summary so far.

00:05:16.110 --> 00:05:19.558
If we've just put in a sentence, we don't
want to put in a variant of that sentence,

00:05:19.558 --> 00:05:20.657
that's mostly the same.

00:05:20.657 --> 00:05:24.251
And we might measure that by just
measuring its cosine similarity to

00:05:24.251 --> 00:05:27.984
the summary and choosing sentences
that have low cosine similarity.

00:05:27.984 --> 00:05:33.999
So one version of MMR might be, we'll have
a lambda for weighing these two factors.

00:05:33.999 --> 00:05:38.338
We want a sentence that has a high
similarity with the query and

00:05:38.338 --> 00:05:42.843
which has a low, a subtract out,
the similarity of the sentence

00:05:42.843 --> 00:05:47.200
with its most similar sentence
that's in the summary so far.

00:05:47.200 --> 00:05:51.660
So pick sentences that don't look like
any of the sentences in the summary so

00:05:51.660 --> 00:05:54.140
far but do look like the query.

00:05:54.140 --> 00:05:56.480
And we'll add those
sentences in iteratively and

00:05:56.480 --> 00:06:00.150
we'll stop by some criterion perhaps
when we've achieved our desired length.

00:06:00.150 --> 00:06:05.620
Now we're going to want to combine
the intuitions of log likelihood ratio,

00:06:05.620 --> 00:06:10.610
picking informative sentences and MMR,
choosing non-redundant sentences.

00:06:10.610 --> 00:06:15.410
So one of the many ways we might combine
intuitions is to start by scoring every

00:06:15.410 --> 00:06:20.150
sentence based on log likelihood ratio
using the words that either occurred

00:06:20.150 --> 00:06:25.610
more than we expect by chance or
words that occurred in the query.

00:06:25.610 --> 00:06:29.190
And now start by including the sentence
with the highest log likelihood ratio

00:06:29.190 --> 00:06:33.270
score in the summary, and now start
iteratively adding into that summary other

00:06:33.270 --> 00:06:36.663
sentences that are not redundant with
the sentences with the summary so far.

00:06:39.300 --> 00:06:43.070
And when we had single document
summaries we could pick document order

00:06:43.070 --> 00:06:45.010
as our ordering for
the sentences in the summary.

00:06:45.010 --> 00:06:48.990
That's a little harder to do if our
sentences come from multiple documents.

00:06:48.990 --> 00:06:50.346
So there's various things we can do,

00:06:50.346 --> 00:06:54.580
we can if we're summarizing news,
we can do chronological ordering.

00:06:54.580 --> 00:06:57.400
We can look at the date that
the article came out and

00:06:57.400 --> 00:07:00.700
order the sentences in
order of actual time.

00:07:01.950 --> 00:07:04.460
We can choose coherence based ordering.

00:07:04.460 --> 00:07:07.720
We can put sentences near each other,
if they have similar meaning.

00:07:07.720 --> 00:07:11.100
So we can look at the cosine
between the two sentences and

00:07:11.100 --> 00:07:14.700
sentences that have a high cosine
we could put next to each other.

00:07:14.700 --> 00:07:18.830
Or perhaps, we could look at
the entities discussed by the sentences,

00:07:18.830 --> 00:07:23.670
in two sentences, talk about the same
entity we can put them near each other.

00:07:23.670 --> 00:07:26.710
Or we can even do some kind of fancy
method where we look at the source

00:07:26.710 --> 00:07:30.870
documents and look at the actual semantic
topics that happen in the documents.

00:07:30.870 --> 00:07:33.290
And we could order those,
they can order any and

00:07:33.290 --> 00:07:35.770
apply that to our output sentences.

00:07:35.770 --> 00:07:37.810
So various ways we can
do information ordering.

00:07:39.400 --> 00:07:41.020
So that's the bottom-up or

00:07:41.020 --> 00:07:45.510
snippet based approach to doing query
focused, multi document summarization.

00:07:45.510 --> 00:07:49.902
We grab a whole bunch of sentences,
we rank them by log likelihood ratio and

00:07:49.902 --> 00:07:53.547
MMR informer summary,
that way we order them by some method.

00:07:56.283 --> 00:07:59.656
The alternative method,
the information extraction method,

00:07:59.656 --> 00:08:03.093
is used when we have a particular
kind of question that we want to,

00:08:03.093 --> 00:08:06.100
we know what kind of things we
expect to put in the answer.

00:08:06.100 --> 00:08:09.822
So we know, for example, that a good
biography contains a birth and death,

00:08:09.822 --> 00:08:13.732
maybe why they're famous, their education,
their nationality, and so on.

00:08:13.732 --> 00:08:17.888
Whereas a good definition contains what
is called a genus or hypernym sentence.

00:08:17.888 --> 00:08:19.349
The Hajj is, what is it?

00:08:19.349 --> 00:08:20.480
It's a type of ritual.

00:08:21.785 --> 00:08:25.095
Or a medical answer about a drug's use.

00:08:25.095 --> 00:08:29.175
We want to talk about a problem,
a medical condition, we want to talk about

00:08:29.175 --> 00:08:32.815
what is this intervention, this drug or
procedure, and what's the outcome?

00:08:32.815 --> 00:08:34.925
So knowing these types of questions,

00:08:34.925 --> 00:08:38.375
we can know the types of things we
might want to see in the answer.

00:08:40.850 --> 00:08:44.445
So we could then build a little detector,

00:08:44.445 --> 00:08:48.236
let's say that finds genus
sentences in the documents.

00:08:48.236 --> 00:08:51.050
Find me a genus sentence,
find me a species sentence,

00:08:51.050 --> 00:08:56.620
find the dates this person was born or
died or was educated.

00:08:56.620 --> 00:08:57.780
Find where they went to school,

00:08:57.780 --> 00:09:01.790
or if we're talking about the drug,
who was the study run on?

00:09:01.790 --> 00:09:03.830
What is the actual intervention and so on.

00:09:03.830 --> 00:09:08.020
So we can build classifiers to
extract these kind of information.

00:09:08.020 --> 00:09:12.030
So let's look at an example of
system from Blair-Goldensohn

00:09:12.030 --> 00:09:15.740
that does this kind of complex question
answering for definition questions.

00:09:15.740 --> 00:09:18.160
We have a question, what is the Hajj?

00:09:18.160 --> 00:09:22.230
And we specify, let's say, that we're
going to look at 20 documents to pull our

00:09:22.230 --> 00:09:25.120
summary out and we want a summary
of length, eight sentences long.

00:09:25.120 --> 00:09:29.560
So we're going to extract all of our
documents, we're going whole out,

00:09:29.560 --> 00:09:35.380
we say this is a definition question so
we know we need a genus species sentence.

00:09:35.380 --> 00:09:37.250
So we run our classifier and

00:09:37.250 --> 00:09:41.470
find all of our genus species
sentences in all of these documents.

00:09:41.470 --> 00:09:45.850
And now we find the other sentences and
we're going to, for each of the pieces of

00:09:45.850 --> 00:09:48.680
information that we need,
build little clusters of sentences and

00:09:48.680 --> 00:09:52.510
use your log likelihood equal ratio and
MMR and other approaches to decide which

00:09:52.510 --> 00:09:56.570
sentence to pick and then we're going
to paste these into a definition.

00:09:58.620 --> 00:10:02.050
So we've talked about two methods for
answering these complex,

00:10:02.050 --> 00:10:05.470
difficult questions that require
looking at multiple documents.

00:10:05.470 --> 00:10:08.180
We talked about the bottom-up,
snippet based method,

00:10:08.180 --> 00:10:12.585
that uses log likelihood ratio and
MMR to choose

00:10:12.585 --> 00:10:16.665
just the right non redundant informative
sentences from lots of documents.

00:10:16.665 --> 00:10:19.665
We've talked about an information
extraction style method

00:10:19.665 --> 00:10:24.485
in which we build separate templates for
each question type and pull out particular

00:10:24.485 --> 00:10:28.665
attributes that are important, things
to put in the answer for that question.

00:10:29.760 --> 00:10:33.280
Of course research on this harder
questions that require looking at multiple

00:10:33.280 --> 00:10:36.830
documents is really just beginning and
this is going to be an important and

00:10:36.830 --> 00:10:41.350
exciting area for us to follow in the
future to see where their research goes.

