WEBVTT
Kind: captions
Language: en

00:00:01.600 --> 00:00:04.110
Let's introduce
The Noisy Channel Model of Spelling.

00:00:05.660 --> 00:00:08.370
The intuition of the noisy channel,
when it comes up

00:00:08.370 --> 00:00:11.930
throughout natural language processing,
is that we have some original signal.

00:00:11.930 --> 00:00:14.210
Let's say, it's a word.

00:00:14.210 --> 00:00:17.090
And, we imagine that it
goes through some channel.

00:00:17.090 --> 00:00:20.890
And the idea was originally invented for
speech, where if you talk into a tube or

00:00:20.890 --> 00:00:25.100
we go over some kind of telecommunications
line, and the word is distorted.

00:00:25.100 --> 00:00:28.203
And so, what comes out from the original
word is some noisy word, and

00:00:28.203 --> 00:00:30.435
we've represented that
here with a weird font.

00:00:30.435 --> 00:00:35.381
But, in the spelling case we imagine that,
somebody mistyped the word.

00:00:35.381 --> 00:00:38.516
So the channel is the typewriter,
or the person typing, or

00:00:38.516 --> 00:00:42.106
the keyboard and at the end you get
a misspelled version of the word.

00:00:42.106 --> 00:00:47.608
And our goal in the noisy channel model is
to take that output of that noisy process,

00:00:47.608 --> 00:00:50.281
and by modeling how this channel works,

00:00:50.281 --> 00:00:53.800
we built a probabilistic
model of this channel.

00:00:53.800 --> 00:00:57.530
We can run all possible original
words through that channel and

00:00:57.530 --> 00:01:01.190
see which one looks the most
like the noisy word.

00:01:01.190 --> 00:01:05.130
So, the decoder will take
a bunch of hypotheses for

00:01:05.130 --> 00:01:08.180
each one, run it through the channel.

00:01:08.180 --> 00:01:12.266
Run hypothesis 2 through the channel,
run hypothesis 3 through the channel,

00:01:12.266 --> 00:01:15.146
and we see which word looks
the most like this noisy word,

00:01:15.146 --> 00:01:18.950
then we pick that as the original
hypothesis for the word that started out.

00:01:20.090 --> 00:01:21.090
So let's look at that.

00:01:21.090 --> 00:01:24.103
First, we'll introduce some probability,
and then we'll look at some examples.

00:01:26.910 --> 00:01:28.560
The Noisy Channel is
a probabilistic model.

00:01:30.290 --> 00:01:35.404
Our goal, given an observation x of some
misspelling, some word we've seen, some

00:01:35.404 --> 00:01:40.891
surface thing we've seen, some observation
x, we'd like to find w, the correct word.

00:01:40.891 --> 00:01:44.719
And we're going to model that
probabilistically by saying, we're

00:01:44.719 --> 00:01:49.569
looking, the best word, the word that we'd
like to replace our misspelling with,

00:01:49.569 --> 00:01:53.493
is that word out of the vocabulary
that maximizes the probability.

00:01:53.493 --> 00:01:55.070
What probability?

00:01:55.070 --> 00:01:57.730
The probability of the word,
given the misspelling.

00:01:57.730 --> 00:02:02.490
So what word, given that we've seen some
misspelling, what's the most likely word?

00:02:02.490 --> 00:02:05.820
The most probable, posterior,
probable word, given that misspelling?

00:02:05.820 --> 00:02:11.700
And we're going to use Bayes Rule
to replace that probability.

00:02:11.700 --> 00:02:13.750
So, the probability of w given x,

00:02:13.750 --> 00:02:17.870
we're going to replace that with
P(x given w) P(w), over P(x).

00:02:19.840 --> 00:02:26.060
And so,
we can also eliminate the denominator.

00:02:26.060 --> 00:02:32.780
So, whatever word maximizes this equation
will also maximize this equation.

00:02:32.780 --> 00:02:36.890
We're asking given a misspelling x,
what's the most likely word?

00:02:36.890 --> 00:02:41.890
And since the formula for that probability
includes the probability of that word,

00:02:41.890 --> 00:02:44.370
the misspelling x,

00:02:44.370 --> 00:02:49.500
we're including that probability
in every w that we're considering.

00:02:49.500 --> 00:02:54.220
So, if some w, let's say, w hypothesis 1,

00:02:54.220 --> 00:02:59.560
has a greater probability than
hypothesis 2, by this equation,

00:02:59.560 --> 00:03:04.200
it'll also have a greater probability by
this equation because x is a constant.

00:03:04.200 --> 00:03:07.400
x is the misspelling that we're
trying to decide if w1 or

00:03:07.400 --> 00:03:08.540
w2 is a better hypothesis for.

00:03:10.750 --> 00:03:14.010
So, that means that the Noisy Channel

00:03:15.530 --> 00:03:20.960
model comes down to maximizing
the product of two factors,

00:03:20.960 --> 00:03:26.390
the likelihood, and the prior.

00:03:29.500 --> 00:03:32.511
And we generally call this
term the language model.

00:03:32.511 --> 00:03:35.950
And you've seen language models before.

00:03:35.950 --> 00:03:38.440
That's the probability of the error.

00:03:38.440 --> 00:03:41.360
Excuse me, that's the probability
of the correct word, w.

00:03:42.610 --> 00:03:46.120
And this likelihood term,
we often call this the Channel

00:03:46.120 --> 00:03:51.670
model, or sometimes the error model.

00:03:53.190 --> 00:03:57.540
So, we've got two factors,
the language model and the channel model.

00:03:57.540 --> 00:04:03.140
And the intuition is that the language
model tells us how likely

00:04:03.140 --> 00:04:07.940
would this word be to be a word, perhaps
in this context, perhaps by itself.

00:04:07.940 --> 00:04:08.950
The channel model says,

00:04:08.950 --> 00:04:13.720
well, if it was that word, how likely
would it be to generate this exact error?

00:04:13.720 --> 00:04:17.310
So a channel model is sort of modeling
that noisy channel that turns

00:04:17.310 --> 00:04:19.300
the correct word into the misspelling.

00:04:20.680 --> 00:04:24.586
Now this noisy channel model for
spelling was proposed around 1990,

00:04:24.586 --> 00:04:27.116
independently at two
separate laboratories.

00:04:27.116 --> 00:04:32.060
And the use of speech recognition models
like Noisy channel, came into natural

00:04:32.060 --> 00:04:37.006
language processing right around then,
mainly, although not exclusively,

00:04:37.006 --> 00:04:41.250
because of the work at these two labs,
at IBM and at AT&amp;T Bell Labs.

00:04:41.250 --> 00:04:45.380
And so, the examples we're going to
take for the rest of this example

00:04:45.380 --> 00:04:50.273
come from these two important early papers
by Mays et al and by Kernighan et al.

00:04:52.431 --> 00:04:54.240
So, let's look at an example.

00:04:54.240 --> 00:04:56.267
Here's a misspelling.

00:04:56.267 --> 00:04:59.313
The word a-c-r-e-s-s, so,
think for yourself for

00:04:59.313 --> 00:05:00.966
a second what this could mean.

00:05:05.397 --> 00:05:08.130
First, we're going to start
with generating candidates.

00:05:08.130 --> 00:05:10.810
What are the possible candidate
words to replace this word?

00:05:12.410 --> 00:05:15.250
And we can think of at least
a couple of obvious ways to do this.

00:05:15.250 --> 00:05:18.790
One is, we're going to pick words
that have similar spelling.

00:05:18.790 --> 00:05:22.985
So, words that have similar spelling
might naturally be mistaken for

00:05:22.985 --> 00:05:24.130
the correct word.

00:05:24.130 --> 00:05:29.001
And we're going to operationalize
similar spelling as having a small

00:05:29.001 --> 00:05:31.060
edit distance to the error.

00:05:31.060 --> 00:05:34.560
Or, we could pick words with similar
pronunciation, and there we're going to

00:05:34.560 --> 00:05:37.770
pick words with a small edit distance
of the pronunciation to the error.

00:05:37.770 --> 00:05:41.072
For the rest of this example,
I'm going to pick the first approach.

00:05:41.072 --> 00:05:43.104
So, we're going to pick words
that have similar spelling as

00:05:43.104 --> 00:05:44.060
our possible candidates.

00:05:45.840 --> 00:05:48.460
How do I operationalize similar spelling?

00:05:48.460 --> 00:05:50.600
Well, we've seen edit distance before.

00:05:50.600 --> 00:05:55.520
And remember, with edit distance we talked
about the distance between two strings.

00:05:55.520 --> 00:05:58.320
The minimal number of edits that
turns one string into another.

00:05:58.320 --> 00:06:01.330
We defined an edit as an insertion,
deletion, or

00:06:01.330 --> 00:06:02.810
a substitution, so any of these three.

00:06:04.180 --> 00:06:06.150
For spell correction,

00:06:06.150 --> 00:06:10.360
we're going to want to add a fourth
possible edit operation, transposition,

00:06:10.360 --> 00:06:13.820
because in practice for spelling
errors we often transpose two letters.

00:06:13.820 --> 00:06:19.290
And that version of edit distance is now
called Damerau-Levenshtein edit distance.

00:06:19.290 --> 00:06:23.747
And it can be computed, again,
by various dynamic programming approaches.

00:06:27.262 --> 00:06:31.816
So, let's look at the candidates
that our words within edit

00:06:31.816 --> 00:06:35.764
distance 1 of our misspelling a-c-r-e-s-s.

00:06:35.764 --> 00:06:39.910
So here's our error, a-c-r-e-s-s.

00:06:39.910 --> 00:06:42.350
And here is different possible candidates.

00:06:42.350 --> 00:06:44.230
So here is a candidate, actress.

00:06:44.230 --> 00:06:47.520
How is actress turn into acress?

00:06:47.520 --> 00:06:51.400
Well, the t turns into nothing,
so t was deleted.

00:06:51.400 --> 00:06:53.040
So we have a deletion of a t.

00:06:53.040 --> 00:06:56.360
So, a deletion of a t
turns actress into acress.

00:06:56.360 --> 00:07:01.230
Here, the proposed candidate is
the word cress, the kind of vegetable.

00:07:01.230 --> 00:07:05.690
So here, cress to turn cress into
acress we have to add, insert an a.

00:07:05.690 --> 00:07:10.003
So here we had a deletion, here we
had an insertion, how about caress?

00:07:10.003 --> 00:07:14.870
Caress is, to turn caress into
acress we turn a ca into the ac, so

00:07:14.870 --> 00:07:17.680
we have a transposition of ca and ac.

00:07:20.270 --> 00:07:22.020
The word could've been access.

00:07:22.020 --> 00:07:26.639
Here we have a substitution,
this c turned into an r.

00:07:26.639 --> 00:07:30.251
Or another substitution,
that the word could've been across, and

00:07:30.251 --> 00:07:31.440
the o turned into an e.

00:07:31.440 --> 00:07:37.982
Or, and s could've been inserted
to turn acres into acress.

00:07:37.982 --> 00:07:42.994
But, the s could've been inserted
either here or here, so there's two

00:07:42.994 --> 00:07:48.790
different ways where this source word
could have turned into this error form.

00:07:48.790 --> 00:07:50.660
So, we'll put two rows down for

00:07:50.660 --> 00:07:53.920
both of these possible insertions,
locations, positions.

00:07:55.000 --> 00:07:58.160
So, I've just shown candidates that
are within edit distance of 1.

00:07:58.160 --> 00:08:02.810
It turns out that 80% of spelling
errors are within edit distance of 1.

00:08:02.810 --> 00:08:06.290
And almost all errors are within edit
distance of 2, so most algorithms

00:08:06.290 --> 00:08:10.510
either consider just edit distance 1 or
edit distance 2, possible candidates.

00:08:11.930 --> 00:08:15.850
In practice, we also want to
allow not just insertion and

00:08:15.850 --> 00:08:18.950
substitution of letters, but
also the spaces or hyphens.

00:08:18.950 --> 00:08:21.830
So for example, if user types thisidea,

00:08:21.830 --> 00:08:26.340
we'd like to realize that there
should be an insertion of a space or

00:08:26.340 --> 00:08:30.710
that the original space was, in fact,
deleted to produce this error form.

00:08:30.710 --> 00:08:35.446
Or here, the original dash in the word
in-law was deleted to produce this error

00:08:35.446 --> 00:08:36.796
form i-n-l-a-w.

00:08:39.205 --> 00:08:40.619
We've seen candidate generation,

00:08:40.619 --> 00:08:43.750
now we're ready to talk about
how to rank the candidates.

00:08:43.750 --> 00:08:47.210
And remember, there are two factors,
we have the language model and

00:08:47.210 --> 00:08:48.550
the channel model.

00:08:48.550 --> 00:08:49.585
Now, the language model,

00:08:49.585 --> 00:08:52.474
we can use any of the language modeling
algorithms we've already learned.

00:08:52.474 --> 00:08:55.125
We can use unigrams,
bigrams, and trigrams,

00:08:55.125 --> 00:08:59.579
we can use any kind of backoff algorithm,
or smoothing algorithm we want to use.

00:08:59.579 --> 00:09:03.891
In practice, for very, very large-scale,
web-scale correction we're going to use,

00:09:03.891 --> 00:09:07.510
as usual for web-scale things,
we're going to use stupid backoff.

00:09:07.510 --> 00:09:13.272
But we might want to use smarter
algorithms for smaller kinds of tasks.

00:09:17.158 --> 00:09:21.422
So let's look at an example
of a language model.

00:09:21.422 --> 00:09:24.114
Here I've picked just
a very simple unigram, and

00:09:24.114 --> 00:09:28.601
in this case we've computed the unigram
from the Corpus of Contemporary English,

00:09:28.601 --> 00:09:30.287
one of many possible corpora.

00:09:30.287 --> 00:09:34.779
And here's some counts, here's counts
of the different possible candidates,

00:09:34.779 --> 00:09:36.937
actress, cress, caress, and so on.

00:09:36.937 --> 00:09:38.820
Here's their frequency, and

00:09:38.820 --> 00:09:42.594
normalized by the total number
of words we get a probability.

00:09:42.594 --> 00:09:46.592
Here's the total number of words,
we get a normalizing, this count,

00:09:46.592 --> 00:09:49.390
by the total count, we get probabilities.

00:09:49.390 --> 00:09:53.870
So, here's probabilities of words
assigned by unigram language model.

00:09:56.230 --> 00:09:58.203
How about computing
channel model probability?

00:09:58.203 --> 00:10:04.075
Remember, the channel model also called
the error model or the edit probability.

00:10:04.075 --> 00:10:09.502
And, we're going to take simplifying
assumption made by Kernighan, Church and

00:10:09.502 --> 00:10:14.810
Gale in 1990, when they first proposed
the use of noisy channel model.

00:10:14.810 --> 00:10:16.658
So, let's first see how to do that.

00:10:16.658 --> 00:10:22.215
Let's assume that misspelled word x
has a set of letters x 1 through x m,

00:10:22.215 --> 00:10:28.070
and the correct word w has a set of
letters, let's call them w1 through wm.

00:10:28.070 --> 00:10:31.777
Now, the probability of the edit x,
given w,

00:10:31.777 --> 00:10:36.054
is going to be some set of deletions,
or insertions, or

00:10:36.054 --> 00:10:40.730
substitutions, or transpositions,
some set of edits.

00:10:40.730 --> 00:10:44.255
The way that we are going to model that is
we're going to create a confusion matrix.

00:10:44.255 --> 00:10:51.250
And a confusion matrix says,
for any given pair of letters,

00:10:51.250 --> 00:10:56.199
how likely is a particular edit to happen?

00:10:56.199 --> 00:11:00.219
So, for example, for
the pair of letters x,y,

00:11:00.219 --> 00:11:04.336
we want to know how often
xy is typed as x, meaning,

00:11:04.336 --> 00:11:09.160
how often is a y deleted when
there's an x right before it?

00:11:10.190 --> 00:11:13.740
We're also going to keep a count for
insertion probabilities.

00:11:13.740 --> 00:11:19.530
How often was an x typed as xy, so
how often is y inserted after x?

00:11:19.530 --> 00:11:22.390
So, y deleted after x, y inserted after x.

00:11:22.390 --> 00:11:24.260
Or, we'll keep a count for substitutions.

00:11:24.260 --> 00:11:26.400
How often is x typed as y?

00:11:26.400 --> 00:11:28.420
So we meant to type x, we typed y.

00:11:28.420 --> 00:11:30.760
That's an x, y substitution.

00:11:30.760 --> 00:11:32.140
Or our transposition.

00:11:32.140 --> 00:11:34.696
How often was xy typed as yx?

00:11:34.696 --> 00:11:35.855
So, these are just counts.

00:11:35.855 --> 00:11:39.860
We'll keep a matrix of these counts for
every x and for every y.

00:11:41.240 --> 00:11:44.770
And notice that what we've done
implicitly, is we've conditioned our

00:11:44.770 --> 00:11:50.880
insertion and
our deletion on the previous character.

00:11:50.880 --> 00:11:54.850
So, whether y is deleted
is condition on x.

00:11:54.850 --> 00:11:57.420
We could have chosen the condition
on the next character, or

00:11:57.420 --> 00:12:00.210
the character five to the left,
or some other thing but

00:12:00.210 --> 00:12:01.941
we generally condition on
the previous character.

00:12:02.970 --> 00:12:07.220
So, here's an example of a confusion
matrix for spelling errors.

00:12:07.220 --> 00:12:08.690
The font is a little small.

00:12:08.690 --> 00:12:12.580
But just to give you a basic idea, here's,

00:12:12.580 --> 00:12:16.341
this is a substitution matrix
that I took from Kernighan et al.

00:12:16.341 --> 00:12:21.240
So, here's the letter e, and
it's very likely in their data,

00:12:21.240 --> 00:12:25.050
388 times to be substituted with an a.

00:12:25.050 --> 00:12:28.914
So, you meant to type e, you incorrectly
typed an a, or you might have typed an i,

00:12:28.914 --> 00:12:30.259
or you might have typed an o.

00:12:30.259 --> 00:12:34.848
So, vowels are very likely to
be mistaken for each other,

00:12:34.848 --> 00:12:39.824
or similarly, the letter m,
very often gets mistyped as an n.

00:12:39.824 --> 00:12:44.680
So, a very high probability of m and
n being substituted for each other.

00:12:44.680 --> 00:12:47.920
Next to each other on the keyboard,
they sound alike, lots of reasons for

00:12:47.920 --> 00:12:48.830
them to be substituted.

00:12:48.830 --> 00:12:51.969
So here is a set of our
confusion matrices, and

00:12:51.969 --> 00:12:54.150
we just compute four of them.

00:12:54.150 --> 00:12:57.820
One for substitution, and
one for insertion, one for

00:12:57.820 --> 00:13:00.880
deletion, and one for transposition.

00:13:01.980 --> 00:13:05.255
Now, I've shown you this table
comes from Kernighan et al,

00:13:05.255 --> 00:13:07.900
but you could also generate
the table yourself.

00:13:09.310 --> 00:13:13.349
So for example, Peter Norvig posted on
his website a lovely list of errors.

00:13:18.210 --> 00:13:20.660
So these are errors taken from Wikipedia,

00:13:20.660 --> 00:13:24.180
and other places that he
talks about on his website.

00:13:24.180 --> 00:13:29.648
And from a set of errors like this,
so here, misspellings

00:13:29.648 --> 00:13:35.110
of adaptable as adabtable, or

00:13:35.110 --> 00:13:40.010
imature with only one m, and so on, so
various kinds of likely misspellings.

00:13:41.360 --> 00:13:46.160
And from this list of errors,
we can get a list of counts for

00:13:46.160 --> 00:13:50.380
every possible single-edit
error of how often it happens.

00:13:50.380 --> 00:13:53.660
And from that we can build, so
we build our little confusion matrix.

00:13:55.010 --> 00:13:58.710
And then, from the confusion matrix,
we can generate probabilities.

00:13:58.710 --> 00:14:03.815
So, every time a particular
previous letter happens,

00:14:03.815 --> 00:14:08.920
we look up in our insertion
confusion matrix and we say,

00:14:08.920 --> 00:14:14.803
how often was xi inserted after
a particular letter w sub i-1?

00:14:14.803 --> 00:14:19.159
And we divide by the number
of times wi-1 occurred, and

00:14:19.159 --> 00:14:25.940
that's going to be the probability of a
particular insertion happening in a word.

00:14:25.940 --> 00:14:30.890
So we can generate our probability
of our surface form for

00:14:30.890 --> 00:14:32.300
each possible single-edit error.

00:14:32.300 --> 00:14:34.290
Again, we are assuming a single-edit now,
so

00:14:34.290 --> 00:14:37.890
only one of these happens
to generate our candidate.

00:14:37.890 --> 00:14:41.980
Whichever one it is, we compute our
probability by just normalizing

00:14:41.980 --> 00:14:44.580
the count of the deletion, or
insertion, or substitution, or

00:14:44.580 --> 00:14:48.490
transposition by the appropriate count and
generate a probability.

00:14:50.860 --> 00:14:57.541
So this channel model, for
example, for a word like actress,

00:14:57.541 --> 00:15:04.991
where we generated a-c-r-e-s-s by
when we should have typed a c.

00:15:04.991 --> 00:15:10.030
Excuse me, when we should have
typed a c-t, we typed a c.

00:15:10.030 --> 00:15:14.070
So, the word had a c-t in it,
but the error had only a c.

00:15:15.140 --> 00:15:19.490
So what's the probability of
deleting a t following a c?

00:15:19.490 --> 00:15:24.080
And if we normalize the probabilities
in our confusion matrix,

00:15:24.080 --> 00:15:30.460
here's the likelihood of this word actress
being realized as this misspelling acress.

00:15:30.460 --> 00:15:33.358
It's 0.00117.

00:15:33.358 --> 00:15:37.110
The language model, so here is
the error model or the channel model.

00:15:38.950 --> 00:15:42.621
And now we can add in the language model,
I'll write LM.

00:15:42.621 --> 00:15:43.460
So we have the channel mode.

00:15:43.460 --> 00:15:47.487
How likely was ct to be
error-fully turned into c, so

00:15:47.487 --> 00:15:51.973
t to be deleted, and
how likely is the word actress anyway?

00:15:51.973 --> 00:15:54.400
And then we can just
multiply these together.

00:15:54.400 --> 00:15:57.978
And, what we'll do is,
because these are very small numbers,

00:15:57.978 --> 00:16:02.036
we'll just multiply everything by
10 to the 9th to make it readable.

00:16:02.036 --> 00:16:05.308
So, this would be 2.7
times 10 to the minus 9th.

00:16:05.308 --> 00:16:07.600
We multiplied everything
by 10 to the 9th here.

00:16:08.600 --> 00:16:13.544
So, you can see that the most
likely word here is across,

00:16:13.544 --> 00:16:16.840
with this particular channel model,

00:16:16.840 --> 00:16:22.410
this particular language model,
the most likely word is across

00:16:25.606 --> 00:16:30.210
But actress is also quite likely, and
acre seems a reasonably likelihood, and

00:16:30.210 --> 00:16:34.262
the word cress, which is just a very
rare word, you can see is a very low

00:16:34.262 --> 00:16:38.590
probability, and has an unusual error
of inserting an a at the beginning,

00:16:38.590 --> 00:16:41.242
makes it a very low
probability correction.

00:16:41.242 --> 00:16:45.430
So, noisy channel model likes the word
across as the possible replacement.

00:16:46.820 --> 00:16:51.560
Unfortunately, we can see from the
original sentence taken from Kernighan et

00:16:51.560 --> 00:16:57.300
al's paper, that the original sentence,
across is the wrong word.

00:16:57.300 --> 00:16:59.350
The original sentence is "a stellar and

00:16:59.350 --> 00:17:04.200
versatile acress whose combination of sass
and glamour...", and it should be clear

00:17:04.200 --> 00:17:08.130
that this word should have been actress,
so across is the wrong word.

00:17:08.130 --> 00:17:12.290
So just using a unigram model,
The noisy channel makes a mistake, so

00:17:12.290 --> 00:17:13.540
let's look at a bigram model.

00:17:13.540 --> 00:17:14.900
How well can we do with a bigram model?

00:17:15.920 --> 00:17:19.946
We computed a very simply bigram model
just using add-1 smoothing from the Corpus

00:17:19.946 --> 00:17:21.740
of Contemporary American English.

00:17:21.740 --> 00:17:25.404
So now, the probability of actress,
given versatile,

00:17:25.404 --> 00:17:29.082
just look at these three words,
ignore the rest for now.

00:17:29.082 --> 00:17:33.454
Actress, given versatile,
that probability is point .000021, and

00:17:33.454 --> 00:17:37.448
whose, given actress is point .0010,
so we'll compute those.

00:17:37.448 --> 00:17:40.444
And now, let's do the same thing for
another candidate,

00:17:40.444 --> 00:17:44.743
the original candidate that was preferred
by the unigram model, the word across.

00:17:49.333 --> 00:17:52.344
We'll put across here instead,
as our hypothesis, and

00:17:52.344 --> 00:17:55.670
we'll again compute the probability
of across in versatile,

00:17:55.670 --> 00:17:58.258
given the probability of whose,
given across.

00:17:58.258 --> 00:18:03.020
So, here's those probabilities and
you can see that the probability of whose,

00:18:03.020 --> 00:18:07.641
given actress, is much higher than
the probability of whose, given across,

00:18:07.641 --> 00:18:10.260
actress whose, is just a likely sequence.

00:18:10.260 --> 00:18:13.380
And sure enough,
if we multiply these things out, and

00:18:13.380 --> 00:18:15.680
the probability of
"versatile actress whose"

00:18:17.030 --> 00:18:21.480
is a much higher probability than
the sequence "versatile across whose".

00:18:21.480 --> 00:18:24.718
So, much higher probability, so the noisy
channel model with the bigram language

00:18:24.718 --> 00:18:28.340
model correctly picks
the correction actress.

00:18:30.670 --> 00:18:35.590
How are we going to evaluate these noisy
channel and other kinds of models?

00:18:36.930 --> 00:18:40.618
There are lots of good
spelling error test sets.

00:18:40.618 --> 00:18:43.995
Wikipedia has a list of
common English misspellings.

00:18:43.995 --> 00:18:46.826
There's a filtered version
of that at Aspell,

00:18:46.826 --> 00:18:50.030
there's a spelling error
corpus at Birkbeck.

00:18:50.030 --> 00:18:51.404
Let's look at the Wikipedia list.

00:18:58.604 --> 00:19:01.690
So, there's Wikipedia's list of
common English misspellings.

00:19:04.248 --> 00:19:07.320
And, I've shown you here on this slide
some various other possible lists

00:19:07.320 --> 00:19:08.480
that you can go look at on your own.

00:19:10.130 --> 00:19:14.321
So, from these lists of misspellings
you would generate a training set to

00:19:14.321 --> 00:19:18.174
train your channel model,
a developing set to test your model, and

00:19:18.174 --> 00:19:21.238
then a final test set to see
how well your model works.

00:19:24.222 --> 00:19:29.730
So, that's The Noisy Channel Model of
Spelling applied to non-real words.

