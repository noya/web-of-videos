WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:04.450
In this section I'll tell you a little
bit more about evaluating the quality

00:00:04.450 --> 00:00:06.560
of a search engine.

00:00:06.560 --> 00:00:09.691
There are many measures for
the quality of a search engine.

00:00:09.691 --> 00:00:13.953
There are technical ones such
as how fast does it index, and

00:00:13.953 --> 00:00:15.751
how fast does it search?

00:00:15.751 --> 00:00:20.522
We can look at things like expressiveness
of the query language, whether we're able

00:00:20.522 --> 00:00:24.697
to express complex informational needs
with things like phrase queries,

00:00:24.697 --> 00:00:26.370
negations, disjunctions.

00:00:27.660 --> 00:00:31.470
People have other desires like
having an uncluttered UI and

00:00:31.470 --> 00:00:34.070
a system that doesn't cost a lot to use.

00:00:34.070 --> 00:00:39.873
All of these measures that are measurable,
that we can quantify them,

00:00:39.873 --> 00:00:44.624
and we can get some kind of
score of what is their goodness.

00:00:44.624 --> 00:00:48.533
But in practice all of those measures,
while important,

00:00:48.533 --> 00:00:52.537
tend to be dominated by another
measure of user happiness.

00:00:52.537 --> 00:00:56.496
That is the user happy in
using this search engine?

00:00:56.496 --> 00:01:01.012
And speed of response and
size of the index are certainly factors.

00:01:01.012 --> 00:01:06.613
But by themselves, blindingly fast,
useless answers won't make a user happy.

00:01:06.613 --> 00:01:09.348
So a huge part of user happiness is,

00:01:09.348 --> 00:01:13.530
are the results returned
results that they want?

00:01:13.530 --> 00:01:19.342
And so that's the metric of relevance of
results to a user's information need.

00:01:19.342 --> 00:01:24.504
I mentioned this right at the beginning,
but just to reiterate once more, when

00:01:24.504 --> 00:01:29.675
evaluating the IR system that we evaluate
with respect to an information need.

00:01:29.675 --> 00:01:32.925
So an information need is
translated into a query, and

00:01:32.925 --> 00:01:35.480
that's what the IR system actually runs.

00:01:35.480 --> 00:01:39.834
But relevance is assessed relative to
the information need, not the query.

00:01:39.834 --> 00:01:43.667
So for example if the information
need is the person is looking for

00:01:43.667 --> 00:01:48.270
information on whether drinking red wine
is more effective than white wine for

00:01:48.270 --> 00:01:52.543
reducing your risk of heart attacks,
they'll come up with some query.

00:01:52.543 --> 00:01:55.963
For example, it might be,
wine red white heart attack effective.

00:01:55.963 --> 00:01:59.110
And that'll be submitted
to a search engine.

00:01:59.110 --> 00:02:03.831
And in evaluating the effectiveness of
the search engine in returning relevant

00:02:03.831 --> 00:02:08.480
results, we're not asking, are the results
that the search engines returns

00:02:08.480 --> 00:02:11.593
documents that simply have
those words on the page?

00:02:11.593 --> 00:02:16.920
Rather we're saying, do these documents
address the users information need?

00:02:18.570 --> 00:02:20.540
Okay, well how can we go about doing that?

00:02:20.540 --> 00:02:26.476
Well, if the search engine returns a set
of results, well then what we can do for

00:02:26.476 --> 00:02:32.146
evaluation is if we start off with three
things, if we have some benchmark,

00:02:32.146 --> 00:02:36.421
collection of documents that
we can use for evaluation.

00:02:36.421 --> 00:02:40.625
And we have some benchmark set of
queries which are in some sense

00:02:40.625 --> 00:02:44.129
representative of information
needs of interest.

00:02:44.129 --> 00:02:48.777
And then we've gathered this third
thing which is assessor judgments of

00:02:48.777 --> 00:02:53.910
whether particular documents
are relevant to particular queries.

00:02:53.910 --> 00:02:58.010
Commonly in practice this can't
be assembled exhaustively,

00:02:58.010 --> 00:03:00.830
certainly not if the document
collection is large.

00:03:00.830 --> 00:03:04.460
But at least for a particular set
of documents that are returned by

00:03:04.460 --> 00:03:07.460
particular search engines,
we can get the assessor to

00:03:07.460 --> 00:03:11.600
judge whether those documents
are relevant to the queries.

00:03:11.600 --> 00:03:16.500
Well, if we have a result set with just
these three things, we're in business.

00:03:16.500 --> 00:03:20.870
Because we can use exactly the same
measures that we looked at previously,

00:03:20.870 --> 00:03:24.510
precision, recall, and
the F measure that combines them.

00:03:24.510 --> 00:03:28.424
And these are suitable good measures for
exactly the same reason that they are good

00:03:28.424 --> 00:03:32.236
measures for when we're talking about
things like named entity recognition.

00:03:32.236 --> 00:03:37.316
That normally only a few documents will
be relevant to a particular query, and so

00:03:37.316 --> 00:03:42.708
we can measure that much better by looking
at these measures of precision and recall.

00:03:42.708 --> 00:03:47.397
But what if we've now moved on to a search
engine that returns ranked results?

00:03:47.397 --> 00:03:52.729
We can't just totally straightforwardly
use these measures of precision,

00:03:52.729 --> 00:03:57.990
recall, and F measure because the system
can return any number of results.

00:03:57.990 --> 00:04:02.527
In fact, the number of returns normally
depends on how often we keep on clicking,

00:04:02.527 --> 00:04:03.524
asking for more.

00:04:03.524 --> 00:04:08.018
But if we sort of look at any
initial subset of the results,

00:04:08.018 --> 00:04:12.807
we can then work out the precision and
recall for that subset.

00:04:12.807 --> 00:04:17.037
And then by putting them together we can
come up with a precision recall curve.

00:04:17.037 --> 00:04:20.996
Let's look at how that works.

00:04:20.996 --> 00:04:25.358
So here are the first 10 results for
a search engine where we've labelled each

00:04:25.358 --> 00:04:30.305
result as either relevant or not relevant
according to an assessor's judgement.

00:04:30.305 --> 00:04:35.515
And so then we can take any initial
subsequence of these documents and

00:04:35.515 --> 00:04:37.770
work out a recall and precision.

00:04:37.770 --> 00:04:41.790
So for the first document,
the system got it right.

00:04:41.790 --> 00:04:43.139
It's a relevant document.

00:04:43.139 --> 00:04:47.500
And let's assume that overall there are 10
relevant documents in the collection.

00:04:47.500 --> 00:04:54.137
So it's gotten one of 1 the 10 relevant
documents, and so its recall is 0.1.

00:04:54.137 --> 00:04:57.262
And well since that document was relevant,

00:04:57.262 --> 00:05:03.190
the system was right on the first answer,
its precision is 1 at this point.

00:05:03.190 --> 00:05:08.860
Well, the next document was not relevant
so the recall of the first two documents,

00:05:08.860 --> 00:05:14.711
we're down to here now, is 0.1,
and the precision is now 0.5.

00:05:14.711 --> 00:05:20.988
Another not relevant document,
so the recall is still 0.1,

00:05:20.988 --> 00:05:25.647
and the precision has now dropped to 0.33.

00:05:25.647 --> 00:05:29.273
And if we look at the set
of the top 4 documents,

00:05:29.273 --> 00:05:34.445
we've now found 2 of the 10 relevant ones,
so recall is 0.2,

00:05:34.445 --> 00:05:38.380
and our precision has gone
back up again to 0.5.

00:05:38.380 --> 00:05:43.426
The fifth one is also relevant so
now our recall is

00:05:43.426 --> 00:05:48.471
up to 0.3, and
our recall is up to 3 out of 5,

00:05:48.471 --> 00:05:52.630
0.6, and we can keep on going down.

00:05:52.630 --> 00:05:58.366
Maybe you guys could work out what
some of these entries are down here.

00:05:58.366 --> 00:06:01.176
The other measure I want to mention,

00:06:01.176 --> 00:06:06.620
one of the most used recent
measures is mean average precision.

00:06:06.620 --> 00:06:10.743
If we look at the rate retrieval results
ordered this way to give me a bit more

00:06:10.743 --> 00:06:13.828
room, and so
the first document returned is relevant.

00:06:13.828 --> 00:06:17.111
The second one is not relevant,
say the third one is not relevant,

00:06:17.111 --> 00:06:21.940
then a relevant one, then another relevant
one, not relevant, relevant, relevant.

00:06:21.940 --> 00:06:25.700
Let's say those are our top eight results.

00:06:25.700 --> 00:06:29.660
What you're doing for
mean average precision for first of all,

00:06:29.660 --> 00:06:34.350
you're working on average precision for
one query.

00:06:34.350 --> 00:06:39.010
And so the way you do that is by saying

00:06:39.010 --> 00:06:43.510
let's work out the precision
at each point that a relevant

00:06:43.510 --> 00:06:48.860
document is returned because that's
when you're increasing recall.

00:06:48.860 --> 00:06:51.932
So here the precision is 1.

00:06:51.932 --> 00:06:57.680
Here there are now four documents so
the precision is a half.

00:06:57.680 --> 00:07:01.889
Here there are five documents so
the precision is 0.6.

00:07:01.889 --> 00:07:09.352
Here there are seven documents,
of which four of them are relevant.

00:07:09.352 --> 00:07:14.970
So this is around 0.58,
you guys can correct my arithmetic.

00:07:14.970 --> 00:07:21.863
Here we now have eight documents, of which
five are relevant, and that's 0.625.

00:07:21.863 --> 00:07:27.140
And then what we're doing to work
out the mean average precision

00:07:27.140 --> 00:07:30.910
is we're kind of keeping on
calculating those numbers.

00:07:30.910 --> 00:07:34.877
In practice normally they aren't
calculated exhaustively, but

00:07:34.877 --> 00:07:38.430
they're calculated up to some point,
let's say 100.

00:07:38.430 --> 00:07:43.040
And then you're calculating an average
function of all those numbers.

00:07:43.040 --> 00:07:48.371
And that's then the average precision for
one query.

00:07:48.371 --> 00:07:51.975
You then calculate the same
average precision for

00:07:51.975 --> 00:07:56.278
all the other queries in your
benchmark query collection.

00:07:56.278 --> 00:07:58.762
And you again take the average of that,
and

00:07:58.762 --> 00:08:01.880
that then gives you
mean average precision.

00:08:01.880 --> 00:08:05.220
So, in particular, this is what's
referred to as macro-averaging.

00:08:05.220 --> 00:08:11.450
It's each query that counts equally in
the calculation of mean average precision.

00:08:11.450 --> 00:08:16.670
So this is a good measure
that evaluates to some extent

00:08:16.670 --> 00:08:21.740
precision at different recall levels
while still being weighted most

00:08:21.740 --> 00:08:27.550
to what the precision is like for
the top few returned documents.

00:08:27.550 --> 00:08:30.660
And at the level of
across different queries,

00:08:30.660 --> 00:08:33.250
it's giving equal weight
to different queries,

00:08:33.250 --> 00:08:37.050
which tends to be a useful thing to do
because you're always interested in how

00:08:37.050 --> 00:08:43.210
systems work on queries of rare terms
as well as queries of common terms.

00:08:43.210 --> 00:08:46.050
So this is a pretty good measure
to think about using for

00:08:46.050 --> 00:08:48.370
evaluating information retrieval systems.

00:08:50.120 --> 00:08:54.140
Okay, there's even more methods
that I could talk about, but

00:08:54.140 --> 00:08:56.140
that's probably a good sense of how

00:08:57.340 --> 00:09:01.160
to go about evaluating the performance
of a ranked retrieval system.

