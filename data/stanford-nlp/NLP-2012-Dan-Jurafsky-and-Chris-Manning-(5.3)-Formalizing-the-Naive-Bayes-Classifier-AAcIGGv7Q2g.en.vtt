WEBVTT
Kind: captions
Language: en

00:00:00.420 --> 00:00:03.480
Let's now formalize
the Naive Bayes classifier.

00:00:03.480 --> 00:00:07.430
In classification,
we have a document d and a class c.

00:00:08.920 --> 00:00:13.280
And our goal is to compute
the probability for

00:00:13.280 --> 00:00:17.450
each class of its conditional
probability given a document.

00:00:17.450 --> 00:00:19.770
And we're going to see that the,

00:00:20.800 --> 00:00:23.960
we're going to use this probability
to pick the best class.

00:00:25.320 --> 00:00:29.440
Now how do we computer the probability
of a class give a document?

00:00:29.440 --> 00:00:34.670
By Bayes rule, this is equal to the
probability of a document given the class

00:00:34.670 --> 00:00:41.080
times the probability of a class over
the probability of the document.

00:00:42.210 --> 00:00:44.530
Let's see how to use
that in the classifier.

00:00:46.240 --> 00:00:50.830
The best class, the maximum a posteriori
class, the class that we're looking for

00:00:50.830 --> 00:00:56.500
to assign this document to,
is out of all classes

00:00:56.500 --> 00:01:00.640
the one that maximizes the probability
of that class given the document.

00:01:01.870 --> 00:01:04.500
So we're looking for
the class whose probability,

00:01:04.500 --> 00:01:07.100
given the document, Is greatest.

00:01:07.100 --> 00:01:12.505
By Bay's Rule, That's the same,

00:01:12.505 --> 00:01:16.240
whichever class maximizes,
probability of c, given d.

00:01:16.240 --> 00:01:20.450
Also maximizes this equation,
the probability of d given c.

00:01:20.450 --> 00:01:23.479
Probability of the class,
over the probability of the document.

00:01:25.620 --> 00:01:31.630
And, as is traditional in
Bayesian classification,

00:01:31.630 --> 00:01:37.580
which ever class maximizes this equation,
also maximizes this equation.

00:01:37.580 --> 00:01:39.900
What we've done here is we've
dropped the denominator.

00:01:39.900 --> 00:01:41.500
Crossed out the denominator.

00:01:41.500 --> 00:01:46.260
Why is it okay to cross
out the denominator d?

00:01:48.010 --> 00:01:51.730
Probability of d is how
likely the document is.

00:01:51.730 --> 00:01:53.680
Now if I give you a document and I say,

00:01:53.680 --> 00:01:58.270
which of these 10 classes does this
document belong to and for each of these

00:01:58.270 --> 00:02:02.080
classes, I'm computing the probability
of the document given the class.

00:02:02.080 --> 00:02:04.790
The probability of the class and
the probability of the document.

00:02:04.790 --> 00:02:08.340
The probability of the document
is identical for all 10 classes.

00:02:08.340 --> 00:02:12.160
For each class, one more time I have to
compute the probability of the document.

00:02:12.160 --> 00:02:14.810
And that means that if
I'm comparing 10 things,

00:02:14.810 --> 00:02:17.720
each of which is divided by
probability of the document.

00:02:17.720 --> 00:02:20.790
The probability of the document is
a constant, and I can eliminate that.

00:02:21.890 --> 00:02:25.090
So, the most likely, C map,

00:02:25.090 --> 00:02:30.670
is that class which maximizes
the product of two probabilities.

00:02:30.670 --> 00:02:34.490
The probability of the document given
the class, we'll call that the likelihood.

00:02:37.570 --> 00:02:39.790
And the probability of the class,
we'll call that the prior.

00:02:40.870 --> 00:02:42.714
Prior probability of the class.

00:02:55.470 --> 00:02:59.636
So the most likely class is the one
that maximizes the product of these two

00:02:59.636 --> 00:03:01.620
probabilities.

00:03:01.620 --> 00:03:06.080
The probability of the class will turn
out to be relatively simple to computer.

00:03:06.080 --> 00:03:09.016
What do I mean by the probability
of a document given the class?

00:03:09.016 --> 00:03:13.644
What do I mean to say, this particular
movie review, how likely is it,

00:03:13.644 --> 00:03:15.450
given the class positive.

00:03:15.450 --> 00:03:19.600
It seems like a very complicated and
confusing thing to compute.

00:03:19.600 --> 00:03:22.280
And one way to operationalize
that is to say,

00:03:22.280 --> 00:03:28.120
let's represent the document by a whole
set of features, x1 through xn.

00:03:28.120 --> 00:03:33.150
So when I say the probability of
a document given a class, I'm going to say

00:03:33.150 --> 00:03:37.780
all that means is the probability of
a vector of features given the class.

00:03:37.780 --> 00:03:39.080
P of d given c.

00:03:39.080 --> 00:03:43.080
We're going to represent that probability
by the joint probability of x1, x2,

00:03:43.080 --> 00:03:45.360
up through xn given the class.

00:03:47.310 --> 00:03:48.660
In other words,

00:03:48.660 --> 00:03:53.280
we're representing this document d
as a set of features x1 through xn.

00:03:53.280 --> 00:03:57.569
That still doesn't tell me how to compute
this probability but it's a start.

00:04:01.702 --> 00:04:03.830
So let's talk about these two pieces now.

00:04:05.630 --> 00:04:08.120
How do I compute probability of a class?

00:04:09.770 --> 00:04:13.700
Well, really, that's just asking
how often does this class occur?

00:04:13.700 --> 00:04:16.209
Are positive reviews much more
common than negative reviews?

00:04:18.800 --> 00:04:22.080
Is Madison a much more frequent author?

00:04:23.620 --> 00:04:29.110
So to decide computing
the probability of a class

00:04:29.110 --> 00:04:33.670
can be done just by counting relative
frequencies in some corpus or data set.

00:04:33.670 --> 00:04:37.670
So the probability of a class
is relatively easy to compute.

00:04:37.670 --> 00:04:39.970
What about the likelihood of the document,

00:04:39.970 --> 00:04:42.030
of these features in a document,
given the class.

00:04:43.270 --> 00:04:46.070
Well there's a lot of parameters for
this probability.

00:04:47.270 --> 00:04:50.420
There's, if there's N different features,

00:04:53.310 --> 00:04:57.880
and each of them has a certain length,
that's a lot of parameters that have to be

00:04:57.880 --> 00:05:02.410
computed, and we have to compute
them one for each class.

00:05:02.410 --> 00:05:05.750
So that's far too many parameters
that we could possibly compute.

00:05:05.750 --> 00:05:10.750
We could only estimate at this number if
we had a huge number of training examples

00:05:10.750 --> 00:05:13.600
and we usually don't have such
an enormous amount of training examples.

00:05:15.330 --> 00:05:18.660
So we're going to make some simplifying
assumptions in the naive based classifier

00:05:18.660 --> 00:05:21.200
to make this computation more possible.

00:05:23.750 --> 00:05:26.270
The first simplifying
assumption we're going to make

00:05:26.270 --> 00:05:28.830
is called the bag of words assumption.

00:05:28.830 --> 00:05:31.570
And we're going to assume that the
position in the document doesn't matter.

00:05:31.570 --> 00:05:34.770
So, this is what I gave you
the intuition of a few slides ago.

00:05:36.010 --> 00:05:39.240
The position of the word in the document
whether it's the first word or

00:05:39.240 --> 00:05:41.970
the seventh word the 150th
word isn't going to matter.

00:05:41.970 --> 00:05:44.780
All we care about is which word or
which feature occurs.

00:05:45.910 --> 00:05:50.100
And the second thing we're going to,
the second assumption we're going to make

00:05:50.100 --> 00:05:54.410
Is we're going to assume that
the different features, X1, X2, X3,

00:05:54.410 --> 00:05:58.010
that their probabilities
are independent given the class.

00:05:59.660 --> 00:06:03.660
So that whether one feature
occurs given a class and

00:06:03.660 --> 00:06:07.720
whether another feature occurs given a
class are independently going to be true.

00:06:07.720 --> 00:06:13.710
And of course, both of these assumptions
are incorrect simplifying assumptions.

00:06:13.710 --> 00:06:16.520
They're absolutely wrong.

00:06:16.520 --> 00:06:20.170
They're terribly completely not true.

00:06:20.170 --> 00:06:24.390
Nonetheless, by making these
incorrect simplifying assumptions,

00:06:24.390 --> 00:06:28.360
we can make our problem so much simpler,
that in practice, we're able to solve

00:06:28.360 --> 00:06:31.970
the problem with a high degree of
accuracy, despite the simplifications.

00:06:33.570 --> 00:06:37.710
So, the result of these two simplifying
assumptions is we're going to represent

00:06:37.710 --> 00:06:43.140
the probability, the joint probability of
a whole set of features x 1 through x 1,

00:06:43.140 --> 00:06:48.825
conditioned on a class as the product of a
whole bunch of independent probabilities.

00:06:48.825 --> 00:06:52.445
Probability of x1 given the class,
probability of x2 given the class,

00:06:52.445 --> 00:06:54.825
probability of x3 given the class,
and so on.

00:06:54.825 --> 00:06:56.615
Up to probability of xn given the class.

00:06:56.615 --> 00:06:58.405
We're just going to
multiply them all together.

00:06:58.405 --> 00:07:01.785
We're not going to care about x1,
which position it occurred in,

00:07:01.785 --> 00:07:05.820
all we care about is that it was
this particular order feature.

00:07:05.820 --> 00:07:09.818
And we're not going to care about
the dependencies between x1 and x2.

00:07:14.671 --> 00:07:20.597
In other words, in order to compute our
simplifying Naive Bayes assumption,

00:07:20.597 --> 00:07:25.356
to compute the most likely class
by multiplying a likelihood,

00:07:25.356 --> 00:07:29.396
the probability of a whole
joint string of features,

00:07:29.396 --> 00:07:33.050
times a prior probability of a class.

00:07:33.050 --> 00:07:38.096
We're going to simplify that and
say that the best class by the Naive Bayes

00:07:38.096 --> 00:07:43.575
assumption is that class that maximizes
the prior probability of the class.

00:07:43.575 --> 00:07:47.579
So that's the same but now more simply,
we're just going to multiply for

00:07:47.579 --> 00:07:52.100
every feature and the set of features,
the probability of that feature.

00:07:52.100 --> 00:07:52.650
Given the class.

00:07:54.670 --> 00:07:55.850
Much simpler equation.

00:07:59.183 --> 00:08:03.816
So now looking specifically at text,
first we're going to assume

00:08:03.816 --> 00:08:08.820
we're going to look at all word
positions in a text document.

00:08:08.820 --> 00:08:12.910
So we have a text document and
it has a hundred words in it.

00:08:12.910 --> 00:08:16.680
So for position of word number one,
position number two,

00:08:16.680 --> 00:08:21.860
position number three,
we're going to look at all the classes.

00:08:21.860 --> 00:08:26.110
And for each class, we're going to say
what's the probability of the class.

00:08:26.110 --> 00:08:30.180
And then for each class, we're going to
walk through every position in the text.

00:08:30.180 --> 00:08:34.820
And for each position, we're going to look
at the word in that position and ask,

00:08:34.820 --> 00:08:37.450
what's its probability given
the class I'm looking at?

00:08:37.450 --> 00:08:44.500
So we'll do this for class 1,
we'll compute P P of class 1,

00:08:44.500 --> 00:08:51.838
times the product over all the I's
of p of word 1 given class 1.

00:08:51.838 --> 00:08:56.821
So we'll compute that and
then we'll do the same thing for

00:08:56.821 --> 00:09:00.379
class 2 we'll compute p of class 2, and

00:09:00.379 --> 00:09:06.320
then the product over all positions
i of the p of word i given class 2.

00:09:06.320 --> 00:09:09.530
And then we're going to pick whichever
of these two is the highest.

00:09:09.530 --> 00:09:13.430
If this is higher, we're going to pick
class 2 and assign it to the document.

00:09:13.430 --> 00:09:16.470
If this is higher,
we'll assign class 1 to the document.

00:09:16.470 --> 00:09:19.010
And of course I've shown you
this with just two classes, but

00:09:19.010 --> 00:09:22.230
in general this is true for
any number of classes.

00:09:25.180 --> 00:09:27.880
So that's the formalization of
the Naive Bayes Classifier.

