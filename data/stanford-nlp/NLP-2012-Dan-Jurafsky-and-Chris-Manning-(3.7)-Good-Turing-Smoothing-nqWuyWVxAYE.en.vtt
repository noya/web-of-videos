WEBVTT
Kind: captions
Language: en

00:00:01.980 --> 00:00:04.980
We're ready to talk now about
advanced methods of smoothing.

00:00:06.380 --> 00:00:09.470
Remember the add one smoothing
that we had earlier.

00:00:09.470 --> 00:00:12.780
In add one smoothing we
add 1 to the numerator and

00:00:12.780 --> 00:00:18.040
V to the denominator, and we saw
a generalization of that at k smoothing,

00:00:18.040 --> 00:00:22.140
where we added k to the numerator and
kV to the denominator.

00:00:22.140 --> 00:00:26.620
And we can modify that slightly, we can
create a new version where we simply

00:00:26.620 --> 00:00:32.010
replace, introduce a new term
a new variable m equals kV

00:00:33.500 --> 00:00:38.680
and now we have a new way
of writing n case moving.

00:00:38.680 --> 00:00:41.910
That is going to be a helpful way
of writing it and the reason is,

00:00:41.910 --> 00:00:47.130
let's see in the next slide,
that when we write it this way,

00:00:47.130 --> 00:00:51.300
we can see that what we're doing
is adding to every bigram,

00:00:51.300 --> 00:00:55.950
we're adding a constant that's
related to 1 over the vocabulary size.

00:00:55.950 --> 00:00:58.820
And instead of doing that,
we could add a constant related

00:00:58.820 --> 00:01:02.400
to the unigram probability of
the word that we're back off to.

00:01:02.400 --> 00:01:06.670
So, the Unigram prior
smoothing algorithm is

00:01:06.670 --> 00:01:10.750
an extension to add a K
that says instead of.

00:01:10.750 --> 00:01:14.215
Using 1 over V as our, to add to every,

00:01:14.215 --> 00:01:19.540
adding some function of one
over V to every bigram count.

00:01:19.540 --> 00:01:23.930
Let's add something about
the unigram probability.

00:01:23.930 --> 00:01:27.464
So really unigram prior is
a kind of interpolation,

00:01:27.464 --> 00:01:31.483
its a variant interpolation
where we're adding a count And

00:01:31.483 --> 00:01:35.920
in some function of the unigram
probability to the bigram count.

00:01:35.920 --> 00:01:41.683
Nonetheless although unigram
prior smoothing works well,

00:01:41.683 --> 00:01:46.429
it still not, well it wasn't,
it still doesn't

00:01:46.429 --> 00:01:51.300
work well enough to be used for
language modeling.

00:01:51.300 --> 00:01:52.491
Nonetheless despite the fact that unigram
prior smoothing works well it doesn't work

00:01:52.491 --> 00:01:54.130
well enough to be used for
language modeling.

00:01:54.130 --> 00:01:58.020
Instead the intuition used by many
smoothing algorithms Good-turing

00:01:58.020 --> 00:02:01.300
smoothing, Kneser-Ney smoothing,
Witten-Bell smoothing.

00:02:01.300 --> 00:02:04.670
Is to use the count of
things we've seen once,

00:02:04.670 --> 00:02:06.970
to estimate the count of
things we've never seen.

00:02:06.970 --> 00:02:11.310
The goal of a smoothing algorithm is to
replace those unseen zeros with something

00:02:11.310 --> 00:02:14.620
else and all these algorithms say
look at the things you've seen once.

00:02:14.620 --> 00:02:19.190
Things that you saw once before are just
like things that you haven't seen yet

00:02:19.190 --> 00:02:21.440
and then you going to see
them once in the test set.

00:02:22.460 --> 00:02:27.055
So to see how this intuition works,
we're going to introduce some notation and

00:02:27.055 --> 00:02:31.335
the notation we're going to
introduce is big N sub c and

00:02:31.335 --> 00:02:34.085
that would mean the frequency
of frequency c.

00:02:34.085 --> 00:02:38.045
Meaning how many things
occurred with frequency c?

00:02:38.045 --> 00:02:41.025
How big is the bin of things
that occurred with frequency c?

00:02:41.025 --> 00:02:45.305
And that's hard to suffer intuitive so
let's look at some intuitions.

00:02:46.930 --> 00:02:49.630
So, let's take a look at a sentence
Sam I am I am Sam I do not eat.

00:02:49.630 --> 00:02:53.480
And let's just look at
the intergram counts in there.

00:02:53.480 --> 00:02:56.700
So, we have I occurring three times.

00:02:56.700 --> 00:03:01.610
Sam occurring twice and do not and
eat occurring once each time.

00:03:01.610 --> 00:03:05.490
So, what is N sub 1?

00:03:05.490 --> 00:03:08.140
How many things occur one time?

00:03:09.390 --> 00:03:10.280
Well here they are.

00:03:10.280 --> 00:03:11.370
There are three of them.

00:03:11.370 --> 00:03:13.200
Three different word types occur one time.

00:03:13.200 --> 00:03:14.670
So N sub 3, sub 1 is 3.

00:03:14.670 --> 00:03:19.840
How about,
how many things occur two times?

00:03:19.840 --> 00:03:21.660
Well there's two of those.

00:03:21.660 --> 00:03:22.480
So N sub 2 is 2.

00:03:22.480 --> 00:03:26.640
And how about things
that occur three times?

00:03:26.640 --> 00:03:28.150
Well, only one of those happens.

00:03:28.150 --> 00:03:29.390
So N sub 3 is 1.

00:03:29.390 --> 00:03:30.278
All right, so

00:03:30.278 --> 00:03:35.535
now that we have the intuition about how
to think about frequencies of frequencies,

00:03:35.535 --> 00:03:39.840
let's apply this to get the intuition for
Good-Turing smoothing.

00:03:41.520 --> 00:03:45.560
Imagine you're fishing, this is
a scenario invented by Josh Goodman, and

00:03:45.560 --> 00:03:51.690
you've caught 10 carp, 3 perch,
2 whitefish, 1 trout, 1 salmon, and 1 eel.

00:03:53.550 --> 00:03:57.045
I don't know what kind of river or
stream or ocean this could be,

00:03:57.045 --> 00:03:59.580
but nonetheless, you've caught 18 fish.

00:03:59.580 --> 00:04:03.870
And we want to estimate how likely is
it that the next species is trout?

00:04:03.870 --> 00:04:08.179
And this, just like words, we have maybe
a word that's occurred ten times or

00:04:08.179 --> 00:04:09.480
three or two or one.

00:04:09.480 --> 00:04:12.410
We want to know how likely is the,
are these ones to occur again?

00:04:14.460 --> 00:04:17.150
Well, there's been 18 fish.

00:04:17.150 --> 00:04:21.820
Trout's occurred one time out of 18, so
the probability ought to be one out of 18.

00:04:21.820 --> 00:04:28.060
But now, let's ask how likely is it
that the next species is a new species?

00:04:28.060 --> 00:04:31.350
Catfish or bass,
some species that we haven't seen before.

00:04:31.350 --> 00:04:33.100
Something that occurred zero times.

00:04:33.100 --> 00:04:38.420
The Good-Turing intuition says let's
use our estimate of things we saw once

00:04:38.420 --> 00:04:40.920
to estimate these new things
we've never seen before.

00:04:40.920 --> 00:04:44.950
So what's our estimate of things once,
are estimate of thing once,

00:04:44.950 --> 00:04:48.140
is drawn from N sub 1.

00:04:48.140 --> 00:04:49.540
And how many things occurred once?

00:04:49.540 --> 00:04:50.520
Well what's N sub 1?

00:04:50.520 --> 00:04:52.470
N sub 1 is 3.

00:04:52.470 --> 00:05:00.080
So out of the 18 things we saw, 3 of them
were things that only occurred one time.

00:05:00.080 --> 00:05:06.560
So let's use 3 out of 18, as our estimate
for things that we've never seen before.

00:05:06.560 --> 00:05:11.350
We're going to use our estimate of things,
a count of things that we've seen once,

00:05:11.350 --> 00:05:14.220
as our estimate of things
that we've never seen before.

00:05:14.220 --> 00:05:18.130
We're going to reserve some probability
mass for all those unseen things.

00:05:19.780 --> 00:05:24.461
Well, now, if we do that, if we use
three out of 18 as our estimate for

00:05:24.461 --> 00:05:27.426
all the unseen things
we could possibly see,

00:05:27.426 --> 00:05:30.330
how likely is it that
next species is trout?

00:05:30.330 --> 00:05:34.639
Well, I already asked you that question
but, before I said one over 18, but

00:05:34.639 --> 00:05:39.143
that can't be true anymore, it must be
less than one over 18 because we've used

00:05:39.143 --> 00:05:43.909
some of our probability mass for, from the
original 18 fish we've saved some of that

00:05:43.909 --> 00:05:46.688
for these new fish that
we've never seen before.

00:05:46.688 --> 00:05:51.479
We've removed 3/18 of our probability
mass and so we now have to

00:05:51.479 --> 00:05:56.870
discount all of our probabilities for
the other fish downward a little bit.

00:05:59.830 --> 00:06:03.850
How are we going to estimate
what this discount factor is.

00:06:03.850 --> 00:06:05.520
How much do we reduce all of these counts?

00:06:10.660 --> 00:06:16.110
Here's the equation for Good Turing,
here's the answer to that question.

00:06:16.110 --> 00:06:19.970
Good Turing tells us
that the probability for

00:06:19.970 --> 00:06:25.215
things that we've never seen before,
P star For things with zero frequency,

00:06:25.215 --> 00:06:28.095
here is exactly what we
used on the previous slide.

00:06:28.095 --> 00:06:31.595
N sub 1, the count of things that have
occurred with frequency 1 over N.

00:06:31.595 --> 00:06:35.625
So it's just a as we saw 3 out
of 18 was our number before.

00:06:35.625 --> 00:06:41.835
Well then, what do you do with things
that didn't occur with zero frequency.

00:06:41.835 --> 00:06:44.565
And for that we use the second
part of the Good Turing equation,

00:06:44.565 --> 00:06:48.020
which says, the new count c star,
the Good Turing count.

00:06:48.020 --> 00:06:54.610
Is going to be N sub c+1
divide it by N sub c x c+1.

00:06:54.610 --> 00:06:58.160
So, let's just work that out and
we'll give you an intuition for

00:06:58.160 --> 00:07:02.610
why this is in a second,
let's work it out a little example first.

00:07:02.610 --> 00:07:07.610
So, unseen fish, let's say it's bass or
catfish, we haven't seen before In

00:07:07.610 --> 00:07:12.520
the training set, the maximum likelihood
probability, estimated probability is 0.

00:07:12.520 --> 00:07:15.030
We didn't see this in
the training set out of 0 words.

00:07:15.030 --> 00:07:17.830
So, it's 0 out of 18 or zero.

00:07:17.830 --> 00:07:21.930
But smooth, we're going to use
the new contouring probability.

00:07:21.930 --> 00:07:25.060
And that says it's N1 out of N.

00:07:25.060 --> 00:07:29.250
N1 is 3, we saw three things once in
the previous slide out of 18 things.

00:07:29.250 --> 00:07:32.450
And so
the new probability is going to be 3 18s.

00:07:32.450 --> 00:07:36.600
What about for
something we've seen once like trout?

00:07:36.600 --> 00:07:38.620
How are we going to re-estimate the trout?

00:07:38.620 --> 00:07:43.770
Well the maximum likelihood estimate tells
us that there was the count of 1 and so

00:07:43.770 --> 00:07:46.990
the maximum likelihood
probability is 1 over 18.

00:07:46.990 --> 00:07:54.660
But the new Good Turing formula here says
the count of trout should be c, c is 1.

00:07:54.660 --> 00:07:59.910
C plus 1, so 2 times N sub 2 over N sub 1,
and that's going to

00:07:59.910 --> 00:08:06.410
be 2 times 1 3rd, because N sub 2 is 1,
from the previous slide, and N sub 1.

00:08:06.410 --> 00:08:10.470
Is three and 2 * 1/3, so 2/3.

00:08:10.470 --> 00:08:15.825
So our Good Turing probability
takes our C*(trout) and

00:08:15.825 --> 00:08:22.941
divides it by the 18 things we've seen so
it's 2/3 / 18 or 1/27.

00:08:22.941 --> 00:08:27.950
So instead of the count of things
we saw once before, we had 1/18.

00:08:27.950 --> 00:08:33.520
And now we've dropped it to two-thirds,
two-thirds over 18.

00:08:33.520 --> 00:08:36.000
So we've discounted our probability

00:08:36.000 --> 00:08:40.020
from one-eighteenth to only
two-thirds of an eighteenth.

00:08:40.020 --> 00:08:43.719
And we've used that extra discount
to probability mass to account for

00:08:43.719 --> 00:08:45.860
the 0 things we've never seen before.

00:08:50.000 --> 00:08:53.362
Let's look tat the Ney's Intuition for
Good Turning developed by Herman Ney and

00:08:53.362 --> 00:08:54.110
his colleagues.

00:08:54.110 --> 00:08:57.130
Imagine the training set,
this is of size c.

00:08:57.130 --> 00:09:00.440
And this is a training
set with words in it.

00:09:00.440 --> 00:09:05.640
And here's a word, here's another word,
here's another word, here's another word.

00:09:05.640 --> 00:09:10.730
And now let's, we're going to hold out
iteratively words from this training set.

00:09:10.730 --> 00:09:13.900
Let's first take one word,
the first word there, the blue word, and

00:09:13.900 --> 00:09:15.160
we'll just write it over here.

00:09:15.160 --> 00:09:18.190
So now we'll think about
the training set without that word.

00:09:18.190 --> 00:09:19.380
That's got c-1 word.

00:09:19.380 --> 00:09:23.320
And this one held out word over here,
the blue word.

00:09:23.320 --> 00:09:25.050
And now let's do the same
thing with a different word.

00:09:25.050 --> 00:09:26.880
Let's take out, let's say the second word.

00:09:26.880 --> 00:09:30.415
So we still have c-1 if
we include this guy.

00:09:30.415 --> 00:09:33.398
C-1, words left in training and
then one more word over here.

00:09:33.398 --> 00:09:34.740
And the held-out set.

00:09:34.740 --> 00:09:38.610
And we'll do this c times, so
each time we'll pull out one word.

00:09:40.410 --> 00:09:42.110
We've pulled out words one by one and

00:09:42.110 --> 00:09:46.910
what we've created is a held-out
set that's of size c.

00:09:46.910 --> 00:09:51.210
But each word in it was created from
a training set that was missing that word,

00:09:51.210 --> 00:09:54.110
a training set of size
c-1 minus that word.

00:09:54.110 --> 00:09:57.290
So imagine each of these words and
their corresponding training sets.

00:09:59.210 --> 00:10:03.640
And we can look at a picture developed by
Dan Klein to think about this intuition.

00:10:04.840 --> 00:10:09.250
And here we've just turned those held
out and training sets on their sides, so

00:10:09.250 --> 00:10:12.950
I still have of length c, but
I've now written them vertically.

00:10:12.950 --> 00:10:15.420
And now let's think about this intuition.

00:10:15.420 --> 00:10:18.890
I've got c training sets,
each one of size c minus one,

00:10:18.890 --> 00:10:22.370
and then each one has
a held out set of size one.

00:10:22.370 --> 00:10:24.140
And let's try to answer the question,

00:10:24.140 --> 00:10:28.370
what fraction of held out
words are unseen in training.

00:10:28.370 --> 00:10:32.250
Well, these words, N sub 0,
the words unseen in training.

00:10:33.580 --> 00:10:37.630
Each word that's unseen in
training occurred one time

00:10:37.630 --> 00:10:41.830
in the original training set before we
took out each of our held out data.

00:10:41.830 --> 00:10:44.470
If there was a word and
it occurred once in training, so

00:10:44.470 --> 00:10:46.790
it's an N sub 1, heard once in training.

00:10:46.790 --> 00:10:48.832
And we take it out of it's training set.

00:10:48.832 --> 00:10:53.070
Leaving c-1 words, then that word
occurs 0 times in its training set,

00:10:53.070 --> 00:10:56.200
the new training set without that word.

00:10:56.200 --> 00:11:02.300
So the held-out words, N sub 0 of them,
those N sub 0 words, were the words

00:11:02.300 --> 00:11:06.110
that were N sub 1 in their original
training set before you removed them.

00:11:06.110 --> 00:11:10.160
So if we want to know how many
words are unseen in training,

00:11:10.160 --> 00:11:14.812
it's the words that occurred one time in
the original training set, or N 1 over c.

00:11:16.190 --> 00:11:19.600
But correspondingly if we want to know,
let me clear that up.

00:11:19.600 --> 00:11:22.060
Where fraction of words
are seen k times in training.

00:11:22.060 --> 00:11:25.820
Let's pick a k perhaps it will be two so
we pick n sub two.

00:11:25.820 --> 00:11:30.670
Then the number of things that occurred
two times in our held-out set is

00:11:30.670 --> 00:11:34.530
the number of things that occurred three
times in the original training before we

00:11:34.530 --> 00:11:39.950
remove one copy of each of those words,
and now they occur only twice.

00:11:39.950 --> 00:11:44.570
So we need to think if we want to know
how many words occur k times in training,

00:11:44.570 --> 00:11:45.380
to estimate that,

00:11:45.380 --> 00:11:49.060
it's really the words that occurred k plus
1 times in our original training set.

00:11:50.570 --> 00:11:56.250
And then we're going to
want to multiply that

00:11:56.250 --> 00:11:59.540
by m the number of words that occur.

00:11:59.540 --> 00:12:03.920
Each of those words occurs k + 1 times.

00:12:03.920 --> 00:12:08.765
So we have k + 1 word occurrences of

00:12:08.765 --> 00:12:13.760
the n sub k + 1 bin,
each of which has n sub k + 1 words in it.

00:12:13.760 --> 00:12:16.250
And we'll express that as a fraction
out of the total words c.

00:12:16.250 --> 00:12:18.240
Remember the total words are c.

00:12:18.240 --> 00:12:21.550
So that's the fraction of held out
words seen k times in training.

00:12:23.010 --> 00:12:27.720
And that means in the future
we expect (k+1) times N

00:12:27.720 --> 00:12:32.060
sub k+1 over to be those
with training count k.

00:12:32.060 --> 00:12:35.490
And since there are N sub k words
with training count k, we want to.

00:12:35.490 --> 00:12:37.220
This fraction, this probability,

00:12:37.220 --> 00:12:40.310
we're going to distribute
that over N sub k words.

00:12:40.310 --> 00:12:45.110
So we'll get each of those N sub k words
is going to occur with probability k+1

00:12:45.110 --> 00:12:48.620
times N sub k plus one
over c over N sub k,

00:12:48.620 --> 00:12:51.080
because we're distributing
over those words.

00:12:51.080 --> 00:12:54.900
And that means that the expected
can be multiplied back by

00:12:54.900 --> 00:12:57.760
c again to turn from
a fraction back into a count.

00:12:57.760 --> 00:13:02.480
The expected count of words that
occur with training count k,

00:13:02.480 --> 00:13:07.645
case of star is k + 1 times the ratio
of n sub k + 1 over n sub k Think we

00:13:07.645 --> 00:13:12.996
talked about, we always compute
the count n sub k from n sub k plus one,

00:13:12.996 --> 00:13:17.531
but what do we do with words that
are in fact the largest set,

00:13:17.531 --> 00:13:22.158
the k plus the largest number,
let's say that the word The,

00:13:22.158 --> 00:13:27.150
is in fact the word that occurred
most frequently in the we

00:13:27.150 --> 00:13:31.310
don't have a more frequent
word to estimate it from.

00:13:31.310 --> 00:13:35.505
So, for large k, this Good-Turing
estimator doesn't work well because

00:13:35.505 --> 00:13:40.100
there's lots of words that may never
have occurred let's say 4 4418 times,

00:13:40.100 --> 00:13:41.510
or even 3722 times.

00:13:41.510 --> 00:13:45.030
We're going to have some gaps.

00:13:45.030 --> 00:13:47.960
And so then we'll have sum 0, so

00:13:47.960 --> 00:13:52.380
the word you'll need is the word the and
this is some other word, of.

00:13:52.380 --> 00:13:55.050
And there's a missing word in here and
there's missing words here.

00:13:55.050 --> 00:13:59.660
So we can't always use the N plot
one word to do the estimation.

00:13:59.660 --> 00:14:05.267
And a simple replacement for that, in
fact I'm going to call simple Good-Turing

00:14:05.267 --> 00:14:10.860
is after the counts gone live or
after the first few counts.

00:14:10.860 --> 00:14:15.950
We just replace our estimator with
some kind of a best fit power law, so

00:14:15.950 --> 00:14:21.180
we don't actually use Good-Turing with
each of these higher order numbers.

00:14:21.180 --> 00:14:22.930
We just use them for the lower bins.

00:14:22.930 --> 00:14:27.401
So let's look at the result Good-Turing
numbers from one example, so

00:14:27.401 --> 00:14:29.487
here's numbers from a Church and

00:14:29.487 --> 00:14:34.540
Gale experiment where they used
22 million words of AP Newswire.

00:14:34.540 --> 00:14:38.530
Here's the, just to remind you here
is the, a good turn equation so

00:14:38.530 --> 00:14:44.920
the count c star is c +
1 times N c+1 over Nc.

00:14:44.920 --> 00:14:48.960
So here is the original count c,
here is the 0s

00:14:48.960 --> 00:14:53.080
now replaced by the Good-Turing estimator
with a little extra probability mass.

00:14:53.080 --> 00:14:58.010
From N sub one here's the one,
the one's alternate in point four four six

00:14:58.010 --> 00:15:02.230
of the things that occurred count two now
occur with count one point two six of all

00:15:02.230 --> 00:15:07.310
the things with count three occur
with count two point two four so

00:15:07.310 --> 00:15:13.690
each of our counts has been discounted
of this each count has been discounted

00:15:13.690 --> 00:15:18.070
To a lower number to leave some room for
the things with zero count.

00:15:19.630 --> 00:15:22.450
And the last thing I want to
leave you on is asking you

00:15:22.450 --> 00:15:25.680
what's the relationship
between each of these counts?

00:15:25.680 --> 00:15:28.670
The original counts,
c, and these counts c*.

00:15:28.670 --> 00:15:30.030
Do you notice any general relationship?

