WEBVTT
Kind: captions
Language: en

00:00:00.790 --> 00:00:03.980
How do we deal with bigrams
with zero probability?

00:00:03.980 --> 00:00:07.020
The simplest idea is
called add-one smoothing.

00:00:08.680 --> 00:00:12.780
Let's look at a picture that gives us
the intuition of smoothing in general,

00:00:12.780 --> 00:00:13.515
from Dan Klein.

00:00:15.480 --> 00:00:21.460
So suppose in our training data
we saw denied the allegations,

00:00:21.460 --> 00:00:24.400
denied the reports, denied the claims,
denied the request.

00:00:24.400 --> 00:00:26.008
And so we've computed probabilities.

00:00:26.008 --> 00:00:28.980
There were seven total things
following denied the and

00:00:28.980 --> 00:00:32.510
we can get our probabilities of
everything of each of these things.

00:00:32.510 --> 00:00:37.020
But we would like to say, well,
maybe denied the report might occur.

00:00:37.020 --> 00:00:38.961
Sorry, denied the report
was in the training data.

00:00:38.961 --> 00:00:42.570
Denied the effort might occur,
denied the outcome might occur.

00:00:42.570 --> 00:00:46.240
So we'd like to steal some
probability mass and save it for

00:00:46.240 --> 00:00:47.930
things we might not see later.

00:00:47.930 --> 00:00:52.450
So this is our training data and
this is the maximum likelihood counts.

00:00:52.450 --> 00:00:55.780
So these things occurred after
denied the and these never occurred.

00:00:55.780 --> 00:01:00.234
We'd like to steal a little probability
mass from each of these words and

00:01:00.234 --> 00:01:03.958
put that probability mass onto
all other possible words, or

00:01:03.958 --> 00:01:06.600
some set of words so
that the zeros go away.

00:01:06.600 --> 00:01:11.512
And the simplest way of doing this
is called add-one estimation,

00:01:11.512 --> 00:01:15.630
or Laplace smoothing, and
the idea is very simple.

00:01:15.630 --> 00:01:19.380
We pretend we saw each word one
more time than we actually did.

00:01:19.380 --> 00:01:21.110
We just add one to all the counts.

00:01:22.130 --> 00:01:27.065
So if our maximum likelihood
estimate is the count of

00:01:27.065 --> 00:01:31.551
the bigram divided by
the count of the unigram,

00:01:31.551 --> 00:01:36.486
our add-one estimate is
the count of the bigram plus

00:01:36.486 --> 00:01:40.210
1 over the count of the unigram plus V.

00:01:41.310 --> 00:01:45.542
We have to add V here in the denominator
because we're adding 1 to

00:01:45.542 --> 00:01:49.480
every word that follows word i-1.

00:01:49.480 --> 00:01:55.330
So our denominator has increased not
just by the total count of times that

00:01:55.330 --> 00:01:59.810
something happened to word i-1 wasn't just
all the previous things that followed it,

00:01:59.810 --> 00:02:02.230
but each one of those got incremented
by 1 and there were V of them,

00:02:02.230 --> 00:02:05.450
so we have to add V to the denominator.

00:02:05.450 --> 00:02:11.470
This is the add-1 estimator and
probability estimator.

00:02:11.470 --> 00:02:13.632
I keep using the term
maximum likelihood estimate,

00:02:13.632 --> 00:02:15.840
and let's just remind you what that means.

00:02:15.840 --> 00:02:20.080
The maximum likelihood estimate
of some parameter of some model

00:02:20.080 --> 00:02:23.510
from a training set is the one
that maximizes the likelihood

00:02:23.510 --> 00:02:25.580
of the training set given the model.

00:02:25.580 --> 00:02:27.512
So we have some training set and

00:02:27.512 --> 00:02:31.260
we're going to maximum likelihood
estimator that lets us learn a model

00:02:31.260 --> 00:02:35.100
from a training set is the one that
makes that training set most likely.

00:02:35.100 --> 00:02:36.670
What do we mean by this?

00:02:36.670 --> 00:02:41.496
Suppose the word bagel occurs 400
times in a corpus of a million words.

00:02:41.496 --> 00:02:46.492
And I ask what's the probability that
a random word from some other text will be

00:02:46.492 --> 00:02:47.030
bagel?

00:02:48.720 --> 00:02:52.386
Well, the maximum
likelihood estimator from

00:02:52.386 --> 00:02:56.714
our corpus is 400 over 1 million,
or 0.004.

00:02:56.714 --> 00:02:59.930
Now this could be a bad estimate for
that other corpus.

00:02:59.930 --> 00:03:04.050
Who knows whether the other corpus
bagel occurs 400 times per million or

00:03:04.050 --> 00:03:05.510
some other probability?

00:03:05.510 --> 00:03:10.530
But this estimate is the one that makes
it most likely that bagel will occur 400

00:03:10.530 --> 00:03:14.610
times in a million word corpus, which is
what it did occur in our training corpus.

00:03:14.610 --> 00:03:17.450
So we're maximizing the likelihood
of our training data.

00:03:18.506 --> 00:03:22.180
So an add-1 smoothing,
and any kind of smoothing,

00:03:22.180 --> 00:03:27.530
is a non-maximum likelihood estimator
because we're changing the counts

00:03:27.530 --> 00:03:31.020
from what they occurred in our training
data to hope to generalize better.

00:03:32.490 --> 00:03:35.940
So if we go back to our
Berkeley Restaurant project and

00:03:35.940 --> 00:03:40.568
we add 1 to all of our counts,
here's our Laplace smoothed bigram counts.

00:03:40.568 --> 00:03:43.373
Now all those 0s that we
had have become 1s, and

00:03:43.373 --> 00:03:45.500
everything else gets 1 added to it.

00:03:46.630 --> 00:03:50.205
So now we can compute the bigram
probabilities from those counts.

00:03:50.205 --> 00:03:56.259
And just using the Laplace add-1
smoothing equation that we saw earlier,

00:03:56.259 --> 00:04:02.050
and now we've got all of our Laplace
that are add-1 smoothed bigrams.

00:04:02.050 --> 00:04:06.727
So we have again the probability
of to given want is 0.26, and

00:04:06.727 --> 00:04:13.884
now all of those 0s have turned into other
things, 0.00042, 0.00026, and so on.

00:04:13.884 --> 00:04:18.997
Now we can also take those
probabilities and reconstitute

00:04:18.997 --> 00:04:24.210
the counts as if we had seen
things the number of times that we

00:04:24.210 --> 00:04:29.980
would have to see to get those
add-1 probabilities naturally.

00:04:29.980 --> 00:04:35.070
So we take our probabilities and
we re-estimate the original counts

00:04:35.070 --> 00:04:38.080
as if they were the numbers that would
have given us these probabilities.

00:04:38.080 --> 00:04:41.310
And we ask, what do those
reconstituted counts look like?

00:04:41.310 --> 00:04:45.310
How much has our add-1 smoothing
changed our probabilities?

00:04:46.620 --> 00:04:48.770
So here's reconstituted counts.

00:04:48.770 --> 00:04:53.870
So we have i is followed
by want 527 times,

00:04:53.870 --> 00:04:58.983
or chinese is followed by food 8.2 times.

00:04:58.983 --> 00:05:04.708
These are reconstituted counts, and
let's compare them to the original counts.

00:05:04.708 --> 00:05:08.381
So here on the top we have
the original counts, and

00:05:08.381 --> 00:05:11.363
here we have our reconstituted counts.

00:05:11.363 --> 00:05:14.390
And I want you to notice
that there is a huge change.

00:05:14.390 --> 00:05:18.730
So in our original account,
to followed want 608 times.

00:05:20.040 --> 00:05:24.950
In our smoothed counts,
to follows want only 238 times.

00:05:24.950 --> 00:05:31.100
So it's almost a third smaller,
three times smaller.

00:05:31.100 --> 00:05:36.039
Or chinese food occurs 82 times
in our original counts and

00:05:36.039 --> 00:05:39.618
only 8.2 in our reconstituted counts.

00:05:39.618 --> 00:05:45.864
So the add-1 smoothing has made
massive changes to our counts.

00:05:45.864 --> 00:05:49.881
And it's sometimes changing a factor
of 10, the original counts,

00:05:49.881 --> 00:05:53.214
in order to steal that probability
mass to give to all those

00:05:53.214 --> 00:05:56.840
massive numbers of 0s that had
to be assigned probabilities.

00:05:58.050 --> 00:06:01.320
In other words, add-1 estimation
is a very blunt instrument.

00:06:02.780 --> 00:06:07.213
It makes very big changes in the counts
in order to get these probability mass to

00:06:07.213 --> 00:06:10.020
assign to this massive number of zeros.

00:06:10.020 --> 00:06:14.010
And so in practice we don't actually
use add-1 smoothing for N-grams.

00:06:14.010 --> 00:06:15.120
We'll have better methods.

00:06:16.460 --> 00:06:21.090
We do use add-1 smoothing for other kinds
of natural language processing models.

00:06:21.090 --> 00:06:25.608
So add-1 smoothing, for example,
is used in text classification or

00:06:25.608 --> 00:06:29.120
in similar kinds of domains where
the number of zeros isn't so enormous.

