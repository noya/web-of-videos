WEBVTT
Kind: captions
Language: en

00:00:00.950 --> 00:00:06.300
In the last segment, we saw how to define
a maximum entropy by exponential model

00:00:06.300 --> 00:00:09.810
which had parameters which were
the weights of the various features.

00:00:09.810 --> 00:00:13.400
In this segment, we're going to look
how we can set those parameters so

00:00:13.400 --> 00:00:16.830
as to maximize the likelihood
of observed data.

00:00:16.830 --> 00:00:21.820
The log conditional likelihood of a maxent
model is a function of the observed data,

00:00:21.820 --> 00:00:26.750
the actual datums in their classes D and
C which we assumed to be

00:00:26.750 --> 00:00:31.880
independent likelihood distributed and
the parameters lambda of the model.

00:00:31.880 --> 00:00:34.890
And it has the form that we see here.

00:00:34.890 --> 00:00:39.970
So this here was the same form for the
model that we saw in the previous section.

00:00:39.970 --> 00:00:44.450
And what we can see is that in principle
it's straightforward to work out the log

00:00:44.450 --> 00:00:48.130
likelihood of some data but

00:00:48.130 --> 00:00:52.510
it's only going to be easy to do in
practice if the number of classes is

00:00:52.510 --> 00:00:56.480
reasonably modest because we're
summing over the classes here.

00:00:56.480 --> 00:00:59.360
And we'll actually come back to
that issue later in this segment.

00:00:59.360 --> 00:01:05.890
We can take this log likelihood and
separate it out into two components.

00:01:05.890 --> 00:01:10.500
So here we have what used
to be the numerator and

00:01:10.500 --> 00:01:12.930
here we have what used
to be the denominator.

00:01:13.950 --> 00:01:21.680
So we can say that the log likelihood of
the entire model is a difference between

00:01:21.680 --> 00:01:26.520
the log likelihood of the numerator and
the log likelihood of the denominator.

00:01:26.520 --> 00:01:31.720
The derivative of the numerator
is really easy to work out.

00:01:31.720 --> 00:01:34.900
We start here with working
out the partial derivative,

00:01:34.900 --> 00:01:38.730
the numerator with respect to
each parameter of the model.

00:01:38.730 --> 00:01:43.480
And so this is the numerator over here.

00:01:43.480 --> 00:01:47.920
And well, first of all we
see that the log exp cancels

00:01:47.920 --> 00:01:51.970
out because they're inverses of each
other which gives us the form over here.

00:01:51.970 --> 00:01:56.070
And then we simply move
the partial derivative

00:01:56.070 --> 00:02:00.170
inside the sum which we can do twice.

00:02:00.170 --> 00:02:00.960
We can then move it in here.

00:02:00.960 --> 00:02:05.050
And then we ask what
are the partial derivatives

00:02:05.050 --> 00:02:09.970
with respect to these lambda i fi terms.

00:02:09.970 --> 00:02:16.150
Well, the partial derivative with respect
to lambda i is going to be zero except for

00:02:16.150 --> 00:02:18.400
the term that involves lambda i.

00:02:18.400 --> 00:02:25.550
And so then, the partial derivative
create that term is simply fi(c,d).

00:02:25.550 --> 00:02:29.150
So by summing that up
we get the numerator.

00:02:29.150 --> 00:02:31.650
And the thing to notice here

00:02:31.650 --> 00:02:36.920
is that the numerator actually has
a very simple and intuitive form.

00:02:36.920 --> 00:02:42.320
So what this is calculating is
precisely the empirical count of

00:02:42.320 --> 00:02:47.200
feature fi, the derivative of the
numerator is just this empirical count.

00:02:47.200 --> 00:02:50.530
Working out the derivative of the
denominator is a fraction more complex,

00:02:50.530 --> 00:02:54.410
and you actually need to remember a little
bit of calculus for this one, but

00:02:54.410 --> 00:02:55.230
it's not that hard.

00:02:55.230 --> 00:03:00.510
So here, we have taken
the denominator term from before,

00:03:00.510 --> 00:03:04.030
and we're taking the derivative
of it with respect

00:03:04.030 --> 00:03:07.240
to each prime of the weight
that's a partial derivative.

00:03:07.240 --> 00:03:13.700
So now first of all we can move
the partial derivative inside the sum,

00:03:13.700 --> 00:03:18.440
but then we need to take the derivative
of the log of something.

00:03:18.440 --> 00:03:21.980
And so to do that we have
to then use the chain rule.

00:03:21.980 --> 00:03:26.980
So the chain rule is that you take the
derivative of the outside function times

00:03:26.980 --> 00:03:31.400
the original function times
the derivative of the inside function.

00:03:31.400 --> 00:03:34.928
So the derivative of log is
the one on exp function so

00:03:34.928 --> 00:03:38.630
we get one on exp of
the original function here.

00:03:38.630 --> 00:03:43.530
And then we take the derivative of
the inside function right here.

00:03:43.530 --> 00:03:49.100
Working then on that right hand
side we now can move the derivative

00:03:49.100 --> 00:03:56.310
inside the sum again and then we gain
the derivative of the exp function.

00:03:56.310 --> 00:04:01.540
So at that point, we have to invoke
the chain rule a second time.

00:04:01.540 --> 00:04:04.160
So the derivative of exp is itself.

00:04:04.160 --> 00:04:08.520
So, then we have exp,
times the inside function here and

00:04:08.520 --> 00:04:13.160
then we take the derivative of
the inside function over here.

00:04:13.160 --> 00:04:17.836
Okay, so at this point to go
down then to the next line,

00:04:17.836 --> 00:04:22.613
what we're doing is we
are regrouping these two terms,

00:04:22.613 --> 00:04:25.680
and that gives this part over here.

00:04:30.068 --> 00:04:34.431
And then on the right-hand side we're
still working on the derivative of this

00:04:34.431 --> 00:04:35.480
function.

00:04:35.480 --> 00:04:41.250
And all this time this function has
the same form we saw in the numerator.

00:04:41.250 --> 00:04:43.981
Since we've taken
the derivative of this sum.

00:04:46.825 --> 00:04:52.047
Here that the only term it's
going to be non zero is the one that

00:04:52.047 --> 00:04:58.430
involves lambda i and so then that term
derivative is just fi of c prime d.

00:04:58.430 --> 00:05:07.470
And so what we end up with here is this
term looks exactly like our model.

00:05:07.470 --> 00:05:10.950
We've got the exp, the same exp for

00:05:10.950 --> 00:05:15.730
a particular class over the sum of
the exp of all the different classes.

00:05:15.730 --> 00:05:23.190
So what we have here is the model
probability of class c prime and

00:05:23.190 --> 00:05:29.820
then here we have the function,
the feature value, of the value c prime.

00:05:29.820 --> 00:05:35.128
And so what we're getting at
here the model expectation or

00:05:35.128 --> 00:05:40.920
the predicted count of if given
the parameter weights lambda.

00:05:40.920 --> 00:05:45.218
So putting those two parts together what
we have is the derivative of the log

00:05:45.218 --> 00:05:50.480
likelihood with respect to
a particular parameter lambda i is

00:05:50.480 --> 00:05:57.140
simply the difference between the actual
count of that feature minus the predictive

00:05:57.140 --> 00:06:01.230
count of that feature according
to our current parameter weights.

00:06:01.230 --> 00:06:04.090
And well how do we maximize a function?

00:06:04.090 --> 00:06:07.510
We maximize a function
in the standard case

00:06:07.510 --> 00:06:10.250
at the point at which
its derivative is zero.

00:06:10.250 --> 00:06:12.890
So we want this difference to be zero.

00:06:12.890 --> 00:06:18.750
And so then in other words is saying the
optimum parameters of the model are found

00:06:18.750 --> 00:06:23.840
when the model's predicted expectation
equals its empirical expectation.

00:06:23.840 --> 00:06:27.090
So this optimum is always unique.

00:06:27.090 --> 00:06:31.780
The actual parameter values that give it
to you may not be unique, but the value

00:06:31.780 --> 00:06:36.650
that is the maximum of the function is
unique because we have a convex function.

00:06:36.650 --> 00:06:41.700
And providing you estimate your models
from real data, it always exists.

00:06:41.700 --> 00:06:43.290
This is something I'll come back to later.

00:06:43.290 --> 00:06:47.714
But these models are also called
maximum entropy models because we

00:06:47.714 --> 00:06:52.533
find that what we are actually doing
is finding the model that has maximum

00:06:52.533 --> 00:06:57.056
entropy while satisfying these
constraints on the expectations.

00:06:59.877 --> 00:07:03.043
Okay, now we know about working
out the partial derivatives for

00:07:03.043 --> 00:07:05.580
the conditional log likelihood function.

00:07:05.580 --> 00:07:10.440
So to recap, what we then want
to do is choose values for

00:07:10.440 --> 00:07:13.981
the parameters lambda1, lambda2, etc.,

00:07:13.981 --> 00:07:18.350
that maximize the conditional
log-likelihood of the training data.

00:07:18.350 --> 00:07:22.790
And what we find is the way
we do that is to make use of

00:07:22.790 --> 00:07:26.370
these partial derivatives in particular
if we take the vectors of all partial

00:07:26.370 --> 00:07:29.690
derivatives that gives us
the gradient of the function.

00:07:29.690 --> 00:07:31.047
Let's see that in a picture.

00:07:34.391 --> 00:07:39.485
So here we are imagining that we have two
parameters lambda one and lambda two and

00:07:39.485 --> 00:07:44.201
depending on the settings of those
parameters we get different values for

00:07:44.201 --> 00:07:46.420
the conditional log likelihood.

00:07:46.420 --> 00:07:50.710
And if we map them out,
we get this kind of likelihood surface.

00:07:50.710 --> 00:07:52.460
So, the idea of what we want to do is,

00:07:52.460 --> 00:07:56.230
we're going to start with the lambda1 and
lambda2 set to some.

00:07:56.230 --> 00:07:59.800
Value and
we can calculate the derivatives,

00:07:59.800 --> 00:08:02.220
the partial derivatives at this point.

00:08:02.220 --> 00:08:06.840
And those partial derivatives give us
the gradient the direction of steepest

00:08:06.840 --> 00:08:09.700
decent on this likelihood surface.

00:08:09.700 --> 00:08:14.290
And so we can walk in that direction
a little calculate the gradient again,

00:08:14.290 --> 00:08:19.200
walk in the direction of gradient a bit,
calculate the gradient again, walk.

00:08:19.200 --> 00:08:21.030
And we can head off and

00:08:21.030 --> 00:08:25.720
come to the maximum value of
the conditional log likelihood function.

00:08:25.720 --> 00:08:27.250
So for accent models,

00:08:27.250 --> 00:08:33.100
this log likelihood surface is
always convex and has a maximum.

00:08:33.100 --> 00:08:36.790
But while it looks fairly easy
to maximize in this example,

00:08:36.790 --> 00:08:39.910
In the real problems we deal with
there might be hundred of thousands or

00:08:39.910 --> 00:08:43.480
millions of parameters, and so
it's considerably more difficult.

00:08:44.890 --> 00:08:49.820
So how we solve that problem is
that we find a good numerical

00:08:49.820 --> 00:08:54.560
optimization package and
get it to find good parameter values.

00:08:54.560 --> 00:08:59.520
In particular, let me just note that
commonly these packages including

00:08:59.520 --> 00:09:03.960
the one that we use for the programming
assign that actually minimizes.

00:09:03.960 --> 00:09:06.370
So instead you minimize the negative,

00:09:06.370 --> 00:09:09.590
the conditional log likelihood
which is equivalent.

00:09:09.590 --> 00:09:13.240
Now there are many numerical
optimization techniques.

00:09:14.450 --> 00:09:17.220
The simplest is gradient descent,

00:09:17.220 --> 00:09:22.820
which just says that you walk always
in the direction of the gradient.

00:09:22.820 --> 00:09:27.800
A variant of it is the stochastic version,
stochastic gradient descent.

00:09:27.800 --> 00:09:32.316
This one is actually quite effective and
is still often used for big problems.

00:09:36.157 --> 00:09:40.863
In the early work on mexent models in
statistics and NLP, people commonly used

00:09:40.863 --> 00:09:45.861
iterative proportional fitting methods,
like generalized iterative scaling or

00:09:45.861 --> 00:09:51.160
improved iterative scaling, these
aren't used much more anymore, though.

00:09:51.160 --> 00:09:55.850
Other standard numerical optimization
methods like conjugate gradient

00:09:55.850 --> 00:09:57.430
are quite effective but

00:09:57.430 --> 00:10:03.120
the method of choice that's usually
used these days is Quasi-Newton methods.

00:10:03.120 --> 00:10:07.680
In particular one well known
algorithm is the L-BFGS algorithm,

00:10:07.680 --> 00:10:11.200
which is the one that we
used in our assignment code.

00:10:11.200 --> 00:10:13.794
And that's a good method
to use in general.

00:10:16.116 --> 00:10:20.489
Okay, so I hope that means you've now
got a good sense of how to calculate

00:10:20.489 --> 00:10:24.219
the partial derivatives of
the log likelihood function, and

00:10:24.219 --> 00:10:28.900
understand how that can be used to find
the optimal parameters for the model.

