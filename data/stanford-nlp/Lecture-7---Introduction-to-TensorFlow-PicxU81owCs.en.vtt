WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.245
[MUSIC]

00:00:05.245 --> 00:00:05.805
Stanford University.

00:00:05.805 --> 00:00:08.242
&gt;&gt; Happened is that Nishith and

00:00:08.242 --> 00:00:13.500
Barak are going to be giving
an introduction to TensorFlow.

00:00:13.500 --> 00:00:16.940
So TensorFlow is Google's
deep learning framework,

00:00:16.940 --> 00:00:19.005
which I hope everyone
will be excited to learn.

00:00:19.005 --> 00:00:21.071
And at any rate, you have to learn it,

00:00:21.071 --> 00:00:24.507
because we're gonna be using it
in assignments two and three.

00:00:24.507 --> 00:00:28.500
So this should really also help out for
the second assignment.

00:00:28.500 --> 00:00:30.950
And so before we get started with that,

00:00:30.950 --> 00:00:34.060
I just want to do a couple
of quick announcements.

00:00:34.060 --> 00:00:37.230
So the first one was on final projects.

00:00:37.230 --> 00:00:40.288
So this is really the time to be
thinking about final projects.

00:00:40.288 --> 00:00:46.267
And if you've got ideas for final projects
and want to do the final project,

00:00:46.267 --> 00:00:51.593
you should be working out how to
talk to one of me, Kevin, Danqi,

00:00:51.593 --> 00:00:56.382
Richard, Ignacio, or
Arun over the next couple of weeks.

00:00:56.382 --> 00:00:59.227
And again, obviously,
you've got to find the time, so

00:00:59.227 --> 00:01:00.840
it's hard to fit everybody in.

00:01:00.840 --> 00:01:04.400
But were are making a real effort to
have project advice office hours.

00:01:04.400 --> 00:01:08.155
There were also some ideas for projects
that been stuck up on the projects page.

00:01:08.155 --> 00:01:11.310
So encourage people to look at that.

00:01:11.310 --> 00:01:14.820
Now, people have also asked
us about assignment four.

00:01:14.820 --> 00:01:18.110
So we've also stuck up
a description of assignment four.

00:01:18.110 --> 00:01:20.786
And so look at that if you're considering
whether to do assignment four.

00:01:20.786 --> 00:01:25.314
So assignment four is gonna be doing
question answering over the SQuAD

00:01:25.314 --> 00:01:29.160
dataset and
you can look in more details about that.

00:01:29.160 --> 00:01:32.610
So then there are two other
things I wanted to mention.

00:01:32.610 --> 00:01:36.290
And we'll also put up messages on Piazza,
etc, about this.

00:01:36.290 --> 00:01:41.910
I mean, the first one is that for
assignment three,

00:01:41.910 --> 00:01:46.020
we want people to have experience
of doing things on a GPU.

00:01:46.020 --> 00:01:51.980
And we've arranged with
Microsoft Azure to use their GPUs for

00:01:51.980 --> 00:01:55.520
doing that and for
people to use for the final project.

00:01:55.520 --> 00:01:59.040
And so we're trying to get that
all organized at the moment.

00:01:59.040 --> 00:02:03.230
There's a limit to how
many GPUs we can have.

00:02:03.230 --> 00:02:06.030
So what we're gonna be doing for
assignment three and for

00:02:06.030 --> 00:02:08.494
the final project is to
allow teams of up to three.

00:02:08.494 --> 00:02:11.093
And really it's in our interest and

00:02:11.093 --> 00:02:16.390
the resource limit's interest if
many people could be teamed up.

00:02:16.390 --> 00:02:20.108
So we'd like to encourage people
to team up for assignment three.

00:02:20.108 --> 00:02:25.050
And so we've put up a Google form for
people to enter their teams.

00:02:25.050 --> 00:02:27.380
And we need people to be
doing that in advance,

00:02:27.380 --> 00:02:32.030
because we need to get that set up at
least a week in advance so we can get

00:02:32.030 --> 00:02:37.260
the Microsoft people to set up accounts
for people so that people can use Azure.

00:02:37.260 --> 00:02:40.450
So please think about groups for
assignment three and

00:02:40.450 --> 00:02:42.800
then fill in the Google form for that.

00:02:42.800 --> 00:02:47.450
And then the final thing is, for next
week, we're gonna make some attempts of

00:02:47.450 --> 00:02:51.340
reorganizing the office hours and
get some rooms for office hours so

00:02:51.340 --> 00:02:55.710
they can hopefully run more smoothly in
the countdown towards the deadline for

00:02:55.710 --> 00:02:59.440
assignment two than they did for
assignment one.

00:02:59.440 --> 00:03:03.600
So keep an eye out for that and expect
that some of the office hour times and

00:03:03.600 --> 00:03:07.090
locations will be varying a bit
compared to what they've been for

00:03:07.090 --> 00:03:09.050
the first three weeks.

00:03:09.050 --> 00:03:11.166
And so that's it from me,
and over to Nishith.

00:03:14.386 --> 00:03:16.616
&gt;&gt; Hi everyone.
Hope you had a great weekend.

00:03:16.616 --> 00:03:19.270
So today we are gonna be
talking about TensorFlow,

00:03:19.270 --> 00:03:21.549
which is another deep learning
framework from Google.

00:03:22.920 --> 00:03:24.610
So why study deep learning frameworks?

00:03:25.780 --> 00:03:29.390
First of all,
much of the research in deep learning and

00:03:29.390 --> 00:03:33.140
machine learning can be attributed because
of these deep learning frameworks.

00:03:33.140 --> 00:03:37.050
They've allowed researchers to iterate
extremely quickly and also have made

00:03:37.050 --> 00:03:41.900
deep learning and other algorithms in ML
much more accessible to practitioners.

00:03:41.900 --> 00:03:44.910
So if you see your phone a lot
smarter than it was three years ago,

00:03:44.910 --> 00:03:48.520
it's probably because one of
these deep learning frameworks.

00:03:48.520 --> 00:03:52.020
So the deep learning frameworks help
to scale machine learning code,

00:03:52.020 --> 00:03:55.580
which is why Google and Facebook
can now scale to billions of users.

00:03:55.580 --> 00:03:57.920
They can compute gradients automatically.

00:03:57.920 --> 00:04:01.510
Obviously, since you all must have
finished your first assignment,

00:04:01.510 --> 00:04:04.040
you must know that gradient
calculation isn't trivial.

00:04:04.040 --> 00:04:05.990
And so
this takes care of it automatically, and

00:04:05.990 --> 00:04:07.711
we can focus on
the high-level math instead.

00:04:08.760 --> 00:04:11.705
It also standardizes ML
across different spaces.

00:04:11.705 --> 00:04:14.702
So regardless of whether I'm at Google or
at Facebook,

00:04:14.702 --> 00:04:18.674
we still use some form of TensorFlow or
another deep learning framework.

00:04:18.674 --> 00:04:23.322
And there's lot of cross-pollination
between the frameworks as well.

00:04:23.322 --> 00:04:27.742
A lot of pre-trained models are also
available online, so people like us who

00:04:27.742 --> 00:04:32.412
have limited resources in terms of GPUs do
not have to start from scratch every time.

00:04:32.412 --> 00:04:35.052
We can stand on
the shoulders of giants and

00:04:35.052 --> 00:04:38.677
on the data that they have collected and
sort of take it up from there.

00:04:38.677 --> 00:04:42.472
They also allow interfacing with GPUs,
which is a fascinating feature,

00:04:42.472 --> 00:04:46.949
because GPUs actually speed up your code a
lot faster because of the parallelization.

00:04:46.949 --> 00:04:51.303
Which is why studying TensorFlow is sort
of almost necessary in order to make

00:04:51.303 --> 00:04:55.724
progress in deep learning, just because
it can facilitate your research and

00:04:55.724 --> 00:04:57.510
your projects.

00:04:57.510 --> 00:05:00.560
We'll be using TensorFlow for
PA two, three, and also for

00:05:00.560 --> 00:05:03.960
the final project, which also is an added
incentive for studying TensorFlow today.

00:05:05.240 --> 00:05:07.270
So what is TensorFlow actually?

00:05:07.270 --> 00:05:10.950
It's just a deep learning framework,
an open source software library for

00:05:10.950 --> 00:05:14.740
numerical computation using
flow graphs from Google.

00:05:14.740 --> 00:05:16.012
It was developed by their Brain team,

00:05:16.012 --> 00:05:17.738
which specializes in
machine learning research.

00:05:17.738 --> 00:05:21.110
And in their words, TensorFlow is
an interface for expressing machine

00:05:21.110 --> 00:05:24.890
learning algorithms, and an implementation
for executing such algorithms.

00:05:26.160 --> 00:05:30.804
So now I'll allow Barak to sort of take
over and give a high-level overview of how

00:05:30.804 --> 00:05:33.949
TensorFlow works and
the underlying paradigms that so

00:05:33.949 --> 00:05:37.317
many researchers have spent so
much time thinking about.

00:05:45.948 --> 00:05:47.705
&gt;&gt; Thanks, Nish, for starting us off.

00:05:47.705 --> 00:05:51.447
I'm gonna be introducing some of
the main ideas behind TensorFlow,

00:05:51.447 --> 00:05:55.730
its programming paradigm, and
some of its main features.

00:05:55.730 --> 00:05:59.410
So the biggest idea of all of
the big ideas about TensorFlow

00:05:59.410 --> 00:06:03.268
is that numeric computation is
expressed as a computational graph.

00:06:03.268 --> 00:06:07.540
If there was one lesson that you took out
of this presentation today, at the back of

00:06:07.540 --> 00:06:12.875
your mind is that the backbone of any
TensorFlow program is going to be a graph

00:06:12.875 --> 00:06:18.390
where the graph nodes are going to be
operations, shorthand as ops in your code.

00:06:18.390 --> 00:06:22.040
And they have any number of inputs and
a single output.

00:06:22.040 --> 00:06:26.370
And the edges between our nodes are going
to be tensors that flow between them.

00:06:26.370 --> 00:06:29.400
And the best way of thinking about
what tensors are in practice

00:06:29.400 --> 00:06:31.630
is as n-dimensional arrays.

00:06:31.630 --> 00:06:36.740
The advantage of using flow graphs as the
backbone of your deep learning framework

00:06:36.740 --> 00:06:40.620
is that it allows you to build
complex models in terms of small and

00:06:40.620 --> 00:06:42.210
simple operations.

00:06:42.210 --> 00:06:44.710
And this is going to make
your gradient calculations

00:06:44.710 --> 00:06:46.770
extremely simple when we get to that.

00:06:46.770 --> 00:06:51.297
You're going to be very, very grateful for
the automatic differentiation when you're

00:06:51.297 --> 00:06:55.250
coding large models in your
final project and in the future.

00:06:55.250 --> 00:06:59.019
Another way of thinking about a TensorFlow
graph is that each operation is

00:06:59.019 --> 00:07:01.468
a function that can be
evaluated at that point.

00:07:01.468 --> 00:07:06.260
And hopefully we will see why that is
the case later in the presentation.

00:07:06.260 --> 00:07:10.380
So let us look at an example of a neural
network with one hidden layer, and

00:07:10.380 --> 00:07:13.430
what its computational graph
in TensorFlow might look like.

00:07:13.430 --> 00:07:17.800
So we have some hidden layer that we are
trying to compute, as the ReLU activation

00:07:17.800 --> 00:07:22.010
of some parameter matrix W times
some input x plus a bias term.

00:07:22.010 --> 00:07:26.470
So if you recall from last lecture, the
ReLU is an activation function standing

00:07:26.470 --> 00:07:31.180
for rectified linear unit in the same way
that a sigmoid is an activation function.

00:07:31.180 --> 00:07:35.690
We are applying some nonlinear
function over our linear input

00:07:35.690 --> 00:07:39.860
that is what gives the neural
networks their expressive function.

00:07:39.860 --> 00:07:43.310
And the ReLU takes the max
of your input and zero.

00:07:43.310 --> 00:07:46.430
On the right, we see what the graph
might look like in TensorFlow.

00:07:46.430 --> 00:07:50.460
We have variables for our b and W, we have
a placeholder, we'll get to that soon,

00:07:50.460 --> 00:07:54.010
with the x, and nodes for
each of the operations in our graph.

00:07:54.010 --> 00:07:56.480
So let's actually dissect
those node types.

00:07:56.480 --> 00:08:01.170
Variables are going to be stateful
nodes which output their current value.

00:08:01.170 --> 00:08:06.360
In our case, it's just b and
W What we mean by saying that variables

00:08:06.360 --> 00:08:11.220
are stateful, is that they retain their
current value over multiple executions,

00:08:11.220 --> 00:08:14.220
and it's easy to restore
saved values to variables.

00:08:14.220 --> 00:08:16.940
So, variables have a number
of other useful features.

00:08:16.940 --> 00:08:19.430
They can be saved to your disk during and

00:08:19.430 --> 00:08:23.000
after training, which is what facilitates
the use the niche talked about earlier,

00:08:23.000 --> 00:08:27.530
that allows people from different
companies and groups to save, store, and

00:08:27.530 --> 00:08:30.760
send over their model
parameters to other people.

00:08:30.760 --> 00:08:35.150
And they also make gradient
updates by default.

00:08:35.150 --> 00:08:37.790
Will apply over all of the variables and
your graph.

00:08:37.790 --> 00:08:41.460
The variables are the things that you
wanna tune to minimize the loss, and

00:08:41.460 --> 00:08:42.750
we will see how to do that soon.

00:08:44.090 --> 00:08:47.490
It is really important to remember
that variables in the graph like b and

00:08:47.490 --> 00:08:49.570
w are still operations.

00:08:49.570 --> 00:08:54.120
By definition, if there can be such
a thing as a definition on this,

00:08:54.120 --> 00:08:56.570
all of your nodes in
the graph are operations.

00:08:56.570 --> 00:09:01.810
So when you evaluate the operation that is
these variables in our run time, and we

00:09:01.810 --> 00:09:06.780
will see what run time means very shortly,
you will get the value of those variables.

00:09:06.780 --> 00:09:09.850
The next type of nodes are placeholders.

00:09:09.850 --> 00:09:14.290
So placeholders are nodes whose
value is fed in at execution time.

00:09:16.370 --> 00:09:21.100
If you have inputs into your network that
depend on some sort of external data

00:09:21.100 --> 00:09:25.680
that you don't want build your graph
that depends on any real value.

00:09:25.680 --> 00:09:29.755
So these are place folders for
values that we're going

00:09:29.755 --> 00:09:34.625
to add into our computation
during training.

00:09:34.625 --> 00:09:36.385
So this is going to be our input.

00:09:36.385 --> 00:09:39.395
So for placeholders,
we don't give any initial values.

00:09:39.395 --> 00:09:44.360
We just assign a data type, and
we assign a shape of a tensor so the graph

00:09:44.360 --> 00:09:48.410
still knows what to compute even though
it doesn't have any stored values yet.

00:09:48.410 --> 00:09:51.980
The third type of node
are mathematical operations.

00:09:51.980 --> 00:09:54.120
This is going to be your
matrix multiplication,

00:09:54.120 --> 00:09:55.760
your addition, and your ReLU.

00:09:55.760 --> 00:09:59.420
All of these are nodes in
your TensorFlow graphs.

00:09:59.420 --> 00:10:02.018
And it's very important that
we're actually calling on

00:10:02.018 --> 00:10:05.278
TensorFlow mathematical operations
as opposed to NumPy operations.

00:10:07.979 --> 00:10:11.520
Okay, so let us actually
see how this works in code.

00:10:11.520 --> 00:10:12.490
So we gonna do three things.

00:10:12.490 --> 00:10:15.630
We're going to create weights
including initialization.

00:10:15.630 --> 00:10:17.720
We're going to create
a placeholder variable for

00:10:17.720 --> 00:10:21.110
our input x, and
then we're going to build our flow graph.

00:10:21.110 --> 00:10:22.260
So how does this look in code?

00:10:22.260 --> 00:10:26.530
We're gonna import our TensorFlow package,
we're gonna build a python variable b,

00:10:26.530 --> 00:10:28.220
that is a TensorFlow variable.

00:10:28.220 --> 00:10:30.630
Taking in initial zeros of size 100.

00:10:30.630 --> 00:10:32.230
A vector of 100 values.

00:10:32.230 --> 00:10:37.014
Our w is going to be a TensorFlow
variable taking uniformly distributed

00:10:37.014 --> 00:10:40.880
values between -1 and
1 of shapes of 184 by 100.

00:10:40.880 --> 00:10:45.440
We're going to create a placeholder for
our input data that doesn't take in any

00:10:45.440 --> 00:10:50.640
initial values, it just takes in a data
type 32 bit floats, as well as a shape.

00:10:50.640 --> 00:10:53.180
Now we're in position to
actually build our flow graph.

00:10:53.180 --> 00:10:55.247
We're going to express h
as the TensorFlow ReLU,

00:10:55.247 --> 00:10:59.620
of the TensorFlow matrix multiplication
of x and w, and we add b.

00:10:59.620 --> 00:11:04.600
So you can actually see that the form
of that line, when we build our h,

00:11:04.600 --> 00:11:08.570
essentially looks exactly the same
as how it would look like a NumPy,

00:11:08.570 --> 00:11:12.520
except we're calling on our
TensorFlow mathematical operations.

00:11:12.520 --> 00:11:17.130
And that is absolutely essential, because
up to this point, we are not actually

00:11:17.130 --> 00:11:21.900
manipulating any data, we're only
building symbols Inside our graph.

00:11:21.900 --> 00:11:25.000
No data is actually moving
in through our system yet.

00:11:25.000 --> 00:11:28.640
You can not print off h, and
actually see the value it expresses.

00:11:28.640 --> 00:11:29.430
First and foremost,

00:11:29.430 --> 00:11:33.190
because x is just a place holder,
it doesn't have any real data in it yet.

00:11:33.190 --> 00:11:37.020
But, even if x wasn't,
you can not print h until we run a tune.

00:11:37.020 --> 00:11:39.420
We are just building a backbone for
our model.

00:11:40.950 --> 00:11:42.710
But, you might wonder now,
where is the graph?

00:11:42.710 --> 00:11:46.500
If you look at the slide earlier, I didn't
build a separate node for this matrix

00:11:46.500 --> 00:11:49.780
multiplication node, and a different node
for add, and a different node for ReLU.

00:11:49.780 --> 00:11:50.660
Well, ReLU is the h.

00:11:50.660 --> 00:11:52.405
We've only defined one line,

00:11:52.405 --> 00:11:55.815
but I claim that we have all
of these nodes in our graph.

00:11:55.815 --> 00:11:58.695
So if you're actually try to analyze
what's happening in the graph,

00:11:58.695 --> 00:12:02.235
what we're gonna do, and
there are not too many reasons for

00:12:02.235 --> 00:12:05.295
you to do this when you're actually
programming a TensorFlow operation.

00:12:05.295 --> 00:12:09.775
But if I'm gonna call on my default graph,
and then I call get_operations on it,

00:12:09.775 --> 00:12:13.525
I see all of the nodes in my graph and
there are a lot of things going on here.

00:12:13.525 --> 00:12:17.610
You can see in the top three lines that we
have three separate nodes just to define

00:12:17.610 --> 00:12:19.010
what is this concept of zeroes.

00:12:19.010 --> 00:12:22.340
There are no values initially assigned yet

00:12:22.340 --> 00:12:26.640
to our b, but the graph is getting
ready to take in those values.

00:12:26.640 --> 00:12:29.640
We see that we have all of these
other nodes just to define

00:12:29.640 --> 00:12:31.850
what the random uniform distribution is.

00:12:31.850 --> 00:12:34.035
And on the right column we
 see we have another node for

00:12:34.035 --> 00:12:36.430
Variable_1 that is probably
going to be our w.

00:12:36.430 --> 00:12:40.420
And then at the bottom four lines we
actually see the nodes as they appear in

00:12:40.420 --> 00:12:44.170
our figure, the placeholder, the matrix
multiplication, the addition and the ReLU.

00:12:44.170 --> 00:12:48.940
So in fact, the figure that we're
presenting on the board is simple for

00:12:48.940 --> 00:12:50.590
what TensorFlow graphs look like.

00:12:50.590 --> 00:12:53.540
There are a lot of things going behind
the scenes that you don't really need to

00:12:53.540 --> 00:12:55.630
interface with as a programmer.

00:12:55.630 --> 00:12:59.970
But it is extremely important to keep in
mind that this is the level of abstraction

00:12:59.970 --> 00:13:03.710
that TensorFlow is working
with above the Python code.

00:13:03.710 --> 00:13:07.010
This is what is actually going
to be computed in your graph.

00:13:07.010 --> 00:13:11.900
And it is also interesting to see that
if you look at the last node, ReLU It is

00:13:11.900 --> 00:13:16.800
pointing to the same object in memory as
the h variable that we defined above.

00:13:16.800 --> 00:13:19.230
Both of them are operations
referring to the same thing.

00:13:19.230 --> 00:13:22.490
So in the code before,
what this h actually stands for

00:13:22.490 --> 00:13:25.780
is the last current node
in the graph that we built.

00:13:25.780 --> 00:13:26.410
So great.

00:13:26.410 --> 00:13:27.491
We define, question?

00:13:43.512 --> 00:13:48.368
So the question was
about how we're deciding

00:13:48.368 --> 00:13:52.490
what the values are, and the types.

00:13:52.490 --> 00:13:55.925
This is purely arbitrary choice,
we're just showing an example,

00:13:55.925 --> 00:13:58.712
It's not related to,
it's just part of our example.

00:14:00.792 --> 00:14:02.074
Okay.

00:14:02.074 --> 00:14:04.190
Great, so we've defined a graph.

00:14:04.190 --> 00:14:06.990
And the next question is
how do we actually run it?

00:14:06.990 --> 00:14:10.590
So the way you run graphs in
TensorFlow is you deploy it

00:14:10.590 --> 00:14:12.280
in something called a session.

00:14:12.280 --> 00:14:18.374
A session is a binding to a particular
execution context like a CPU or a GPU.

00:14:18.374 --> 00:14:21.642
So we're going to take
the graph that we built, and

00:14:21.642 --> 00:14:24.970
we're going to deploy it on to a CPU or
a GPU.

00:14:24.970 --> 00:14:28.600
And you might actually be interested to
know that Google is developing their own

00:14:28.600 --> 00:14:32.170
integrated circuit called
a tensor processing unit,

00:14:32.170 --> 00:14:35.360
just to make tensor
computation extremely quickly.

00:14:35.360 --> 00:14:40.290
It's in fact orders of magnitude
more quick then even a GPU,

00:14:40.290 --> 00:14:43.920
and they did use a tender alpha
go match against lissdell.

00:14:43.920 --> 00:14:48.054
So the session is any like
hardware environment that supports

00:14:48.054 --> 00:14:52.030
the execution of all
the operations in your graph.

00:14:52.030 --> 00:14:54.380
So that's how you deploy a graph, great.

00:14:54.380 --> 00:14:56.290
So lets see how this is run in code.

00:14:56.290 --> 00:14:58.450
We're going to build a session object, and

00:14:58.450 --> 00:15:02.320
we're going to call run on two arguments,
fetches and feeds.

00:15:02.320 --> 00:15:07.425
Fetches are the list of graph nodes
that return the outputs of the nodes.

00:15:07.425 --> 00:15:11.015
These are the nodes that we're interested
in actually computing the values of.

00:15:11.015 --> 00:15:13.725
The feeds is going to
be a dictionary mapping

00:15:13.725 --> 00:15:17.545
from graph nodes to actual values
that we want to run in our model.

00:15:17.545 --> 00:15:21.680
So this is where we actually fill in the
placeholders that we talked about earlier.

00:15:22.840 --> 00:15:27.020
So this is the code that we have earlier,
and we're gonna add some new lines.

00:15:27.020 --> 00:15:30.900
We're first going to build
a session object called tf.Session.

00:15:30.900 --> 00:15:32.800
It's gonna take some default environment.

00:15:34.240 --> 00:15:35.650
Most likely a CPU, but

00:15:35.650 --> 00:15:38.650
you're able to add in as an argument
what device you want to run it on.

00:15:38.650 --> 00:15:40.805
And then we're going to call,
first of all,

00:15:40.805 --> 00:15:44.070
session.run on initialize
all the variables.

00:15:44.070 --> 00:15:46.880
This is concept intensive
flow called lazy evaluation.

00:15:46.880 --> 00:15:51.890
It means that the evaluation of your
graph only ever happens at run time, and

00:15:51.890 --> 00:15:55.900
run time now we can add an interpretation
to run time in TensorFlow, so and so

00:15:55.900 --> 00:15:57.130
means the session.

00:15:57.130 --> 00:15:59.860
Once we build the session we're
ready to actually call unlike

00:15:59.860 --> 00:16:04.140
the tensa flow run time, so
it is only then that we actually stick or

00:16:04.140 --> 00:16:08.570
assign the values that we initialize
our BMW on to those notes.

00:16:08.570 --> 00:16:12.660
BMW never mind [LAUGHS].

00:16:12.660 --> 00:16:16.700
After those two lines, we're finally in
a position to call run on the note we're

00:16:16.700 --> 00:16:21.870
actually interested in, the H, and we feed
in our second argument of dictionary for

00:16:21.870 --> 00:16:24.410
X, it's our placeholder With
the values that we're interested.

00:16:24.410 --> 00:16:26.333
For now just some random values, question?

00:16:28.874 --> 00:16:32.519
Initialize all variables will initialize
all the things that are formerly called

00:16:32.519 --> 00:16:34.782
variables in your graph like b and
w in this case.

00:16:34.782 --> 00:16:42.801
[BLANK

00:16:42.801 --> 00:16:51.665
AUDIO] And so the question was.

00:16:51.665 --> 00:16:54.755
What is the difference between variables
and place holders, and why we might,

00:16:54.755 --> 00:16:56.347
we might want to use which.

00:16:56.347 --> 00:17:01.225
So, place, sorry, variables are in most
cases will be the parameters that we're

00:17:01.225 --> 00:17:05.175
interested in, you can almost think
of them as the direct correspondence,

00:17:05.175 --> 00:17:08.380
X are data is not a parameter,
we're interested in tuning.

00:17:08.380 --> 00:17:11.350
In the models we are working with.

00:17:11.350 --> 00:17:15.540
Additionally, it's important that our
parameters have initializations in our

00:17:15.540 --> 00:17:16.690
model to begin with.

00:17:16.690 --> 00:17:17.610
They have a state.

00:17:17.610 --> 00:17:20.670
Our input doesn't really have
a state as part of our model.

00:17:20.670 --> 00:17:24.460
If we're gonna take our model and
Export it to somebody else.

00:17:24.460 --> 00:17:28.070
There's no reason for it to actually
include any real data values.

00:17:28.070 --> 00:17:29.110
The data is arbitrary,

00:17:29.110 --> 00:17:34.050
it's the model parameters that
are the foundation of your model.

00:17:34.050 --> 00:17:37.920
They are what makes your model interesting
and computing what it computes.

00:17:39.360 --> 00:17:41.890
Great, so what have we covered so far?

00:17:41.890 --> 00:17:45.040
We first built a graph using variables and
placeholders.

00:17:45.040 --> 00:17:49.710
We then deploy that graph onto a session
which is the execution environment.

00:17:49.710 --> 00:17:52.480
And next we will see
how to train the model.

00:17:53.560 --> 00:17:57.150
So the first question that we might
ask in terms of optimization is

00:17:57.150 --> 00:17:58.340
how do we define the loss?

00:17:58.340 --> 00:18:02.680
So we're going to use placeholder for
labels as data that we feed in only at run

00:18:02.680 --> 00:18:06.530
time and then we're going to build a loss
node using our labels and prediction.

00:18:08.310 --> 00:18:11.980
The first line in code here is we're going
to have this Python variable that is

00:18:11.980 --> 00:18:14.400
the prediction at the end
of your neural network.

00:18:14.400 --> 00:18:18.870
It's going to be the top of some soft
max over whatever it is that your

00:18:18.870 --> 00:18:22.510
neural network is outputting a probability
vector could be a regression.

00:18:22.510 --> 00:18:27.420
The first sign is where is the end of the
feed forward stage of your neural network?

00:18:27.420 --> 00:18:29.700
It's what your network
is trying to predict.

00:18:29.700 --> 00:18:33.060
We're then going to create a variable
called label that is a place holder for

00:18:33.060 --> 00:18:36.070
the ground truth that our model
is trying to train against.

00:18:36.070 --> 00:18:38.470
Now we are ready to
create our cross entropy

00:18:39.600 --> 00:18:42.800
node which is just like
in our assignment one.

00:18:42.800 --> 00:18:47.020
It's going to be the sum of the labels
times the TensorFlow log of the prediction

00:18:47.020 --> 00:18:47.660
on our column.

00:18:49.330 --> 00:18:52.714
So just an interesting point, so
the sum and log do need to be

00:18:52.714 --> 00:18:57.325
TensorFlow functions, but TensorFlow
will automatically convert addition,

00:18:57.325 --> 00:19:00.919
subtraction and element wise
multiplication into TensorFlow

00:19:00.919 --> 00:19:06.307
operations Question.

00:19:06.307 --> 00:19:14.416
&gt;&gt; [INAUDIBLE]
&gt;&gt; Yep.

00:19:14.416 --> 00:19:19.250
&gt;&gt; [INAUDIBLE]
&gt;&gt; It's going to sum

00:19:19.250 --> 00:19:24.700
the row altogether which is what we want
to do since label in the label each row

00:19:24.700 --> 00:19:27.030
&gt;&gt; Is going to be a one hard vector.

00:19:27.030 --> 00:19:29.650
So you wanna multiply
that by our prediction.

00:19:29.650 --> 00:19:34.170
And it's going to multiply it at
the point of the target index.

00:19:34.170 --> 00:19:36.950
And when we sum that,
it's going to give us the correct result.

00:19:36.950 --> 00:19:41.814
Everything else will be a zero in that
row, so it's squashing it into a column.

00:19:41.814 --> 00:19:45.510
Since zero access is the rows,
axis 1 is the columns.

00:19:45.510 --> 00:19:48.382
So it's gonna collapse The colons.

00:19:48.382 --> 00:19:48.882
Yes?

00:19:54.552 --> 00:19:57.032
The question was are the feeds just for
the placeholders?

00:19:57.032 --> 00:19:59.030
Yes, that is correct.

00:19:59.030 --> 00:20:02.130
The feeds are just used as a dictionary to
fill in the values of our placeholders.

00:20:05.970 --> 00:20:09.040
Great, all right, so
we've now defined the loss and

00:20:09.040 --> 00:20:11.320
we are ready to compute the gradients.

00:20:11.320 --> 00:20:15.620
So the way this is done in TensorFlow
is we're first going to create

00:20:15.620 --> 00:20:17.180
an optimizer object.

00:20:17.180 --> 00:20:20.650
So there's a general abstract class
in TensorFlow called optimizer.

00:20:20.650 --> 00:20:25.150
Where each of the subclasses in that
class is going to be an optimizer for

00:20:25.150 --> 00:20:27.020
a particular learning algorithm.

00:20:27.020 --> 00:20:29.690
So the learning algorithm that
we already use in this class

00:20:29.690 --> 00:20:32.510
is the gradient descent algorithm,
but there are many

00:20:32.510 --> 00:20:36.250
other choices that you might want to
experiment with in your final project.

00:20:36.250 --> 00:20:38.560
They have different advantages So

00:20:38.560 --> 00:20:42.950
that is just the object to create
an optimization node in our graph.

00:20:42.950 --> 00:20:46.270
We're going to call on the method of it,
it's called minimize, and

00:20:46.270 --> 00:20:50.060
it's gonna take in its argument to the
node that we actually want to minimize.

00:20:52.020 --> 00:20:57.240
So this adds an optimization operation
to the top of our computational graph,

00:20:57.240 --> 00:21:01.840
which when we evaluate that node,
when we evaluate this variable I

00:21:01.840 --> 00:21:04.520
wrote in the top line called
train_step equals the line.

00:21:04.520 --> 00:21:08.020
When we call session on run on trainstep,
it is going to

00:21:08.020 --> 00:21:12.680
actually apply the gradients onto
all of the variables in our model.

00:21:12.680 --> 00:21:16.580
This is because the dot minimize function
actually does two things in Tensor flow.

00:21:16.580 --> 00:21:21.310
It first computes the gradient of our
argument, in this case cross entropy.

00:21:21.310 --> 00:21:25.200
With respect to all of the things that
we defined as variables in our graph,

00:21:25.200 --> 00:21:26.380
in this case the B and W.

00:21:26.380 --> 00:21:31.190
And then it's actually going to apply
the gradient updates to those variables.

00:21:31.190 --> 00:21:34.140
So I'm sure the question in the back of
all your minds now is how do we actually

00:21:34.140 --> 00:21:35.370
compute the gradients?

00:21:35.370 --> 00:21:39.590
So the way it works in TensorFlow is that,
every graph node has an attached gradient

00:21:39.590 --> 00:21:44.570
operation, has a prebuilt gradient of
the output with respect to the input.

00:21:46.320 --> 00:21:46.880
And.
So

00:21:46.880 --> 00:21:49.630
when we want to calculate
the gradient of our cross

00:21:49.630 --> 00:21:51.730
entropy with respect
to all the parameters,

00:21:51.730 --> 00:21:57.130
it is extremely easy to just backpropagate
through the graph using the chain rule.

00:21:57.130 --> 00:22:00.000
So this is where you actually
get to see the main advantage

00:22:00.000 --> 00:22:03.650
of expressing this machine-learning
framework as this computational graph,

00:22:03.650 --> 00:22:08.250
because it is very easy for
the application to step backwards,

00:22:08.250 --> 00:22:10.860
to traverse backwards through your graph,
and at each point,

00:22:10.860 --> 00:22:16.120
multiply the error signal by
the predefined gradient of our node.

00:22:18.060 --> 00:22:20.170
And all of this happens automatically, and

00:22:20.170 --> 00:22:22.640
it actually happens behind
the programmer's interface.

00:22:22.640 --> 00:22:23.140
Question?

00:22:31.979 --> 00:22:35.346
The question was is
the gradients are competed

00:22:35.346 --> 00:22:40.620
with respect to the cross With
respect to all of our variables.

00:22:40.620 --> 00:22:44.360
So the argument into the minimize function
is going to be the node that it's

00:22:44.360 --> 00:22:47.320
computing the gradient of,
in the numerator, with respect to

00:22:47.320 --> 00:22:51.090
automatically all of the things we
defined as variables in our graph.

00:22:51.090 --> 00:22:55.080
Doesn't that you can add as
another argument what variables to

00:22:55.080 --> 00:22:58.250
actually apply gradients to, but
if you don't it's just going to

00:22:58.250 --> 00:23:01.490
automatically do it to everything
defined as a variable in our graph.

00:23:01.490 --> 00:23:04.590
Which also answers a question earlier
about why we wouldn't want to call x

00:23:04.590 --> 00:23:08.380
a variable because we're not actually
we don't actually want to update that.

00:23:08.380 --> 00:23:09.790
So how does this look like in code?

00:23:09.790 --> 00:23:12.330
We're just going to add the top
line in the previous slide.

00:23:12.330 --> 00:23:15.600
We're gonna create a python variable
called train_step that takes in

00:23:15.600 --> 00:23:18.620
our Gradient Descen tOptimizer
object with learning rate of 0.5.

00:23:18.620 --> 00:23:22.315
We're gonna Minimize on it
over the cross_entropy.

00:23:22.315 --> 00:23:25.205
So you can kinda see that that
line encapsulates everything,

00:23:25.205 --> 00:23:29.335
all of the important information
about doing optimization.

00:23:29.335 --> 00:23:34.105
It knows what gradient step algorithm
to use, the gradient descent.

00:23:34.105 --> 00:23:35.585
And knows what learning rate and

00:23:35.585 --> 00:23:40.840
knows what node to compute the gradients
over and an oath to minimize it of course.

00:23:40.840 --> 00:23:42.520
Okay.
So let's actually see how to run this

00:23:42.520 --> 00:23:46.180
in code, the last thing we have to do.

00:23:46.180 --> 00:23:46.680
Question.

00:23:46.680 --> 00:23:47.689
Let me answer that.

00:23:55.350 --> 00:23:58.865
The question was how does session
know what variables to link it to?

00:23:58.865 --> 00:24:01.235
I think that this answers it.

00:24:01.235 --> 00:24:04.710
The session is going to deploy
all of the nodes in your graph

00:24:04.710 --> 00:24:07.390
Onto the runtime environment.

00:24:07.390 --> 00:24:12.610
Everything in the graph is already on it,
so when you call minimize

00:24:12.610 --> 00:24:17.510
on this particular node,
it's already there inside your session

00:24:17.510 --> 00:24:22.470
to like compute, if that answers it.

00:24:22.470 --> 00:24:25.420
Okay, so the last thing we need to
do now that we have the gradients,

00:24:25.420 --> 00:24:26.300
we have the gradient update.

00:24:26.300 --> 00:24:29.720
It's just to create
an iterative learning schedule.

00:24:29.720 --> 00:24:35.970
So we're going to iterate over, say 1,000
iterations, the 1,000 is arbitrary.

00:24:35.970 --> 00:24:40.100
We're going to call on our favorite data
sets, we're gonna take our next batch

00:24:40.100 --> 00:24:43.130
Data is just any abstract data
in this arbitrary program.

00:24:43.130 --> 00:24:47.960
So we're gonna get a batch for
our inputs, a batch for our labels.

00:24:47.960 --> 00:24:51.830
We're then going to call sess.run
on our training step variable.

00:24:51.830 --> 00:24:53.670
So remember, when we call run on that,

00:24:53.670 --> 00:24:57.500
it already applies the gradients
onto all the variables in our graph.

00:24:57.500 --> 00:24:59.760
And it's gonna take a feed dictionary for

00:24:59.760 --> 00:25:01.650
the two place holders that
we've defined so far.

00:25:01.650 --> 00:25:06.570
The x and the label where x and
label are graph nodes.

00:25:06.570 --> 00:25:11.350
The keys in our dictionary are graph nodes
and the items are going to be NumPy data.

00:25:11.350 --> 00:25:15.590
And this is actually a good place to talk
about just how well TensorFlow interfaces

00:25:15.590 --> 00:25:20.480
with NumPy because TensorFlow will
automatically convert NumPy arrays

00:25:20.480 --> 00:25:23.160
when we feed it in to
our graph in to tensors.

00:25:23.160 --> 00:25:28.500
So we can insert in to our feed dictionary
numpy arrays which are batch_x and

00:25:28.500 --> 00:25:32.370
batch_label and we are also going
to get is an output from sess.run.

00:25:32.370 --> 00:25:35.873
If I defined some variable
like output equals sess.run,

00:25:35.873 --> 00:25:40.742
that would also be a NumPy array of what
the nodes, of what the nodes evaluate to.

00:25:40.742 --> 00:25:44.740
Though train_step would return
you the gradient I believe.

00:25:44.740 --> 00:25:45.985
Are there any other questions up to that

00:25:45.985 --> 00:25:47.480
point before we take
a little bit of a turn?

00:25:47.480 --> 00:25:48.219
Yes.

00:26:01.563 --> 00:26:06.953
So I actually believe there's
some ways to create queues for

00:26:06.953 --> 00:26:13.515
inputting in data and labels,
that might be the answer to your question.

00:26:14.970 --> 00:26:19.390
I can testify to why this
might be the best method too.

00:26:19.390 --> 00:26:23.236
But it certainly is a simple one,
where you can just work with NumPy data,

00:26:23.236 --> 00:26:26.080
which is what Python
programmers are used to.

00:26:26.080 --> 00:26:30.890
And that is the insert
point into our placeholder.

00:26:30.890 --> 00:26:32.120
One more question, yes?

00:26:56.805 --> 00:27:01.200
Your question was, how does a cross
entropy know what to compute?

00:27:02.430 --> 00:27:06.240
So the cross entropy is going to take an,
I haven't defined

00:27:06.240 --> 00:27:09.810
what prediction it is fully,
I just wanted to abstract that part.

00:27:09.810 --> 00:27:13.460
The prediction is going to be something
at the end of your no network,

00:27:13.460 --> 00:27:15.670
where all of those are symbols
inside your graph.

00:27:15.670 --> 00:27:18.430
Something before that's going to be,
all these notes in your graph.

00:27:19.790 --> 00:27:22.630
I think this might be a better answer to
your question, when you evaluate some node

00:27:22.630 --> 00:27:26.630
in the graph, like if i were to call
session.runonprediction It automatically

00:27:26.630 --> 00:27:30.950
computes all of the nodes before it
in the graph that need to be computed

00:27:30.950 --> 00:27:33.570
to actually know what
the value of prediction is.

00:27:33.570 --> 00:27:36.920
Behind the season TensorFlow it's going
to transverse backwards in your graph and

00:27:36.920 --> 00:27:42.190
compute all of those operations and
that happens behind you.

00:27:42.190 --> 00:27:44.890
That happens automatically
inside my session.

00:27:45.980 --> 00:27:48.940
So the last important concept that I
wanna talk about before we move over to

00:27:48.940 --> 00:27:52.836
the live demo is the concept
of variable sharing.

00:27:52.836 --> 00:27:57.430
So when you wanna build the large model,
you often need to share

00:27:57.430 --> 00:28:01.560
large sets of variables and you might
want to initialize them all in one place.

00:28:01.560 --> 00:28:05.600
For example, I might want to
instantiate my graph multiple times or

00:28:05.600 --> 00:28:09.890
even more interestingly, I want to
train over like a cluster of GPUs.

00:28:09.890 --> 00:28:12.930
We might not have the benefit to do that
in the class because of the research

00:28:12.930 --> 00:28:16.720
limitations you wanna talk about, but
especially moving on from this class it's

00:28:16.720 --> 00:28:20.540
often the case that you wanna train your
model on many different devices at one go.

00:28:20.540 --> 00:28:22.590
So how does this concept work
if it's instantiating or

00:28:22.590 --> 00:28:27.540
model on each of these devices,
but we wanna share the variables.

00:28:27.540 --> 00:28:30.590
So one naive way you
might think of doing this

00:28:30.590 --> 00:28:34.220
is creating this variable's
dictionary at the top of your code

00:28:34.220 --> 00:28:38.360
that a dictionary of some strings into
the variables that they represent.

00:28:38.360 --> 00:28:43.810
And in this way if I wanna build locks
below it that depends on this parameters.

00:28:43.810 --> 00:28:46.110
I would just use this dictionary.

00:28:46.110 --> 00:28:49.630
I would call variables_dic and
I would take the key as these values.

00:28:49.630 --> 00:28:52.500
And that might be how I would
want to share my variables.

00:28:52.500 --> 00:28:54.420
But there are many reasons
this is not a good idea.

00:28:54.420 --> 00:28:57.442
And it's mostly because it
breaks the encapsulation.

00:28:57.442 --> 00:29:03.800
So what the code that builds your graph's
intensive flow should always have

00:29:03.800 --> 00:29:08.790
all of the relevant information about the
nodes and operations that you are using.

00:29:08.790 --> 00:29:15.255
You want to be able to, in your code
document the names of your neurons.

00:29:15.255 --> 00:29:18.252
You wanna be able to document
the types of your operations and

00:29:18.252 --> 00:29:19.794
the shapes of your variables.

00:29:19.794 --> 00:29:23.682
And you kind of lose this information
if you just have this massive variables

00:29:23.682 --> 00:29:25.827
dictionary at the top of your code.

00:29:25.827 --> 00:29:29.885
So TensorFlow inspired solution for
this is something called variable scope.

00:29:29.885 --> 00:29:33.975
A variable scope provides a simple
name spacing scheme to avoid clashes,

00:29:33.975 --> 00:29:36.600
and the other relevant
function to go along with that

00:29:36.600 --> 00:29:38.355
is something called get_variable.

00:29:38.355 --> 00:29:41.110
So get_variable will create a variable for

00:29:41.110 --> 00:29:44.980
you if a variable with
a certain name doesn't exist.

00:29:44.980 --> 00:29:48.808
Or it will access that variable
if it finds it to exist.

00:29:48.808 --> 00:29:51.330
So let us see some examples
about how this works.

00:29:51.330 --> 00:29:54.580
Let me open a new variable
scope called foo.

00:29:54.580 --> 00:29:56.700
And I'm gonna called
get_variable with the name v.

00:29:56.700 --> 00:30:00.480
So this is the first time I'm calling
get_variable on v, so it's going to create

00:30:00.480 --> 00:30:06.019
a new variable and you'll find that
the name of that variable is foo/v.

00:30:06.019 --> 00:30:09.160
So kind of calling this
variable scope on foo.

00:30:09.160 --> 00:30:13.290
It's kind of accessing a directory
that we're calling foo.

00:30:13.290 --> 00:30:15.270
Let me close that variable score an reopen

00:30:16.480 --> 00:30:19.420
it with another argument
called reuse to be true.

00:30:19.420 --> 00:30:22.960
Now if I call get_variable
with the same name v,

00:30:22.960 --> 00:30:26.490
I'm actually going to find the same
variable, I'm gonna access the variable,

00:30:26.490 --> 00:30:29.060
I'm gonna access the same
variable that I created before.

00:30:29.060 --> 00:30:32.715
So you will see that v1 and
v are pointing to the same object.

00:30:32.715 --> 00:30:36.865
If I close this variable scope again, and
reopen it, but I set reuse to be false,

00:30:36.865 --> 00:30:41.045
your program will absolutely crash
if I try to run that line again.

00:30:41.045 --> 00:30:44.940
Because you've set it to not reuse
any variables so it tries to create

00:30:44.940 --> 00:30:49.050
this new variable but it has the same
name as the variable we defined earlier.

00:30:49.050 --> 00:30:52.940
The uses of variable scope will become
apparent in the next assignment and

00:30:52.940 --> 00:30:56.440
over the class but it is something
useful to keep in the back of your mind.

00:30:56.440 --> 00:30:58.170
So in summary, what have we looked at?

00:30:58.170 --> 00:31:01.960
We learned how to build a graph in
TensorFlow that has some sort of

00:31:01.960 --> 00:31:02.800
feedforward or

00:31:02.800 --> 00:31:07.540
prediction stage where you are using
your model to predict some values.

00:31:07.540 --> 00:31:11.700
I then showed you how to optimize
those values in your neural network,

00:31:11.700 --> 00:31:16.950
how TensorFlow computes the gradients, and
how to build this train_step operation

00:31:16.950 --> 00:31:19.420
that applies gradient
updates to your parameters.

00:31:19.420 --> 00:31:22.400
I then showed you what it
means to initialize a session,

00:31:22.400 --> 00:31:25.090
which deploys your graph
onto some hardware

00:31:25.090 --> 00:31:28.340
that creates like the runtime
environment to run your program.

00:31:28.340 --> 00:31:32.330
I then showed you how to build some
sort of simple iterating schedule

00:31:32.330 --> 00:31:35.090
to continuously run and train our model.

00:31:35.090 --> 00:31:38.850
Are there any questions up to this stage
before we move on in this lecture?

00:31:40.330 --> 00:31:40.830
Yes?

00:31:48.108 --> 00:31:53.547
It doesn't because in feed_dict,
you can see that,

00:31:53.547 --> 00:31:57.450
in feed_dict it takes in some node.

00:31:57.450 --> 00:32:00.230
We're not really understanding
feed_dict with what

00:32:00.230 --> 00:32:01.490
the names of those variables are.

00:32:01.490 --> 00:32:04.500
So whenever you create a variable or
a placeholder

00:32:04.500 --> 00:32:07.690
There's always an argument that allows
you to give the name of that node.

00:32:07.690 --> 00:32:10.716
So when you create the name of that node,

00:32:10.716 --> 00:32:15.618
not name in my Python variable,
but name as a TensorFlow symbol.

00:32:15.618 --> 00:32:17.604
That's a great question,

00:32:17.604 --> 00:32:22.969
the naming scope changes the name of
the actual symbol of that operation.

00:32:22.969 --> 00:32:26.198
So if I were to scroll back in the slides
and look at my list of operations.

00:32:26.198 --> 00:32:30.487
The names of all those operations will be
appended with foo as we created earlier.

00:32:30.487 --> 00:32:33.179
Maybe one more question before
we move on if there's anything?

00:32:35.491 --> 00:32:36.201
Yes?

00:32:44.874 --> 00:32:48.297
Yes, if you load a graph
using the get variable,

00:32:48.297 --> 00:32:51.910
it will call the same
variable across devices.

00:32:51.910 --> 00:32:55.030
This is why it's extremely important to
introduce this idea of variable scope

00:32:55.030 --> 00:32:56.610
to shared devices.

00:32:56.610 --> 00:32:57.568
One more question.

00:32:57.568 --> 00:33:04.413
[BLANK AUDIO]
&gt;&gt; Can we share variables-

00:33:04.413 --> 00:33:05.457
&gt;&gt; The question was,

00:33:05.457 --> 00:33:07.747
can we share variables across sessions?

00:33:07.747 --> 00:33:10.633
I believe the answer to
that question is correct.

00:33:10.633 --> 00:33:12.388
I might be wrong.

00:33:12.388 --> 00:33:15.535
But I'm not entirely sure,
as of this time.

00:33:15.535 --> 00:33:18.620
Okay, so
we just have a couple of acknowledgements.

00:33:18.620 --> 00:33:20.240
When we created this presentation,

00:33:20.240 --> 00:33:23.360
we consulted with a few other people
who have done TensorFlow tutorials.

00:33:23.360 --> 00:33:25.910
Most of these slides
are inspired by Jon Gauthier

00:33:25.910 --> 00:33:27.710
in a similar presentation he gave.

00:33:27.710 --> 00:33:29.810
We also talked with Bharath and Chip.

00:33:29.810 --> 00:33:35.360
Chip is teaching a class, CS20SI,
TensorFlow for Deep Learning Research.

00:33:35.360 --> 00:33:40.170
So we are very grateful to all the people
we talked with to create these slides.

00:33:40.170 --> 00:33:43.412
And now, we will move on
to the research highlights,

00:33:43.412 --> 00:33:45.510
before we move on to the live demo.

00:33:56.888 --> 00:33:58.606
&gt;&gt; Hi everyone.

00:33:58.606 --> 00:34:01.051
Can you guys hear me okay?

00:34:01.051 --> 00:34:02.514
Hi my name's Alan.

00:34:02.514 --> 00:34:04.602
And let's take a break from TensorFlow and

00:34:04.602 --> 00:34:07.560
talk about something
also very interesting.

00:34:07.560 --> 00:34:11.920
I'm gonna present a paper
called Visual Dialog.

00:34:11.920 --> 00:34:13.231
Here's a brief introduction.

00:34:13.231 --> 00:34:17.828
Basically, in recent years
we are witnessing rapid

00:34:17.828 --> 00:34:20.761
development improvement in AI.

00:34:20.761 --> 00:34:24.015
Especially in natural language
processing and computer vision.

00:34:24.015 --> 00:34:29.074
And many people believe that the next
generation of intelligent systems.

00:34:29.074 --> 00:34:33.537
Will be able to hold meaningful
conversations with humans in

00:34:33.537 --> 00:34:37.041
natural language based
on the visual content.

00:34:37.041 --> 00:34:41.688
So for example it should be able
to help blind people to understand

00:34:41.688 --> 00:34:45.416
their surroundings by
answering their questions.

00:34:45.416 --> 00:34:50.297
Or you can integrate them together
with AI assistants such as

00:34:50.297 --> 00:34:54.420
Alexa to understand
people's questions better.

00:34:55.830 --> 00:35:01.740
And before I move on to the paper,
let's talk about some related work.

00:35:01.740 --> 00:35:05.790
There have been a lot of efforts trying
to combine natural language parsing and

00:35:05.790 --> 00:35:06.830
computer vision.

00:35:06.830 --> 00:35:09.900
And the first category
is image captioning.

00:35:12.210 --> 00:35:14.538
Here, I'm gonna introduce two works.

00:35:14.538 --> 00:35:17.738
The first one is a paper called Show,
Attend, and Tell.

00:35:17.738 --> 00:35:20.806
Which is an extension of
another paper called Show and

00:35:20.806 --> 00:35:23.065
Tell with some attention mechanisms.

00:35:23.065 --> 00:35:28.732
And the second one is an open-source
code written by Andrej Karpathy.

00:35:28.732 --> 00:35:32.792
In both models the models are able

00:35:32.792 --> 00:35:37.930
to give you a description of the image.

00:35:37.930 --> 00:35:41.580
And for the second case,
that's a typo right there.

00:35:41.580 --> 00:35:43.210
It should be Video Summary.

00:35:43.210 --> 00:35:48.647
Basically, the model's able to
summarize the content of the video.

00:35:48.647 --> 00:35:54.042
So imagine if you are watching a movie and
you don't wanna watch the whole movie.

00:35:54.042 --> 00:35:59.600
You wanna see what's the main
content of the movie.

00:35:59.600 --> 00:36:02.500
This model would be pretty useful.

00:36:02.500 --> 00:36:06.590
And in this category is
Visual-Semantic Alignment.

00:36:06.590 --> 00:36:10.509
So instead of giving a description for
each image, this model actually

00:36:10.509 --> 00:36:14.116
gives a description for
each individual component in the image.

00:36:14.116 --> 00:36:19.308
And as we can see on the right, the data
collection process, it's very tedious.

00:36:19.308 --> 00:36:22.453
Because you actually need to draw
a lot of boundary boxes, and

00:36:22.453 --> 00:36:24.520
give a description to every single one.

00:36:25.870 --> 00:36:30.670
And the next one is more related to our
paper, which is called visual question and

00:36:30.670 --> 00:36:31.370
answering.

00:36:31.370 --> 00:36:34.190
Basically given an image and a question,

00:36:34.190 --> 00:36:38.820
the model answers the question
based on the visual content.

00:36:38.820 --> 00:36:42.990
And in this case as you can see
the answers are either binary, yes or

00:36:42.990 --> 00:36:44.420
no, or very short.

00:36:44.420 --> 00:36:48.740
So one number, or a circle or
different types of shapes.

00:36:48.740 --> 00:36:56.269
And this paper, Visual Dialog, actually
tries to solve the issue I just mentioned.

00:36:56.269 --> 00:36:59.532
And it proposes a new AI
task called Visual Dialog.

00:36:59.532 --> 00:37:04.406
Which requires an AI agent to
hold meaningful conversation

00:37:04.406 --> 00:37:08.380
with humans based on the visual content.

00:37:08.380 --> 00:37:12.270
And also develop a novel
data collection protocol.

00:37:12.270 --> 00:37:15.367
And in my opinion,
this is the best invention ever.

00:37:15.367 --> 00:37:18.982
Because you make contributions to science,
make money, and

00:37:18.982 --> 00:37:21.562
socialize with people
all at the same time.

00:37:21.562 --> 00:37:28.144
And it also introduces the family of
deep learning models for visual dialog.

00:37:28.144 --> 00:37:32.032
And I'm not gonna go into
too many details today,

00:37:32.032 --> 00:37:37.413
because we are gonna cover deep
neural networks later in this class.

00:37:37.413 --> 00:37:44.452
This model encodes the image using
a convolutional neural network.

00:37:44.452 --> 00:37:46.432
And encodes the question and

00:37:46.432 --> 00:37:49.576
the chat history using two
recurrent neural network.

00:37:49.576 --> 00:37:54.440
And then concatenates three
representations together as a vector.

00:37:54.440 --> 00:37:57.600
It is then followed by
a fully connected layer and

00:37:57.600 --> 00:38:01.808
a decoder which generate the answer
based on the representation.

00:38:01.808 --> 00:38:07.674
And here's some analysis of the dataset.

00:38:07.674 --> 00:38:11.724
As you can see the dataset is much
better than the previous work,

00:38:11.724 --> 00:38:14.281
because there are more unique answers.

00:38:14.281 --> 00:38:21.510
And also the question and answers tend
to be longer, and here are some results.

00:38:21.510 --> 00:38:26.680
They actually show the model in
the form of a visual chat bot.

00:38:26.680 --> 00:38:30.680
Basically you can chat with
a robot online, in real time.

00:38:30.680 --> 00:38:35.640
And if you guys are interested,
please try it, [LAUGH] and that's it.

00:38:38.521 --> 00:38:43.193
&gt;&gt; [APPLAUSE]

00:38:46.142 --> 00:38:48.550
&gt;&gt; All right, let's get started then.

00:38:48.550 --> 00:38:50.430
So we're gonna start
with linear regression.

00:38:50.430 --> 00:38:53.515
I'm sure all of you if
you have taken CS 221 or

00:38:53.515 --> 00:38:57.990
CS 229 then you have heard and
coded up linear regression before.

00:38:57.990 --> 00:39:04.060
This is just gonna be a start to get us
familiarized with TensorFlow even better.

00:39:04.060 --> 00:39:08.000
So we're gonna start at,
what does linear regression do again?

00:39:08.000 --> 00:39:11.470
It takes all your data and
tries to find the best linear fit to it.

00:39:11.470 --> 00:39:14.932
So imagine house prices with time for
example or location,

00:39:14.932 --> 00:39:17.420
it's probably a linear fit.

00:39:17.420 --> 00:39:21.629
And so we generate our data set
artificially using y equals 2 x plus

00:39:21.629 --> 00:39:25.630
epsilon where epsilon is sampled
from a normal distribution.

00:39:25.630 --> 00:39:28.523
I won't really go much into
how we obtain the data.

00:39:28.523 --> 00:39:32.512
Because that, we assume, is normal Python
processing and not really TensorFlow so

00:39:32.512 --> 00:39:33.310
we will move on.

00:39:33.310 --> 00:39:37.470
And actually start implementing linear
regression and the function run.

00:39:37.470 --> 00:39:39.420
So in this first function,
linear regression,

00:39:39.420 --> 00:39:42.710
we will be actually
implementing the graph itself.

00:39:42.710 --> 00:39:45.950
As [INAUDIBLE] said, we will be
implementing and defining the flow graph.

00:39:45.950 --> 00:39:47.610
So let's get started.

00:39:47.610 --> 00:39:50.360
So, first,
we're gonna create our placeholders,

00:39:50.360 --> 00:39:53.150
because we're gonna see how
we can feed in our data.

00:39:53.150 --> 00:39:55.480
So, we have two placeholders here,
x and y.

00:39:55.480 --> 00:40:00.340
So, let's just start with creating x
first, and this is gonna be of type float,

00:40:00.340 --> 00:40:05.320
so we are gonna make float32 And
it's gonna be of shape.

00:40:05.320 --> 00:40:10.476
So we're gonna make this likely more
general and have it of shape None.

00:40:10.476 --> 00:40:14.330
And what this means that you can
dynamically change the number of

00:40:14.330 --> 00:40:17.110
batches that you can send to your network.

00:40:17.110 --> 00:40:19.500
Or in this case, your linear model.

00:40:19.500 --> 00:40:21.200
And it's just a row vector here.

00:40:21.200 --> 00:40:23.929
All right, and we're gonna name it x.

00:40:26.520 --> 00:40:31.824
We're gonna create y
which is the label and

00:40:31.824 --> 00:40:38.316
which will also be of the same type and
shape as well.

00:40:40.370 --> 00:40:44.910
All right, and we're gonna name it y.

00:40:44.910 --> 00:40:47.200
All right, so
now that we have defined our placeholders,

00:40:47.200 --> 00:40:49.450
we're gonna start
creating other variables.

00:40:49.450 --> 00:40:51.210
So we start with first
by defining our scope.

00:40:51.210 --> 00:40:54.350
So let's say tf.variable_scope, and

00:40:54.350 --> 00:40:57.980
we're gonna name it just a lreg,
because linear regression.

00:40:59.280 --> 00:41:02.220
And we're gonna call it scope, all right?

00:41:02.220 --> 00:41:06.017
So now that we are here,
we're gonna create our matrix which is w.

00:41:06.017 --> 00:41:10.758
So we're gonna call it tf.Variable and
since it's just a linear regression,

00:41:10.758 --> 00:41:15.514
it'll just be a single integer or not
an integer, my bad, but just one number.

00:41:15.514 --> 00:41:21.918
And we're gonna randomly initialize
it with np.random.random.

00:41:21.918 --> 00:41:24.640
We're gonna start with
the normal distribution rather.

00:41:24.640 --> 00:41:26.860
Let's do that, yeah.

00:41:26.860 --> 00:41:28.389
And we're gonna call it w.

00:41:31.349 --> 00:41:36.226
Now we're gonna actually build
the graph now that we have defined

00:41:36.226 --> 00:41:38.808
our variables and placeholders.

00:41:38.808 --> 00:41:43.848
We're gonna define y_pred
which is just prediction and

00:41:43.848 --> 00:41:47.632
it's gonna be given by tf.mul(w, x).

00:41:47.632 --> 00:41:49.078
So far so clear, any questions?

00:41:49.078 --> 00:41:49.830
Yes.

00:41:54.061 --> 00:41:57.665
Yeah, so as I mentioned earlier, none in
this case is so that you can dynamically

00:41:57.665 --> 00:42:00.710
change the number of batches that
you can send to your network.

00:42:00.710 --> 00:42:04.395
So imagine like if I'm doing
hyper parameter tuning,

00:42:04.395 --> 00:42:09.213
I don't want to go and change shape
to be 10 or 32 or 256 later on.

00:42:09.213 --> 00:42:12.618
Almost, you can imagine that you're
dynamically saying that okay,

00:42:12.618 --> 00:42:16.268
I'm gonna change the number of batches
that I'm gonna send to my network.

00:42:16.268 --> 00:42:18.297
Does that answer your question?

00:42:18.297 --> 00:42:19.177
Yes.

00:42:23.394 --> 00:42:26.694
So as we mentioned,
it'll just go into the variable scope and

00:42:26.694 --> 00:42:29.238
then define the name as it pleases,
so, yeah.

00:42:31.529 --> 00:42:33.230
All right, so let's go ahead.

00:42:33.230 --> 00:42:35.007
So now that we have our prediction,

00:42:35.007 --> 00:42:38.000
the next logical thing is to
actually compute the loss.

00:42:38.000 --> 00:42:41.700
So this we are gonna do by just,
there are two norm.

00:42:42.790 --> 00:42:47.410
So let's just first get the norm itself.

00:42:47.410 --> 00:42:53.332
And that's gonna be given by square,
and we just do, (y_pred- y).

00:42:53.332 --> 00:42:59.314
And since we wanted to be of a particular
shape, its gonna be over reduce some.

00:43:02.577 --> 00:43:05.154
Let's reduce_mean rather, all right.

00:43:05.154 --> 00:43:11.730
Okay, so now, with this we have
finished building our graph.

00:43:11.730 --> 00:43:14.640
And so now we'll return x, y, y_pred, and

00:43:14.640 --> 00:43:18.860
we'll return the loss as well
from our linear regression model.

00:43:20.240 --> 00:43:25.045
Now we're gonna actually start
computing what's in the graph.

00:43:25.045 --> 00:43:28.739
And we first start by
generating our dataset.

00:43:28.739 --> 00:43:30.747
And I'm gonna fill in code here,

00:43:30.747 --> 00:43:33.918
which we'll define
the training procedure for it.

00:43:33.918 --> 00:43:35.864
All right, so
let's get started on that part.

00:43:37.883 --> 00:43:41.110
So first we get what we call the model.

00:43:41.110 --> 00:43:47.183
We make an instance of it, and
that's just gonna be given by this.

00:43:47.183 --> 00:43:50.569
All right, so once we have that,
we are gonna create our optimizer.

00:43:50.569 --> 00:43:55.551
And this is gonna be given by,
as Barack mentioned

00:43:55.551 --> 00:44:00.071
earlier in his slides, DescentOptimizer.

00:44:00.071 --> 00:44:02.539
And we are gonna define
the learning rate to be 0.1.

00:44:02.539 --> 00:44:06.000
And we are gonna minimize over the loss
that we just got from our model.

00:44:06.000 --> 00:44:09.140
All right, any questions so far?

00:44:09.140 --> 00:44:10.430
We just created our optimizer.

00:44:11.740 --> 00:44:13.803
Okay, now we are gonna start a session.

00:44:13.803 --> 00:44:20.566
So with tf.Session, As session.

00:44:20.566 --> 00:44:24.780
And we are first gonna initialize,
yeah, that's one thing I forgot.

00:44:26.250 --> 00:44:30.890
We are gonna first initialize our
variables as someone earlier asked.

00:44:30.890 --> 00:44:33.890
Why would we do that?

00:44:33.890 --> 00:44:35.214
So this is actually a new function.

00:44:35.214 --> 00:44:37.084
So it's likely different
from what Barrack mentioned.

00:44:37.084 --> 00:44:39.765
And this sort of explains our tens
of our base really quickly, and

00:44:39.765 --> 00:44:42.330
since the time Barrack made
the slide then I need the code.

00:44:42.330 --> 00:44:45.810
It's already been updated so
we're going to change that.

00:44:45.810 --> 00:44:49.160
[Albeit so this is just
initializing the variables here.

00:44:51.000 --> 00:44:52.920
Initializer, all right.

00:44:52.920 --> 00:44:54.050
So we created a session.

00:44:54.050 --> 00:44:54.708
And now,

00:44:54.708 --> 00:45:00.145
we are gonna run the init function which
is just initialization of variables.

00:45:00.145 --> 00:45:02.136
Now, we are gonna create our feed_dict.

00:45:04.229 --> 00:45:07.159
And so what we are gonna feed in
is essentially just x_batch and

00:45:07.159 --> 00:45:10.106
y_batch which you got from our
regenerate dataset function.

00:45:12.775 --> 00:45:15.675
And y here would be y_batch.

00:45:15.675 --> 00:45:20.144
All right, now we're gonna actually just
loop over our data set multiple times,

00:45:20.144 --> 00:45:22.328
because it's a pretty small dataset.

00:45:22.328 --> 00:45:25.680
 30s is just our arbitrary chosen here.

00:45:25.680 --> 00:45:30.630
We are gonna get our loss value and
optimize it.

00:45:30.630 --> 00:45:31.850
I'll explain the step in a second.

00:45:33.600 --> 00:45:39.430
So now we're gonna call run and
what we want to fetch is the loss and

00:45:39.430 --> 00:45:43.010
the optimizer and
we are gonna feed in our feed dict.

00:45:44.980 --> 00:45:46.948
Does anyone have any
questions on this line?

00:45:49.688 --> 00:45:52.981
All right, and we are just gonna print for
the loss here.

00:45:58.985 --> 00:46:01.307
And then since this is an array,

00:46:01.307 --> 00:46:06.651
we are just going to want the mean
because we have almost 101 examples.

00:46:06.651 --> 00:46:10.568
All right, so now that we're done
with that, we can actually go and

00:46:10.568 --> 00:46:15.500
train our model, but we'd also like to
see how it actually ends up performing.

00:46:15.500 --> 00:46:20.520
So, what we are gonna do is we are gonna
actually see what it predicts, and

00:46:20.520 --> 00:46:25.340
how we get that is, again,
calling the session.run on y_pred.

00:46:25.340 --> 00:46:27.010
So we are gonna fetch y_pred.

00:46:27.010 --> 00:46:31.218
And our feed dictionary
here will be just this.

00:46:36.052 --> 00:46:37.182
All right.
Yes.

00:46:42.676 --> 00:46:46.677
So the optimizer was defined as
a GradientDescentOptimizer here.

00:46:47.800 --> 00:46:50.090
So you can see we are not
returning anything for

00:46:50.090 --> 00:46:52.740
that, which is why I just
ended up with a blank there.

00:46:52.740 --> 00:46:54.940
It's just a syntax here.

00:46:54.940 --> 00:46:57.890
So over here you see I'm
returning nothing over there.

00:46:57.890 --> 00:47:02.711
Yeah, all right, so we can actually go and
start running our code,

00:47:02.711 --> 00:47:05.005
and see how it performs, okay?

00:47:16.456 --> 00:47:17.484
All right.

00:47:20.764 --> 00:47:26.436
So let's actually go and
run our [INAUDIBLE].

00:47:26.436 --> 00:47:27.728
Let's see how that performs.

00:47:30.977 --> 00:47:33.964
So you see the loss decrease, and

00:47:33.964 --> 00:47:38.347
we can actually go ahead and
see how it turns out.

00:47:38.347 --> 00:47:41.644
Okay, I guess it didn't like my tmux.

00:47:41.644 --> 00:47:49.232
Anyways, So

00:47:49.232 --> 00:47:53.139
you see we fed a linear
line over the data.

00:47:53.139 --> 00:47:55.219
All right, so far so good.

00:47:55.219 --> 00:47:57.183
All right, so
now we are actually gonna go and

00:47:57.183 --> 00:48:00.610
implement word2vec using Skip-gram which
is slightly gonna be more complex.

00:48:00.610 --> 00:48:04.460
This was just to see how we create
very simple models in TensorFlow.

00:48:05.570 --> 00:48:06.795
All right, let's go ahead.

00:48:12.509 --> 00:48:14.047
So now.

00:48:16.510 --> 00:48:17.830
Any questions so far?

00:48:17.830 --> 00:48:18.490
Yes?

00:48:29.772 --> 00:48:31.320
All right.
So, in this.

00:48:31.320 --> 00:48:34.705
This is refine our understanding
of word2vec again.

00:48:34.705 --> 00:48:36.005
If you have the following sentence.

00:48:36.005 --> 00:48:37.685
A completely unbiased statement here.

00:48:37.685 --> 00:48:40.635
The first cs224 homework was a lot of fun.

00:48:40.635 --> 00:48:44.358
And if you were suppose to make
a dataset out of this sentence here.

00:48:44.358 --> 00:48:45.136
We would have.

00:48:45.136 --> 00:48:47.291
Consider a window size one here.

00:48:47.291 --> 00:48:51.505
And we are going have the cs221 For
a 224 end [INAUDIBLE] first.

00:48:51.505 --> 00:48:56.246
And so we are just basically decomposing
other sentence into a data set.

00:48:56.246 --> 00:49:01.639
Remember that Skipgram tries to predict
each context word given this target word.

00:49:01.639 --> 00:49:05.261
And since the number of context
words here is just two,

00:49:05.261 --> 00:49:07.401
because our window size is one.

00:49:07.401 --> 00:49:11.056
And so the task now becomes
to predict D from cs224n.

00:49:11.056 --> 00:49:12.270
From first, a lot.

00:49:12.270 --> 00:49:14.640
And fun from off and so on.

00:49:14.640 --> 00:49:16.020
And so this is our data set.

00:49:16.020 --> 00:49:19.670
So just clarifying what
word2vec actually was.

00:49:20.690 --> 00:49:21.420
Alright.
So let's go ahead and

00:49:21.420 --> 00:49:22.290
start implementing that.

00:49:22.290 --> 00:49:24.530
I've already made the data
processing functions here.

00:49:24.530 --> 00:49:25.750
So we won't have to deal with that.

00:49:25.750 --> 00:49:27.190
We have our batches.

00:49:27.190 --> 00:49:31.450
And this function load data already
loads the pre-process training and

00:49:31.450 --> 00:49:32.130
validation set.

00:49:33.160 --> 00:49:37.320
Training data is a list of batch
inputs and their label pairs.

00:49:37.320 --> 00:49:39.370
And we have about 30,000 of those.

00:49:39.370 --> 00:49:41.400
And we are going to see
a train as well here.

00:49:41.400 --> 00:49:44.610
The valuation data is just
a list of all validation inputs.

00:49:44.610 --> 00:49:47.890
And the reverse dictionary is a Python
dictionary from word index to word.

00:49:49.220 --> 00:49:50.230
Right?

00:49:50.230 --> 00:49:52.929
So let's start and go ahead and
implement Skipgram first.

00:49:56.087 --> 00:49:58.196
All right.

00:49:58.196 --> 00:50:02.154
So we are, again, going to start
by defining our placeholders.

00:50:02.154 --> 00:50:04.585
And so this is going to be batch inputs.

00:50:04.585 --> 00:50:07.450
And we are going to define
a placeholder here.

00:50:07.450 --> 00:50:10.886
But in this case,
since you just have integers.

00:50:10.886 --> 00:50:12.810
We can define with int32.

00:50:12.810 --> 00:50:16.880
And the shape is going to
be batch_size and nothing.

00:50:18.960 --> 00:50:21.000
So we have that.

00:50:21.000 --> 00:50:22.676
And we can avoid naming here.

00:50:22.676 --> 00:50:25.984
Because we are not going to
call multiple variable scopes.

00:50:25.984 --> 00:50:27.090
That will be fine.

00:50:27.090 --> 00:50:28.730
Then we go and create our batch labels.

00:50:28.730 --> 00:50:31.375
Which is, again, tf.placeholder.

00:50:33.503 --> 00:50:34.586
And 32.

00:50:34.586 --> 00:50:38.547
This will also be of
the same shape as previous.

00:50:45.054 --> 00:50:48.019
And finally, we will go and
create a constant for our validation set.

00:50:48.019 --> 00:50:50.100
Because that is not going
to change anytime soon.

00:50:50.100 --> 00:50:54.168
And that is going to be defined by

00:50:54.168 --> 00:50:59.450
a val_data which we previously loaded.

00:50:59.450 --> 00:51:03.790
And we have to define what type it is.

00:51:03.790 --> 00:51:06.623
And the type for that is int32 again.

00:51:06.623 --> 00:51:08.524
Just like our training set.

00:51:08.524 --> 00:51:09.211
All right.

00:51:09.211 --> 00:51:11.631
Now that we have defined, yes?

00:51:16.171 --> 00:51:18.190
So since I'll be applying
transposes later.

00:51:18.190 --> 00:51:20.460
I just wanted to make sure that it's one.

00:51:20.460 --> 00:51:22.714
It doesn't really make
that big of a difference.

00:51:24.610 --> 00:51:28.580
So in this case,
I'll be calling transpose on labels.

00:51:28.580 --> 00:51:31.906
Which is why I just wanted to make
sure that it transposes fine.

00:51:45.254 --> 00:51:46.520
You wouldn't.
It's just,

00:51:46.520 --> 00:51:49.270
I wanna make it absolutely
clear that it's a row vector.

00:51:49.270 --> 00:51:49.978
Not a column vector.

00:51:49.978 --> 00:51:52.610
&gt;&gt; [Question]
&gt;&gt; Yeah, exactly.

00:51:52.610 --> 00:51:54.380
All right.

00:51:54.380 --> 00:51:57.780
So now we can go and
start creating our scope for.

00:51:59.227 --> 00:52:00.843
All right.

00:52:07.055 --> 00:52:09.909
So, this is where we'll define our model.

00:52:09.909 --> 00:52:13.703
And first, we are going to go and
create an embeddings,

00:52:13.703 --> 00:52:15.902
as we all did in our assignment.

00:52:15.902 --> 00:52:18.558
And that's going to be a huge variable.

00:52:18.558 --> 00:52:23.114
And it's going to be initialized
randomly with uniform distribution.

00:52:23.114 --> 00:52:25.761
And this is going to kick vocabulary size,

00:52:25.761 --> 00:52:28.636
which you have previously
defined in the top.

00:52:28.636 --> 00:52:31.020
And it's going to take embedding size.

00:52:34.430 --> 00:52:37.682
So this is going to be a number of words
in your dictionary times the size of your

00:52:37.682 --> 00:52:38.444
embedding size.

00:52:38.444 --> 00:52:39.550
And we are going to define.

00:52:40.560 --> 00:52:43.588
Since it's a randomly
uniform distribution.

00:52:43.588 --> 00:52:47.870
We just going to also give
the parameters for that.

00:52:47.870 --> 00:52:49.160
So far so good?

00:52:49.160 --> 00:52:51.810
All right, so
we just created our embeddings.

00:52:51.810 --> 00:52:53.872
Now, since we want to
index with our batch.

00:52:53.872 --> 00:52:56.620
We are going to create batch embeddings.

00:52:56.620 --> 00:53:00.347
And you are going to use this function.

00:53:00.347 --> 00:53:06.669
Which is actually going to be pretty
handy for our current assignment.

00:53:06.669 --> 00:53:10.929
And so we do an embedding
lookup with the embeddings.

00:53:10.929 --> 00:53:14.833
And we are going to put
in the batch inputs here.

00:53:14.833 --> 00:53:15.630
All right.

00:53:15.630 --> 00:53:19.375
Finally, we go and create our weights.

00:53:19.375 --> 00:53:23.237
And we are going to call it,
here, .variable, here.

00:53:23.237 --> 00:53:28.115
So we are going to use truncated
normal distribution here.

00:53:28.115 --> 00:53:30.630
Which is just normal distribution
where it's cut off at

00:53:30.630 --> 00:53:33.150
two standard deviations instead
of going up the internet.

00:53:34.570 --> 00:53:35.592
Okay.
All right.

00:53:35.592 --> 00:53:37.212
This is also going to be of
the same size as previously.

00:53:37.212 --> 00:53:41.401
But this is going to be vocabulary
size and embedding size.

00:53:47.433 --> 00:53:51.500
And this is because I turn
tracks with our input directly.

00:53:51.500 --> 00:53:55.370
Since this is truncated normal, we need
to define what the standard deviation is.

00:53:55.370 --> 00:54:02.172
And this is going be given by one over the
square root of the embedding size, itself.

00:54:02.172 --> 00:54:07.570
[BLANK AUDIO] Okay.

00:54:07.570 --> 00:54:13.780
Finally we go and create our biases,
Which are also going to be variables.

00:54:13.780 --> 00:54:18.659
And this is going to be initialized
with zeros of size vocabularies.

00:54:22.429 --> 00:54:24.360
All right.

00:54:24.360 --> 00:54:28.050
Now we define our loss function,
now that we have all our variables.

00:54:33.220 --> 00:54:37.668
So in this case, we used a soft max cross
entropy in our assignment are the negative

00:54:37.668 --> 00:54:38.645
log likelihood.

00:54:38.645 --> 00:54:40.645
In this case,
you'd be using something similar.

00:54:40.645 --> 00:54:42.505
And this is where
Tensorflow really shines.

00:54:42.505 --> 00:54:44.660
It has a lot of loss functions built in.

00:54:44.660 --> 00:54:49.100
And say we are going to use this called
negative constraint, negative concentrate.

00:54:49.100 --> 00:54:50.785
I forgot the exact name.

00:54:50.785 --> 00:54:55.156
But it is very similar, in the sense
that the words that need to come up with

00:54:55.156 --> 00:54:57.496
a higher probability are emphasized.

00:54:57.496 --> 00:54:59.854
And the words which should not appear with
lower probability are not emphasized.

00:55:12.096 --> 00:55:14.410
And so we are going to call tf.nn.

00:55:14.410 --> 00:55:17.700
Nn is the neural network library
in TensorFlow, our module.

00:55:17.700 --> 00:55:19.860
And this is going to take
a couple of parameters.

00:55:19.860 --> 00:55:21.020
You can look up the API.

00:55:21.020 --> 00:55:21.712
Yes?

00:55:24.263 --> 00:55:24.972
Embeddings?

00:55:24.972 --> 00:55:26.699
All right.
What vector presentation.

00:55:26.699 --> 00:55:27.477
Which is what you're trying to learn.

00:55:29.380 --> 00:55:32.914
No, w is the weight matrix that is
a parameter that you're trying to

00:55:32.914 --> 00:55:34.080
also learn.

00:55:34.080 --> 00:55:37.497
But it's interacting
with other presentations.

00:55:37.497 --> 00:55:41.787
Effectively, you can think of these
embeddings as sort of semantic

00:55:41.787 --> 00:55:44.509
representations of those words, right?

00:55:44.509 --> 00:55:45.638
Yes?

00:55:48.125 --> 00:55:49.478
Right.

00:55:49.478 --> 00:55:50.148
So, imagine.
So,

00:55:50.148 --> 00:55:53.460
our embeddings is defined
as the vocabulary size.

00:55:53.460 --> 00:55:55.730
So let's say we have 10,000
words in our dictionary.

00:55:55.730 --> 00:55:59.620
And each row is now the word
vector that goes with that word.

00:55:59.620 --> 00:56:00.640
Index of that word.

00:56:00.640 --> 00:56:03.030
And since our batch is only
a subset of the vocabulary,

00:56:03.030 --> 00:56:05.360
we need to index into that EH matrix.

00:56:05.360 --> 00:56:11.770
With our batch, which is why we used
the embedding lookup function, okay.

00:56:11.770 --> 00:56:14.327
All right, so we're gonna go and just use,

00:56:14.327 --> 00:56:19.250
this API obviously everyone would need to
look up on the TensorFlow website itself.

00:56:22.450 --> 00:56:26.174
But what this would do is now take
the weights and the biases and

00:56:26.174 --> 00:56:27.400
the labels as well.

00:56:29.965 --> 00:56:32.110
Okay.

00:56:32.110 --> 00:56:33.700
I defined them as batch_labels.

00:56:39.162 --> 00:56:42.900
And they also take an input,
which is batch_inputs.

00:56:42.900 --> 00:56:45.117
Okay.

00:56:45.117 --> 00:56:50.513
And so here's where TensorFlow really
shines again, the num_sampled.

00:56:50.513 --> 00:56:53.438
So in our data set,
we only have positive samples, or

00:56:53.438 --> 00:56:57.170
in the sense that we had the context
words and the target word.

00:56:57.170 --> 00:57:00.670
We also need context words and
noisy words.

00:57:00.670 --> 00:57:03.295
And this is where num_samples
will come in use.

00:57:03.295 --> 00:57:05.765
We have defined num_samples
to be 64 earlier.

00:57:05.765 --> 00:57:09.345
And what it would essentially do is
look up 64 words which are not in our

00:57:09.345 --> 00:57:11.305
training set and which are noise words.

00:57:11.305 --> 00:57:13.495
And this would serve as sort
of negative examples so

00:57:13.495 --> 00:57:17.985
that our network learns which words are
actually context words and which are not.

00:57:21.759 --> 00:57:25.240
And finally, our num_classes is
defined by our vocabulary size again.

00:57:29.149 --> 00:57:29.848
All right.

00:57:29.848 --> 00:57:31.334
With that,
we have defined our loss function.

00:57:31.334 --> 00:57:34.575
And now we have to take the mean of that,

00:57:34.575 --> 00:57:39.630
because loss needs to be
the size of the batch itself.

00:57:39.630 --> 00:57:42.044
And we get that by reduced mean.

00:57:42.044 --> 00:57:47.019
This is gonna be

00:57:47.019 --> 00:57:52.763
slightly nasty.

00:58:06.220 --> 00:58:10.235
So we get, the loss is given for
that particular batch.

00:58:10.235 --> 00:58:11.715
And yes, exactly.

00:58:11.715 --> 00:58:13.375
It's given for multiple samples.

00:58:13.375 --> 00:58:14.955
And since we have multiple
samples in a batch,

00:58:14.955 --> 00:58:16.810
we want to take the average of those.

00:58:16.810 --> 00:58:17.510
Exactly.

00:58:17.510 --> 00:58:19.800
Okay.
And, so great.

00:58:19.800 --> 00:58:22.150
And now we have completely
defined our loss function.

00:58:23.470 --> 00:58:26.740
Now we can go ahead and actually,
if you remember from the assignment,

00:58:26.740 --> 00:58:29.420
we take the norm of these word vectors.

00:58:29.420 --> 00:58:31.954
So let's go ahead and do that first.

00:58:31.954 --> 00:58:37.271
So that will be reduce_mean, this is just

00:58:37.271 --> 00:58:42.292
API calling, which is very valuable and

00:58:42.292 --> 00:58:47.913
detailed on the TensorFlow website itself.

00:58:47.913 --> 00:58:48.627
All right.

00:58:50.721 --> 00:58:53.288
Keep_dims=True.

00:58:53.288 --> 00:58:55.950
So this is where, in this, I have added
an argument called keep dimensions.

00:58:55.950 --> 00:58:59.790
And this is where, if you sum over a
dimension, you don't it to disappear, but

00:58:59.790 --> 00:59:01.240
just leave it as 1.

00:59:01.240 --> 00:59:03.918
Okay.

00:59:03.918 --> 00:59:10.308
And now we divide the embeddings with the
norm, to get the normalized_embeddings.

00:59:10.308 --> 00:59:15.431
embeddings/norm.

00:59:15.431 --> 00:59:17.821
Great.

00:59:17.821 --> 00:59:22.290
And now we return from,
we get batch inputs,

00:59:22.290 --> 00:59:27.592
we return batch labels because
this will be our feed.

00:59:27.592 --> 00:59:30.592
We have normalized embeddings.

00:59:30.592 --> 00:59:32.300
And we have loss.

00:59:32.300 --> 00:59:34.382
All right.

00:59:34.382 --> 00:59:38.279
With this done,
we can come back to this function later.

00:59:38.279 --> 00:59:41.386
There's a slide part missing,
which we'll get back to.

00:59:41.386 --> 00:59:42.098
Yes?

00:59:48.518 --> 00:59:51.115
Thank you.

00:59:51.115 --> 00:59:51.631
All right.

00:59:54.873 --> 00:59:57.220
So now we go and define our run function.

00:59:57.220 --> 00:59:58.159
How are we doing on time?

00:59:58.159 --> 00:59:59.398
Okay, we have 20 minutes, great.

00:59:59.398 --> 01:00:06.140
We actually make a object of our model.

01:00:07.510 --> 01:00:08.620
And that's just by calling.

01:00:14.015 --> 01:00:15.174
Embeddings.

01:00:15.174 --> 01:00:21.139
And loss from our function, which was
just called word2, or skipgram, rather.

01:00:25.270 --> 01:00:26.444
Okay, and now we initialize the session.

01:00:31.320 --> 01:00:36.970
And over here, again,
I forgot to initialize our variables.

01:00:36.970 --> 01:00:39.227
We can call.

01:00:53.701 --> 01:00:57.532
We just initialized all of our
variables for the default values,

01:00:57.532 --> 01:00:59.290
as Barak mentioned again.

01:00:59.290 --> 01:01:00.897
Now we are gonna go and

01:01:00.897 --> 01:01:06.981
actually loop over our data to see if we
can actually go ahead and train our model.

01:01:06.981 --> 01:01:10.567
And so let's actually do that first step.

01:01:10.567 --> 01:01:12.313
And batch_data.

01:01:14.837 --> 01:01:15.857
Train_data.

01:01:15.857 --> 01:01:21.312
So for each iteration in this for
loop, we are gonna obtain a batch,

01:01:21.312 --> 01:01:25.910
which has its input data
as well as the labels.

01:01:25.910 --> 01:01:30.212
Okay, so we have inputs and

01:01:30.212 --> 01:01:33.970
labels from our batch.

01:01:33.970 --> 01:01:37.127
Great.
And we can now define our feed_dictionary

01:01:37.127 --> 01:01:39.880
accordingly where the batch_inputs.

01:01:43.050 --> 01:01:44.370
So this is a dictionary.

01:01:44.370 --> 01:01:46.680
And our batch_labels would just be labels.

01:01:50.296 --> 01:01:51.240
Any questions so far?

01:01:53.450 --> 01:01:54.450
Okay.

01:01:54.450 --> 01:01:57.060
We go ahead and
call our loss function again.

01:01:57.060 --> 01:02:01.311
And we do this by calling session.run,

01:02:01.311 --> 01:02:06.442
where we fetch the optimizer again and
the loss.

01:02:06.442 --> 01:02:12.439
And we pass in our feed dictionary,
which we already defined above.

01:02:12.439 --> 01:02:13.171
Okay.

01:02:13.171 --> 01:02:14.299
We'll get the loss.

01:02:14.299 --> 01:02:18.044
And since we are trying to get
the average, we're gonna add it first and

01:02:18.044 --> 01:02:20.930
then divide by the number of
examples that we just saw.

01:02:20.930 --> 01:02:23.277
All right.

01:02:25.267 --> 01:02:29.346
So we're just gonna put a couple of
print statements now just to make sure,

01:02:29.346 --> 01:02:31.825
to see if our model actually goes and
trains.

01:02:31.825 --> 01:02:36.133
And see.

01:02:45.293 --> 01:02:46.363
Print loss.

01:02:53.286 --> 01:02:58.377
Step [INAUDIBLE]
and

01:02:58.377 --> 01:03:03.790
then average loss.

01:03:03.790 --> 01:03:08.482
Since the loss will be

01:03:08.482 --> 01:03:13.695
zero in the first step,

01:03:13.695 --> 01:03:18.915
we can just [INAUDIBLE].

01:03:18.915 --> 01:03:20.281
All right, so
now we have our average loss.

01:03:20.281 --> 01:03:25.757
And we reset our average loss again,
just so

01:03:25.757 --> 01:03:30.941
that we don't for every iteration loop.

01:03:30.941 --> 01:03:31.441
Okay.
So

01:03:31.441 --> 01:03:33.177
we have almost finished
our implementation here.

01:03:33.177 --> 01:03:36.103
However, one thing that's, yes?

01:03:38.779 --> 01:03:39.889
I forgot to define that.

01:03:39.889 --> 01:03:40.468
Good call.

01:03:45.197 --> 01:03:52.027
So we can define that as
the beginning of a run step.

01:03:52.027 --> 01:03:53.243
Gradient.

01:03:59.668 --> 01:04:00.448
Optimizer.

01:04:00.448 --> 01:04:03.990
And we'll take a learning rate of zero,
and we're gonna minimize the loss.

01:04:05.596 --> 01:04:10.657
All right, thanks for that, okay.

01:04:10.657 --> 01:04:14.270
One thing that we're missing here is
we haven't really dealt with our value

01:04:14.270 --> 01:04:15.260
addition set.

01:04:15.260 --> 01:04:17.540
So, although we are training
in our training set,

01:04:17.540 --> 01:04:21.200
we would wanna make sure that it actually
generalizes to the value addition set.

01:04:21.200 --> 01:04:23.190
And that's the last part that's missing.

01:04:23.190 --> 01:04:24.040
And we just gonna do that now.

01:04:26.620 --> 01:04:29.366
But before we do that,
there's only one step missing.

01:04:29.366 --> 01:04:31.994
Where we, once we have the validation set,

01:04:31.994 --> 01:04:35.773
we still need to see how similar
our word vectors are with that.

01:04:35.773 --> 01:04:38.010
And we do that in our flow graph itself.

01:04:39.380 --> 01:04:42.600
So, let's go back to
our skip gram function.

01:04:44.840 --> 01:04:48.654
Anyway here we can

01:04:48.654 --> 01:04:54.026
implement that, okay.

01:04:54.026 --> 01:04:59.420
So, we have our val_embeddings against
index into the embeddings matrix

01:04:59.420 --> 01:05:02.239
to get the embeddings that
correspond to the validation words.

01:05:03.670 --> 01:05:10.970
And we use the embedding look up
function here, embedding_lookup,

01:05:10.970 --> 01:05:16.830
embedding and we call in train data set or
val data set.

01:05:16.830 --> 01:05:20.630
We'll actually use the
normalized_embedding because we are very

01:05:20.630 --> 01:05:22.610
concerned about the cosine similarity and

01:05:22.610 --> 01:05:28.344
not necessarily about
the magnitude of the similarity.

01:05:28.344 --> 01:05:36.130
Okay, and we use val_dataset here.

01:05:36.130 --> 01:05:39.870
Okay, and the similarity is
essentially just a cosine similarity.

01:05:39.870 --> 01:05:43.500
So, how this works is we matrix multiply

01:05:44.600 --> 01:05:47.708
the val_embeddings which
we just obtained and

01:05:47.708 --> 01:05:52.580
the normalized_embeddings.

01:05:52.580 --> 01:05:56.720
And since they won't work, you can
just may just multiply both of them

01:05:56.720 --> 01:05:59.920
because of dimensional incompatibility,
we'll have to transpose_b.

01:05:59.920 --> 01:06:03.431
And this is just another flag.

01:06:03.431 --> 01:06:08.760
All right, since we also
returned this from our function,

01:06:08.760 --> 01:06:12.270
again this is just a part of the graph.

01:06:12.270 --> 01:06:14.930
And we need to actually execute the graph,
in order to obtain values from it.

01:06:17.010 --> 01:06:18.269
And here we have similarity.

01:06:20.328 --> 01:06:24.186
Okay, and let's do,
since this is a pretty expensive process,

01:06:24.186 --> 01:06:29.041
computationally expensive,
let's do this only every 5,000 iterations.

01:06:32.811 --> 01:06:33.515
All right.

01:06:38.127 --> 01:06:42.928
So, the way you're gonna do this is by
calling eval on the similarity matrix,

01:06:42.928 --> 01:06:47.543
what this does is, since we had this
noted it actually goes and evaluates.

01:06:47.543 --> 01:06:51.780
This is equal on to calling session.run
on similarity and fetching that, okay.

01:06:51.780 --> 01:06:58.048
So, we go on call, and we get similarity,
and for every word in our validation set,

01:07:05.636 --> 01:07:08.940
You gonna find the top_k
words that are closest to it.

01:07:10.070 --> 01:07:11.943
And we can define k to be 8 here.

01:07:15.118 --> 01:07:17.820
And we will now get the nearest words.

01:07:20.390 --> 01:07:21.162
So let's do that.

01:07:23.009 --> 01:07:27.670
And we'll sort it according
to their magnitude.

01:07:29.180 --> 01:07:32.660
And since the first word that will
be closest will be the word itself,

01:07:32.660 --> 01:07:38.130
we'll want the other eight words and
not the first eight words and

01:07:38.130 --> 01:07:44.150
this will be given by top_k+1,
any questions so far?

01:07:47.180 --> 01:07:47.864
Yes?

01:07:54.830 --> 01:07:59.708
Right, so your embedding is on number
of words you have in your vocabulary,

01:07:59.708 --> 01:08:03.080
times the size of your word embedding for
each word.

01:08:04.280 --> 01:08:09.057
So, it's a huge matrix, and since your
batch that you're currently working with

01:08:09.057 --> 01:08:13.357
is only subset of that vocabulary,
this function, embedding to lookup,

01:08:13.357 --> 01:08:17.069
actually indexes into that matrix for
you and obtains the word.

01:08:17.069 --> 01:08:20.719
This is the equivalent to some complicated
Python splicing that you do with matrices,

01:08:20.719 --> 01:08:22.624
but it's just good syntax
should go over it.

01:08:25.716 --> 01:08:30.918
Okay, all right, almost there, okay.

01:08:35.065 --> 01:08:39.992
So, we have our nearest key words, we'll
just go and, I have this function in my

01:08:39.992 --> 01:08:44.928
utils, you can check this on the GitHub
that we'll post after the class is over.

01:08:44.928 --> 01:08:51.441
And you can play with it as you wish,
and In the past a nearest,

01:08:51.441 --> 01:08:56.332
and a reverse_dictrionary gesture actually

01:08:56.332 --> 01:09:01.363
see the words and
not just numbers, all right.

01:09:01.363 --> 01:09:06.260
Finally, be open our
final_embeddings which will be

01:09:06.260 --> 01:09:10.836
a normalized_embedding at
the end of the train and

01:09:10.836 --> 01:09:15.946
we're just going to call eval
on that again which is equal

01:09:15.946 --> 01:09:21.069
to calling session.run and
passing and fetching this.

01:09:21.069 --> 01:09:25.266
All right, we are done with the coding and
we can actually see and

01:09:25.266 --> 01:09:27.302
visualize how this performs.

01:09:29.109 --> 01:09:29.768
Okay.

01:09:32.405 --> 01:09:39.733
And python word2vec Oops,
I must have missed something.

01:09:45.753 --> 01:09:47.370
I missed a bracket again.

01:10:03.476 --> 01:10:06.777
So, we'll first load up our data set, and

01:10:06.777 --> 01:10:11.755
then it will iterate over it and
we will use our scripting model.

01:10:11.755 --> 01:10:14.277
Oops, let's see that.

01:10:19.739 --> 01:10:21.639
All right, where did this group?

01:10:31.237 --> 01:10:38.138
You know why,
please tell me why have to be there, okay.

01:10:38.138 --> 01:10:38.792
Perfect.

01:10:42.579 --> 01:10:47.858
So, as you can see here we have 30,000

01:10:47.858 --> 01:10:52.433
batches each with a bad set of 128.

01:10:52.433 --> 01:10:55.760
Ahh, man, [LAUGH] Let's see.

01:10:59.638 --> 01:11:04.026
All right, so as we see,
the loss started off as 259, all right,

01:11:04.026 --> 01:11:08.964
ends up at 145 and then decreases,
I think it goes somewhere to around 6.

01:11:10.160 --> 01:11:13.840
Here we can also see as a printing,
the nearest word for

01:11:13.840 --> 01:11:18.990
this is e leaders orbit, this gets better
with time and with more training data.

01:11:18.990 --> 01:11:23.075
We only use around 30,000 examples,

01:11:23.075 --> 01:11:25.820
so there's a lot of potential
to actually get better.

01:11:25.820 --> 01:11:31.447
And in the interest of time,
I'm only limited to around 30 epochs, yes.

01:12:01.254 --> 01:12:03.965
So TensorFlow comes with TensorBoard,

01:12:03.965 --> 01:12:07.350
which I didn't show in
the interest of time.

01:12:07.350 --> 01:12:10.650
Essentially, you can go up
to your local host, and

01:12:10.650 --> 01:12:13.580
then see the entire graph,
and how it's organized.

01:12:13.580 --> 01:12:17.147
And so, that'll actually be a huge
debugging help, and you can use that for

01:12:17.147 --> 01:12:18.171
your final project.

01:12:25.215 --> 01:12:28.140
Enter bold, yeah.

01:12:28.140 --> 01:12:29.745
All right, well, thank you for your time.

01:12:29.745 --> 01:12:33.020
&gt;&gt; [APPLAUSE]

