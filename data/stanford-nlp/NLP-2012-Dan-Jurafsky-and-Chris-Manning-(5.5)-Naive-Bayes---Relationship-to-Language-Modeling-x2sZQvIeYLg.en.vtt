WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:03.760
It turns out that Naive Bayes has a very
close relationship to language modeling.

00:00:04.810 --> 00:00:06.130
Let's see how that is.

00:00:06.130 --> 00:00:10.240
We'll start by looking at the generative
model for multinominal Naive Bayes.

00:00:10.240 --> 00:00:12.950
So imagine I have a class,
let's say it's China.

00:00:14.530 --> 00:00:19.570
And imagine that we were randomly
generating a document about China.

00:00:21.120 --> 00:00:24.610
So we might start by saying
the first word, X of 1 is Shanghai,

00:00:24.610 --> 00:00:29.110
and the second word is and, and
the third one is Shenzhen, and

00:00:29.110 --> 00:00:32.820
the fourth word is issue, and
the fifth word is bonds, and so on.

00:00:32.820 --> 00:00:36.700
We've generated a random
little document about China.

00:00:39.210 --> 00:00:45.610
So, what this generative model shows you
is that each word is an independently

00:00:45.610 --> 00:00:49.230
generated word from a class,
generated with a certain probability.

00:00:49.230 --> 00:00:51.510
We have a little set of probabilities
we're keeping for each word.

00:00:52.600 --> 00:00:53.777
Let's think about that.

00:00:56.658 --> 00:01:00.505
Now, in general, Naive Bayes classifiers
can use all sorts of features,

00:01:00.505 --> 00:01:04.007
URLs email addresses,
we'll talk about that for spam detection.

00:01:04.007 --> 00:01:08.590
But if in the previous slide, we just
used the word features, and if we use

00:01:08.590 --> 00:01:12.290
all the words in the text, then it turns
out that this generative model for

00:01:12.290 --> 00:01:16.370
Naive Bayes gives it an important
similarity to language modeling.

00:01:16.370 --> 00:01:19.100
Naive Bayes turns out to be
a kind of language model.

00:01:20.260 --> 00:01:25.562
And in particular,
each class in a Naive Bayes classifier,

00:01:25.562 --> 00:01:27.650
each class is a unigram language model.

00:01:29.860 --> 00:01:34.370
And the way we can think about that is
each word in a Naive Bayes classifier,

00:01:34.370 --> 00:01:40.520
the likelihood term, assigns a word the
probability of the word, given the class.

00:01:40.520 --> 00:01:43.150
And a sentence in
a Naive Bayes classifier,

00:01:43.150 --> 00:01:46.590
since we're multiplying together
a sentence or even a whole document,

00:01:46.590 --> 00:01:49.450
since we're multiplying together
the probabilities of all the words,

00:01:49.450 --> 00:01:51.960
we compute the probability
of a sentence given a class.

00:01:51.960 --> 00:01:54.930
We're just multiplying
together all the words,

00:01:54.930 --> 00:01:56.480
the likelihoods of all
the words in the class.

00:01:56.480 --> 00:01:58.300
So let's see how that works.

00:01:59.510 --> 00:02:02.200
Imagine that we have the class positive.

00:02:04.560 --> 00:02:10.070
And we have our likelihoods,
the likelihood of I given positive,

00:02:10.070 --> 00:02:14.160
that's P of I given positive, and
we have P of love given positive,

00:02:14.160 --> 00:02:17.883
and we have P of this given positive,
and so on.

00:02:17.883 --> 00:02:24.620
And P of I given positive is 0.1, and P
of love given positive is 0.1, and so on.

00:02:24.620 --> 00:02:27.110
Okay, so
here's our Naive Bayes classifier.

00:02:27.110 --> 00:02:29.590
Well, we can think of that
exactly as a language model.

00:02:29.590 --> 00:02:35.240
We have a sequence of words, I generate
some words, I love this fun film,

00:02:35.240 --> 00:02:39.810
and Naive Bayes is assigning that

00:02:39.810 --> 00:02:44.789
sequence of words a set of probabilities,
one for each word from the class.

00:02:44.789 --> 00:02:47.700
0.1, 0.1, 0.5 and so on.

00:02:49.370 --> 00:02:53.430
So that if we multiply all these
together we can get the probability

00:02:53.430 --> 00:02:54.208
of the sentence.

00:02:54.208 --> 00:02:55.344
So Naive Bayes,

00:02:55.344 --> 00:03:00.141
each class is just a unigram language
model conditioned on the class.

00:03:03.252 --> 00:03:06.546
So when we ask the question which
class assigns a higher probability

00:03:06.546 --> 00:03:08.030
to a document?

00:03:08.030 --> 00:03:10.980
It's like we're running two
separate language models.

00:03:10.980 --> 00:03:14.600
So here I've shown you two separate
language models, the positive class and

00:03:14.600 --> 00:03:15.780
the negative class.

00:03:15.780 --> 00:03:17.363
And each one has separate probabilities.

00:03:17.363 --> 00:03:22.380
So here's the probability
of I given negative.

00:03:22.380 --> 00:03:25.180
And here's probability
of I given positive.

00:03:25.180 --> 00:03:29.170
I guess people are more likely to use
the word I when they don't like something.

00:03:30.460 --> 00:03:36.876
And now if we take a particular sentence,
I love this fun film, and

00:03:36.876 --> 00:03:44.459
we say what's the probability of this
sequence according to our first model?

00:03:47.106 --> 00:03:49.800
And what's the probability
according to our second model?

00:03:49.800 --> 00:03:51.780
Each one assigns a probability for
each word.

00:03:51.780 --> 00:03:53.960
That's the Naive Bayes likelihoods.

00:03:53.960 --> 00:03:55.940
We can multiply them all together.

00:03:55.940 --> 00:03:59.870
And we can show that if we multiply them
all together, you can sort of see from

00:03:59.870 --> 00:04:04.270
inspection that the positive probability
multiplied together are going to be,

00:04:04.270 --> 00:04:05.410
I mean, because of this one and

00:04:05.410 --> 00:04:08.510
this one, are going to be much higher
than the negative probabilities.

00:04:08.510 --> 00:04:11.590
So you can think of Naive Bayes as

00:04:12.640 --> 00:04:17.730
each class is a separate class
condition language model.

00:04:17.730 --> 00:04:23.410
We're going to run each language model to
compute the likelihood of a test sentence,

00:04:23.410 --> 00:04:26.100
and we'll just pick
whichever language model

00:04:26.100 --> 00:04:29.670
has the higher probability as the class
which is the more likely class.

00:04:29.670 --> 00:04:34.860
So that's the close relationship between
Naive Bayes and language modeling.

