WEBVTT
Kind: captions
Language: en

00:00:00.810 --> 00:00:03.340
Now that we have a much better
understanding of the role of

00:00:03.340 --> 00:00:04.900
independence assumptions in PCFGs.

00:00:04.900 --> 00:00:09.240
In this segment I'm going to show
you how you can make a much,

00:00:09.240 --> 00:00:11.990
much better PCFG without
doing lexicalization.

00:00:13.470 --> 00:00:18.660
And so, this was worked those done in the
accurate unlexicalized parsing paper by me

00:00:18.660 --> 00:00:20.740
and Dan Klein in 1993.

00:00:20.740 --> 00:00:24.820
So first,
what do I mean by unlexicalized PCFG?

00:00:24.820 --> 00:00:29.270
What thatâ€™s going to mean is that in
general the grammar rules are not

00:00:29.270 --> 00:00:33.210
systematically specified down
to the level of lexical items.

00:00:33.210 --> 00:00:36.260
So we're not going to be able to
have lexicalized categories like in

00:00:36.260 --> 00:00:41.380
a lexicalized PCFG,
like NP stocks or VP routers.

00:00:42.400 --> 00:00:46.990
On the other hand, we are going
to be allowed to do things like

00:00:46.990 --> 00:00:48.931
parent down [INAUDIBLE] of categories or

00:00:48.931 --> 00:00:52.900
to note other things about the [INAUDIBLE]
of categories like saying.

00:00:52.900 --> 00:00:56.970
This is a noun phrase that is
a coordinator noun phrase.

00:00:56.970 --> 00:01:01.400
To drill down on that just
a teeny bit more, in particular

00:01:01.400 --> 00:01:05.975
we wish to make a distinction between
close versus open-class words.

00:01:05.975 --> 00:01:11.065
So this a long tradition in
linguistic syntax to make use of

00:01:11.065 --> 00:01:16.785
function words as features or
markers for categories and selection.

00:01:16.785 --> 00:01:23.172
So in English the Verbs have v and
do act as verbal auxiliaries.

00:01:23.172 --> 00:01:26.982
In things like I am running,
he has eaten, and so

00:01:26.982 --> 00:01:31.410
it's okay to recognize that those
function words have a special role.

00:01:31.410 --> 00:01:37.170
And to treat them specially or
if you have a compliment it's okay

00:01:37.170 --> 00:01:41.490
to know whether it's a that compliment
versus an if/whether compliment.

00:01:41.490 --> 00:01:46.840
Those kind of conditioning features
are allowed in state splits

00:01:46.840 --> 00:01:51.780
because they're fundamentally different
to this idea of marking the semantic

00:01:51.780 --> 00:01:56.620
heads of phrases for working out things
like prepositional phrase attachment.

00:01:56.620 --> 00:02:01.750
Where really you're using the lexical
heads as a kind of proxy for semantics.

00:02:03.300 --> 00:02:07.940
And so those thesis was, is that
a large percent of what you need for

00:02:07.940 --> 00:02:09.310
accurate parsing.

00:02:09.310 --> 00:02:14.090
And indeed much of what was actually
being captured by lexicalized PCFGs

00:02:14.090 --> 00:02:16.810
wasn't actually anything
to do with publistic

00:02:16.810 --> 00:02:18.980
dependencies being
captured by content words.

00:02:18.980 --> 00:02:24.500
But, it was actually really just these
basic grammatical features like Verb form,

00:02:24.500 --> 00:02:28.570
finiteness, presence of a verbal
auxiliary that were well known

00:02:28.570 --> 00:02:31.450
from people working out
traditional grammars.

00:02:31.450 --> 00:02:35.620
In particular people who'd worked on
feature based grammars for languages.

00:02:36.860 --> 00:02:41.180
So the way we went about investigating
this Is that, with the Penn Treebank,

00:02:41.180 --> 00:02:45.920
Wall Street Journal we chose a small
development set that we could

00:02:45.920 --> 00:02:47.620
do a whole bunch of experiments on.

00:02:47.620 --> 00:02:52.930
And that they thoroughly did it
on computers were much slower, so

00:02:52.930 --> 00:02:57.730
doing a 100 files of the Wall Street
Journal seemed like it take a long time.

00:02:57.730 --> 00:03:02.580
So we just used 20 files of the Wall
Street Journal development section and

00:03:02.580 --> 00:03:07.770
we've run on again and again and we'd
make manual state splits in the grammar.

00:03:07.770 --> 00:03:08.350
And try and

00:03:08.350 --> 00:03:13.410
improve performance by breaking down
wrong independence assumptions.

00:03:13.410 --> 00:03:17.660
And so, we have a couple of statistics.

00:03:17.660 --> 00:03:25.560
One is our performance level which will be
our usual F1 of label precision recall.

00:03:25.560 --> 00:03:30.780
And then, the other one is the size of our
grammar that as we make more state splits,

00:03:30.780 --> 00:03:34.810
the grammar will be bigger in terms
of its number of non-terminals.

00:03:34.810 --> 00:03:40.030
And if this number gets too big,
that's both dangerous for two reasons.

00:03:40.030 --> 00:03:42.650
It will both slow down the power sign.

00:03:42.650 --> 00:03:47.530
And we'll stop to get problems with
sparseness because we're not smoothing

00:03:47.530 --> 00:03:50.320
the rebites of our PCFG

00:03:50.320 --> 00:03:54.490
except down at the lexical level
where we're rewriting those words.

00:03:55.570 --> 00:03:59.400
And so, our goal will be to
state-split as sparingly as possible.

00:03:59.400 --> 00:04:03.880
So we just want to make a limited number
of state-splits that give the most bang

00:04:03.880 --> 00:04:09.410
for the buck in terms of capturing
necessary probabilistic dependencies.

00:04:09.410 --> 00:04:12.550
So let's just look at a few
examples of how we did that.

00:04:14.500 --> 00:04:17.760
Now motivation was in
part having looked at

00:04:17.760 --> 00:04:19.940
what had been done in lexicized pcfgs's.

00:04:19.940 --> 00:04:25.170
And it turns out that some of the things
that are being done in the lexicized

00:04:25.170 --> 00:04:26.450
pcfg models.

00:04:26.450 --> 00:04:30.920
The prominent ones of those days
were Eugene and Mike Collins'.

00:04:30.920 --> 00:04:35.970
Didn't actually have anything to do
with content word lexiconization at all.

00:04:35.970 --> 00:04:38.960
They had been doing other things.

00:04:38.960 --> 00:04:42.420
So one idea was to split up

00:04:42.420 --> 00:04:46.950
context free grammar rules which
had long right hand sides So,

00:04:46.950 --> 00:04:52.170
you only do limited conditioning on
the other things on the right-hand side.

00:04:52.170 --> 00:04:56.690
So, if you have a flat rule, of which
there are lots in the Penn Treebank,

00:04:56.690 --> 00:05:00.520
we've already discussed
needing to binarize those.

00:05:00.520 --> 00:05:05.430
And there's sort of a straightforward way
of binarization, where at each point.,

00:05:05.430 --> 00:05:07.480
you preserve your left context.

00:05:07.480 --> 00:05:11.040
And then you have the probabilities
of expanding further.

00:05:11.040 --> 00:05:15.660
So now, we're again preserving
our left context and

00:05:15.660 --> 00:05:18.060
saying what are the probabilities
of expanding further?

00:05:18.060 --> 00:05:24.370
But as you go on you're conditioning on
more and more stuff that might not make

00:05:24.370 --> 00:05:29.430
much of a difference because you're also
going to have places in the pen tree bank

00:05:29.430 --> 00:05:34.310
where a noun phrase expands as
five proper nouns in a row.

00:05:34.310 --> 00:05:38.482
If you have something like crater,

00:05:38.482 --> 00:05:43.665
Duluth, investment advancement
committee or Or something like that.

00:05:43.665 --> 00:05:47.385
You're going to get five
proper nouns in a row.

00:05:47.385 --> 00:05:51.615
And it seems like at some point
you just want to know that

00:05:51.615 --> 00:05:55.860
you're expanding a bunch o proper
nouns and how many there were before.

00:05:55.860 --> 00:05:58.140
Forehand doesn't matter.

00:05:58.140 --> 00:06:02.910
And so, what you can do is get
rid of some of the history

00:06:02.910 --> 00:06:07.170
in the same way as we do with language
models, where we markovize them and

00:06:07.170 --> 00:06:10.650
say, let's not use the entire
proceeding context.

00:06:10.650 --> 00:06:13.230
Let's just use a bit
of proceeding context.

00:06:13.230 --> 00:06:18.270
But our proceeding context
now Is in terms of these

00:06:18.270 --> 00:06:22.240
categories of what we've seen
previously on the right hand side.

00:06:22.240 --> 00:06:28.350
So if we only keep what we're expanding,
and the thing that we saw most recently

00:06:28.350 --> 00:06:33.680
on the right hand side, we get rid of
some of the prior conditioning context.

00:06:33.680 --> 00:06:37.390
And then,
these two states become the same.

00:06:37.390 --> 00:06:40.100
Up grandma will actually
gets smaller again.

00:06:40.100 --> 00:06:44.840
So, from this perspective, if you do naive

00:06:44.840 --> 00:06:51.020
finalization of those very flat rules,
you actually get a big grammar.

00:06:51.020 --> 00:06:56.010
And you pass at about the 73%
number that we've talked about for

00:06:56.010 --> 00:06:57.340
a plain vanilla PCFG.

00:06:57.340 --> 00:07:01.910
It turns out that if you condition on less

00:07:01.910 --> 00:07:06.580
context like only the two
preceding tags or

00:07:06.580 --> 00:07:11.060
only the one preceding thing
on the right hand side.

00:07:11.060 --> 00:07:17.150
Your grammar gets enormously smaller and
actually it's performance goes up.

00:07:17.150 --> 00:07:21.550
So, in particular if you
condition on just the two previous

00:07:21.550 --> 00:07:24.850
categories in your expansion
of the right hand side.

00:07:24.850 --> 00:07:27.220
Your performance goes up a little bit.

00:07:27.220 --> 00:07:29.760
But we had found that we could do
a little bit better than that.

00:07:29.760 --> 00:07:34.510
We could condition on sometimes 1
piece of preceding context, and

00:07:34.510 --> 00:07:40.630
sometimes 2 bits of preceding context,
depending on how often

00:07:40.630 --> 00:07:46.400
you've seen expansions for that category
as to how common this non terminal is.

00:07:47.440 --> 00:07:51.018
So that's Horizontal Markovization.

00:07:51.018 --> 00:07:56.978
The idea of using parent categories you
can think of as Veritcal Markovization.

00:07:56.978 --> 00:08:02.408
So before we were saying, okay,
we're expanding just a verb phrase and

00:08:02.408 --> 00:08:08.830
then we could say, well, let's also look
at the thing above that in the tree.

00:08:08.830 --> 00:08:11.930
And so
we're then marking that in our category.

00:08:11.930 --> 00:08:14.560
So this looking at the thing
above you in the tree

00:08:14.560 --> 00:08:17.600
is kind of looking into
a history going upwards.

00:08:17.600 --> 00:08:21.760
And you can think of having your
whole history going upwards and

00:08:21.760 --> 00:08:24.850
then progressively deleting
out some of that, and

00:08:24.850 --> 00:08:27.100
that's a form of Vertical Markovization.

00:08:28.260 --> 00:08:31.415
So from this perspective a standard PCFG,

00:08:31.415 --> 00:08:37.525
you're just expanding your own category
and you're looking at nothing above you.

00:08:37.525 --> 00:08:40.230
So your Vertical Markov Order 1,

00:08:40.230 --> 00:08:45.005
which is again at slightly
under 73% performance level.

00:08:45.005 --> 00:08:50.105
As Johnson noticed,
that by simply doing nothing else but

00:08:50.105 --> 00:08:54.980
also knowing your parent category,
that gives you a lot of value.

00:08:54.980 --> 00:08:59.448
So that's pushing up your parsing
numbers by about 4% here.

00:08:59.448 --> 00:09:04.430
It turns out that if you put
in grandparents as well,

00:09:04.430 --> 00:09:08.010
you can push the numbers up
even a little bit further.

00:09:08.010 --> 00:09:11.910
But the problem is that, as you do that,
you start having a bigger and

00:09:11.910 --> 00:09:14.890
bigger grammar with more nonterminals.

00:09:14.890 --> 00:09:19.870
So the model we used as a basis for
doing more linguistic state splits

00:09:19.870 --> 00:09:24.550
was actually again,
to take this in-between parent and

00:09:24.550 --> 00:09:29.240
nothing model, where you're using
the parent category most of the time but

00:09:29.240 --> 00:09:33.300
not always for
certain very rare non-terminals.

00:09:33.300 --> 00:09:35.445
So they're some of
the ones we saw mentioned.

00:09:35.445 --> 00:09:40.086
Occasionally there are things
like where the things like

00:09:40.086 --> 00:09:45.017
where you have special non-terminals for
fragments and for

00:09:45.017 --> 00:09:50.950
reduced relative clause and
things like that occur rarely.

00:09:50.950 --> 00:09:54.401
Okay.
So, once we took the vertical and

00:09:54.401 --> 00:09:59.810
horizontal Markovization at level 2V,
between 1 and

00:09:59.810 --> 00:10:06.022
2, we then get to a PCFG where
our accuracy is 77.8% F1 and

00:10:06.022 --> 00:10:13.140
the number of non-terminals we
have in the grammar is now 7.5K.

00:10:13.140 --> 00:10:17.282
And that'll be the basis for introducing
some furthermore linguistic state splits.

00:10:19.590 --> 00:10:27.163
Okay, well what other problems do the
independent assumptions of PCFGs cause?

00:10:27.163 --> 00:10:30.182
An easy way to find them
is to just run your PCFG,

00:10:30.182 --> 00:10:33.804
see where it makes pass errors
on your development set and

00:10:33.804 --> 00:10:38.960
then scratch your head and think,
why is it choosing the wrong role here?

00:10:38.960 --> 00:10:41.860
What information could I encode into

00:10:41.860 --> 00:10:45.610
the non-terminals that cause
it to stop doing that?

00:10:45.610 --> 00:10:47.620
And so, here's an example of this.

00:10:47.620 --> 00:10:52.780
So, you find in a basic PCFG
that unary roles are used

00:10:52.780 --> 00:10:58.140
too often because they make it easy
to change one category into another.

00:10:58.140 --> 00:11:01.120
So if you have a high category role here,

00:11:01.120 --> 00:11:05.530
like a verb phrase going to and
verb ending in i-n-g and

00:11:05.530 --> 00:11:10.480
a noun phase, it's going to want to use
it and so it finds that it can just

00:11:10.480 --> 00:11:15.110
change a sentence into a verb phrase
with fairly high probability.

00:11:15.110 --> 00:11:20.380
That's a high probability unary role and
so it will use it and do that.

00:11:20.380 --> 00:11:25.260
But there's a problem here which
is that this unary rewrite

00:11:25.260 --> 00:11:29.350
isn't appropriate for
this higher level rule.

00:11:29.350 --> 00:11:34.464
That this higher level rule is
expecting here to have a finite

00:11:34.464 --> 00:11:42.410
sentence with a subject, and so therefore,
it shouldn't then expand as a unary role.

00:11:42.410 --> 00:11:47.326
So the way we can capture
that is by saying, well,

00:11:47.326 --> 00:11:52.610
let's mark the unary rewrites
in the training data.

00:11:52.610 --> 00:11:57.113
Whenever there's a unary rule applied,
we'll just stick unary on

00:11:57.113 --> 00:12:02.255
the parent category so we know the kinds
of places that unary rewrites occur.

00:12:02.255 --> 00:12:07.446
And then at that point, it'll decide on
at parsing time, this isn't a place that

00:12:07.446 --> 00:12:12.755
unary rewrites occur and so it'll choose
a different structure for the sentence.

00:12:12.755 --> 00:12:17.460
It'll decide to choose this
structure where this is actually

00:12:17.460 --> 00:12:23.250
a modifier phrase, modifying the noun
phrase, and that's the right answer.

00:12:23.250 --> 00:12:25.860
And so just making this little change here

00:12:25.860 --> 00:12:31.060
will immediately already move our
parsing numbers up by a half a percent.

00:12:31.060 --> 00:12:32.150
So let's keep going.

00:12:34.120 --> 00:12:38.930
Another problem of the independence
assumptions is that in various places,

00:12:38.930 --> 00:12:42.200
the part of speech tags are too coarse.

00:12:42.200 --> 00:12:48.060
One of the worst examples of that is with
the tag IN in the pink treebank tags set.

00:12:48.060 --> 00:12:52.702
So that's used for
all three of the sentential

00:12:52.702 --> 00:12:57.231
complementizers of sentence complements.

00:12:57.231 --> 00:13:03.531
Words like that, whether, and if, or
subordinating conjunctions like while and

00:13:03.531 --> 00:13:08.485
after, and for true propositions,
words like in, of, and to.

00:13:08.485 --> 00:13:14.232
Well, that causes wrong parsers, so here's
an example of how it causes a wrong parse.

00:13:14.232 --> 00:13:21.401
So here we have this sentential
complement with advertising works,

00:13:21.401 --> 00:13:27.097
where we should have be
a sentence advertising works.

00:13:27.097 --> 00:13:32.899
But instead, what it's chosen to do
is just give a regular prepositional

00:13:32.899 --> 00:13:38.140
phrase analysis despite the fact
that the preposition here is if,

00:13:38.140 --> 00:13:41.690
which is not really a true preposition.

00:13:41.690 --> 00:13:44.220
It's one of these
sentential complementizers.

00:13:45.450 --> 00:13:51.530
Okay, well, the way to deal with this
is to split this IN tag here and

00:13:51.530 --> 00:13:54.290
have different kinds of words here.

00:13:54.290 --> 00:14:01.496
So if we say that if you have a noun
phrase under your preposition,

00:14:01.496 --> 00:14:06.475
then what we want is
the kind of IN that appears

00:14:06.475 --> 00:14:10.683
with a noun phrase as its complement.

00:14:10.683 --> 00:14:15.654
Well, then what's going to happen
is we're going to learn that if

00:14:15.654 --> 00:14:17.689
is not an example of that.

00:14:17.689 --> 00:14:22.626
If is instead an example of
the kind of IN that appears

00:14:22.626 --> 00:14:27.456
with the sentential
complement because it appears

00:14:27.456 --> 00:14:32.400
in this SBAR construction
that we have here.

00:14:32.400 --> 00:14:36.610
And so that means the parser is then
going to choose a different and

00:14:36.610 --> 00:14:38.470
correct construction here.

00:14:38.470 --> 00:14:43.260
It's going to say that this part
here is an SBAR with an s in it.

00:14:44.990 --> 00:14:49.338
Splitting the I intake in
this way is a big change and

00:14:49.338 --> 00:14:54.294
by itself it gives you an extra
2% in parsing accuracy but

00:14:54.294 --> 00:14:57.444
we can keep on and do more of the same.

00:14:57.444 --> 00:15:04.118
So let's just look at a couple
of other examples of that.

00:15:04.118 --> 00:15:10.740
So sometimes we also want to
refine phrasal categories.

00:15:10.740 --> 00:15:14.924
So if you have a verb phrase,
it makes a fair bit of

00:15:14.924 --> 00:15:19.530
a difference knowing what
kind of a verb phrase it is.

00:15:19.530 --> 00:15:24.894
Is it a finite verb or
is it a non-finite or infinitive verb?

00:15:24.894 --> 00:15:30.318
And one way that you can capture
that is by knowing what the part

00:15:30.318 --> 00:15:36.783
of speech tag of the verb is because this
here is a finite tag for the verb and

00:15:36.783 --> 00:15:41.812
this here is an infinitive
non-finite tag for the verb.

00:15:41.812 --> 00:15:50.332
And so we can represent that information
also on the verb phrase category.

00:15:50.332 --> 00:15:55.899
And if we do that we will
find that this structure

00:15:55.899 --> 00:16:00.908
is bad because although
the verb is can take

00:16:00.908 --> 00:16:06.335
VP complements, it won't take infinitive,

00:16:06.335 --> 00:16:11.800
vr infinitive VB compliments like panic.

00:16:11.800 --> 00:16:17.280
Instead, it's going to take
things like participial for

00:16:17.280 --> 00:16:21.680
the progressives, like she is running or
things like that.

00:16:21.680 --> 00:16:27.410
And so when we annotate that
extra information, the poss again

00:16:27.410 --> 00:16:32.150
know that this isn't a good structure,
and it will choose a different structure.

00:16:32.150 --> 00:16:37.000
And so here it's choosing to make
panic buying a noun phrase correctly.

00:16:37.000 --> 00:16:40.400
Okay, so that's the idea of
splitting verb-phrase categories.

00:16:40.400 --> 00:16:43.610
And we mentioned earlier why we
wanted to split off possessive

00:16:43.610 --> 00:16:45.402
noun phrase categories.

00:16:45.402 --> 00:16:49.110
Both of those are worth quite
a bit in pausing accuracy.

00:16:49.110 --> 00:16:53.100
So we gain almost a percent
from the possessives and

00:16:53.100 --> 00:16:56.620
we gain over 2% by knowing what kind
of verb phrase we're dealing with.

00:16:58.640 --> 00:17:04.400
There's one other category of splits that
we introduced and that is at somehow.

00:17:04.400 --> 00:17:09.710
We want to know something about attachment
sites for prepositional phrases.

00:17:09.710 --> 00:17:14.854
Even though we're not doing lexical
conditioning, we want to have some kind of

00:17:14.854 --> 00:17:20.018
an idea as to do prepositional phrases
tend to attach low or tend to attach high.

00:17:20.018 --> 00:17:24.510
And we have put in some features
that can capture that, so

00:17:24.510 --> 00:17:29.658
we mark whether any phrase contains
a verb, that tends to capture

00:17:29.658 --> 00:17:34.267
whether it's already something big and
sentence sized.

00:17:34.267 --> 00:17:35.421
And also mark for

00:17:35.421 --> 00:17:40.218
noun phrases whether they've already
had things attached to them.

00:17:40.218 --> 00:17:44.690
Or whether there's still a base noun phase
that hasn't had anything attached to them.

00:17:44.690 --> 00:17:50.360
And these ideas also push up
the performance numbers a little.

00:17:50.360 --> 00:17:54.230
Indeed on the development set,
we've now gone up to 87 F1.

00:17:54.230 --> 00:17:59.170
So at the end of the day,
what we've constructed here is

00:17:59.170 --> 00:18:04.240
that we transform the training
data with manual rules done

00:18:04.240 --> 00:18:10.560
as state splits to produce this
richer set of non-terminals here.

00:18:10.560 --> 00:18:14.290
A verb phrase, that's under an S category.

00:18:14.290 --> 00:18:20.580
It has a finite verb inert, and
it's a phrase that contains a verb.

00:18:20.580 --> 00:18:22.640
And so these are our categories.

00:18:22.640 --> 00:18:27.520
We then build a straightforward PCFG
with those categories and paths.

00:18:27.520 --> 00:18:28.370
How well do we do?

00:18:28.370 --> 00:18:33.100
Well, the answer is that
this unlexicalized PCFG can

00:18:33.100 --> 00:18:35.990
suddenly actually do rather well.

00:18:35.990 --> 00:18:37.020
So these are the results.

00:18:37.020 --> 00:18:42.040
So These were two of the earliest
lexicalized PCFG models and

00:18:42.040 --> 00:18:45.360
they got almost 85 and 86%.

00:18:45.360 --> 00:18:50.815
And here we have this hand
built on lexicalized PCFG and

00:18:50.815 --> 00:18:53.955
it's actually doing a little
bit better than those.

00:18:53.955 --> 00:18:58.915
It isn't doing quite as well as the model
of Charniak that I showed earlier or

00:18:58.915 --> 00:19:01.815
indeed Collins's slightly later model.

00:19:01.815 --> 00:19:07.293
These ones were getting you know
a little bit more, 87, 88%.

00:19:07.293 --> 00:19:12.279
But any rate,
that shows you that if we're considering,

00:19:12.279 --> 00:19:16.773
we started off at about 72.7 for
our plain PCFG.

00:19:16.773 --> 00:19:22.521
And then if we look at what
they've mileage to get up to here,

00:19:22.521 --> 00:19:27.030
it then looks like well,
no about 13 to 14%

00:19:27.030 --> 00:19:31.113
of the mileage is coming from these ideas.

00:19:31.113 --> 00:19:35.281
Like knowing more about context,
parent annotation,

00:19:35.281 --> 00:19:41.402
refining critical categories, knowing
that you have possessive down phrases,

00:19:41.402 --> 00:19:46.368
knowing about the type of phrases,
participial and infinitive,

00:19:46.368 --> 00:19:51.067
infinite verb phrases and
the amount of these at a models we're

00:19:51.067 --> 00:19:56.332
capturing from having true
lexicalization was really rather small.

00:19:56.332 --> 00:20:00.991
So at somewhere between 1% and 3%.

00:20:03.150 --> 00:20:09.046
And so that changed the orientation
somewhat as to better understanding

00:20:09.046 --> 00:20:14.758
what is and isn't important in being
able to choose the right parses.

00:20:14.758 --> 00:20:19.628
Okay, so that showed that although
the idea of lexicalization is

00:20:19.628 --> 00:20:24.941
clearly important for capturing
certain kinds of parsing decisions,

00:20:24.941 --> 00:20:29.721
such as prepositional phrase attachments,
you can do a lot with

00:20:29.721 --> 00:20:34.768
just a little bit of linguistic
modeling rather than actually going

00:20:34.768 --> 00:20:39.673
the whole way of having this
massive space of lexicalized PCFGs.

00:20:39.673 --> 00:20:44.098
And the kind of modeling that you're doing
there is actually a kind that was very

00:20:44.098 --> 00:20:48.525
familiar from work on feature based and
unification based models that have been

00:20:48.525 --> 00:20:51.787
explored in linguistics in the 1980s and
1990s.

