WEBVTT
Kind: captions
Language: en

00:00:00.490 --> 00:00:04.000
Word tokenization is an important
part of text processing.

00:00:04.000 --> 00:00:08.160
Every natural language processing task
has to normalize the text in some way.

00:00:08.160 --> 00:00:11.720
We start by segmenting or
tokenizing the words off.

00:00:11.720 --> 00:00:15.700
We often have to normalize
the format of each of the words and,

00:00:15.700 --> 00:00:19.510
as part of this process, we're going to
have to break out sentences from the text.

00:00:19.510 --> 00:00:23.020
So let's start by talking about
this kind of word tokenization.

00:00:23.020 --> 00:00:25.400
How many words are there in a sentence?

00:00:25.400 --> 00:00:26.380
Here's a sentence.

00:00:26.380 --> 00:00:30.230
I do main-,
mainly business data processing.

00:00:30.230 --> 00:00:32.310
How many words are in that sentence?

00:00:32.310 --> 00:00:33.370
It's a complicated question.

00:00:33.370 --> 00:00:34.440
There is a word like.

00:00:34.440 --> 00:00:35.140
Is a word?

00:00:35.140 --> 00:00:38.060
Or how about the cutoff main of mainly?

00:00:38.060 --> 00:00:42.560
So we call things like main a fragment,
we call things like a filled pause.

00:00:42.560 --> 00:00:44.900
So for certain applications we
might want to be counting these,

00:00:44.900 --> 00:00:49.840
if we're dealing with speech synthesis or
speech recognition or correcting things.

00:00:49.840 --> 00:00:51.480
What about cat and cats?

00:00:51.480 --> 00:00:53.280
We talked about the Cat in the Hat.

00:00:53.280 --> 00:00:57.360
So we defined the term lemma,
two words are the same lemma if

00:00:57.360 --> 00:01:01.480
they have the same stem, the same part
of speech, roughly the same word sense.

00:01:01.480 --> 00:01:03.350
So cat and cats are both nouns.

00:01:03.350 --> 00:01:04.650
They have similar meanings.

00:01:04.650 --> 00:01:08.990
We say that cat and cats are the same
lemma, so the same word in that sense.

00:01:08.990 --> 00:01:13.200
We define the term wordform to mean
the full inflected surface form.

00:01:13.200 --> 00:01:16.415
So cat and cats, by that definition
of a word, are different words and

00:01:16.415 --> 00:01:18.000
the different wordforms.

00:01:18.000 --> 00:01:20.770
So we're going to use different
definitions depending on our goals.

00:01:20.770 --> 00:01:22.650
So let's look at an example sentence.

00:01:22.650 --> 00:01:26.030
They lay back on the San Francisco
grass and looked at the stars and

00:01:26.030 --> 00:01:27.760
their, and so on.

00:01:27.760 --> 00:01:30.540
And let's ask how many words
there are in this sentence.

00:01:30.540 --> 00:01:31.909
So count for yourself.

00:01:34.700 --> 00:01:37.590
We can define words in a couple ways.

00:01:37.590 --> 00:01:40.800
Word types,
how many vocabulary elements there are,

00:01:40.800 --> 00:01:42.460
how many unique words there are.

00:01:42.460 --> 00:01:43.600
And word tokens,

00:01:43.600 --> 00:01:47.590
how many instances of that particular
type there are in a running text.

00:01:47.590 --> 00:01:51.070
So, how many tokens do we have in here?

00:01:51.070 --> 00:01:52.880
Well, should be easy to count.

00:01:52.880 --> 00:01:56.210
One, two, three, four, five, and so on.

00:01:56.210 --> 00:02:00.470
So, if we count San and
Francisco separately, we end up with 15.

00:02:00.470 --> 00:02:05.210
If we count San Francisco as one token,
we end up with 14.

00:02:05.210 --> 00:02:08.010
So even the definition of a word

00:02:08.010 --> 00:02:10.040
depends a little bit in what we're
going to do with our spaces.

00:02:11.480 --> 00:02:13.897
How about types, count for yourself.

00:02:16.510 --> 00:02:20.200
Well, there's 13 types, again,
depending on how we count.

00:02:20.200 --> 00:02:23.050
So we have multiple copies.

00:02:23.050 --> 00:02:26.480
The word the, there's the and the.

00:02:26.480 --> 00:02:29.680
Again, it depends if we count
San Francisco as one word or two.

00:02:29.680 --> 00:02:33.620
And remember our lemmas,
we might decide that they and

00:02:33.620 --> 00:02:35.900
their, since they are the same lemma,

00:02:35.900 --> 00:02:39.770
although they're different word forms, we
might want to count them as the same type.

00:02:39.770 --> 00:02:40.800
Depending, again, on our goal.

00:02:42.240 --> 00:02:46.380
In general, we're going to be
referring to the number of tokens,

00:02:46.380 --> 00:02:49.820
which comes up whenever we're
counting things, with capital N.

00:02:49.820 --> 00:02:54.970
And we'll use capital V to mean the
vocabulary, the set of different types.

00:02:54.970 --> 00:02:56.930
And we'll use set notation,

00:02:56.930 --> 00:03:00.120
so the cardinality of the set V
is the size of the vocabulary.

00:03:00.120 --> 00:03:01.939
Although sometimes, for simplification,

00:03:01.939 --> 00:03:06.720
we'll just use capital V to mean the
vocabulary size when it's not ambiguous.

00:03:06.720 --> 00:03:09.770
So, how many words and tokens and types

00:03:09.770 --> 00:03:14.390
are there in the kind of data sets that we
look at in natural language processing?

00:03:14.390 --> 00:03:16.140
Let's look at a couple of these.

00:03:16.140 --> 00:03:20.400
Data sets of text are called corpora and
here's three important corpora,

00:03:20.400 --> 00:03:25.230
the switchboard corpus of phone
conversations has 2.4 million word

00:03:25.230 --> 00:03:30.540
tokens and has 20,000 word types
in those 2.4 million words.

00:03:30.540 --> 00:03:33.030
Shakespeare has just under
a million word tokens,

00:03:33.030 --> 00:03:34.440
Shakespeare's quite a small corpus.

00:03:34.440 --> 00:03:36.980
He wrote a 800,000 words in his lifetime.

00:03:36.980 --> 00:03:40.670
And, in that less than a million words,
he actually used 31,000 distinct words.

00:03:40.670 --> 00:03:43.610
So he had a very broad vocabulary,
famously.

00:03:43.610 --> 00:03:48.095
And if you look at a very huge corpus,
the Google N-grams corpus that has

00:03:48.095 --> 00:03:53.460
a trillion different tokens, a very large
number of words, there's 13 million types.

00:03:53.460 --> 00:03:55.670
So how many words are there in English?

00:03:55.670 --> 00:03:57.890
Well, if you look at conversation,
20,000 different words.

00:03:57.890 --> 00:04:01.036
If you look at Shakespeare, 30,000 words.

00:04:01.036 --> 00:04:04.770
And if you combine the two, probably
somewhere, not quite the sum of the two,

00:04:04.770 --> 00:04:05.920
but some larger number.

00:04:05.920 --> 00:04:09.300
But if you look at the Google N-grams,
we have 13 million words.

00:04:09.300 --> 00:04:13.460
And of course, some of those
are probably URLs and email addresses.

00:04:13.460 --> 00:04:15.190
But even if you eliminate all of those,

00:04:15.190 --> 00:04:18.540
the number of words in
the language is very large.

00:04:18.540 --> 00:04:20.690
Maybe there's a million words of English.

00:04:20.690 --> 00:04:26.310
And in fact, Church and Gale have
suggested that the size of the vocabulary

00:04:26.310 --> 00:04:31.040
grows greater with the square
root of the number of tokens.

00:04:31.040 --> 00:04:35.820
So as you get n tokens, the square
root of n more vocabulary items.

00:04:35.820 --> 00:04:38.640
So vocabulary keeps growing and
growing and its names and

00:04:38.640 --> 00:04:41.550
other kinds of things that contribute
to this growing in vocabulary.

00:04:41.550 --> 00:04:46.440
We're going to introduce some
standard Unix tools that are used for

00:04:46.440 --> 00:04:47.350
text processing.

00:04:48.460 --> 00:04:53.270
So I have here a corpus of Shakespeare,
Shakespeare's complete works.

00:04:54.460 --> 00:04:57.690
You can see, here's the sonnets, and
it goes on through all the plays.

00:04:57.690 --> 00:05:02.920
So let's start by extracting
all the words in the corpus.

00:05:02.920 --> 00:05:05.565
So we're going to do this
using the tr program.

00:05:11.090 --> 00:05:13.960
All right, so
the tr program takes character and

00:05:13.960 --> 00:05:18.200
it maps every instance of that
character into another character.

00:05:18.200 --> 00:05:21.240
And we specify tr -c,
which means compliment.

00:05:21.240 --> 00:05:25.560
So it means take every character
that's not one of these characters and

00:05:25.560 --> 00:05:27.080
turn it into this character.

00:05:27.080 --> 00:05:30.080
So in this case it's take every
non-alphabetic character and

00:05:30.080 --> 00:05:31.800
turn it into a carriage return.

00:05:31.800 --> 00:05:34.110
So we're going to replace all
the periods and commas and

00:05:34.110 --> 00:05:36.760
spaces in Shakespeare with newlines.

00:05:36.760 --> 00:05:39.770
So we're going to create one line,
one word per line in this way.

00:05:39.770 --> 00:05:40.670
So let's look at that.

00:05:41.840 --> 00:05:44.821
So there's, we've now turned
the sonnets into one word per line.

00:05:47.950 --> 00:05:55.620
And now we're going to sort those words
to let us look at the unique word types.

00:05:56.630 --> 00:05:57.170
So let's do that.

00:05:58.490 --> 00:06:02.860
And you can see here's all the A's,
there's a lot of them.

00:06:02.860 --> 00:06:04.230
A occurs a lot in Shakespeare.

00:06:06.220 --> 00:06:09.280
And this is a very boring way to
look through all of Shakespeare,

00:06:09.280 --> 00:06:10.088
we don't want to do this.

00:06:10.088 --> 00:06:15.030
So let's, instead, use the program uniq.

00:06:15.030 --> 00:06:20.700
And the program uniq will
take that sorted file and

00:06:20.700 --> 00:06:24.590
tell us, for each unique type,
the count of times that it occurs.

00:06:24.590 --> 00:06:26.180
So let's try that.

00:06:26.180 --> 00:06:30.300
So here we have all the words in
Shakespeare with a count along the left,

00:06:30.300 --> 00:06:33.290
this is the product of the uniq program.

00:06:33.290 --> 00:06:34.420
And we can walk through.

00:06:36.860 --> 00:06:40.190
So, we know that in Shakespeare,
the word Achievement, with a capital A,

00:06:40.190 --> 00:06:41.380
occurs once.

00:06:41.380 --> 00:06:43.000
The word Achilles appears 79 times.

00:06:43.000 --> 00:06:46.174
The word Acquaint six times, and so on.

00:06:49.780 --> 00:06:54.850
So that's interesting, but it would be
nice if we didn't have to just look at

00:06:54.850 --> 00:07:00.400
these words in alphabetical order, but if
we could look at them in frequency order.

00:07:00.400 --> 00:07:05.760
So let's take the same list of words and
now resort it by frequency.

00:07:11.210 --> 00:07:16.260
So now we have the most frequent word in
Shakespeare is the word the followed by

00:07:16.260 --> 00:07:21.480
the word I followed by the word and and
we have the actual counts in Shakespeare.

00:07:21.480 --> 00:07:25.790
So that here is our lexicon of
Shakespeare, sorted in frequency order.

00:07:25.790 --> 00:07:27.050
There's some problems.

00:07:27.050 --> 00:07:28.502
One is that the word and

00:07:28.502 --> 00:07:33.480
occurs twice because we didn't map our
uppercase words to lowercase words.

00:07:33.480 --> 00:07:36.690
So let's fix the mapping of case first.

00:07:36.690 --> 00:07:37.458
Let's try that again.

00:07:37.458 --> 00:07:42.694
We're going to map all
of the uppercase letters

00:07:42.694 --> 00:07:47.250
to lowercase letters in Shakespeare.

00:07:47.250 --> 00:07:53.790
Then we're going to pipe that to
another instance of the tr program

00:07:57.130 --> 00:08:02.470
which replaces all of
the non-alphabetics with newlines.

00:08:02.470 --> 00:08:05.320
And now we're going to do our
sorting as we did before.

00:08:05.320 --> 00:08:08.285
We're going to use uniq to
find all the individual types.

00:08:08.285 --> 00:08:10.490
Uniq -c tells us the actual count.

00:08:10.490 --> 00:08:12.550
And then we're going to sort again.

00:08:12.550 --> 00:08:15.160
[INAUDIBLE] Means numerically and
r means start from the highest one.

00:08:15.160 --> 00:08:17.520
And then we'll look at those,
so let's do that.

00:08:19.860 --> 00:08:24.430
All right, so now we've solved
the problem of the ands, so

00:08:24.430 --> 00:08:29.010
now we only have lowercase and,
we don't have our uppercase and appearing.

00:08:29.010 --> 00:08:30.670
But we have another problem.

00:08:30.670 --> 00:08:33.630
We have this d here.

00:08:33.630 --> 00:08:40.180
Why is the word d or the word s,
why are they so frequent in Shakespeare?

00:08:42.930 --> 00:08:48.630
We also have to decide a standard that
we're going to need for [INAUDIBLE] words.

00:08:48.630 --> 00:08:52.930
So for example, if our input is
Finland apostrophe s capital,

00:08:52.930 --> 00:08:56.340
how are we going to tokenize
Finland's depends on our goal.

00:08:56.340 --> 00:08:59.180
So we might choose to keep
all the apostrophes in and

00:08:59.180 --> 00:09:01.070
then we have Finland apostrophe s.

00:09:01.070 --> 00:09:04.533
We might choose to replace all
the apostrophes with nothing or

00:09:04.533 --> 00:09:07.190
we might choose to eliminate
all the apostrophe s's.

00:09:07.190 --> 00:09:11.940
Similarly, we might choose to
expand what'res to what are and

00:09:11.940 --> 00:09:14.550
the I'ms to I ams because if we're,
for example,

00:09:14.550 --> 00:09:19.390
looking for all the cases of I, if for
some sentimental analysis task.

00:09:19.390 --> 00:09:21.720
Or if we're looking for
cases of negation for

00:09:21.720 --> 00:09:25.368
some task,
we might want to turn isn't into is not.

00:09:25.368 --> 00:09:28.090
How about Hewlett-Packard.

00:09:28.090 --> 00:09:35.940
We've to decide whether Hewlett-Packard
are going to be represented Or.

00:09:35.940 --> 00:09:36.840
Or with a space.

00:09:36.840 --> 00:09:39.270
The same is true with phrases
like state of the art.

00:09:39.270 --> 00:09:42.760
We'll have to decide for words like
lowercase, should they have a dash?

00:09:42.760 --> 00:09:43.890
Should they have no dash at all?

00:09:43.890 --> 00:09:45.420
Should they have a space?

00:09:45.420 --> 00:09:46.890
We talked about the issue
of San Francisco.

00:09:46.890 --> 00:09:49.750
And then,
issues with periods become a huge issue.

00:09:49.750 --> 00:09:52.900
We have to decide if we're going to
represent m.p.h., leave the periods in.

00:09:52.900 --> 00:09:55.160
And then all of our algorithms
that use periods for

00:09:55.160 --> 00:09:58.120
splitting things are going to
have to be sensitive to this.

00:09:58.120 --> 00:10:01.790
The issue of tokenization becomes even
more complicated in other languages.

00:10:01.790 --> 00:10:07.275
We have the French phrase l'ensemble,
[INAUDIBLE] the l' be a separate word and,

00:10:07.275 --> 00:10:10.970
if so,
do we turn it into the full article le or

00:10:10.970 --> 00:10:14.240
do we keep as l' or just an l by itself.

00:10:14.240 --> 00:10:16.720
We'd like it to match the same

00:10:16.720 --> 00:10:19.980
word ensemble even if a different
article occurs before it.

00:10:19.980 --> 00:10:21.190
So we're going to want to
break them up for

00:10:21.190 --> 00:10:24.910
some reasons, but then we're stuck
with these sort of non-words.

00:10:24.910 --> 00:10:27.290
So another issue we have to deal with.

00:10:27.290 --> 00:10:32.340
In German, the long nouns are not
segmented as they are in English.

00:10:32.340 --> 00:10:36.150
So a word like life insurance company
employee in English would be segmented up

00:10:36.150 --> 00:10:36.880
in German.

00:10:36.880 --> 00:10:41.320
We're going to into these very long
phrase, but spelled as a single word.

00:10:41.320 --> 00:10:44.270
So, for
German tasks like information retrieval,

00:10:44.270 --> 00:10:46.331
we're going to need to
do a compound splitting.

00:10:49.030 --> 00:10:51.560
In Chinese and
Japanese we have a different problem.

00:10:51.560 --> 00:10:53.474
There's no spaces at
all between the words.

00:10:53.474 --> 00:10:57.445
So here's [INAUDIBLE] and we've shown
you the original Chinese sentence here.

00:10:58.485 --> 00:11:01.015
And now here's the sentence segmented out.

00:11:01.015 --> 00:11:07.005
So here's Sharapova now lives in US,
and so on.

00:11:07.005 --> 00:11:10.435
And so in English, we segment out
[INAUDIBLE], Chinese we don't.

00:11:10.435 --> 00:11:11.985
So if we want to do an actual
language processing

00:11:11.985 --> 00:11:15.405
on Chinese [INAUDIBLE] applications,
we need to break things up into words.

00:11:15.405 --> 00:11:17.910
And so we'll need some way of doing that.

00:11:17.910 --> 00:11:21.740
Similarly, in Japanese, we have
the problem that there's no spaces between

00:11:21.740 --> 00:11:26.310
words and we have the problem there are
multiple alphabets that are intermingled.

00:11:26.310 --> 00:11:32.110
There's the Katakana alphabet, there's
the Hiragana alphabet, there are Kanji,

00:11:32.110 --> 00:11:35.920
which are like the Chinese characters,
and there's Romaji, the Roman letters.

00:11:35.920 --> 00:11:39.440
Another complicating issue that has to
be dealt with in tokenizing Japanese.

00:11:42.320 --> 00:11:46.400
Word tokenization in Chinese is a common
research problem that has to be addressed

00:11:46.400 --> 00:11:49.090
when doing any kind of Chinese
natural language processing.

00:11:49.090 --> 00:11:53.030
And the characters in Chinese
represent a single syllable,

00:11:53.030 --> 00:11:57.550
often a single morpheme, and the average
word is about 2.4 characters long.

00:11:57.550 --> 00:12:01.160
So a word has to be broken up into
approximately two or three characters.

00:12:01.160 --> 00:12:03.950
And there are lots of complicated
algorithms for this, but

00:12:03.950 --> 00:12:08.280
there is a standard baseline segmentation
algorithm called the max-match,

00:12:08.280 --> 00:12:12.970
the maximum matching algorithm,
also called the greedy algorithm.

00:12:12.970 --> 00:12:15.300
So let's look at the max-match
as an algorithm.

00:12:17.740 --> 00:12:20.993
We're given a word list of Chinese,
so a vocabulary of Chinese,

00:12:20.993 --> 00:12:22.910
a dictionary, and a string.

00:12:22.910 --> 00:12:25.670
We'll start a pointer at
the beginning of the string.

00:12:25.670 --> 00:12:29.190
We'll find the longest word in the
dictionary that matches the string so far,

00:12:29.190 --> 00:12:31.090
starting at the pointer.

00:12:31.090 --> 00:12:35.190
We'll move the pointer over the word in
the string and then we'll go back and

00:12:35.190 --> 00:12:36.520
move on for the next word.

00:12:36.520 --> 00:12:38.370
Let's just see an example of that working.

00:12:38.370 --> 00:12:40.910
I'm going to pick an English example
that's easier to think about.

00:12:40.910 --> 00:12:44.810
We'll take the phrase, imagine English
was written like Chinese with no spaces.

00:12:44.810 --> 00:12:48.630
We'd have a phrase like thecatinthehat,
all run together.

00:12:48.630 --> 00:12:55.860
And we have a dictionary that has
words like the and a and cat.

00:12:55.860 --> 00:12:56.700
So we look at this and

00:12:56.700 --> 00:13:00.980
we say what's the longest word in our
dictionary that matches the beginning?

00:13:00.980 --> 00:13:05.840
And the longest word in our dictionary
is the, because thec is not a word,

00:13:05.840 --> 00:13:09.070
and theca is not a word and so
on, so we'll start with the.

00:13:09.070 --> 00:13:10.710
And now we've gotten to here.

00:13:10.710 --> 00:13:13.630
And now we say what's the longest
word starting with c?

00:13:13.630 --> 00:13:15.930
And the longest word is cat.

00:13:15.930 --> 00:13:19.990
So now we say, what's the longest
word starting with the i?

00:13:19.990 --> 00:13:21.480
And so on, and we do a good job.

00:13:22.540 --> 00:13:24.840
How about the phrase Thetabledownthere,

00:13:24.840 --> 00:13:27.330
where we've taken the spaces
out of the table down there.

00:13:27.330 --> 00:13:29.600
What's our segmentation,
our max-match segmentation algorithm,

00:13:29.600 --> 00:13:31.860
going to do with Thetabledownthere?

00:13:31.860 --> 00:13:32.860
Think a little for yourself.

00:13:34.310 --> 00:13:38.150
You may think that what it's going to
do is produce the table down there, but

00:13:38.150 --> 00:13:38.930
there's a problem.

00:13:38.930 --> 00:13:41.260
English has a lot of long words.

00:13:41.260 --> 00:13:43.612
English has the word theta for
the variable.

00:13:43.612 --> 00:13:49.220
And so, instead of the table down there,
we're going to get theta,

00:13:50.420 --> 00:13:56.360
right after that, bled, and
then own and then there.

00:13:56.360 --> 00:13:58.512
So we're going to get
theta bled own there.

00:13:58.512 --> 00:14:02.580
So max-match is, in fact,
not a generally good algorithm for

00:14:02.580 --> 00:14:05.070
this kind of pseudo-English,
English without spaces.

00:14:05.070 --> 00:14:09.220
Because English has these very long words
and short words all mixed together.

00:14:09.220 --> 00:14:10.350
But since Chinese, in general,

00:14:10.350 --> 00:14:15.050
has relatively consistent word length,
this works very well for Chinese.

00:14:15.050 --> 00:14:20.410
And it turns out that modern probabilistic
segmentation algorithms work even better.

00:14:20.410 --> 00:14:24.270
So that's the end of our
section on word tokenization.

