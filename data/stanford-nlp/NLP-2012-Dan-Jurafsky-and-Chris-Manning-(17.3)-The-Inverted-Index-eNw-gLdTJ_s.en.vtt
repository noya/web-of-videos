WEBVTT
Kind: captions
Language: en

00:00:02.260 --> 00:00:03.380
Hello again.

00:00:03.380 --> 00:00:04.060
In this segment,

00:00:04.060 --> 00:00:08.750
we're going to talk about the inverted
index and how it's constructed.

00:00:08.750 --> 00:00:12.440
An inverted index is the key data
structure that underlies all modern

00:00:12.440 --> 00:00:16.840
information retrieval systems,
from systems running on a single laptop

00:00:16.840 --> 00:00:20.100
to those running in the biggest
commercial search engines.

00:00:20.100 --> 00:00:24.630
An inverted index is a data structure
that exploits this sparsity

00:00:24.630 --> 00:00:29.120
of the term document matrix that
we talked about in the preceding

00:00:29.120 --> 00:00:34.400
segment and allows for
very efficient retrieval.

00:00:34.400 --> 00:00:38.770
It's essentially without peer as the data
structure used in information retrieval

00:00:38.770 --> 00:00:39.439
systems.

00:00:42.240 --> 00:00:45.320
So let me go through what's
in an inverted index.

00:00:45.320 --> 00:00:46.460
So for each term,

00:00:46.460 --> 00:00:51.410
each word, we must store a list of all
the documents that contain the word.

00:00:52.720 --> 00:00:57.570
Let's identify each document by a docID,
which is just a document serial number.

00:00:57.570 --> 00:01:01.442
So you can think of it starting
with the first document called one,

00:01:01.442 --> 00:01:02.908
then two, three, etc.

00:01:02.908 --> 00:01:06.788
And then, the question is what
data structure should we use?

00:01:06.788 --> 00:01:11.702
I mean, one idea might be to use
fixed arrays, like the vectors there

00:01:11.702 --> 00:01:16.160
in the term document matrix,
but that's very inefficient.

00:01:16.160 --> 00:01:19.848
Because while for some words,
they'll appear in a lot of documents,

00:01:19.848 --> 00:01:22.950
other words will appear
in very few documents.

00:01:22.950 --> 00:01:27.040
Moreover, there are perhaps other problems
if we think about a dynamic index, where

00:01:27.040 --> 00:01:32.180
some documents are added later on that
then we have more documents that changed.

00:01:32.180 --> 00:01:36.930
Then we'll have difficult things
in adjusting our vector sizes.

00:01:39.460 --> 00:01:43.730
For these reasons, one way or
another, we need to use variable size

00:01:43.730 --> 00:01:48.100
lists to store the documents
in which a word occurs.

00:01:48.100 --> 00:01:48.780
And instead,

00:01:48.780 --> 00:01:53.640
the information retrieval terminology,
these lists are called postings lists.

00:01:53.640 --> 00:01:57.660
Postings lists traditionally were
usually stored on disks, but

00:01:57.660 --> 00:02:00.750
that may not be the case now for
big search engines.

00:02:00.750 --> 00:02:03.450
And if you're storing
postings lists on disks,

00:02:03.450 --> 00:02:07.980
the right way to store them is as one
continuous run of postings because that

00:02:07.980 --> 00:02:11.680
gives the most efficient method of
then being able to load them off disk

00:02:11.680 --> 00:02:15.570
back into memory when you're interested
in the postings from a particular word.

00:02:15.570 --> 00:02:20.410
In memory, a postings list can be
represented as a data structure,

00:02:20.410 --> 00:02:22.710
like a linked list or
variable length's arrays,

00:02:22.710 --> 00:02:26.820
with some obvious tradeoffs in
the size versus the ease of insertion.

00:02:27.840 --> 00:02:29.750
So the data structure
that we end up with for

00:02:29.750 --> 00:02:33.670
an inverted index is like
the one that I'm showing here.

00:02:33.670 --> 00:02:41.010
So we have the terms that are in any of
our documents, and then for each term,

00:02:41.010 --> 00:02:46.650
we've then got a pointer to a postings
list that is then giving the different

00:02:46.650 --> 00:02:52.350
documents, which are described by
their document ID in which it occurs.

00:02:53.370 --> 00:03:01.210
Okay, so one occurrence of a word document
pair is referred to as a posting,

00:03:01.210 --> 00:03:07.070
and the sum of all of the postings list
are then referred to as the postings.

00:03:07.070 --> 00:03:11.050
And so over all then, we have
the parts of on the left hand side,

00:03:11.050 --> 00:03:12.650
we have the dictionary.

00:03:12.650 --> 00:03:16.598
And then on the right hand side,
we have the postings.

00:03:16.598 --> 00:03:22.660
And a property of the postings is
that they're sorted by document ID,

00:03:22.660 --> 00:03:25.390
and very soon now,
we'll explain why that's essential.

00:03:26.900 --> 00:03:30.950
So these two data structures,
the dictionary and

00:03:30.950 --> 00:03:34.350
the postings have somewhat
different statuses.

00:03:34.350 --> 00:03:39.570
Because in global size,
the dictionary is relatively small,

00:03:39.570 --> 00:03:42.390
but it's normally essential
that it's in memory.

00:03:43.700 --> 00:03:49.710
Whereas the postings are large,
but at least for something like

00:03:49.710 --> 00:03:54.399
a small scale enterprise search engine,
these will normally be stored on disk.

00:03:57.450 --> 00:04:00.720
Let me move now to how
an inverted index is constructed.

00:04:00.720 --> 00:04:04.603
So, the starting off point is we have
a bunch of documents to be indexed, and

00:04:04.603 --> 00:04:08.446
each of those documents we'll think
of as being a sequence of characters.

00:04:08.446 --> 00:04:13.314
We'll assume that we've already dealt
with, perhaps by someone else's software,

00:04:13.314 --> 00:04:17.458
conversion from PDF from Microsoft Word
documents and things like that.

00:04:17.458 --> 00:04:21.610
So then we're going to go through
first some pre-processing steps.

00:04:21.610 --> 00:04:27.200
So we need a tokenizer that turns the
document into a sequence of word tokens,

00:04:27.200 --> 00:04:30.440
which are the basic units of indexing.

00:04:30.440 --> 00:04:35.250
But we often don't index exactly the words
that are contained in the document.

00:04:35.250 --> 00:04:39.020
There might be various linguistic
modules that in some way

00:04:39.020 --> 00:04:42.930
modify the tokens to put them
into some kind of canonical form.

00:04:42.930 --> 00:04:46.810
So for instance, here,
we're saying that for friends here,

00:04:46.810 --> 00:04:53.100
it's being both lowercased and is being
stemmed to remove the s plural ending.

00:04:53.100 --> 00:04:58.760
Okay, so then it's those modified tokens,
which will be fed to the index which

00:04:58.760 --> 00:05:02.910
is the thing that builds the inverted
index that I was just talking about.

00:05:02.910 --> 00:05:04.690
So here's the inverted index.

00:05:04.690 --> 00:05:08.665
And it's this step here of the indexer
that is the main thing that I want to

00:05:08.665 --> 00:05:09.388
talk about.

00:05:09.388 --> 00:05:15.158
But let me first just briefly mention
those initial stages of text processing.

00:05:15.158 --> 00:05:18.090
So, in just a fraction more detail,

00:05:18.090 --> 00:05:22.770
the things that happen in those initial
stages is firstly, tokenizations.

00:05:22.770 --> 00:05:28.130
That's just how we decide to cut
the character sequence into word tokens.

00:05:28.130 --> 00:05:29.530
And there are various issues there.

00:05:29.530 --> 00:05:32.630
There are punctuation that
come up against words,

00:05:32.630 --> 00:05:35.800
how to treat possessives,
hyphenated terms, and

00:05:35.800 --> 00:05:41.940
all that kind of stuff that we
can talk about in more detail.

00:05:41.940 --> 00:05:47.040
Then normalization was this issue that
while certain things like USA with and

00:05:47.040 --> 00:05:51.500
without the dots, you probably
want to treat as the same term.

00:05:51.500 --> 00:05:56.630
And map both text and the query terms to
the same form, so that they will match.

00:05:58.280 --> 00:06:02.210
You might want to do other kinds
of mapping, such as stemming, so

00:06:02.210 --> 00:06:06.900
the authorize and authorization are both
being mapped to the same stem, so

00:06:06.900 --> 00:06:09.120
that they straightforwardly
match in a query.

00:06:10.670 --> 00:06:15.240
And finally, you may not want to
index at all the most common words.

00:06:15.240 --> 00:06:19.630
Traditionally, many search engines have
left out very common words, like the,

00:06:19.630 --> 00:06:22.070
a, to, and of from the indexing.

00:06:22.070 --> 00:06:26.150
It's not clear that in the modern
world when our amount of storage is so

00:06:26.150 --> 00:06:30.857
vast that that's such a good idea because
there are queries that you might like to

00:06:30.857 --> 00:06:35.518
do, such as for the song To Be Or Not To
Be, where you really need the stop words.

00:06:35.518 --> 00:06:41.398
And it turns out that with modern indexes,
it's not that inefficient to store them.

00:06:41.398 --> 00:06:46.035
Okay, now let's go through in
detail how the indexer goes from

00:06:46.035 --> 00:06:51.830
the sequence of perhaps normalized
tokens to building an inverted index.

00:06:51.830 --> 00:06:57.460
So for this example, we're assuming we
have two documents, Doc 1 and Doc 2 here.

00:06:57.460 --> 00:07:00.360
So there are key sequence of
steps that we go through.

00:07:01.710 --> 00:07:07.200
So our input is that we
have the sequence of tokens

00:07:07.200 --> 00:07:12.110
of the first document in the order that
they come in the text and the sequence of

00:07:12.110 --> 00:07:16.580
tokens of the second document in
the order in which they come in the text.

00:07:16.580 --> 00:07:19.920
So the first step is that we do a sort,
and

00:07:19.920 --> 00:07:25.920
we sort as primary key by the terms,
putting them in alphabetical order.

00:07:25.920 --> 00:07:29.200
So here we have this
alphabetical list of terms.

00:07:29.200 --> 00:07:34.090
And if we have the same term
appearing in multiple documents,

00:07:34.090 --> 00:07:37.530
we do a second resort by the document ID.

00:07:37.530 --> 00:07:40.612
So the word Caesar appears twice,

00:07:40.612 --> 00:07:45.298
once in document ID one and
twice in document ID two.

00:07:45.298 --> 00:07:49.200
And we're sorting it
secondarily by document id.

00:07:49.200 --> 00:07:54.455
And so that's a core and
expensive indexing step.

00:07:54.455 --> 00:07:58.315
Once we've got that far,
what we then do is essentially

00:07:58.315 --> 00:08:03.015
a consolidation of what we
found over here on the right.

00:08:03.015 --> 00:08:07.845
So we take that, here it is again, and

00:08:07.845 --> 00:08:11.365
multiple entries in a single
document are merged.

00:08:11.365 --> 00:08:16.610
So that's the two instances of Caesar,
and they're just treated as one.

00:08:16.610 --> 00:08:21.730
And then we also merge all instances

00:08:21.730 --> 00:08:27.920
of a particular term, and so
then we re-represent that as over here.

00:08:27.920 --> 00:08:31.470
So we say we have
the dictionary entry Caesar,

00:08:31.470 --> 00:08:34.670
we record its total
frequency in the collection.

00:08:34.670 --> 00:08:37.200
I'll come back to that a bit later.

00:08:37.200 --> 00:08:38.280
And then we build for

00:08:38.280 --> 00:08:43.172
it the postings list, which is the list
of documents in which it occurs.

00:08:43.172 --> 00:08:47.610
And straightforwardly, because of
a consequence of our sort in the previous

00:08:47.610 --> 00:08:52.950
step that this postings list is itself
going to be sorted by the document ID.

00:08:55.330 --> 00:08:59.080
So thinking about the size of
inverted index, we can think for

00:08:59.080 --> 00:09:02.470
a minute about where do we pay in storage?

00:09:02.470 --> 00:09:05.420
So we pay some amount for

00:09:05.420 --> 00:09:10.530
the list of terms in the accounts, but the
number of terms will be relatively modest.

00:09:10.530 --> 00:09:14.938
In our example beforehand,
there were 500,000 terms.

00:09:14.938 --> 00:09:19.530
We pay for a pointer or two that
identifies where the postings list are,

00:09:19.530 --> 00:09:24.080
but again,
that's of the order of 500,000 things.

00:09:24.080 --> 00:09:28.030
And then we pay for
the actual postings list themselves.

00:09:28.030 --> 00:09:31.548
And these postings lists
are by far the biggest part.

00:09:31.548 --> 00:09:36.730
But even then, they're bounded by the size
of the number of tokens in the collection.

00:09:36.730 --> 00:09:42.816
So in our example before of the million
documents with an average length 1,000

00:09:42.816 --> 00:09:47.950
words, we're sill less than 1 billion
items there, and so storage is manageable.

00:09:50.490 --> 00:09:55.160
So when we are actually
building an efficient

00:09:55.160 --> 00:09:59.460
IR system implementation,
we think further about these questions.

00:09:59.460 --> 00:10:01.900
We think about how can we make

00:10:01.900 --> 00:10:05.530
the index as efficient as
possible through retrieval.

00:10:05.530 --> 00:10:09.820
And how can we minimize
the storage on both sides of this,

00:10:09.820 --> 00:10:14.380
both on this side and this side,
in terms of various sorts of compression?

00:10:14.380 --> 00:10:17.240
We're not going to get into
the details of that now, but

00:10:17.240 --> 00:10:21.860
what I hope you can start to
see is that the inverted index

00:10:21.860 --> 00:10:27.020
gives an efficient basis on which
to do retrieval operations.

00:10:27.020 --> 00:10:31.800
And that's something that we'll talk
about in more detail in the next segment.

00:10:31.800 --> 00:10:36.450
But at any rate, now you know the
underlying data structure, it's really not

00:10:36.450 --> 00:10:41.250
that complex, that underlies all
modern information retrieval systems.

