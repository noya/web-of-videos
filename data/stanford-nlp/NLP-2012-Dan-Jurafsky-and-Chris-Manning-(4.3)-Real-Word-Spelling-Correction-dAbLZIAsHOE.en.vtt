WEBVTT
Kind: captions
Language: en

00:00:01.340 --> 00:00:04.450
We've seen how to correct
non-words of English, but

00:00:04.450 --> 00:00:06.390
what happens if the error
produces a real word.

00:00:07.960 --> 00:00:09.560
This turns out to be
a very common problem.

00:00:10.780 --> 00:00:13.600
Maybe between a quarter and
a half of spelling errors,

00:00:13.600 --> 00:00:17.170
depending on the application,
turn out to be real words.

00:00:17.170 --> 00:00:24.110
And, so,
in the examples from a classic paper by.

00:00:24.110 --> 00:00:29.100
The word minutes is misspelled as minuets,
perfectly reasonable English word.

00:00:29.100 --> 00:00:33.940
The word and is misspelled as the word an,
a very common English word.

00:00:33.940 --> 00:00:40.110
Leave as lave, by as be and so on and
so, not only are these English words,

00:00:40.110 --> 00:00:43.390
but some of them are quite common and
frequently used English words so

00:00:43.390 --> 00:00:48.490
a much tougher problem solving
the real word spelling error Task.

00:00:50.170 --> 00:00:52.830
Again what we're going to do for the
real-world spelling errors is very similar

00:00:52.830 --> 00:00:58.060
to what we did for the non real words.

00:00:58.060 --> 00:01:01.396
We're going to generate a candidate
set which includes the word itself and

00:01:01.396 --> 00:01:07.690
all single-letter edits that produce
English words and maybe we'll also,

00:01:07.690 --> 00:01:11.410
in some versions, produce words that
are homophones that sound alike.

00:01:11.410 --> 00:01:15.380
And given this candidate set for each
word, we'll choose the best candidates,

00:01:15.380 --> 00:01:16.880
either using the noisy channel model or

00:01:16.880 --> 00:01:22.580
you can imagine we'll talk later about
more complex models that use classifiers.

00:01:22.580 --> 00:01:24.350
So, let's look at that in detail.

00:01:25.960 --> 00:01:31.560
Given a sentence with word 1 through
word n, we're going to generate for

00:01:31.560 --> 00:01:34.180
each word A set of candidates.

00:01:34.180 --> 00:01:38.650
So for word 1, we have the candidates Word
1 itself, and then a bunch of variants.

00:01:38.650 --> 00:01:41.720
The single edit distance
neighbors of that word.

00:01:41.720 --> 00:01:45.060
Word 1 prime, Word 1 double-prime,
Word 1 triple-prime.

00:01:45.060 --> 00:01:49.900
Word 2, Word 2 prime, Word 2 double-prime,
Word 2 triple-prime, and so on.

00:01:49.900 --> 00:01:52.960
For each of the words, so we have a whole
lot of candidates preach the words.

00:01:54.540 --> 00:01:57.480
And now we're going to choose
the sequence, capital W,

00:01:57.480 --> 00:02:03.660
the sequence of candidates
that has maximal probability.

00:02:03.660 --> 00:02:12.000
In other words we might pick word one
from this candidate set and word.

00:02:12.000 --> 00:02:13.950
Two prime prime from
this candidate set and

00:02:13.950 --> 00:02:16.750
word three prime prime prime from
this candidate set and so on.

00:02:16.750 --> 00:02:20.150
For each word we're going to pick some
candidate which might be the word itself

00:02:20.150 --> 00:02:21.500
or some correction of that word.

00:02:21.500 --> 00:02:24.810
And we're going to pick
the sequence that is most likely.

00:02:26.180 --> 00:02:27.220
Let's look at an example of that.

00:02:28.950 --> 00:02:37.510
We have the imagine the three
word mini-sentence, two of thew.

00:02:37.510 --> 00:02:38.120
T-H-W, thew.

00:02:38.120 --> 00:02:41.930
So for each word,

00:02:41.930 --> 00:02:47.600
the word two, the word of,
the word thew, we generate potential.

00:02:47.600 --> 00:02:51.560
Corrections, each of which is a word of
English that is at a distance of one.

00:02:51.560 --> 00:02:52.490
So I've shown some here.

00:02:52.490 --> 00:02:55.550
So two could have been the word to,

00:02:55.550 --> 00:03:00.880
if the original word two was,
the error was an insertion of a W.

00:03:00.880 --> 00:03:06.150
Or it could have been the word tao, where
the error was a substitution of A for W.

00:03:07.350 --> 00:03:13.760
Or could've been the word Too
substitution of an O by a W.

00:03:13.760 --> 00:03:18.700
Or could've been,
the word Two could be correct.

00:03:18.700 --> 00:03:22.980
Similarly, of could've been the correct
word could've been off, and

00:03:22.980 --> 00:03:24.440
there was a deletion of an F.

00:03:24.440 --> 00:03:28.990
So again three candidates Off, On,
including the word Of itself and

00:03:28.990 --> 00:03:34.490
the word Thew,
which is a real-word of english.

00:03:34.490 --> 00:03:38.300
Could have been the word threw and
the r got deleted, or the word thaw or

00:03:38.300 --> 00:03:40.900
the word the, a very common word.

00:03:41.950 --> 00:03:45.990
And ew, a very likely error turns
out because w is right next to

00:03:45.990 --> 00:03:50.320
e in the keyboard And so on, so
we have each of our candidate sets.

00:03:50.320 --> 00:03:55.190
And then we just want to ask,
of all the possible sets of sentences

00:03:56.400 --> 00:04:02.930
produced by paths in this graph,
so here's one, to of threw.

00:04:02.930 --> 00:04:07.530
Here's another one, to on thaw,
here's another one,

00:04:07.530 --> 00:04:13.090
two of the and someone for
each of those possible sentences,

00:04:13.090 --> 00:04:16.670
what's the most likely one
according to the noisy channel?

00:04:16.670 --> 00:04:19.570
Pick the most probable one
according to the noisy channel.

00:04:20.780 --> 00:04:27.480
And hopefully the that noisy channel,
a good noisy channel model,

00:04:27.480 --> 00:04:32.330
will pick the correct answer two of
the as the most likely sequence here.

00:04:33.640 --> 00:04:38.300
In practice, for spelling correction,
we also make the simplification that we're

00:04:38.300 --> 00:04:42.860
only seeing one error, rather than letting
every word have a possible error in it.

00:04:42.860 --> 00:04:48.270
In other words, the set of sequences
we consider is the sequences in which

00:04:48.270 --> 00:04:52.660
only one of the words is an error and the
rest of the words were correct as typed.

00:04:52.660 --> 00:04:59.880
So here w1, w3, and
w4 were correct as typed and it was word

00:04:59.880 --> 00:05:04.150
two that was misspelled, and we replace
it by word two double prime let's say.

00:05:04.150 --> 00:05:09.720
Or in this sequence it was the word
three that was misspelled.

00:05:09.720 --> 00:05:13.370
The was misspelled as thew, and so

00:05:13.370 --> 00:05:16.930
here is the error and
these words are correct, and so on.

00:05:16.930 --> 00:05:20.650
So, these smaller sets of
possible candid sequences.

00:05:20.650 --> 00:05:23.900
So instead of having to consider
n squared possible sequences,

00:05:23.900 --> 00:05:29.260
we're just considering a constant
times n possible sequences.

00:05:30.270 --> 00:05:35.410
From this set,
now we choose the sequence that maximizes,

00:05:35.410 --> 00:05:37.330
that has the maximum probability.

00:05:37.330 --> 00:05:41.760
So we picked the most likely,
most probable, most

00:05:42.800 --> 00:05:48.270
conditionally most probable
sequence of candidates.

00:05:48.270 --> 00:05:52.910
Where do we get these probabilities?

00:05:52.910 --> 00:05:57.230
Again, the language model,
just as we saw before,

00:05:57.230 --> 00:06:00.322
we have our unigram, we have or
bigram, smoothing method we'd like.

00:06:01.600 --> 00:06:05.250
The Channel model is just the same again
as for the non-word spelling error

00:06:05.250 --> 00:06:09.700
correction, the only difference is, we now
need a probability of having no error.

00:06:09.700 --> 00:06:13.770
Because of course, we're assuming that
only one of the words is an error, so

00:06:13.770 --> 00:06:17.050
we have to have a probability for all
those other words that are not an error.

00:06:17.050 --> 00:06:21.680
We need to be able to decide when we
have an error and when a word is,

00:06:21.680 --> 00:06:22.950
in fact, correct.

00:06:22.950 --> 00:06:26.540
Meaning that the probability of the word
itself, giving the word, is high.

00:06:26.540 --> 00:06:27.830
So I'm likely to have an error.

00:06:29.280 --> 00:06:31.510
How do we compute this
probability of no error?

00:06:31.510 --> 00:06:35.100
What's the channel probability for
a correctly typed word?

00:06:35.100 --> 00:06:37.140
And this obviously depends
on the application.

00:06:38.400 --> 00:06:42.160
And so, we might make the assumption that,
in a particular application,

00:06:42.160 --> 00:06:44.760
1 word in 10 is typed wrong.

00:06:44.760 --> 00:06:49.260
And that means that the probability
of a correctly typed word is 0.90.

00:06:49.260 --> 00:06:53.930
Or we might have instead the assumption
that 1 word in 200 is wrong.

00:06:53.930 --> 00:06:58.175
And so now, the probability of any
word being typed correctly is .995.

00:06:58.175 --> 00:07:02.810
So there's our channel model
probability of a word not changing.

00:07:02.810 --> 00:07:08.530
Let's assume the channel model of a task

00:07:08.530 --> 00:07:13.148
it Has a probability of
one in twenty of an error.

00:07:13.148 --> 00:07:18.100
Meaning that 95% of the time
the word is correct as typed,

00:07:19.150 --> 00:07:21.630
so here's one from Peter Norwig.

00:07:21.630 --> 00:07:24.250
Again we have the spelling error t.h.e.w.

00:07:24.250 --> 00:07:27.924
And we want to know whether
it should be the word 'the'.

00:07:29.160 --> 00:07:35.450
The word few, that it was correct as
typed, or thaw or threw or thwe and so on.

00:07:37.280 --> 00:07:40.830
And again, for
each one we generate our channel model,

00:07:40.830 --> 00:07:43.960
and channel model were exactly
computed the same way as before.

00:07:43.960 --> 00:07:49.260
We have the probability of
a substitution of a being

00:07:49.260 --> 00:07:55.020
substituted by an e or
of a R being deleted after an H.

00:07:55.020 --> 00:07:56.980
And so on that we can compute
just from our channel models.

00:07:56.980 --> 00:07:59.310
So here's our channel model probabilities.

00:07:59.310 --> 00:08:02.980
And again we have our language
model probability just as before.

00:08:04.370 --> 00:08:10.510
And these are examples Peter Norvig
computed from the Google ingram accounts.

00:08:10.510 --> 00:08:17.830
And again, we've assumed the channel model
of a word not changing of the error x.

00:08:17.830 --> 00:08:21.350
We can generate it by correctly
generated by the word x.

00:08:21.350 --> 00:08:25.230
And we can multiply these together that
multiply together the channel model.

00:08:26.270 --> 00:08:28.470
With the language model and

00:08:28.470 --> 00:08:32.770
I'm again showing you things multiplied
by 10 to the 9th make me easy to read.

00:08:32.770 --> 00:08:38.350
You can see that the word
the is correctly chosen.

00:08:38.350 --> 00:08:43.770
Very high probability as
the misspelling of the word thew

00:08:43.770 --> 00:08:48.240
by itself and in context and
this is using a unigram language model.

00:08:48.240 --> 00:08:52.840
If we're using a bigram or a trigram we're
even more likely probably to distinguish

00:08:52.840 --> 00:08:56.940
when the word thew really was the word
thew and when it was the word the.

00:08:58.570 --> 00:09:00.150
So that's real word spelling correction.

00:09:00.150 --> 00:09:04.480
We simply take the, the standard
algorithm, noisy channel algorithm for

00:09:04.480 --> 00:09:09.100
non real words Add and
probability of an edit not happening, and

00:09:09.100 --> 00:09:13.640
then allow every word, to imagine
every word could have been an error.

00:09:13.640 --> 00:09:15.900
And then look for
the most likely sequence.

00:09:15.900 --> 00:09:18.670
Simplifying usually by assuming
only 1 error per sentence.

