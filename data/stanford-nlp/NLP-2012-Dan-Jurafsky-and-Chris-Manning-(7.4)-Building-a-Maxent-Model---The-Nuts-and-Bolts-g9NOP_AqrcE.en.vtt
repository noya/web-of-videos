WEBVTT
Kind: captions
Language: en

00:00:02.265 --> 00:00:05.634
So let me now just go
through quite concretely,

00:00:05.634 --> 00:00:08.850
how you go about building a Maxent model.

00:00:08.850 --> 00:00:12.740
And this is the kind of stuff that you'll
be doing in the assignment for this week.

00:00:14.311 --> 00:00:17.740
So the first step is
that we define features.

00:00:17.740 --> 00:00:21.370
So our features are Boolean functions,

00:00:21.370 --> 00:00:25.380
binary indicator functions
over the data points.

00:00:25.380 --> 00:00:30.410
And what we're thinking about is
what are things that will pick out

00:00:30.410 --> 00:00:36.830
sets of data point, which say something
distinctive about our classification task.

00:00:36.830 --> 00:00:38.860
If it's something like
text classification or

00:00:38.860 --> 00:00:43.250
sentiment, we're definitely going to want
to include individual words as features.

00:00:43.250 --> 00:00:47.980
But sometimes those aren't the best or
the only good features.

00:00:47.980 --> 00:00:53.500
For example, there are an infinite
space of numbers like 3.42 or 7.86,

00:00:53.500 --> 00:00:58.410
and it's going to be hard
to usefully use these

00:00:59.780 --> 00:01:05.425
numbers just as words, because most likely
the numbers that turn up at test time,

00:01:05.425 --> 00:01:07.120
weren't in your training data.

00:01:07.120 --> 00:01:11.030
So you can get a lot of value from
defining more general features.

00:01:11.030 --> 00:01:14.760
Which might be just simply,
word contains a digit.

00:01:14.760 --> 00:01:19.246
Or it might be something more specific
like, is floating point number.

00:01:25.114 --> 00:01:31.420
But there are are also many other
cases of useful kinds of features.

00:01:31.420 --> 00:01:34.300
So in many languages including English

00:01:34.300 --> 00:01:37.570
the end of a word gives you
information about its class.

00:01:37.570 --> 00:01:43.796
So if you have words like helping, making,

00:01:43.796 --> 00:01:48.720
drying that you can tell that

00:01:48.720 --> 00:01:54.260
these words are participial forms of
verbs by looking at that ing ending.

00:01:54.260 --> 00:01:55.960
Of course, this isn't always true.

00:01:55.960 --> 00:02:01.740
You'll get a word like sting, but
nevertheless it's a good indicator and

00:02:01.740 --> 00:02:05.940
so a feature like this is something that
you can be expecting to get a positive

00:02:05.940 --> 00:02:10.420
weight in a model, if you're wanting to
have a model that's classifying words for

00:02:10.420 --> 00:02:12.014
something like part of speech.

00:02:14.196 --> 00:02:19.640
So what we will do in class that's
commonly done in practice Is that for

00:02:19.640 --> 00:02:25.640
each phi feature that picks out a data
context that we will just represented as

00:02:25.640 --> 00:02:31.044
a string, so the string could just
be the word computer or whatever.

00:02:32.797 --> 00:02:37.219
But since, we're also going to have
other features like word contains number

00:02:37.219 --> 00:02:40.650
commonly what we want to do is
have some informal namespace.

00:02:40.650 --> 00:02:44.300
So this might be,
the word equals computer.

00:02:44.300 --> 00:02:49.360
And this one might be,
the num equals decimal.

00:02:52.380 --> 00:02:56.420
Or the feature here might
be the end equals ing.

00:02:56.420 --> 00:02:59.880
So we got this informal
name space of features.

00:02:59.880 --> 00:03:02.450
And the reason why we want
to do that is we want to

00:03:02.450 --> 00:03:06.920
represent each feature as an unique
string, so we can use that

00:03:06.920 --> 00:03:10.830
in like a hash map where we can then
look up the weight for each feature.

00:03:10.830 --> 00:03:14.740
Now of course you don't have to
encode the features as strings.

00:03:14.740 --> 00:03:18.704
You could encode them as some
more general, structured object.

00:03:18.704 --> 00:03:23.030
But in practice a lot of the time strings
I use because they're just a fairly

00:03:23.030 --> 00:03:25.810
flexible and
formal way to encode features.

00:03:27.170 --> 00:03:32.780
Okay, and then remember that this
part gives us our five feature.

00:03:32.780 --> 00:03:35.501
And then from the five feature we're
going to construct the f features,

00:03:35.501 --> 00:03:42.940
where the f features are going to be
a five feature and a particular class.

00:03:42.940 --> 00:03:48.100
So for a particular five feature, we're
then going to build a set of f features,

00:03:48.100 --> 00:03:50.120
one for each choice of class cj.

00:03:50.120 --> 00:03:56.087
And then it's the particular
f features like f1,

00:03:56.087 --> 00:04:01.636
f2, f3 that are then
going to be given weights

00:04:01.636 --> 00:04:07.350
like 0.3 minus 0.7, 1.2 or whatever.

00:04:07.350 --> 00:04:10.560
We concentrate on defining

00:04:10.560 --> 00:04:15.500
the phi features in terms of what are
useful features for the problem domain.

00:04:15.500 --> 00:04:20.010
But you should remember that in the
presentation that follows and in the code,

00:04:20.010 --> 00:04:25.260
that the code is working
in terms of indices i

00:04:25.260 --> 00:04:30.370
of the individual f features, where
each phi feature is then being turned

00:04:30.370 --> 00:04:34.520
into a set of f features,
one for each class.

00:04:36.270 --> 00:04:38.820
How do you go about
building a Maxent Model?

00:04:38.820 --> 00:04:43.110
Well normally to start off with,
you define some features that you are,

00:04:43.110 --> 00:04:45.470
just basically,
sure are going to be useful.

00:04:45.470 --> 00:04:48.100
So that might be things like
words in the document or

00:04:48.100 --> 00:04:52.430
the would before the word you want
to classify or something like that.

00:04:52.430 --> 00:04:57.060
But typically, we go through an iterative
development process where we initially

00:04:57.060 --> 00:05:01.580
build one model with some features.

00:05:01.580 --> 00:05:06.950
We test it on some developmental
data not our final test data.

00:05:06.950 --> 00:05:11.400
And we see how it does and
it gets some performance level,

00:05:11.400 --> 00:05:15.350
like it might be 57%, and
we'd like to do a bit better.

00:05:15.350 --> 00:05:21.020
And so what we're then going to do is
come up with a Refined second model and

00:05:21.020 --> 00:05:25.640
then repeat over and
we'll commonly do this a number of times

00:05:25.640 --> 00:05:28.620
until we've tried to come up with
the best model that we think we can.

00:05:28.620 --> 00:05:33.530
And so the question then is what
do we do for this next model?

00:05:33.530 --> 00:05:34.360
And.

00:05:34.360 --> 00:05:36.710
In general,
you can look at the features and

00:05:36.710 --> 00:05:39.990
see which ones are useful as
good features and bed features.

00:05:39.990 --> 00:05:44.180
Often, the easiest way to make
progress in practice is to try and

00:05:44.180 --> 00:05:48.180
define features that mark bad situations.

00:05:48.180 --> 00:05:52.290
So if you look through your
classified development data,

00:05:52.290 --> 00:05:56.760
you'll see some piece of data and
it's classified something as a drug.

00:05:56.760 --> 00:05:58.730
And it just shouldn't have been a drug.

00:05:58.730 --> 00:06:03.270
And so then maybe you can look
at the actual observed data and

00:06:03.270 --> 00:06:04.590
see something about it.

00:06:04.590 --> 00:06:08.950
So maybe there's a smiley face here or
something like that.

00:06:08.950 --> 00:06:13.375
And you can say, well,
it just isn't likely That a piece of text

00:06:13.375 --> 00:06:17.585
which has a smiley face near it is going
to be talking about a drug, And so

00:06:17.585 --> 00:06:21.105
you can define a feature for
this combination.

00:06:21.105 --> 00:06:25.110
And so this kind of style of feature
is where you target errors And

00:06:25.110 --> 00:06:29.860
find some way to explain to the model
why that's a bad configuration

00:06:29.860 --> 00:06:32.830
is often one of the most
effective ways to add features.

00:06:34.280 --> 00:06:37.020
Then for
any particular set of features and

00:06:37.020 --> 00:06:41.620
their weights what we're going to want
to be able to do Is calculate the data

00:06:41.620 --> 00:06:46.690
conditional likelihood the probability
of a class, given the data item.

00:06:46.690 --> 00:06:48.440
Well we already know how to do that,

00:06:48.440 --> 00:06:53.170
that's what we did in the preceding
slides with the worked examples.

00:06:53.170 --> 00:06:58.525
But then second We're also going to
want to work out the derivative

00:06:58.525 --> 00:07:02.855
of the conditional likelihood with
respect to each feature weight.

00:07:05.555 --> 00:07:10.155
So we kind of want a partial
derivative of our overall probability

00:07:13.075 --> 00:07:17.355
according to a particular
feature weight lambda i.

00:07:20.630 --> 00:07:24.590
This is what we're going to work it
through the method in the next session.

00:07:24.590 --> 00:07:28.290
But crucially to do that, what we're
going to end up doing is using those two

00:07:28.290 --> 00:07:31.010
feature expectations
that I defined earlier.

00:07:33.920 --> 00:07:35.460
And so using those two things,

00:07:35.460 --> 00:07:38.559
we'll then be able to work out
these optimum feature weights.

00:07:41.050 --> 00:07:46.160
Okay so I hope that's given you a good
sense of what features in discriminate

00:07:46.160 --> 00:07:50.600
models and at least some idea of
how to go about defining them or

00:07:50.600 --> 00:07:55.255
classify in a practical Context, though
that question of what kinds of features

00:07:55.255 --> 00:07:59.465
are good is something we'll come back to
later when looking at particular problems.

00:07:59.465 --> 00:08:02.285
After we've worked through more
of the math and properties and

00:08:02.285 --> 00:08:03.565
maximum entropy models.

