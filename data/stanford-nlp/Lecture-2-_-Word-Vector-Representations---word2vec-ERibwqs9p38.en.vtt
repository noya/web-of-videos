WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.314
[MUSIC]

00:00:06.314 --> 00:00:07.713
Stanford University.

00:00:11.743 --> 00:00:14.257
&gt;&gt; Okay, so let's get going.

00:00:14.257 --> 00:00:18.609
Welcome back to the second
class of CS224N /Ling 284,

00:00:18.609 --> 00:00:22.570
Natural Language Processing
with Deep Learning.

00:00:22.570 --> 00:00:28.115
So this class is gonna be almost
the complete opposite of the last class.

00:00:28.115 --> 00:00:29.837
So in the last class,

00:00:29.837 --> 00:00:35.600
it was a very high level picture of
sort of trying from the very top down.

00:00:35.600 --> 00:00:39.462
Sort of say a little bit about what
is natural language processing,

00:00:39.462 --> 00:00:43.866
what is deep learning, why it's exciting,
why both of them are exciting and

00:00:43.866 --> 00:00:46.220
how I'd like to put them together?

00:00:46.220 --> 00:00:50.687
So for today's class we're gonna go
completely to the opposite extreme.

00:00:50.687 --> 00:00:54.324
We're gonna go right down
to the bottom of words,

00:00:54.324 --> 00:00:58.928
and we're gonna have vectors,
and we're gonna do baby math.

00:00:58.928 --> 00:01:04.965
Now for some of you this will seem
like tedious repetitive baby math.

00:01:04.965 --> 00:01:08.479
But I think that there are probably
quite a few of you for

00:01:08.479 --> 00:01:12.157
which having some math review
is just going to be useful.

00:01:12.157 --> 00:01:17.200
And this is really the sort of foundation
on which everything else builds.

00:01:17.200 --> 00:01:21.853
And so if you don't have sort of straight
the fundamentals right at the beginning of

00:01:21.853 --> 00:01:26.306
how you can use neural networks on the
sort of very simplest kind of structures,

00:01:26.306 --> 00:01:28.652
it's sort of really all over from there.

00:01:28.652 --> 00:01:32.599
So what I'd like to do today is
sort of really go slowly and

00:01:32.599 --> 00:01:38.034
carefully through the foundations of how
you can start to do things with neural

00:01:38.034 --> 00:01:43.250
networks in this very simple case of
learning representations for words.

00:01:43.250 --> 00:01:47.850
And hope that's kind of a good foundation
that we can build on forwards.

00:01:47.850 --> 00:01:51.060
And indeed that's what we're gonna
keep on doing, building forward.

00:01:51.060 --> 00:01:56.188
So next week Richard is gonna keep on
doing a lot of math from the ground up to

00:01:56.188 --> 00:02:01.593
try, and really help get straight some
of the foundations of Deep Learning.

00:02:01.593 --> 00:02:05.620
Okay, so this is basically the plan.

00:02:05.620 --> 00:02:11.463
So tiny bit word meaning and

00:02:11.463 --> 00:02:14.261
no, [LAUGH].

00:02:14.261 --> 00:02:19.793
&gt;&gt; [LAUGH]
&gt;&gt; Tiny bit on word meaning then start to

00:02:19.793 --> 00:02:25.160
introduce this model of learning
word vectors called Word2vec.

00:02:25.160 --> 00:02:29.196
And this was a model that was
introduced by Thomas Mikolov and

00:02:29.196 --> 00:02:31.582
colleagues at Google in 2013.

00:02:31.582 --> 00:02:35.278
And so there are many other
ways that you could think about

00:02:35.278 --> 00:02:37.815
having representations of words.

00:02:37.815 --> 00:02:41.417
And next week, Richard's gonna talk
about some of those other mechanisms.

00:02:41.417 --> 00:02:45.005
But today, I wanna sort of avoid
having a lot of background and

00:02:45.005 --> 00:02:46.810
comparative commentary.

00:02:46.810 --> 00:02:50.305
So I'm just gonna present
this one way of doing it.

00:02:50.305 --> 00:02:52.948
And you'd also pretty study
the good way of doing it, so

00:02:52.948 --> 00:02:55.220
it's not a bad one to know.

00:02:55.220 --> 00:02:58.663
Okay, so then after that,
we're gonna have the first or

00:02:58.663 --> 00:03:01.614
was it gonna be one of
the features of this class.

00:03:01.614 --> 00:03:07.811
We decided that all the evidence says that
students can't concentrate for 75 minutes.

00:03:07.811 --> 00:03:11.971
So we decided we'd sort of mix
it up a little, and hopefully,

00:03:11.971 --> 00:03:16.932
also give people an opportunity to sort
of get more of a sense of what some of

00:03:16.932 --> 00:03:22.100
the exciting new work that's coming
out every month in Deep Learning is.

00:03:22.100 --> 00:03:27.730
And so what we're gonna do is have one TA
each time, do a little research highlight.

00:03:27.730 --> 00:03:30.903
Which will just be sort of a like
a verbal blog post of telling

00:03:30.903 --> 00:03:35.163
you a little bit about some recent paper
and why it's interesting, exciting.

00:03:35.163 --> 00:03:38.334
We're gonna start that today with Danqi.

00:03:38.334 --> 00:03:39.343
Then after that,

00:03:39.343 --> 00:03:44.066
I wanna go sort of carefully through the
word to vec objective function gradients.

00:03:44.066 --> 00:03:47.102
Refresher little on optimization,
mention the assignment,

00:03:47.102 --> 00:03:51.180
tell you all about Word2vec
that's basically the plan, okay?

00:03:51.180 --> 00:03:55.528
So we kinda wonder sort
of have word vectors as I

00:03:55.528 --> 00:03:59.995
mentioned last time as
a model of word meaning.

00:03:59.995 --> 00:04:02.525
That's a pretty
controversial idea actually.

00:04:02.525 --> 00:04:07.335
And I just wanna give kind of a few words
of context before we dive into that and

00:04:07.335 --> 00:04:08.650
do it anyway.

00:04:08.650 --> 00:04:12.795
Okay, so
if you look up meaning in a dictionary cuz

00:04:12.795 --> 00:04:17.548
a dictionary is a storehouse
of word meanings after all.

00:04:17.548 --> 00:04:21.922
What the Webster's dictionary says is
meaning is the idea that is represented by

00:04:21.922 --> 00:04:24.020
a word, phrase, etc.

00:04:24.020 --> 00:04:29.448
The idea that a person wants to express
by using words, signs, etc, etc.

00:04:30.660 --> 00:04:34.210
In some sense, this is fairly close

00:04:34.210 --> 00:04:38.260
to what is the commonest linguistic
way of thinking of meaning.

00:04:38.260 --> 00:04:45.058
So standardly in linguistics,
you have a linguistic sign like a word,

00:04:45.058 --> 00:04:50.176
and then it has things that
it signifies in the world.

00:04:50.176 --> 00:04:55.651
So if I have a word like glasses then
it's got a signification which includes

00:04:55.651 --> 00:05:01.487
these and there are lots of other pairs of
glasses I can see in front of me, right?

00:05:01.487 --> 00:05:05.533
And those things that it signifies,

00:05:05.533 --> 00:05:09.585
the denotation of the term glasses.

00:05:09.585 --> 00:05:15.352
That hasn't proven to be a notion of
meaning that's been very easy for people

00:05:15.352 --> 00:05:21.260
to make much use of in computational
systems for dealing with language.

00:05:21.260 --> 00:05:25.730
So in practice, if you look at what
computational systems have done for

00:05:25.730 --> 00:05:29.310
meanings of words over
the last several decades.

00:05:29.310 --> 00:05:33.760
By far the most common thing that's
happened is, people have tried

00:05:33.760 --> 00:05:38.600
to deal with the meaning of words by
making use of taxonomic resources.

00:05:38.600 --> 00:05:42.800
And so if they're English, the most
famous taxonomic resource is WordNet.

00:05:42.800 --> 00:05:46.481
And it's famous,
maybe not like Websters is famous.

00:05:46.481 --> 00:05:48.629
But it's famous among
computational linguists.

00:05:48.629 --> 00:05:51.160
Because it's free to download a copy and

00:05:51.160 --> 00:05:55.713
that's much more useful than having
a copy of Webster's on your shelf.

00:05:55.713 --> 00:06:00.180
And it provides a lot of
taxonomy information about words.

00:06:01.500 --> 00:06:03.490
So this little bit of Python code.

00:06:03.490 --> 00:06:07.892
This is showing you getting a hold of
word net using the nltk which is one of

00:06:07.892 --> 00:06:10.240
the main Python packages for nlp.

00:06:10.240 --> 00:06:13.801
And so then I'm asking it for
the word panda,

00:06:13.801 --> 00:06:17.568
not the Python package Panda,
the word panda.

00:06:17.568 --> 00:06:18.648
Then I'm saying,

00:06:18.648 --> 00:06:23.190
well tell me about the hypernym the kind
of things that it's the kind of.

00:06:23.190 --> 00:06:28.003
And so for Panda it's sort of heading
up through carnivores, placentals,

00:06:28.003 --> 00:06:31.399
mammals up into sort of
abstract types like objects.

00:06:31.399 --> 00:06:34.412
Or on the right hand side,
I'm sort of asking for

00:06:34.412 --> 00:06:38.130
the word good,
will tell me about synonyms of good.

00:06:38.130 --> 00:06:42.890
And part of what your finding there is,
well WordNet is saying,

00:06:42.890 --> 00:06:45.987
well the word good has different senses.

00:06:45.987 --> 00:06:49.870
So for each sense, let me tell
you some synonyms for each sense.

00:06:49.870 --> 00:06:54.460
So one sense, the second one is sort
of the kind of good person sense.

00:06:54.460 --> 00:06:58.928
And they're suggesting synonyms
like honorable and respectable.

00:06:58.928 --> 00:07:04.087
But there are other ones here
where this pair is good to eat and

00:07:04.087 --> 00:07:06.930
that's sort of meaning is ripe.

00:07:08.750 --> 00:07:11.280
Okay, so
you get this sort of sense of meaning.

00:07:12.402 --> 00:07:16.043
That's been,
that's been a great resource, but

00:07:16.043 --> 00:07:20.437
it's also been a resource that
people have found in practice.

00:07:20.437 --> 00:07:26.030
It's hard to get nearly as much value out
of it as you'd like to get out of it.

00:07:26.030 --> 00:07:27.939
And why is that?

00:07:27.939 --> 00:07:30.210
I mean there are a whole bunch of reasons.

00:07:30.210 --> 00:07:35.534
I mean one reason is that at this level
of this sort of taxonomic relationships,

00:07:35.534 --> 00:07:38.161
you lose an enormous amount of nuance.

00:07:38.161 --> 00:07:43.146
So one of those synonym sets for
good was adept, expert, good, practiced,

00:07:43.146 --> 00:07:44.844
proficient, skillful.

00:07:44.844 --> 00:07:49.458
But I mean, it seems like those mean
really different things, right?

00:07:49.458 --> 00:07:52.820
It seems like saying I'm
an expert at deep learning.

00:07:53.965 --> 00:07:59.510
Means something slightly different
to I'm good at deep learning.

00:07:59.510 --> 00:08:01.538
So there's a lot of nuance there.

00:08:01.538 --> 00:08:06.018
There's a lot of incompleteness in WordNet
so for a lot of the ways that people,

00:08:06.018 --> 00:08:10.797
Use words more flexibly.

00:08:10.797 --> 00:08:14.166
So if I say I'm a deep-learning ninja, or

00:08:14.166 --> 00:08:18.765
something like that,
that that's not in WordNet at all.

00:08:18.765 --> 00:08:23.438
What kind of things you put into these
synonym sets ends up very subjective,

00:08:23.438 --> 00:08:23.956
right?

00:08:23.956 --> 00:08:27.238
Which sense distinctions you make and
which things you do and

00:08:27.238 --> 00:08:29.950
don't say are the same,
it's all very unclear.

00:08:29.950 --> 00:08:32.540
It requires,
even to the extent that it's made,

00:08:32.540 --> 00:08:37.050
it's required many person
years of human labor.

00:08:37.050 --> 00:08:43.030
And at the end of the day,
it's sort of, it's kind

00:08:43.030 --> 00:08:47.750
of hard to get anything accurate out of it
in the way of sort of word similarities.

00:08:47.750 --> 00:08:53.830
Like I kind of feel that proficient is
more similar to expert than good, maybe.

00:08:53.830 --> 00:08:57.085
But you can't get any of this
kind of stuff out of WordNet.

00:08:58.330 --> 00:09:03.740
Okay, so therefore,
that's sort of something of a problem.

00:09:03.740 --> 00:09:08.610
And it's part of this
general problem of discrete,

00:09:08.610 --> 00:09:13.190
or categorical, representations
that I started on last time.

00:09:13.190 --> 00:09:16.660
So, the fundamental thing
to note is that for

00:09:16.660 --> 00:09:21.930
sorta just about all NLP,
apart from both modern deep learning and

00:09:21.930 --> 00:09:25.670
a little bit of neural net work
NLP that got done in the 1980s,

00:09:25.670 --> 00:09:31.560
that it's all used atomic symbols
like hotel, conference, walk.

00:09:31.560 --> 00:09:36.720
And if we think of that from our kind
of jaundiced neural net direction,

00:09:36.720 --> 00:09:40.500
using atomic symbols is kind of like using

00:09:40.500 --> 00:09:45.100
big vectors that are zero everywhere
apart from a one and one position.

00:09:45.100 --> 00:09:49.080
So what we have, is we have a lot of words
in the language that are equivalent to our

00:09:49.080 --> 00:09:53.060
symbols and we're putting a one
in the position, in the vector,

00:09:53.060 --> 00:09:55.935
that represents the particular symbol,
perhaps hotel.

00:09:55.935 --> 00:09:59.220
And these vectors are going to be really,
really long.

00:09:59.220 --> 00:10:01.400
I mean,
how long depends on how you look at it.

00:10:01.400 --> 00:10:05.500
So sometimes a speech recognizer
might have a 20,000 word vocabulary.

00:10:05.500 --> 00:10:07.250
So it'd be that long.

00:10:07.250 --> 00:10:10.910
But, if we're kinda building
a machine translation system,

00:10:10.910 --> 00:10:16.000
we might use a 500,000 word vocabulary,
so that's very long.

00:10:16.000 --> 00:10:21.130
And Google released sort of
a 1-terabyte corpus of web crawl.

00:10:21.130 --> 00:10:24.470
That's a resource that's been
widely used for a lot of NLP.

00:10:24.470 --> 00:10:27.900
And while the size of the vocabulary
in that is 13 million words, so

00:10:27.900 --> 00:10:29.690
that's really, really long.

00:10:29.690 --> 00:10:33.968
So, it's a very, very big vector.

00:10:33.968 --> 00:10:37.730
And so, why are these vectors problematic?

00:10:37.730 --> 00:10:43.150
I'm sorry, I'm not remembering my slides,
so I should say my slides first.

00:10:43.150 --> 00:10:47.890
Okay, so this is referred to in
neural net land as one-hot in coding

00:10:47.890 --> 00:10:51.647
because there's just this
one on zero in the vector.

00:10:51.647 --> 00:10:55.580
And so, that's the example of
a localist representation.

00:10:57.000 --> 00:10:59.270
So why is this problematic?

00:10:59.270 --> 00:11:04.000
And the reason why it's
problematic is it doesn't give any

00:11:04.000 --> 00:11:07.870
inherent notion of
relationships between words.

00:11:07.870 --> 00:11:12.390
So, very commonly what we want
to know is when meanings and

00:11:12.390 --> 00:11:15.106
words and
phrases are similar to each other.

00:11:15.106 --> 00:11:19.520
So, for example, in a web search
application, if the user searches for

00:11:19.520 --> 00:11:21.670
Dell notebook battery size,

00:11:21.670 --> 00:11:26.120
we'd like to match a document that
says Dell laptop battery capacity.

00:11:26.120 --> 00:11:29.650
So we sort of want to know that
notebooks and laptops are similar,

00:11:29.650 --> 00:11:33.750
and size and capacity are similar,
so this will be equivalent.

00:11:33.750 --> 00:11:37.480
We want to know that hotels and
motels are similar in meaning.

00:11:37.480 --> 00:11:42.397
And the problem is that if we're
using one-hot vector encodings,

00:11:42.397 --> 00:11:45.600
they have no natural notion of similarity.

00:11:45.600 --> 00:11:48.075
So if we take these two vectors and say,

00:11:48.075 --> 00:11:51.909
what is the dot product between
those vectors, it's zero.

00:11:51.909 --> 00:11:56.050
They have no inherent
notion of similarity.

00:11:56.050 --> 00:12:00.888
And, something I just wanna stress
a little, since this is important,

00:12:00.888 --> 00:12:05.806
is note this problem of symbolic
encoding applies not only to traditional

00:12:05.806 --> 00:12:10.329
rule base logical approaches to
natural language processing, but

00:12:10.329 --> 00:12:15.246
it also applies to basically all of
the work that was done in probabilistic

00:12:15.246 --> 00:12:20.790
statistical conventional machine learning
base natural language processing.

00:12:20.790 --> 00:12:25.498
Although those Latin models normally had
real numbers, they had probabilities of

00:12:25.498 --> 00:12:29.871
something occurring in the context of
something else, that nevertheless,

00:12:29.871 --> 00:12:32.960
they were built over
symbolic representations.

00:12:32.960 --> 00:12:37.900
So that you weren't having any kind of
capturing relationships between words and

00:12:37.900 --> 00:12:42.190
the models,
each word was a nation to itself.

00:12:42.190 --> 00:12:46.200
Okay, so that's bad, and
we have to do something about it.

00:12:46.200 --> 00:12:51.316
Now, as I've said, there's more than
one thing that you could do about it.

00:12:53.451 --> 00:12:56.008
And so, one answer is to say, okay gee,

00:12:56.008 --> 00:12:59.715
we need to have a similarity
relationship between words.

00:12:59.715 --> 00:13:00.695
Let's go over here and

00:13:00.695 --> 00:13:05.165
start building completely separately
a similarity relationship between words.

00:13:05.165 --> 00:13:06.782
And, of course, you could do that.

00:13:06.782 --> 00:13:09.542
But I'm not gonna talk about that here.

00:13:09.542 --> 00:13:14.662
What instead I'm going to talk about and
suggest is that

00:13:14.662 --> 00:13:19.932
what we could do is we could
explore this direct approach,

00:13:19.932 --> 00:13:25.172
where the representation of
a word encodes its meaning

00:13:25.172 --> 00:13:29.860
in such a way that you can
just directly read off

00:13:29.860 --> 00:13:34.220
from these representations,
the similarity between words.

00:13:34.220 --> 00:13:36.300
So what we're gonna do is
have these vectors and

00:13:36.300 --> 00:13:38.720
do something like a dot product.

00:13:38.720 --> 00:13:42.620
And that will be giving us a sense
of the similarity between words.

00:13:44.050 --> 00:13:46.850
Okay, so how do we go about doing that?

00:13:46.850 --> 00:13:51.930
And so the way we gonna go about
doing that is by making use of this

00:13:53.320 --> 00:13:57.350
very simple, but
extremely profound and widely used,

00:13:57.350 --> 00:14:00.930
NLP idea called distributional similarity.

00:14:00.930 --> 00:14:03.630
So this has been a really powerful notion.

00:14:03.630 --> 00:14:09.460
So the notion of distributional similarity
is that you can get a lot of value for

00:14:09.460 --> 00:14:14.040
representing the meaning of a word
by looking at the context in

00:14:14.040 --> 00:14:18.130
which it appears and
doing something with those contexts.

00:14:19.860 --> 00:14:22.950
So, if I want to know what
the word banking means,

00:14:22.950 --> 00:14:28.030
what I'm gonna do is find thousands of
instances of the word banking in text and

00:14:28.030 --> 00:14:31.850
I'm gonna look at the environment
in which each one appeared.

00:14:31.850 --> 00:14:35.250
And I'm gonna see debt problems,
governments,

00:14:35.250 --> 00:14:39.240
regulation, Europe, saying unified and

00:14:39.240 --> 00:14:43.590
I'm gonna start counting up all of these
things that appear and by some means,

00:14:43.590 --> 00:14:48.470
I'll use those words in the context
to represent the meaning of banking.

00:14:49.630 --> 00:14:55.280
The most famous slogan that you
will read everywhere if you look

00:14:55.280 --> 00:15:01.380
into distributional similarity is this one
by JR Firth, who was a British linguist,

00:15:01.380 --> 00:15:06.358
who said, you shall know a word
by the company it keeps.

00:15:06.358 --> 00:15:11.825
But this is also really exactly the same
notion that Wittgenstein proposed in

00:15:11.825 --> 00:15:17.035
his later writings where he
suggested a use theory of meaning.

00:15:17.035 --> 00:15:21.840
Where, somewhat controversially,
this not the main stream in semantics,

00:15:21.840 --> 00:15:26.980
he suggested that the right way to
think about the meaning of words is

00:15:26.980 --> 00:15:29.120
understanding their uses in text.

00:15:29.120 --> 00:15:33.570
So, essentially,
if you could predict which textual context

00:15:33.570 --> 00:15:37.490
the word would appear in, then you
understand the meaning of the word.

00:15:38.680 --> 00:15:41.110
Okay, so that's what we're going to do.

00:15:41.110 --> 00:15:46.348
So what we want to do is say for
each word we're going to come up for

00:15:46.348 --> 00:15:52.850
it a vector and that dense vector
is gonna be chosen so that

00:15:52.850 --> 00:15:58.690
it'll be good at predicting other words
that appear in the context of this word.

00:15:58.690 --> 00:15:59.770
Well how do we do that?

00:15:59.770 --> 00:16:03.918
Well, each of those other words will also
have a word that are attached to them and

00:16:03.918 --> 00:16:07.090
then we'll be looking at sort
of similarity measures like dot

00:16:07.090 --> 00:16:08.981
product between those two vectors.

00:16:08.981 --> 00:16:11.867
And we're gonna change
them as well to make it so

00:16:11.867 --> 00:16:14.330
that good at being able to be predicted.

00:16:14.330 --> 00:16:17.881
So it all kind off gets a little
bit recursive or circular, but

00:16:17.881 --> 00:16:21.431
we're gonna come up with this
clever algorithm to do that, so

00:16:21.431 --> 00:16:25.650
that words will be able to predict
their context words and vice-versa.

00:16:25.650 --> 00:16:30.880
And so I'm gonna go on and
say a little bit more about that.

00:16:30.880 --> 00:16:34.480
But let me just underline one bit

00:16:34.480 --> 00:16:39.220
of terminology that was
appearing before in the slide.

00:16:39.220 --> 00:16:42.359
So we saw two keywords.

00:16:42.359 --> 00:16:47.710
One was distributional, which was here.

00:16:47.710 --> 00:16:51.880
And then we've had
distributed representations

00:16:51.880 --> 00:16:55.440
where we have these dense vectors to
represent the meaning of the words.

00:16:55.440 --> 00:16:59.810
Now people tend to
confuse those two words.

00:16:59.810 --> 00:17:02.950
And there's sort of two
reasons they confuse them.

00:17:02.950 --> 00:17:08.190
One is because they both start with
distribute and so they're kind of similar.

00:17:08.190 --> 00:17:16.388
And the second reason people confuse them
is because they very strongly co-occur,.

00:17:16.388 --> 00:17:22.070
So that distributed representations and
meaning have almost always,

00:17:22.070 --> 00:17:26.160
up until now, been built by
using distributional similarity.

00:17:26.160 --> 00:17:31.520
But I did just want people to gather
that these are different notions, right?

00:17:31.520 --> 00:17:37.420
So the idea of distributional similarity
is a theory about semantics of word

00:17:37.420 --> 00:17:42.640
meaning that you can describe the meaning
of words by as a use theory of meaning,

00:17:42.640 --> 00:17:45.380
understanding the context
in which they appear.

00:17:45.380 --> 00:17:50.157
So distributional contrasts with,
way back here when I said but

00:17:50.157 --> 00:17:53.967
didn't really explain,
denotational, right?

00:17:53.967 --> 00:17:57.634
The denotational idea of
word meaning is the meaning

00:17:57.634 --> 00:18:02.310
of glasses is the set of pairs of
glasses that are around the place.

00:18:02.310 --> 00:18:05.160
That's different from
distributional meaning.

00:18:05.160 --> 00:18:10.752
And distributed then contrasts
with our one-hot word vector.

00:18:10.752 --> 00:18:15.740
So the one-hot word vectors are localist
representation where you're storing in

00:18:15.740 --> 00:18:16.460
one place.

00:18:16.460 --> 00:18:19.000
You're saying here is the symbol glasses.

00:18:19.000 --> 00:18:23.610
It's stored right here whereas
in distributed representations

00:18:23.610 --> 00:18:27.058
we're smearing the meaning of
something over a large vector space.

00:18:27.058 --> 00:18:32.950
Okay, so that's part one.

00:18:32.950 --> 00:18:38.534
And we're now gonna sorta be heading into
part two, which is what is Word2vec?

00:18:38.534 --> 00:18:42.570
Okay, and so
I'll go almost straight into this.

00:18:42.570 --> 00:18:47.176
But this is sort of the recipe in
general for what we're doing for

00:18:47.176 --> 00:18:49.990
learning neural word embeddings.

00:18:49.990 --> 00:18:55.000
So we're gonna define a model
that aims to predict between

00:18:55.000 --> 00:19:00.010
a center word and
words that appear in it's context.

00:19:00.010 --> 00:19:03.460
Kind of like we are here,
the distributional wording.

00:19:03.460 --> 00:19:06.972
And we'll sort of have some,
perhaps probability measure or

00:19:06.972 --> 00:19:10.292
predicts the probability of
the context given the words.

00:19:10.292 --> 00:19:14.811
And then once we have that we can
have a loss function as to whether

00:19:14.811 --> 00:19:17.200
we do that prediction well.

00:19:17.200 --> 00:19:21.950
So ideally we'd be able to perfectly
predict the words around the word so

00:19:21.950 --> 00:19:27.260
the minus t means the words that aren't
word index t so the words around t.

00:19:27.260 --> 00:19:31.260
If we could predict those perfectly
from t we'd have probability one so

00:19:31.260 --> 00:19:34.350
we'd have no loss but
normally we can't do that.

00:19:34.350 --> 00:19:37.604
And if we give them probability a quarter
then we'll have sort of three quarters

00:19:37.604 --> 00:19:38.780
loss or something, right?

00:19:38.780 --> 00:19:40.677
So we'll have a loss function and

00:19:40.677 --> 00:19:44.270
we'll sort of do that in many
positions in a large corpus.

00:19:44.270 --> 00:19:49.270
And so our goal will be to change
the representations of words so

00:19:49.270 --> 00:19:51.720
as to minimize our loss.

00:19:51.720 --> 00:19:55.670
And at this point sort
of a miracle occurs.

00:19:55.670 --> 00:20:00.690
It's sort of surprising, but
true that you can do no more

00:20:01.830 --> 00:20:05.130
than set up this kind of
prediction objective.

00:20:05.130 --> 00:20:10.370
Make it the job of every words word
vectors to be such that they're

00:20:10.370 --> 00:20:15.680
good at predicting their words that
appear in their context or vice versa.

00:20:15.680 --> 00:20:20.778
You just have that very simple goal and
you say nothing else about how this is

00:20:20.778 --> 00:20:26.880
gonna be achieved, but you just pray and
depend on the magic of deep learning.

00:20:26.880 --> 00:20:28.825
And this miracle happens and

00:20:28.825 --> 00:20:33.490
outcome these word vectors that
are just amazingly powerful

00:20:33.490 --> 00:20:38.480
at representing the meaning of words and
are useful for all sorts of things.

00:20:38.480 --> 00:20:43.698
And so that's where we want to get into
more detail and say how that happens.

00:20:43.698 --> 00:20:44.577
Okay.

00:20:48.986 --> 00:20:54.065
So that representation was
meant to be meaning all words

00:20:54.065 --> 00:20:59.090
apart from the wt, yes,
what is this w minus t mean?

00:20:59.090 --> 00:21:01.710
I'm actually not gonna use that
notation again in this lecture.

00:21:01.710 --> 00:21:06.560
But the w minus t, minus is sometimes
used to mean everything except t.

00:21:06.560 --> 00:21:12.470
So wt is my focus word, and w minus
t is in all the words in the context.

00:21:14.615 --> 00:21:19.009
Okay, so this idea that you can
learn low dimensional vector

00:21:19.009 --> 00:21:23.975
representations is an idea that
has a history in neural networks.

00:21:23.975 --> 00:21:26.165
It was certainly present in the 1980s,

00:21:26.165 --> 00:21:30.800
parallel distributed processing
era including work by

00:21:30.800 --> 00:21:34.760
Rumelhart on learning representations
by back-propagating errors.

00:21:34.760 --> 00:21:42.080
It really was demonstrated for
word representations in this pioneering

00:21:42.080 --> 00:21:47.660
early paper by Yoshua Bengio in 2003 and
neural probabilistic language model.

00:21:47.660 --> 00:21:52.870
I mean, at the time, sort of not so many
people actually paid attention to this

00:21:52.870 --> 00:21:58.260
paper, this was sort of before
the deep learning boom started.

00:21:58.260 --> 00:22:03.390
But really this was the paper
where the sort of showed

00:22:03.390 --> 00:22:08.800
how much value you could get from having
distributed representations of words and

00:22:08.800 --> 00:22:11.800
be able to predict other words in context.

00:22:11.800 --> 00:22:17.338
But then as things started to take off
that idea was sort of built on and

00:22:17.338 --> 00:22:18.207
revived.

00:22:18.207 --> 00:22:23.504
So in 2008, Collobert and Weston started
in the sort of modern direction by saying,

00:22:23.504 --> 00:22:27.809
well, if we just want good word
representations, we don't even have to

00:22:27.809 --> 00:22:31.906
necessarily make a probabilistic
language model that can predict,

00:22:31.906 --> 00:22:35.756
we just need to have a way of
learning our word representations.

00:22:35.756 --> 00:22:40.054
And that's something that's then being
continued in the model that I'm gonna look

00:22:40.054 --> 00:22:42.040
at now, the word2vec model.

00:22:42.040 --> 00:22:47.195
That the emphasis of the word2vec model
was how can we build a very simple,

00:22:47.195 --> 00:22:53.550
scalable, fast to train model
that we can run over billions

00:22:53.550 --> 00:22:57.700
of words of text that will produce
exceedingly good word representations.

00:23:00.190 --> 00:23:03.130
Okay, word2vec, here we come.

00:23:03.130 --> 00:23:08.440
The basic thing word2vec is trying
to do is use theory of meaning,

00:23:08.440 --> 00:23:12.600
predict between every word and
its context words.

00:23:12.600 --> 00:23:15.450
Now word2vec is a piece of software,
I mean,

00:23:15.450 --> 00:23:19.350
actually inside word2vec it's kind
of a sort of a family of things.

00:23:19.350 --> 00:23:24.230
So there are two algorithms inside it for
producing word vectors and

00:23:24.230 --> 00:23:28.240
there are two moderately
efficient training methods.

00:23:28.240 --> 00:23:32.310
So for this class what I'm
going to do is tell you about

00:23:32.310 --> 00:23:36.480
one of the algorithms which
is a skip-gram method and

00:23:36.480 --> 00:23:40.100
about neither of the moderately
efficient training algorithms.

00:23:40.100 --> 00:23:43.920
Instead I'm gonna tell you about
the hopelessly inefficient training

00:23:43.920 --> 00:23:48.990
algorithm but is sort of the conceptual
basis of how this is meant to work and

00:23:48.990 --> 00:23:52.640
that the moderately efficient ones,
which I'll mention at the end.

00:23:52.640 --> 00:23:55.920
And then what you'll have to do to
actually make this a scalable process

00:23:55.920 --> 00:23:57.500
that you can run fast.

00:23:57.500 --> 00:24:01.300
And then, today is also the day
when we're handing out assignment

00:24:02.400 --> 00:24:07.289
one and Major part of what you
guys get to do in assignment

00:24:07.289 --> 00:24:11.682
one is to implement one of
the efficient training algorithms, and

00:24:11.682 --> 00:24:16.481
to work through the method one of
those efficient training algorithms.

00:24:16.481 --> 00:24:19.770
So this is the picture
of the skip-gram model.

00:24:19.770 --> 00:24:24.130
So the idea of the skip-gram model is for

00:24:24.130 --> 00:24:30.130
each estimation step,
you're taking one word as the center word.

00:24:30.130 --> 00:24:36.100
So that's here, is my word banking and
then what you're going to do

00:24:36.100 --> 00:24:42.160
is you're going to try and predict words
in its context out to some window size.

00:24:42.160 --> 00:24:46.880
And so, the model is going to define
a probability distribution that

00:24:46.880 --> 00:24:52.470
is the probability of a word appearing
in the context given this center word.

00:24:52.470 --> 00:24:57.106
And we're going to choose vector
representations of words so

00:24:57.106 --> 00:25:01.395
we can try and
maximize that probability distribution.

00:25:01.395 --> 00:25:04.085
And the thing that we'll come back to.

00:25:04.085 --> 00:25:09.225
But it's important to realize is there's
only one probability distribution,

00:25:09.225 --> 00:25:10.175
this model.

00:25:10.175 --> 00:25:12.625
It's not that there's
a probability distribution for

00:25:12.625 --> 00:25:16.190
the word one to the left and the word
one to the right, and things like that.

00:25:16.190 --> 00:25:20.493
We just have one probability
distribution of a context word,

00:25:20.493 --> 00:25:24.713
which we'll refer to as the output,
because it's what we,

00:25:24.713 --> 00:25:29.866
produces the output, occurring in
the context close to the center word.

00:25:29.866 --> 00:25:32.318
Is that clear?

00:25:32.318 --> 00:25:35.530
Yeah, okay.

00:25:37.070 --> 00:25:42.420
So that's what we kinda wanna do so
we're gonna have a radius m and

00:25:42.420 --> 00:25:47.390
then we're going to predict
the surrounding words from sort of

00:25:47.390 --> 00:25:52.410
positions m before our center
word to m after our center word.

00:25:52.410 --> 00:25:56.540
And we're gonna do that a whole bunch
of times in a whole bunch of places.

00:25:56.540 --> 00:26:00.620
And we want to choose word

00:26:00.620 --> 00:26:05.870
vectors such as that we're maximizing
the probability of that prediction.

00:26:05.870 --> 00:26:14.080
So what our loss function or objective
function is is really this J prime here.

00:26:14.080 --> 00:26:18.980
So the J prime is saying we're going to,
so we're going to take a big

00:26:18.980 --> 00:26:23.730
long amount of text, we take the whole
of Wikipedia or something like that so

00:26:23.730 --> 00:26:28.240
we got big long sequence of words, so
there are words in the context and

00:26:28.240 --> 00:26:33.140
real running text, and we're going to
go through each position in the text.

00:26:33.140 --> 00:26:37.340
And then, for each position in the text,
we're going to have a window

00:26:37.340 --> 00:26:41.720
of size 2m around it,
m words before and m words after it.

00:26:41.720 --> 00:26:47.180
And we're going to have a probability
distribution that will give a probability

00:26:47.180 --> 00:26:51.400
to a word appearing in
the context of the center word.

00:26:52.400 --> 00:26:57.560
And what we'd like to do is set
the parameters of our model so

00:26:57.560 --> 00:27:00.170
that these probabilities
of the words that actually

00:27:00.170 --> 00:27:04.100
do appear in the context of the center
word are as high as possible.

00:27:05.625 --> 00:27:10.895
So the parameters in this model of these
theta here that I show here and here.

00:27:10.895 --> 00:27:13.140
After this slide,
I kinda drop the theta over here.

00:27:13.140 --> 00:27:17.055
But you can just assumed
that there is this theta.

00:27:17.055 --> 00:27:17.555
What is this theta?

00:27:17.555 --> 00:27:20.335
What is theta is?

00:27:20.335 --> 00:27:24.455
It's going to be the vector
representation of the words.

00:27:24.455 --> 00:27:29.770
The only parameters in this model of
the vector representations of each word.

00:27:29.770 --> 00:27:34.024
There are no other parameters whatsoever
in this model as you'll see pretty

00:27:34.024 --> 00:27:34.630
quickly.

00:27:34.630 --> 00:27:39.670
So conceptually this is
our objective function.

00:27:39.670 --> 00:27:45.000
We wanna maximize the probability
of this predictions.

00:27:45.000 --> 00:27:47.950
In practice, we just slightly tweak that.

00:27:47.950 --> 00:27:52.328
Firstly, almost unbearably when
we're working with probabilities and

00:27:52.328 --> 00:27:57.059
we want to do maximization, we actually
turn things into log probabilities cuz

00:27:57.059 --> 00:27:59.458
then all that products turn into sums and

00:27:59.458 --> 00:28:04.140
our math gets a lot easier to work with
and so that's what I've done down here.

00:28:09.405 --> 00:28:10.611
Good points.

00:28:10.611 --> 00:28:14.115
And the question is, hey, wait a minute
you're cheating, windows size,

00:28:14.115 --> 00:28:16.070
isn't that a parameter of the model?

00:28:16.070 --> 00:28:18.600
And you are right,
this is the parameter of the model.

00:28:18.600 --> 00:28:22.320
So I guess I was a bit loose there.

00:28:22.320 --> 00:28:26.100
Actually, it turns out that there are
several hyper parameters of the model, so

00:28:26.100 --> 00:28:26.720
I did cheat.

00:28:26.720 --> 00:28:31.690
It turns out that there are a few
hyper parameters of the model.

00:28:31.690 --> 00:28:35.340
One is Windows sized and it turns out
that we'll come across a couple of

00:28:35.340 --> 00:28:37.750
other fudge factors later in the lecture.

00:28:37.750 --> 00:28:42.110
And all of those things are hyper
parameters that you could adjust.

00:28:42.110 --> 00:28:43.920
But let's just ignore those for
the moment,

00:28:43.920 --> 00:28:46.370
let's just assume those are constant.

00:28:46.370 --> 00:28:49.150
And given those things
aren't being adjusted,

00:28:49.150 --> 00:28:54.990
the only parameters in the model,
the factor representations of the words.

00:28:54.990 --> 00:28:59.401
What I'm meaning is that there's
sort of no other probability

00:28:59.401 --> 00:29:02.239
distribution with its own parameters.

00:29:02.239 --> 00:29:03.370
That's a good point.

00:29:03.370 --> 00:29:05.221
I buy that one.

00:29:05.221 --> 00:29:11.349
So we've gone to the log probability and
the sums now and,

00:29:11.349 --> 00:29:18.230
and then rather than having
the probability of the whole corpus,

00:29:18.230 --> 00:29:26.155
we can sort of take the average over
each positions so I've got 1 on T here.

00:29:26.155 --> 00:29:32.850
And that's just sort of a making it per
word as sort of a kinda normalization.

00:29:32.850 --> 00:29:35.280
So that doesn't affect what's the maximum.

00:29:35.280 --> 00:29:39.480
And then, finally,
the machine learning people

00:29:39.480 --> 00:29:43.300
really love to minimize things
rather than maximizing things.

00:29:43.300 --> 00:29:46.990
And so, you can always swap
between maximizing and minimizing,

00:29:46.990 --> 00:29:51.430
when you're in plus minus land, by
putting a minus sign in front of things.

00:29:51.430 --> 00:29:55.950
And so, at this point,
we get the negative log likelihood,

00:29:55.950 --> 00:29:59.050
the negative log probability
according to our model.

00:29:59.050 --> 00:30:03.563
And so, that's what we will be formally

00:30:03.563 --> 00:30:08.090
minimizing as our objective function.

00:30:08.090 --> 00:30:12.858
So if there were objective function, cost
function, loss function, all the same,

00:30:12.858 --> 00:30:17.365
this negative log likelihood criterion
really that means that we're using this

00:30:17.365 --> 00:30:21.141
our cross-entropy loss which is
gonna come back to this next week so

00:30:21.141 --> 00:30:23.100
I won't really go through it now.

00:30:23.100 --> 00:30:26.890
But the trick is since we
have a one hot target,

00:30:26.890 --> 00:30:30.130
which is just predict the word
that actually occurred.

00:30:30.130 --> 00:30:34.796
Under that criteria the only
thing that's left in cross

00:30:34.796 --> 00:30:39.814
entropy loss is the negative
probability of the true class.

00:30:39.814 --> 00:30:43.210
Well, how are we gonna actually do this?

00:30:43.210 --> 00:30:47.777
How can we make use of
these word vectors to

00:30:47.777 --> 00:30:52.381
minimize that negative log likelihood?

00:30:52.381 --> 00:30:55.690
Well, the way we're gonna
do it is we're gonna come

00:30:55.690 --> 00:31:00.400
with the probably
distribution of context word,

00:31:00.400 --> 00:31:06.050
given the center word, which is
constructed out of our word vectors.

00:31:06.050 --> 00:31:09.990
And so, this is what our probability
distribution is gonna look like.

00:31:09.990 --> 00:31:14.655
So just to make sure we're clear on
the terminology I'm gonna use forward

00:31:14.655 --> 00:31:15.415
from here.

00:31:15.415 --> 00:31:22.845
So c and o are indices in the space
of the vocabulary, the word types.

00:31:22.845 --> 00:31:29.380
So up here, the t and the t plus j, where
in my text there are positions in my text.

00:31:29.380 --> 00:31:34.700
Those are sort of words,
763 in words 766 in my text.

00:31:34.700 --> 00:31:39.988
But here o and c in my vocabulary
words I have word types and

00:31:39.988 --> 00:31:45.730
so I have my p for words 73 and
47 in my vocabulary words.

00:31:45.730 --> 00:31:53.140
And so, each word type they're going to
have a vector associated with them so

00:31:53.140 --> 00:31:59.410
u o is the vector associated
with context word in index o and

00:31:59.410 --> 00:32:04.600
vc is the vector that's
associated with the center word.

00:32:04.600 --> 00:32:10.460
And so, how we find this probability
distribution is we're going to use this,

00:32:10.460 --> 00:32:16.540
what's called a Softmax form,
where we're taking dot products between

00:32:16.540 --> 00:32:22.510
the the two word vectors and then we're
putting them into a Softmax form.

00:32:22.510 --> 00:32:25.890
So just to go through that kind
of maximally slowly, right?

00:32:25.890 --> 00:32:30.890
So we've got two word vectors and
we're gonna dot product them,

00:32:30.890 --> 00:32:33.930
which means that we so
take the corresponding terms and

00:32:33.930 --> 00:32:37.490
multiply them together and
sort of sum them all up.

00:32:37.490 --> 00:32:42.170
So may adopt product is sort of like
a loose measure of similarity so

00:32:42.170 --> 00:32:46.320
the contents of the vectors
are more similar to each other

00:32:46.320 --> 00:32:48.386
the number will get bigger.

00:32:48.386 --> 00:32:52.790
So that's kind of a similarity
measure through the dot product.

00:32:52.790 --> 00:32:56.730
And then once we've worked out
dot products between words

00:32:56.730 --> 00:33:00.000
we're then putting it
in this Softmax form.

00:33:00.000 --> 00:33:03.454
So this Softmax form is a standard way to

00:33:03.454 --> 00:33:07.757
turn numbers into
a probability distribution.

00:33:07.757 --> 00:33:12.460
So when we calculate dot products,
they're just numbers, real numbers.

00:33:12.460 --> 00:33:14.740
They could be minus 17 or 32.

00:33:14.740 --> 00:33:19.990
So we can't directly turn those
into a probability distribution so

00:33:19.990 --> 00:33:23.290
an easy thing that we can
do is exponentiate them.

00:33:23.290 --> 00:33:27.266
Because if you exponentiate things
that puts them into positive land so

00:33:27.266 --> 00:33:29.020
it's all gonna be positive.

00:33:29.020 --> 00:33:34.510
And that's a good basis for
having a probability distribution.

00:33:34.510 --> 00:33:39.170
And if you have a bunch of numbers that
come from anywhere that are positive and

00:33:39.170 --> 00:33:43.190
you want to turn them into a probability
distribution that's proportional to

00:33:43.190 --> 00:33:47.260
the size of those numbers,
there's a really easy way to do that.

00:33:47.260 --> 00:33:52.110
Which is you sum all the numbers together
and you divide through by the sum and

00:33:52.110 --> 00:33:55.430
that then instantly gives you
a probability distribution.

00:33:55.430 --> 00:34:00.260
So that's then denominated that is
normalizing to give a probability and so

00:34:00.260 --> 00:34:05.450
when you put those together, that then
gives us this form that we're using

00:34:05.450 --> 00:34:10.450
as our Softmax form which is now
giving us a probability estimate.

00:34:10.450 --> 00:34:13.460
So that's giving us this
probability estimate

00:34:13.460 --> 00:34:18.060
here built solely in terms of
the word vector representations.

00:34:18.060 --> 00:34:19.630
Is that good?

00:34:19.630 --> 00:34:20.130
Yeah.

00:34:24.767 --> 00:34:29.526
That is an extremely good question and
I was hoping to delay saying that for

00:34:29.526 --> 00:34:33.370
just a minute but you've asked and
so I will say it.

00:34:33.370 --> 00:34:41.820
Yes, you might think that one word should
only have one vector representation.

00:34:41.820 --> 00:34:47.980
And if you really wanted to you could
do that, but it turns out you can make

00:34:47.980 --> 00:34:53.400
the math considerably easier by
saying now actually each word has two

00:34:53.400 --> 00:34:57.920
vector representation that has one vector
representation when it synthesis the word.

00:34:57.920 --> 00:35:02.050
And it has another vector representation
when it's a context word.

00:35:02.050 --> 00:35:04.690
So that's formally what we have here.

00:35:04.690 --> 00:35:10.770
So the v is the center word vectors,
and the u are the context word vectors.

00:35:10.770 --> 00:35:13.920
And it turns out not only does
that make the math a lot easier,

00:35:13.920 --> 00:35:16.870
because the two
representations are separated

00:35:16.870 --> 00:35:19.990
when you do optimization rather
than tied to each other.

00:35:19.990 --> 00:35:24.160
It's actually in practice empirically
works a little better as well,

00:35:24.160 --> 00:35:28.900
so if your life is easier and
better, who would not choose that?

00:35:28.900 --> 00:35:31.170
So yes, we have two vectors for each word.

00:35:32.290 --> 00:35:33.251
Any other questions?

00:35:52.837 --> 00:35:55.696
Yeah, so the question is,
well wait a minute,

00:35:55.696 --> 00:35:59.340
you just said this was a way to
make everything positive, but

00:35:59.340 --> 00:36:03.627
actually you also simultaneously
screwed with the scale of things a lot.

00:36:03.627 --> 00:36:05.280
And that's true, right?

00:36:05.280 --> 00:36:09.420
The reason why this is called a Softmax
function is because it's kind of

00:36:09.420 --> 00:36:14.260
close to a max function,
because when you exponentiate things,

00:36:14.260 --> 00:36:18.325
the big things get way bigger and
so they really dominate.

00:36:18.325 --> 00:36:23.805
And so this really sort of blows out
in the direction of a max function,

00:36:23.805 --> 00:36:24.945
but not fully.

00:36:24.945 --> 00:36:27.055
It's still a sort of a soft thing.

00:36:27.055 --> 00:36:30.180
So you might think that
that's a bad thing to do.

00:36:30.180 --> 00:36:34.070
Doing things like this is the most
standard underlying a lot of math,

00:36:34.070 --> 00:36:37.780
including all those super
common logistic regressions,

00:36:37.780 --> 00:36:40.120
you see another class's
way of doing things.

00:36:40.120 --> 00:36:41.390
So it's a good way to know,

00:36:41.390 --> 00:36:44.030
but people have certainly worked
on a whole bunch of other ways.

00:36:44.030 --> 00:36:46.480
And there are reasons that you might
think they're interesting, but

00:36:46.480 --> 00:36:48.190
I won't do them now.

00:36:48.190 --> 00:36:48.718
Yes?

00:37:00.734 --> 00:37:04.610
Yeah, so the question was,
when I'm dealing with the context words,

00:37:04.610 --> 00:37:08.050
am I paying attention to where they are or
just their identity?

00:37:08.050 --> 00:37:11.730
Yeah, where they are has nothing
to do with it in this model.

00:37:11.730 --> 00:37:15.940
It's just, what is the identity of
the word somewhere in the window?

00:37:15.940 --> 00:37:19.660
So there's just one
probability distribution and

00:37:19.660 --> 00:37:21.710
one representation of the context word.

00:37:21.710 --> 00:37:25.335
Now you know, it's not that
that's necessarily a good idea.

00:37:25.335 --> 00:37:30.925
There are other models which absolutely
pay attention to position and distance.

00:37:30.925 --> 00:37:34.415
And for some purposes,
especially more syntactic

00:37:34.415 --> 00:37:38.455
purposes rather than semantic purposes,
that actually helps a lot.

00:37:38.455 --> 00:37:43.189
But if you're sort of more interested
in just sort of word meaning,

00:37:43.189 --> 00:37:45.847
it turns out that not paying attention

00:37:45.847 --> 00:37:50.348
to position actually tends to
help you rather than hurting you.

00:37:50.348 --> 00:37:51.269
Yeah.

00:38:05.663 --> 00:38:10.610
Yeah, so the question is how, wait
a minute, is there a unique solution here?

00:38:10.610 --> 00:38:14.280
Could there be different rotations
that would be equally good?

00:38:15.310 --> 00:38:19.880
And the answer is yes, there can be.

00:38:19.880 --> 00:38:24.730
I think we should put off discussing
this cuz actually there's a lot to

00:38:24.730 --> 00:38:29.770
say about optimization in neural networks,
and there's a lot of exciting new work.

00:38:29.770 --> 00:38:34.860
And the one sentence headline is
it's all good news, people spent

00:38:34.860 --> 00:38:39.630
years saying that minimal work ought to be
a big problem and it turns out it's not.

00:38:39.630 --> 00:38:40.820
It all works.

00:38:40.820 --> 00:38:45.760
But I think we better off talking
about that in any more detail.

00:38:45.760 --> 00:38:50.990
Okay, so

00:38:50.990 --> 00:38:56.660
yeah this is my picture of what the skip
gram model ends up looking like.

00:38:56.660 --> 00:38:59.020
It's a bit confusing and hard to read, but

00:38:59.020 --> 00:39:01.500
also I've got it thrown
from left to right.

00:39:01.500 --> 00:39:05.110
Right, so we have the center
word that's a one hot vector.

00:39:06.260 --> 00:39:13.808
We then have a matrix of
the representations of center words.

00:39:13.808 --> 00:39:22.700
So if we kind of do a multiplication
of this matrix by that vector.

00:39:22.700 --> 00:39:26.970
We just sort of actually select
out the column of the matrix

00:39:26.970 --> 00:39:30.670
which is then the representation
of the center word.

00:39:31.870 --> 00:39:35.050
Then what we do is we have a second matrix

00:39:35.050 --> 00:39:39.660
which stores the representations
of the context words.

00:39:39.660 --> 00:39:42.850
And so for each position in the context,

00:39:42.850 --> 00:39:46.070
I show three here because
that was confusing enough.

00:39:46.070 --> 00:39:50.590
We're going to multiply
the vector by this matrix

00:39:51.620 --> 00:39:55.960
which is the context word representations.

00:39:55.960 --> 00:40:00.250
And so
we will be picking out sort of the dot

00:40:00.250 --> 00:40:04.830
products of the center word
with each context word.

00:40:04.830 --> 00:40:07.880
And it's the same matrix for
each position, right?

00:40:07.880 --> 00:40:10.766
We only have one context word matrix.

00:40:10.766 --> 00:40:12.752
And then these dot products,

00:40:12.752 --> 00:40:17.132
we're gonna soft max then turn
into a probability distribution.

00:40:17.132 --> 00:40:22.535
And so our model, as a generative model,
is predicting the probability of

00:40:22.535 --> 00:40:29.250
each word appearing in the context given
that a certain word is the center word.

00:40:29.250 --> 00:40:32.907
And so if we are actually using
it generatively, it would say,

00:40:32.907 --> 00:40:35.948
well, the word you should
be using is this one here.

00:40:35.948 --> 00:40:41.061
But if there is sort of actual ground
truth as to what was the context word,

00:40:41.061 --> 00:40:46.520
we can sort of say, well, the actual
ground truth was this word appeared.

00:40:46.520 --> 00:40:50.350
And you gave a probability
estimate of 0.1 to that word.

00:40:50.350 --> 00:40:54.185
And so that's the basis, so if you
didn't do a great job at prediction,

00:40:54.185 --> 00:40:57.580
then there's going to be some loss, okay?

00:40:57.580 --> 00:40:59.780
But that's the picture of our model.

00:40:59.780 --> 00:41:03.420
Okay, and so what we wanna do is now learn

00:41:04.890 --> 00:41:09.790
parameters, these word vectors,
in such a way that we

00:41:09.790 --> 00:41:14.620
do as good a job at prediction
as we possibly can.

00:41:16.670 --> 00:41:21.530
And so standardly when we do these things,
what we do

00:41:21.530 --> 00:41:26.760
is we take all the parameters in our model
and put them into a big vector theta.

00:41:26.760 --> 00:41:31.730
And then we're gonna say we're gonna do
optimization to change those parameters so

00:41:31.730 --> 00:41:35.408
as to maximize objective
function of our model.

00:41:35.408 --> 00:41:38.990
So what our parameters are is that for

00:41:38.990 --> 00:41:43.470
each word, we're going to have
a little d dimensional vector,

00:41:43.470 --> 00:41:47.480
when it's a center word and
when it's a context word.

00:41:47.480 --> 00:41:50.176
And so
we've got a vocabulary of some size.

00:41:50.176 --> 00:41:54.923
So we're gonna have a vector for
aardvark as a context word,

00:41:54.923 --> 00:41:57.740
a vector for art as a context word.

00:41:57.740 --> 00:42:00.330
We're going to have a vector
of aardvark as a center word,

00:42:00.330 --> 00:42:02.240
a vector of art as a center word.

00:42:02.240 --> 00:42:06.398
So our vector in total is
gonna be of length 2dV.

00:42:06.398 --> 00:42:10.783
There's gonna be a big long vector that
has everything that was in what was shown

00:42:10.783 --> 00:42:12.560
in those matrices before.

00:42:12.560 --> 00:42:15.630
And that's what we then gonna
be saying about optimizing.

00:42:15.630 --> 00:42:19.870
And so after the break, I'm going to be so

00:42:19.870 --> 00:42:23.570
going through concretely how
we do that optimization.

00:42:23.570 --> 00:42:25.384
But before the break,

00:42:25.384 --> 00:42:30.552
we have the intermission with
our special guest, Danqi Chen.

00:42:30.552 --> 00:42:32.425
&gt;&gt; Hi, everyone.

00:42:32.425 --> 00:42:36.080
I'm Danqi Chen, and
I'm the head TA of this class.

00:42:36.080 --> 00:42:39.210
So today I will start our first
research highlight session,

00:42:39.210 --> 00:42:42.390
and I will introduce you
a paper from Princeton.

00:42:42.390 --> 00:42:46.920
The title is A Simple but Tough-to-beat
Baseline for Sentence Embeddings.

00:42:46.920 --> 00:42:50.205
So today we are learning the word
vector representations, so

00:42:50.205 --> 00:42:53.750
we hope these vectors can
encode the word meanings.

00:42:53.750 --> 00:42:58.405
But our central question in natural
language processing, and also this class,

00:42:58.405 --> 00:43:02.924
is that how we could have the vector
representations that encode the meaning of

00:43:02.924 --> 00:43:06.159
sentences like,
natural language processing is fun.

00:43:08.120 --> 00:43:12.695
So with these sentence representations,
we can compute

00:43:12.695 --> 00:43:18.260
the sentence similarity using
the inner product of the two vectors.

00:43:18.260 --> 00:43:22.753
So, for example, Mexico wishes to
guarantee citizen's safety, and,

00:43:22.753 --> 00:43:25.510
Mexico wishes to avoid more violence.

00:43:25.510 --> 00:43:29.898
So we can use the vector
representation to predict these two

00:43:29.898 --> 00:43:32.302
sentences are pretty similar.

00:43:32.302 --> 00:43:35.970
We can also use this sentence
representation to use as

00:43:35.970 --> 00:43:39.935
features to do some sentence
classification task.

00:43:39.935 --> 00:43:41.885
For example, sentiment analysis.

00:43:41.885 --> 00:43:45.485
So given a sentence like,
natural language processing is fun,

00:43:45.485 --> 00:43:49.152
we can put our classifier on top
of the vector representations and

00:43:49.152 --> 00:43:51.525
predict if sentiment is positive.

00:43:51.525 --> 00:43:54.318
Hopefully this is right, so.

00:43:54.318 --> 00:43:58.408
So there are a wide range of
measures that compose word vector

00:43:58.408 --> 00:44:02.920
representations into sentence
vector representations.

00:44:02.920 --> 00:44:06.164
So the most simple way is
to use the bag-of-words.

00:44:06.164 --> 00:44:09.750
So the bag-of-words is just like
the vector representation of

00:44:09.750 --> 00:44:11.520
the natural language processing.

00:44:11.520 --> 00:44:15.764
It's a average of the three single
word vector representations,

00:44:15.764 --> 00:44:18.577
the natural, language, and processing.

00:44:18.577 --> 00:44:24.059
Later in this quarter, we'll learn a bunch
of complex models, such as recurrent

00:44:24.059 --> 00:44:29.394
neural nets, the recursing neural nets,
and the convolutional neural nets.

00:44:29.394 --> 00:44:34.202
But today, for this paper from Princeton,
I want to introduce

00:44:34.202 --> 00:44:39.115
that this paper introduces a very
simple unsupervised method.

00:44:39.115 --> 00:44:43.787
That is essentially just
a weighted bag-of-words sentence

00:44:43.787 --> 00:44:47.930
representation plus remove
some special direction.

00:44:47.930 --> 00:44:49.600
I will explain this.

00:44:50.690 --> 00:44:52.160
So they have two steps.

00:44:52.160 --> 00:44:57.040
So the first step is that just like how
we compute the average of the vector

00:44:57.040 --> 00:45:03.450
representations, they also do this,
but each word has a separate weight.

00:45:03.450 --> 00:45:05.493
Now here, a is a constant.

00:45:05.493 --> 00:45:09.690
And the p(w),
it means the frequency of this word.

00:45:09.690 --> 00:45:12.230
So this basically means that

00:45:12.230 --> 00:45:15.930
the average representation down
weight the frequent words.

00:45:15.930 --> 00:45:19.064
That's the very simple Step 1.

00:45:19.064 --> 00:45:23.924
So for the Step 2, after we compute
all of these sentence vector

00:45:23.924 --> 00:45:28.965
representations, we compute
the first principal components and

00:45:28.965 --> 00:45:34.290
also subtract the projections onto
this first principle component.

00:45:35.600 --> 00:45:40.582
You might be familiar with this
if you have ever taken CS 229 and

00:45:40.582 --> 00:45:42.043
also learned PCA.

00:45:42.043 --> 00:45:43.127
So that's it.

00:45:43.127 --> 00:45:44.461
That's their approach.

00:45:46.010 --> 00:45:50.378
So in this paper,
they also give a probabilistic

00:45:50.378 --> 00:45:55.148
interpretation about why
they want to do this.

00:45:55.148 --> 00:46:00.160
So basically, the idea is that given the
sentence representation, the probability

00:46:00.160 --> 00:46:05.790
of the limiting or single word, they're
related to the frequency of the word.

00:46:05.790 --> 00:46:12.085
And also related to how close the word is
related to this sentence representation.

00:46:12.085 --> 00:46:17.043
And also there's a C0 term that
means common discourse vector.

00:46:17.043 --> 00:46:19.538
That's usually related to some syntax.

00:46:21.774 --> 00:46:24.510
So, finally, the results.

00:46:24.510 --> 00:46:29.583
So first, they take context parents
on the sentence similarity and

00:46:29.583 --> 00:46:34.567
they show that this simple approach
is much better than the average

00:46:34.567 --> 00:46:37.860
of word vectors, all the TFIDF rating, and

00:46:37.860 --> 00:46:43.020
also all the performance of
other sophisticated models.

00:46:43.020 --> 00:46:47.429
And also for some supervised tasks
like sentence classification,

00:46:47.429 --> 00:46:52.390
they're also doing pretty well,
like the entailment and sentiment task.

00:46:52.390 --> 00:46:54.392
So that's it, thanks.

00:46:54.392 --> 00:46:55.790
&gt;&gt; Thank you.

00:46:55.790 --> 00:47:00.581
[LAUGH]
&gt;&gt; [APPLAUSE]

00:47:00.581 --> 00:47:08.829
&gt;&gt; Okay, Okay,

00:47:08.829 --> 00:47:13.662
so, and we'll go back from there.

00:47:17.970 --> 00:47:23.985
All right, so now we're wanting to sort
of actually work through our model.

00:47:23.985 --> 00:47:26.860
So this is what we had, right?

00:47:26.860 --> 00:47:33.485
We had our objective function where we
wanna minimize negative log likelihood.

00:47:33.485 --> 00:47:38.438
And this is the form of the probability
distribution up there, where we have these

00:47:38.438 --> 00:47:44.663
sort of word vectors with both center
word vectors and context word vectors.

00:47:44.663 --> 00:47:50.157
And the idea is we want to change
our parameters, these vectors, so

00:47:50.157 --> 00:47:56.637
as to minimize the negative log likelihood
item, maximize the probability we predict.

00:47:56.637 --> 00:48:00.667
So if that's what we want to do,

00:48:00.667 --> 00:48:06.057
how can we work out how
to change our parameters?

00:48:11.254 --> 00:48:13.951
Gradient, yes,
we're gonna use the gradient.

00:48:13.951 --> 00:48:18.880
So, what we're gonna have to do
at this point is to start to do

00:48:18.880 --> 00:48:24.160
some calculus to see how
we can change the numbers.

00:48:24.160 --> 00:48:29.710
So precisely, what we'll going
to want to do is to say, well,

00:48:29.710 --> 00:48:36.600
we have this term for
working out log probabilities.

00:48:36.600 --> 00:48:44.366
So, we have the log of the probability
of the word t plus j word t.

00:48:44.366 --> 00:48:46.010
Well, what is the form of that?

00:48:46.010 --> 00:48:47.710
Well, we've got it right here.

00:48:47.710 --> 00:48:52.800
So, we have the log of v
maybe I can save a line.

00:48:54.020 --> 00:49:00.050
We've got this log of this.

00:49:00.050 --> 00:49:06.840
And then, what we're gonna want to do is
that we're going to want to change this so

00:49:06.840 --> 00:49:11.470
that we have, I'm sorry,
minimized in this objective.

00:49:11.470 --> 00:49:15.790
So, let's suppose we sort of
look at these center vectors.

00:49:15.790 --> 00:49:21.137
So, what we're gonna want to do is start
working out the partial derivatives

00:49:21.137 --> 00:49:26.260
of this with respect to the center
vector which is then, going to give us,

00:49:26.260 --> 00:49:32.790
how we can go about working out,
in which way to change this vector

00:49:34.570 --> 00:49:38.240
to minimize our objective function.

00:49:38.240 --> 00:49:40.590
Okay, so, we want to deal with this.

00:49:41.610 --> 00:49:44.720
So, what's the first thing we can
do with that to make it simpler?

00:49:47.810 --> 00:49:49.840
Subtraction, yeah.

00:49:49.840 --> 00:49:55.855
So, this is a log of a division so, we can
turn that into a log of a subtraction,

00:49:55.855 --> 00:50:00.340
and then, we can do the partial
derivatives separately.

00:50:00.340 --> 00:50:05.749
So, we have the derivative

00:50:05.749 --> 00:50:11.034
with Vc of the log of the exp of

00:50:11.034 --> 00:50:16.563
u0^T vc and then, we've got

00:50:16.563 --> 00:50:21.734
minus the log of the sum of w

00:50:21.734 --> 00:50:27.165
equals 1 to V of exp of u w^T vc.

00:50:27.165 --> 00:50:32.041
And at that point,
we can separate it into two pieces, right,

00:50:32.041 --> 00:50:37.760
cuz when there's addition or
subtraction we can do them separately.

00:50:37.760 --> 00:50:42.254
So, we can do this piece 1 and
we can do the,

00:50:42.254 --> 00:50:47.492
work out the partial
derivatives of this piece 2.

00:50:47.492 --> 00:50:52.465
So, piece 1 looks kind of easy so,
let's start here.

00:50:52.465 --> 00:50:55.065
So, what's the first thing I
should do to make this simpler?

00:50:57.490 --> 00:51:00.179
Easy question.

00:51:01.850 --> 00:51:08.621
Cancel some things out, log and x inverses
of each other so, they can just go away.

00:51:08.621 --> 00:51:13.275
So, for 1,
we can say that this is going to be

00:51:13.275 --> 00:51:18.810
the partial derivative with
respect to Vc of u0^T vc.

00:51:18.810 --> 00:51:25.848
Okay, that's looking kind of simpler so,

00:51:25.848 --> 00:51:30.793
what is the partial derivative

00:51:30.793 --> 00:51:35.180
of this with respect to vc?

00:51:35.180 --> 00:51:38.640
u0, so, this just comes out as u0.

00:51:40.478 --> 00:51:47.370
Okay, and so, I mean, effectively, this is
the kind of level of calculus that you're

00:51:47.370 --> 00:51:52.850
gonna have to be able to do to be okay on
assignment one that's coming out today.

00:51:52.850 --> 00:51:58.350
So, it's nothing that life threatening,
hopefully, you've seen this before.

00:51:58.350 --> 00:52:04.830
But nevertheless, we are here using
calculus with vectors, right?

00:52:04.830 --> 00:52:09.730
So, vc here is not just a single number,
it's a whole vector.

00:52:09.730 --> 00:52:16.550
So, that's sort of the Math 51,
CME 100 kind of content.

00:52:16.550 --> 00:52:21.860
Now, if you want to,
you can pull it all apart.

00:52:21.860 --> 00:52:26.564
And you can work out
the partial derivative

00:52:26.564 --> 00:52:30.618
with respect to Vc, some index, k.

00:52:30.618 --> 00:52:33.301
And then,

00:52:33.301 --> 00:52:38.668
you could have this as

00:52:38.668 --> 00:52:44.032
the sum of l = 1 to d of

00:52:44.032 --> 00:52:49.112
(u0)l (Vc)l.

00:52:49.112 --> 00:52:55.032
And what will happen then is if you're
working out of with respect to only one

00:52:55.032 --> 00:53:01.610
index, then, all of these terms will go
away apart from the one where k equals l.

00:53:01.610 --> 00:53:09.720
And you'll sort of end up with
that being the (uo)k term.

00:53:09.720 --> 00:53:14.745
And I mean, if things get confusing and
complicated, I think it can actually,

00:53:14.745 --> 00:53:19.695
and your brain is small like mine, it can
actually be useful to sort of go down to

00:53:19.695 --> 00:53:24.570
the level of working it out with real
numbers and actually have all the indices

00:53:24.570 --> 00:53:28.750
there and you can absolutely do that and
it comes out the same.

00:53:28.750 --> 00:53:32.750
But a lot of the time it's sort
of convenient if we can just

00:53:32.750 --> 00:53:37.400
stay at this vector level and
work out vector derivatives, okay.

00:53:37.400 --> 00:53:41.110
So, now, this was the easy part and

00:53:41.110 --> 00:53:44.760
we've got it right there and
we'll come back to that, okay.

00:53:44.760 --> 00:53:49.355
So then, the trickier part is we then,
go on to number 2.

00:53:52.675 --> 00:53:58.127
So now, if we just ignore the minus

00:53:58.127 --> 00:54:02.451
sign for a little bit, so,

00:54:02.451 --> 00:54:07.715
we'll subtract it afterwards,

00:54:07.715 --> 00:54:14.107
we've then got the partial derivatives

00:54:14.107 --> 00:54:19.935
with respect to vc of the log of the sum

00:54:19.935 --> 00:54:26.160
from w = 1 to v of the exp of uw^T vc,
okay.

00:54:26.160 --> 00:54:28.870
Well, how can we make
progress with this half?

00:54:39.296 --> 00:54:44.252
Yeah, so that's right,
before you're going to do that?

00:54:44.252 --> 00:54:49.966
The chain rule, okay, so, our key tool
that we need to know how to use and

00:54:49.966 --> 00:54:55.180
we'll just use everywhere
is the chain rule, right?

00:54:55.180 --> 00:55:00.530
So, neural net people talk all
the time about backpropagation,

00:55:00.530 --> 00:55:07.380
it turns out that backpropagation
is nothing more than the chain rule

00:55:07.380 --> 00:55:12.560
with some efficient storage
of partial quantities so

00:55:12.560 --> 00:55:16.980
that you don't keep on calculating
the same quantity over and over again.

00:55:16.980 --> 00:55:20.010
So, it's sort of like chain
rule with memorization,

00:55:20.010 --> 00:55:22.990
that is the backpropagation algorithm.

00:55:22.990 --> 00:55:30.080
So, now, key tool is the chain rule so,
what is the chain rule?

00:55:30.080 --> 00:55:34.180
So, within saying, okay, well,

00:55:34.180 --> 00:55:38.968
what overall are we going to have
is some function where we're taking

00:55:38.968 --> 00:55:45.290
f(g(u)) of something.

00:55:45.290 --> 00:55:49.630
And so, we have this inside part z and so,

00:55:49.630 --> 00:55:54.720
what we're going to be doing is that
we're going to be taking the derivative

00:55:54.720 --> 00:56:01.610
of the outside part then,
with the value of the inside.

00:56:01.610 --> 00:56:06.230
And then, we're gonna be taking
the derivative of the inside part So for

00:56:06.230 --> 00:56:11.490
this here, so the outside part,
here's our F.

00:56:11.490 --> 00:56:14.430
And then here's our inside part Z.

00:56:14.430 --> 00:56:19.090
So the outside part is F,
which is a log function.

00:56:19.090 --> 00:56:23.444
And so the derivative of a log
function is the one on X function.

00:56:23.444 --> 00:56:29.276
So that we're then gonna be having

00:56:29.276 --> 00:56:34.301
that this is 1 over the sum of w

00:56:34.301 --> 00:56:39.542
equals 1 to V of the exp of uw^T vc.

00:56:39.542 --> 00:56:45.257
And then we're going to be multiplying
it by, what do we get over there.

00:56:54.407 --> 00:57:00.499
So we get the partial
derivative with respect to

00:57:06.824 --> 00:57:10.567
With respect to vc,

00:57:10.567 --> 00:57:17.143
of This inside part.

00:57:17.143 --> 00:57:23.130
The sum of, and it's a little trickier.

00:57:23.130 --> 00:57:25.930
We really need to be careful of indices so

00:57:25.930 --> 00:57:32.020
we're gonna get in the bad mess if
we have W here, and we reuse W here.

00:57:32.020 --> 00:57:34.505
We really need to change
it into something else.

00:57:34.505 --> 00:57:37.448
So we're gonna have X equals 1 to V.

00:57:37.448 --> 00:57:45.629
And then we've got the exp of UX,

00:57:45.629 --> 00:57:49.900
transpose VC.

00:57:49.900 --> 00:57:52.640
So that's made a little bit of progress.

00:57:52.640 --> 00:57:56.320
We want to make a bit more progress here.

00:57:56.320 --> 00:57:57.703
So what's the next thing we're gonna do.

00:58:04.118 --> 00:58:06.570
Distribute the derivative.

00:58:06.570 --> 00:58:08.350
This is just adding some stuff.

00:58:09.930 --> 00:58:15.750
We can do the same trick of we can do
each part of the derivative separately.

00:58:15.750 --> 00:58:20.469
So X equals 1 to big V of
the partial derivative

00:58:20.469 --> 00:58:24.598
with respect to VC of the exp of ux^T vc.

00:58:24.598 --> 00:58:31.472
Okay, now we wanna keep
going What can we do next.

00:58:33.923 --> 00:58:35.460
The chain rule again.

00:58:36.580 --> 00:58:40.280
This is also the form of here's our F and
here's our

00:58:40.280 --> 00:58:45.580
inner values V which is in
turn sort of a function.

00:58:45.580 --> 00:58:50.200
Yeah, so we can apply the chain
rule a second time and

00:58:50.200 --> 00:58:55.790
so we need the derivative of X.

00:58:55.790 --> 00:58:57.212
What's the derivative of X.

00:58:57.212 --> 00:59:02.350
X, so this part here is gonna be staying.

00:59:02.350 --> 00:59:06.990
The sum of X equals 1 to V
of the partial derivative.

00:59:06.990 --> 00:59:08.440
Hold on no.

00:59:08.440 --> 00:59:10.520
Not that one, moving that inside.

00:59:10.520 --> 00:59:18.461
So it's still exp at its value of UX T VC.

00:59:18.461 --> 00:59:24.203
And then we're having the partial

00:59:24.203 --> 00:59:30.941
derivative with respect to VC of UXT VC.

00:59:30.941 --> 00:59:33.460
And then we've got a bit
more progress to make.

00:59:33.460 --> 00:59:36.980
So we now need to work out what this is.

00:59:36.980 --> 00:59:37.667
So what's that.

00:59:40.102 --> 00:59:43.356
Right, so
that's the same as sort of back over here.

00:59:43.356 --> 00:59:49.384
At this point this is just going to be,
that' s coming out as UX.

00:59:49.384 --> 00:59:54.939
And here we still have the sum

00:59:54.939 --> 01:00:01.670
of X equals 1 to V of the X of UX T VC.

01:00:01.670 --> 01:00:08.630
So at this point we kind of wanna
put this together with that.

01:00:08.630 --> 01:00:11.540
Cuz we're still, I stopped writing that.

01:00:11.540 --> 01:00:15.860
But we have this one over

01:00:15.860 --> 01:00:20.612
the sum of W equals 1 to V of

01:00:20.612 --> 01:00:26.041
the exp of UW, transpose VC.

01:00:26.041 --> 01:00:34.615
Can we put those things together
in a way that makes it prettier.

01:00:50.406 --> 01:00:54.492
So I can move this inside this sum.

01:00:54.492 --> 01:01:00.933
Cuz this is just the sort of number that's
a multiplier that's distributed through.

01:01:00.933 --> 01:01:05.928
And in particular when I do that,
I can start to sort of

01:01:05.928 --> 01:01:10.701
notice this interesting
thing that I'm going to be

01:01:10.701 --> 01:01:15.807
reconstructing a form that
looks very like this form.

01:01:15.807 --> 01:01:17.881
Sorry, leaving this part up aside.

01:01:17.881 --> 01:01:23.820
It looks very like the Softmax
form that I started off with.

01:01:23.820 --> 01:01:28.901
And so I can then be saying that

01:01:28.901 --> 01:01:33.982
this is the sum from X equals 1

01:01:33.982 --> 01:01:39.265
to V of the exp of UX transpose VC

01:01:39.265 --> 01:01:44.720
over the sum of W equals 1 to V.

01:01:44.720 --> 01:01:50.940
So this is where it's important that I
have X and W with different variables

01:01:50.940 --> 01:01:57.434
of the X of U W transpose VC times U of X.

01:01:59.560 --> 01:02:03.740
And so well, at that point,
that's kind of interesting cuz,

01:02:03.740 --> 01:02:09.430
this is kind of exactly the form
that I started of with,

01:02:09.430 --> 01:02:13.160
for my softmax probability distribution.

01:02:13.160 --> 01:02:15.113
So what we're doing is we.

01:02:19.840 --> 01:02:26.779
What we're doing is that that part is then

01:02:26.779 --> 01:02:32.133
being the sum over X equals one to

01:02:32.133 --> 01:02:38.096
V of the probability of [INAUDIBLE].

01:02:38.096 --> 01:02:39.873
It was wait.

01:02:39.873 --> 01:02:44.176
The probability of O given

01:02:44.176 --> 01:02:50.600
the probability of X given C times UX.

01:02:50.600 --> 01:02:54.230
So that's what we're getting
from the denominator.

01:02:54.230 --> 01:02:56.940
And then we still had the numerator.

01:02:56.940 --> 01:02:58.800
The numerator was U zero.

01:03:00.220 --> 01:03:07.369
What we have here is our
final form is U0 minus that.

01:03:07.369 --> 01:03:12.460
And if you look at this a bit
it's sort of a form that you

01:03:12.460 --> 01:03:17.870
always get from these
softmax style formulations.

01:03:17.870 --> 01:03:19.980
So this is what we observed.

01:03:19.980 --> 01:03:25.720
There was the actual output
context word appeared.

01:03:25.720 --> 01:03:28.960
And this has the form of an expectation.

01:03:28.960 --> 01:03:31.330
So what we're doing is right here.

01:03:31.330 --> 01:03:35.350
We're calculating expectation
though we're working out

01:03:35.350 --> 01:03:39.900
the probability of every possible
word appearing in the context, and

01:03:39.900 --> 01:03:44.800
based on that probability we get
taking that much of that UX.

01:03:44.800 --> 01:03:49.210
So this is in some,
this is the expectation vector.

01:03:49.210 --> 01:03:52.710
It's the average over all
the possible context vectors,

01:03:52.710 --> 01:03:54.640
weighted by their
likelihood of occurrence.

01:03:56.880 --> 01:03:59.610
That's the form of our derivative.

01:03:59.610 --> 01:04:05.620
What we're going to want to be doing is
changing the parameters in our model.

01:04:05.620 --> 01:04:10.743
In such a way that these become
equal cause that's when we're

01:04:10.743 --> 01:04:15.485
then finding the maximum and
minimum for us to minimize.

01:04:15.485 --> 01:04:21.860
[INAUDIBLE] Okay and so that gives
us the derivatives in that model.

01:04:21.860 --> 01:04:25.490
Does that make sense?

01:04:25.490 --> 01:04:27.548
Yeah, that's gonna be question.

01:04:27.548 --> 01:04:28.705
Anyway, so

01:04:28.705 --> 01:04:34.280
precisely doing things like this is what
will expect you to do for assignment one.

01:04:34.280 --> 01:04:37.610
And I'll take the question, but
let me just mention one point.

01:04:37.610 --> 01:04:41.520
So in this case,
I've only done this for the VC,

01:04:41.520 --> 01:04:46.420
the center vectors.

01:04:46.420 --> 01:04:49.360
We do this to every
parameter of the model.

01:04:49.360 --> 01:04:53.650
In this model, our only other
parameters are the context vectors.

01:04:53.650 --> 01:04:55.880
We're also gonna do it for those.

01:04:55.880 --> 01:04:59.280
It's very similar cuz if you look
at the form of the equation,

01:04:59.280 --> 01:05:01.930
there's a certain
symmetry between the two.

01:05:01.930 --> 01:05:05.400
But we're gonna do it for that as well but
I'm not gonna do it here.

01:05:05.400 --> 01:05:07.500
That's left to you guys.

01:05:07.500 --> 01:05:08.000
Question.

01:05:09.220 --> 01:05:15.298
Yeah.
&gt;&gt; [INAUDIBLE]

01:05:24.580 --> 01:05:25.623
&gt;&gt; From here to here.

01:05:25.623 --> 01:05:26.880
Okay.

01:05:26.880 --> 01:05:27.380
So.

01:05:28.450 --> 01:05:32.550
So, right, so this is a sum right?

01:05:32.550 --> 01:05:36.190
And this is just the number
at the end of the day.

01:05:36.190 --> 01:05:41.790
So I can divide every term in
this sum through by that number.

01:05:41.790 --> 01:05:43.080
So that's what I'm doing.

01:05:43.080 --> 01:05:48.500
So now I've got my sum with every term
in that divided through by this number.

01:05:48.500 --> 01:05:54.080
And then I say, wait a minute,
the form of this piece here

01:05:54.080 --> 01:05:58.530
is precisely my softmax
probably distribution,

01:05:58.530 --> 01:06:02.210
where this is the probability
of x given C.

01:06:02.210 --> 01:06:06.890
And so then I'm just rewriting
it as probability of x given c.

01:06:06.890 --> 01:06:10.310
Where that is meaning,
I kind of did double duty here.

01:06:10.310 --> 01:06:14.920
But that's sort of meaning that you're
using this probability of x given c

01:06:14.920 --> 01:06:16.565
using this probability form.

01:06:16.565 --> 01:06:22.200
&gt;&gt; [INAUDIBLE]

01:06:26.367 --> 01:06:27.019
&gt;&gt; Yeah,

01:06:27.019 --> 01:06:32.905
the probability that x occurs as
a context word of center word c.

01:06:32.905 --> 01:06:36.697
&gt;&gt; [INAUDIBLE]
&gt;&gt; Well,

01:06:36.697 --> 01:06:39.673
we've just assumed some
fixed window size M.

01:06:39.673 --> 01:06:44.735
So maybe our window size is five and so
we're considering sort of ten words,

01:06:44.735 --> 01:06:47.200
five to the left, five to the right.

01:06:48.580 --> 01:06:52.470
So that's a hypergrameter,
and that stuff's nowhere.

01:06:52.470 --> 01:06:55.513
We're not dealing with that, we just
assume that God's fixed that for us.

01:06:59.506 --> 01:07:02.371
The problem, so
it's done at each position.

01:07:02.371 --> 01:07:09.513
So for any position, and
all of them are treated equivalently,

01:07:09.513 --> 01:07:14.539
for any position,
the probability that word

01:07:14.539 --> 01:07:19.565
x is the word that occurs
within this window at

01:07:19.565 --> 01:07:24.475
any position given
the center word was of C.

01:07:26.452 --> 01:07:27.509
Yeah?

01:07:27.509 --> 01:07:30.943
&gt;&gt; [INAUDIBLE]

01:07:40.475 --> 01:07:43.029
&gt;&gt; All right, so the question is,

01:07:43.029 --> 01:07:46.720
why do we choose the dot
product as our basis for

01:07:46.720 --> 01:07:50.138
coming up with this probability measure?

01:07:50.138 --> 01:07:58.109
And you know I think the answer
is there's no necessary reason,

01:07:58.109 --> 01:08:02.537
that there are clearly other things

01:08:02.537 --> 01:08:07.620
that you could have done and might do.

01:08:07.620 --> 01:08:12.010
On the other hand,
I kind of think in terms of

01:08:13.030 --> 01:08:20.290
Vector Algebra it's sort of the most
obvious and simple thing to do.

01:08:20.290 --> 01:08:28.420
Because it's sort of a measure of
the relatedness and similarity.

01:08:28.420 --> 01:08:32.520
I mean I sort of said loosely it was
a measure of similarity between vectors.

01:08:32.520 --> 01:08:37.200
Someone could have called me on that
because If you say, well wait a minute.

01:08:37.200 --> 01:08:41.380
If you don't control for
the scale of the vectors,

01:08:41.380 --> 01:08:44.720
you can make that number as big
as you want, and that is true.

01:08:44.720 --> 01:08:50.030
So really the common measure of similarity
between vectors is the cosine measure.

01:08:50.030 --> 01:08:53.370
Where what you do is in the numerator.

01:08:53.370 --> 01:08:58.170
You take a dot product and then you divide
through by the length of the vectors.

01:08:58.170 --> 01:08:59.970
So you've got scale and variance and

01:08:59.970 --> 01:09:02.054
you can't just cheat by
making the vectors bigger.

01:09:02.054 --> 01:09:08.400
And so, that's a bigger,
better measure of similarity.

01:09:08.400 --> 01:09:12.390
But to do that you have to
do a whole lot more math and

01:09:12.390 --> 01:09:15.760
it's not actually necessary here
because since you're sort of

01:09:15.760 --> 01:09:18.990
predicting every word
against every other word.

01:09:18.990 --> 01:09:22.812
If you sort of made one
vector very big to try and

01:09:22.812 --> 01:09:26.547
make some probability
of word k being large.

01:09:26.547 --> 01:09:30.177
Well the consequence would be it would
make the probability of every other word

01:09:30.177 --> 01:09:31.002
be large as well.

01:09:31.002 --> 01:09:34.000
So you kind of can't cheat
by lengthening the vectors.

01:09:34.000 --> 01:09:38.396
And therefore you can get away with
just using the dot product as a kind of

01:09:38.396 --> 01:09:39.870
a similarity measure.

01:09:39.870 --> 01:09:41.799
Does that sort of satisfy?

01:09:56.556 --> 01:09:58.487
So yes.

01:09:58.487 --> 01:10:01.430
I mean, it's not necessary, right?

01:10:01.430 --> 01:10:04.430
And if we were going to argue,
you could sort of argue with me and

01:10:04.430 --> 01:10:08.950
say no look, this is crazy,
because by construction,

01:10:08.950 --> 01:10:13.869
this means the most likely word to appear
in the context of a word is itself.

01:10:15.520 --> 01:10:17.715
That doesn't seem like a good result,

01:10:17.715 --> 01:10:21.399
[LAUGH] because presumably
different words occur.

01:10:21.399 --> 01:10:28.320
And you could then go from there and say
well no let's do something more complex.

01:10:28.320 --> 01:10:32.110
Why don't we put a matrix to mediate
between the two vectors to express what

01:10:32.110 --> 01:10:39.420
appears in the context of each other,
it turns out you don't need to.

01:10:39.420 --> 01:10:44.420
Now one thing of course is since we
have different representations for

01:10:44.420 --> 01:10:49.400
the context and center word vectors, it's
not necessarily true that the same word

01:10:49.400 --> 01:10:53.740
would be highest because there're
two different representations.

01:10:53.740 --> 01:10:57.410
But in practice they often have a lot
of similarity between themselves not

01:10:57.410 --> 01:11:00.210
really that that's the reason.

01:11:00.210 --> 01:11:04.800
It's more that it's sort
of works out pretty well.

01:11:04.800 --> 01:11:07.340
Because although it is true
that you're not likely to

01:11:07.340 --> 01:11:10.020
get exactly the same word in the context,

01:11:10.020 --> 01:11:13.800
you're actually very likely to get words
that are pretty similar in meaning.

01:11:13.800 --> 01:11:18.640
And are strongly associated and when
those words appear as the center word,

01:11:18.640 --> 01:11:22.290
you're likely to get your
first word as a context word.

01:11:22.290 --> 01:11:25.340
And so at a sort of a macro level,
you are actually

01:11:25.340 --> 01:11:28.540
getting this effect that the same
words are appearing on both sides.

01:11:30.370 --> 01:11:32.390
More questions, yeah,
there are two of them.

01:11:32.390 --> 01:11:32.950
I don't know.

01:11:32.950 --> 01:11:34.455
Do I do the behind person first and
then the in front person?

01:11:34.455 --> 01:11:35.120
[LAUGH]

01:11:52.809 --> 01:11:54.250
So I haven't yet done gradient descent.

01:11:54.250 --> 01:11:58.730
And maybe I should do that in a minute and
I will see try then.

01:11:58.730 --> 01:12:00.460
Okay?
&gt;&gt; [INAUDIBLE]

01:12:00.460 --> 01:12:00.960
&gt;&gt; Yeah

01:12:11.612 --> 01:12:12.961
&gt;&gt; So that truth is well,

01:12:12.961 --> 01:12:15.960
we've just clicked to
the huge amount text.

01:12:15.960 --> 01:12:20.189
So if our word at any position, we know
what are the five words to the left and

01:12:20.189 --> 01:12:23.027
the five words to the right and
that's the truth.

01:12:23.027 --> 01:12:26.581
And so we're actually giving some
probability estimate to every

01:12:26.581 --> 01:12:28.454
word appearing in that context and

01:12:28.454 --> 01:12:32.472
we can say, well, actually the word
that appeared there was household.

01:12:32.472 --> 01:12:36.590
What probability did you give to that and
there's some answer.

01:12:36.590 --> 01:12:38.830
And so, that's our truth.

01:12:38.830 --> 01:12:43.520
Time is running out, so maybe I'd sort
of just better say a little bit more

01:12:43.520 --> 01:12:48.300
before we finish which is sort of
starting to this optimization.

01:12:48.300 --> 01:12:53.274
So this is giving us our derivatives,
we then want to use our

01:12:53.274 --> 01:12:57.664
derivatives to be able to
work out our word vectors.

01:12:57.664 --> 01:13:03.214
And I mean, I'm gonna spend
a super short amount time on this,

01:13:03.214 --> 01:13:08.055
the hope is through 221,
229 or similar class.

01:13:08.055 --> 01:13:15.457
You've seen a little bit of optimization
and you've seen some gradient descent.

01:13:15.457 --> 01:13:18.195
And so, this is just a very quick review.

01:13:18.195 --> 01:13:22.951
So the idea is once we have gradient
set at point x that if what we

01:13:22.951 --> 01:13:27.250
do is we subtract off a little
fraction of the gradient,

01:13:27.250 --> 01:13:31.010
that will move us downhill
towards the minimum.

01:13:31.010 --> 01:13:36.067
And so if we then calculate the gradient
there again and subtract off

01:13:36.067 --> 01:13:42.305
a little fraction of it, we'll sort of
start walking down towards the minimum.

01:13:42.305 --> 01:13:46.187
And so,
that's the algorithm of gradient descent.

01:13:46.187 --> 01:13:51.838
So once we have an objective function and
we have the derivatives of the objective

01:13:51.838 --> 01:13:56.741
function with respect to all of
the parameters, our gradient descent

01:13:56.741 --> 01:14:02.335
algorithm would be to say,
you've got some current parameter values.

01:14:02.335 --> 01:14:05.082
We've worked out the gradient
at that position.

01:14:05.082 --> 01:14:09.007
We subtract off a little
fraction of that and

01:14:09.007 --> 01:14:14.204
that will give us new parameter
values which we will expect

01:14:14.204 --> 01:14:21.660
to be give us a lower objective value,
and we'll walk towards the minimum.

01:14:21.660 --> 01:14:25.720
And in general, that is true and
that will work.

01:14:28.850 --> 01:14:31.551
So then, to write that up as Python code,

01:14:31.551 --> 01:14:36.278
it's really sort of super simple that
you just go in this while true loop.

01:14:36.278 --> 01:14:41.048
You have to have some stopping condition
actually where you evaluating the gradient

01:14:41.048 --> 01:14:45.889
of given your objective function, your
corpus and your current parameters, so

01:14:45.889 --> 01:14:50.659
you have the theta grad and then you're
sort of subtracting a little fraction of

01:14:50.659 --> 01:14:55.960
the theta grad after the current
parameters and then you just repeat over.

01:14:55.960 --> 01:15:00.884
And so the picture is, so the red lines
that are sort of the contour lines of

01:15:00.884 --> 01:15:03.435
the value of the objective function.

01:15:03.435 --> 01:15:07.993
And so what you do is when you
calculate the gradient, it's giving you

01:15:07.993 --> 01:15:12.784
the direction of the steepest descent and
you walk a little bit each time in

01:15:12.784 --> 01:15:18.145
that direction and you will hopefully
walk smoothly towards the minimum.

01:15:18.145 --> 01:15:22.509
Now the reason that might not work is
if you actually take a first step and

01:15:22.509 --> 01:15:26.900
you go from here to over there,
you've greatly overshot the minimum.

01:15:26.900 --> 01:15:31.822
So, it's important that alpha be small
enough that you're still walking calmly

01:15:31.822 --> 01:15:35.082
down towards the minimum and
then all work.

01:15:35.082 --> 01:15:40.407
And so, gradient descent is the most
basic tool to minimize functions.

01:15:40.407 --> 01:15:46.640
So it's the conceptually first thing to
know, but then the sort of last minute.

01:15:46.640 --> 01:15:49.763
What I wanted to explain is actually,

01:15:49.763 --> 01:15:54.850
we might have 40 billion tokens
in our corpus to go through.

01:15:54.850 --> 01:15:59.470
And if you have to work out
the gradient of your objective function

01:15:59.470 --> 01:16:04.426
relative to a 40 billion word corpus,
that's gonna take forever,

01:16:04.426 --> 01:16:09.740
so you'll wait for an hour before
you make your first gradient update.

01:16:09.740 --> 01:16:13.654
And so, you're not gonna be able train
your model in a realistic amount of time.

01:16:13.654 --> 01:16:17.549
So for basically,
all neural nets doing naive batch

01:16:17.549 --> 01:16:22.160
gradient descent hopeless algorithm,
you can't use that.

01:16:22.160 --> 01:16:24.125
It's not practical to use.

01:16:24.125 --> 01:16:28.724
So instead, what we do Is used
stochastic gradient descent.

01:16:28.724 --> 01:16:34.100
So, the stochastic gradient descent or
SGD is our key tool.

01:16:34.100 --> 01:16:40.263
And so what that's meaning is, so
we just take one position in the text.

01:16:40.263 --> 01:16:45.253
So we have one center word and
the words around it and we say, well,

01:16:45.253 --> 01:16:49.979
let's adjust it at that one
position work out the gradient with

01:16:49.979 --> 01:16:52.830
respect to all of our parameters.

01:16:52.830 --> 01:16:57.422
And using that estimate of
the gradient in that position,

01:16:57.422 --> 01:17:00.902
we'll work a little bit in that direction.

01:17:00.902 --> 01:17:05.047
If you think about it for
doing something like word vector learning,

01:17:05.047 --> 01:17:09.551
this estimate of the gradient is
incredibly, incredibly noisy, because

01:17:09.551 --> 01:17:14.580
we've done it at one position which just
happens to have a few words around it.

01:17:14.580 --> 01:17:18.724
So the vast majority of the parameters
of our model, we didn't see at all.

01:17:18.724 --> 01:17:22.489
So, it's a kind of incredibly
noisy estimate of the gradient.

01:17:22.489 --> 01:17:26.656
walking a little bit in that direction
isn't even guaranteed to have make you

01:17:26.656 --> 01:17:29.910
walk downhill,
because it's such a noisy estimate.

01:17:29.910 --> 01:17:33.317
But in practice, this works like a gem.

01:17:33.317 --> 01:17:36.049
And in fact, it works better.

01:17:36.049 --> 01:17:37.622
Again, it's a win, win.

01:17:37.622 --> 01:17:42.115
It's not only that doing things
this way is orders of magnitude

01:17:42.115 --> 01:17:44.829
faster than batch gradient descent,

01:17:44.829 --> 01:17:50.410
because you can do an update after you
look at every center word position.

01:17:50.410 --> 01:17:55.200
It turns out that neural
network algorithms love noise.

01:17:55.200 --> 01:18:00.965
So the fact that this gradient descent,
the estimate of the gradient is noisy,

01:18:00.965 --> 01:18:05.784
actually helps SGD to work better
as an optimization algorithm and

01:18:05.784 --> 01:18:07.775
neural network learning.

01:18:07.775 --> 01:18:09.900
And so, this is what we're
always gonna use in practice.

01:18:09.900 --> 01:18:14.129
I have to stop there for today even
though the fire alarm didn't go off.

01:18:14.129 --> 01:18:14.866
Thanks a lot.

01:18:14.866 --> 01:18:17.940
&gt;&gt; [APPLAUSE]

