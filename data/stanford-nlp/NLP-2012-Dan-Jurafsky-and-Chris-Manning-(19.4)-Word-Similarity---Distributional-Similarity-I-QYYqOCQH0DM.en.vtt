WEBVTT
Kind: captions
Language: en

00:00:01.400 --> 00:00:03.450
All right, let's turn to
the second important method for

00:00:03.450 --> 00:00:06.100
computing word similarity,
distributional similarity.

00:00:07.620 --> 00:00:10.730
Now, thesauri have problems for
computing similarity.

00:00:10.730 --> 00:00:13.500
We don't always have a thesaurus for
a particular language, and

00:00:13.500 --> 00:00:17.940
even when we do, thesauri have a problem
with recall, so words are missing,

00:00:19.790 --> 00:00:24.070
phrases are missing,
connections between senses may be missing.

00:00:24.070 --> 00:00:27.280
And in general, thesauri don't work
as well for verbs or adjectives,

00:00:27.280 --> 00:00:29.180
which have less structured
hyponymy relations.

00:00:30.880 --> 00:00:34.560
So, for all these reasons, we often
use distributional models of meaning,

00:00:34.560 --> 00:00:36.430
often called vector-space
models of meaning.

00:00:36.430 --> 00:00:39.630
And these tend to give you higher
recall than hand-built thesauri,

00:00:39.630 --> 00:00:41.430
although they might have lower precision.

00:00:43.440 --> 00:00:46.900
The intuition of these
distributional models of meaning,

00:00:46.900 --> 00:00:49.160
comes from early linguistic work.

00:00:49.160 --> 00:00:52.965
So, for example, Zellig Harris in 1954,
said oculist and

00:00:52.965 --> 00:00:56.580
eye-doctor occur in almost
the same environments.

00:00:56.580 --> 00:01:01.300
So, if A and B have almost identical
environments, we say they're synonyms.

00:01:01.300 --> 00:01:06.110
And Firth, back in 57 said, you shall
know a word by the company it keeps.

00:01:07.380 --> 00:01:12.220
So, here is an example, we have
a bunch of sentences about tesgüino.

00:01:12.220 --> 00:01:15.210
Nida says a bottle of
tesgüino is on the table.

00:01:15.210 --> 00:01:16.830
Everybody likes tesgüino.

00:01:16.830 --> 00:01:18.090
Tesgüino makes you drunk.

00:01:18.090 --> 00:01:19.770
We make tesgüino out of corn.

00:01:19.770 --> 00:01:24.860
So, from these context words for a human
is very easy to guess that tesguino

00:01:24.860 --> 00:01:27.850
means some kind of alcoholic beverage,
some kind of beer made out of corn.

00:01:29.150 --> 00:01:30.280
So the intuition for

00:01:30.280 --> 00:01:35.220
an algorithm is two words are similar if
they're just surrounded by similar words.

00:01:35.220 --> 00:01:35.990
Very simple idea.

00:01:37.720 --> 00:01:39.500
Lets see how to make that work.

00:01:39.500 --> 00:01:43.480
Remember the term document matrix,
we saw an information retrieval, so

00:01:43.480 --> 00:01:48.890
each cell in the term document matrix was
the count of the term t in a document d so

00:01:48.890 --> 00:01:51.680
we call it the term frequency of t in d.

00:01:53.170 --> 00:01:58.240
And we thought about it, was was meant to
be a document was to be a count vector, so

00:01:58.240 --> 00:02:00.400
a column in this term document matrix.

00:02:00.400 --> 00:02:04.720
So, the document As You Like It, the
Shakespeare play is a count vector over

00:02:04.720 --> 00:02:09.550
lots of words, I show just four
worlds; battle, soldier, fool, clown.

00:02:09.550 --> 00:02:11.212
So, As You Like It is a count vector.

00:02:14.265 --> 00:02:18.190
So, two documents are similar
if their vectors were similar.

00:02:19.440 --> 00:02:23.610
So Julius Caesar, high counts for battle
and soldier, low counts for fool and

00:02:23.610 --> 00:02:24.500
clown.

00:02:24.500 --> 00:02:28.650
Similarly, Henry V high counts for
battle and soldier, low counts for

00:02:28.650 --> 00:02:30.020
fool and clown.

00:02:30.020 --> 00:02:34.060
So we saw that Julius Caesar and Henry V,
if they were to query in a document or

00:02:34.060 --> 00:02:38.440
two documents, They were similar
by this vector similarity method.

00:02:39.650 --> 00:02:42.070
And we're just going to use
the same exact intuition for

00:02:42.070 --> 00:02:44.170
deciding if two words are similar.

00:02:44.170 --> 00:02:48.560
So, look at the words in a term-document
matrix, and now a word is a count vector,

00:02:50.100 --> 00:02:53.340
and two words are similar if
their vectors are similar.

00:02:53.340 --> 00:02:57.010
So, fool has high counts
in the document as you

00:02:57.010 --> 00:03:01.670
like it in the document Twelfth Night and
low counts in Julius Caesar and Henry V.

00:03:01.670 --> 00:03:04.230
Clown has count in As You Like It and
Twelfth Night and

00:03:04.230 --> 00:03:06.180
low counts in Julius Caesar and Henry V.

00:03:06.180 --> 00:03:10.150
So, we say that fool and
clown are similar.

00:03:12.913 --> 00:03:16.801
But probably battle and
fool are not similar,

00:03:16.801 --> 00:03:23.687
because battle has high counts here and
low counts here but fool has the opposite.

00:03:23.687 --> 00:03:27.479
It has high counts here and
low counts here.

00:03:30.255 --> 00:03:34.159
So, the intuition of word similarity,
distributional word similarity,

00:03:34.159 --> 00:03:38.185
is instead of using entire documents
like we used for information retrieval,

00:03:38.185 --> 00:03:39.720
let's use smaller context.

00:03:39.720 --> 00:03:43.920
We could use a paragraph, or we could
use just a window of ten words, and

00:03:43.920 --> 00:03:47.380
now we define a word by a vector
over these context counts for

00:03:47.380 --> 00:03:49.320
whatever this context is.

00:03:49.320 --> 00:03:52.370
So, let's suppose we use context
of ten words to the left and

00:03:52.370 --> 00:03:53.180
ten words to the right.

00:03:54.330 --> 00:03:57.370
Here's sample example of
graph from the Brown corpus.

00:03:57.370 --> 00:04:02.350
So here's some words, t he word apricot,
the word pineapple, the word digital and

00:04:02.350 --> 00:04:03.200
the word information.

00:04:03.200 --> 00:04:06.150
They've shown you for
each of them just one set of context.

00:04:06.150 --> 00:04:08.910
Ten words before apricot,
ten words after apricot for

00:04:08.910 --> 00:04:13.020
one of the uses of apricot in
one document in the corpus and

00:04:13.020 --> 00:04:16.480
here's 10 words before and
after pineapple and so long.

00:04:16.480 --> 00:04:20.780
So from the various documents, examples
I've grabbed from the Brown corpus some

00:04:20.780 --> 00:04:24.030
examples for each of these words,
looking like these examples.

00:04:24.030 --> 00:04:28.110
I can compute little counts and I can
build myself the term context matrix.

00:04:29.610 --> 00:04:33.860
So, here's the term-context matrix for
these four words, apricot, pineapple,

00:04:33.860 --> 00:04:34.850
digital, information.

00:04:36.350 --> 00:04:40.640
They don't ever appear with the word
aardvark, but the words digital

00:04:40.640 --> 00:04:45.240
information have the word computer within
ten words of them twice for digital and

00:04:45.240 --> 00:04:49.450
once for information, or the word pinch
occurs with apricot and pineapple,

00:04:49.450 --> 00:04:51.080
which sugar occurs with them.

00:04:51.080 --> 00:04:52.300
Where as the word data,

00:04:52.300 --> 00:04:56.570
and the word result tends to occur with
the word digital and the word information.

00:04:58.400 --> 00:05:00.900
Now again,
we say two words are similar in meaning

00:05:00.900 --> 00:05:02.840
if their context factors are similar.

00:05:02.840 --> 00:05:07.380
So, apricot has a one for
pinch and a one for sugar.

00:05:07.380 --> 00:05:10.400
Pineapple also has a one for pinch and
a one for sugar, and zero for

00:05:10.400 --> 00:05:11.510
computer and data.

00:05:11.510 --> 00:05:14.440
So, that tells us that probably
these words are similar.

00:05:14.440 --> 00:05:19.120
Whereas digital and information, their
counts occur in other words like computer,

00:05:19.120 --> 00:05:21.910
data, and result, so
they're probably similar as well.

00:05:23.440 --> 00:05:27.216
Simple intuition, just like we saw in
information retrieval for comparing

00:05:27.216 --> 00:05:30.877
documents, but now we're comparing
words and using a reduced context.

00:05:34.697 --> 00:05:36.791
Now for the term-document matrix for

00:05:36.791 --> 00:05:39.680
information retrieval we
used tf-idf weighting.

00:05:39.680 --> 00:05:41.150
We didn't use raw counts.

00:05:41.150 --> 00:05:43.670
We used various kinds of weighting,
sometimes tf, sometimes idf,

00:05:43.670 --> 00:05:45.160
sometimes both.

00:05:45.160 --> 00:05:49.240
Now for the term context matrix,
it's very common to use a version of

00:05:49.240 --> 00:05:52.880
pointwise mutual information called
Positive Pointwise Mutual Information.

00:05:52.880 --> 00:05:53.580
So, let's look at that.

00:05:56.090 --> 00:05:59.830
Pointwise Mutual Information is
an information theoretic method that says,

00:05:59.830 --> 00:06:04.830
do events x and y occur     more
often than if they were independent.

00:06:04.830 --> 00:06:09.740
So, the Pointwise Mutual Information
between   two things x and

00:06:09.740 --> 00:06:14.040
y is  the probability of the two
occurring together.    , divided by

00:06:14.040 --> 00:06:17.730
the probability of the two, the product of
the probability of the two independently,

00:06:17.730 --> 00:06:19.420
it would take a lot of that.

00:06:19.420 --> 00:06:23.030
So you can see that, if the two things
occur more often than you would expect

00:06:23.030 --> 00:06:25.774
by chance, more often than you
would expect by independents,

00:06:25.774 --> 00:06:28.485
then the numerator will be much
higher than the denominator.

00:06:29.925 --> 00:06:33.105
So, between two words
the Pointwise Mutual Information

00:06:33.105 --> 00:06:36.835
is the log of the probability of
the two words occurring together

00:06:36.835 --> 00:06:40.535
times the product of the two
words occurring independently.

00:06:40.535 --> 00:06:45.649
And positive PMI simply replaces
all the negative values with zero.

00:06:49.109 --> 00:06:53.750
So, imagine that we have a matrix F or
term-context matrix,

00:06:53.750 --> 00:06:58.929
we'll call it F for frequency and
we've got rows labeled by words and

00:06:58.929 --> 00:07:04.250
we have columns labeled by context
which could be a context word.

00:07:04.250 --> 00:07:08.770
So we saw, for example, a context word
aardvark, or computer, or data, or

00:07:08.770 --> 00:07:10.280
pinch or so on.

00:07:10.280 --> 00:07:15.730
And we're going to take our,
here's our counts in each of these,

00:07:15.730 --> 00:07:22.930
each of these counts is f sub ij,
the frequency of row i column j.

00:07:24.060 --> 00:07:26.920
So, we're going to turn those
into probabilities first.

00:07:26.920 --> 00:07:30.430
So, we'll say that the probability of
a word i and j occurring together,

00:07:30.430 --> 00:07:35.500
the joint probability of a word i and a
context j is the frequency with which they

00:07:35.500 --> 00:07:40.600
appear normalized by the frequency
of all words in all contexts.

00:07:40.600 --> 00:07:45.130
We sum over the entire Matrix,
that's the denominator N.

00:07:46.530 --> 00:07:50.170
The probability of a word,
that's a row, i star,

00:07:50.170 --> 00:07:55.410
is the count of all of the contexts

00:07:55.410 --> 00:08:00.580
that word occur in, so
we sum over all possible contexts.

00:08:00.580 --> 00:08:03.620
And we sum all the counts for that word
in those contexts, normalized by N.

00:08:03.620 --> 00:08:08.620
And the probability of
a context is the sum

00:08:08.620 --> 00:08:12.320
over all words that that context occurs
in, those counts again normalized.

00:08:12.320 --> 00:08:16.550
And we take these probabilities,
the probability of a word and

00:08:16.550 --> 00:08:20.490
a context occurring together times
the probability over probability of

00:08:20.490 --> 00:08:24.390
a word times the probability of a context,
we take that log and that's our PMI.

00:08:25.890 --> 00:08:30.340
And our positive PMI is if it's less
than 0, we will replace it with 0.

00:08:30.340 --> 00:08:37.120
So, let's see how that works in practice.

00:08:37.120 --> 00:08:40.696
I made a simpler little matrix of counts
here for working through our example.

00:08:40.696 --> 00:08:46.730
So, the word digital occurs in
the context computer twice,

00:08:46.730 --> 00:08:51.740
in the context data once, in the context
result once, and never for pinch or sugar.

00:08:51.740 --> 00:08:52.600
Okay, we saw before.

00:08:53.950 --> 00:08:59.210
And again, our probability of
the joint probability of a word, and

00:08:59.210 --> 00:09:03.610
a context is the frequency with which the
word occurs in the context, normalize by

00:09:03.610 --> 00:09:08.140
the total N, and the sum for all counts,
for all words and for all contexts.

00:09:08.140 --> 00:09:10.235
Let's first compute N.

00:09:10.235 --> 00:09:15.225
N is the sum of all these things 2,
3, 4, 10, 11, 12, 13, 14, 15, 19 so,

00:09:15.225 --> 00:09:20.315
end is 19,
our denominator is going to be 19 for

00:09:20.315 --> 00:09:23.525
all of our various probabilities.

00:09:25.145 --> 00:09:26.665
So now, the probability,

00:09:26.665 --> 00:09:31.157
the joint probability of the word
information in the context data.

00:09:31.157 --> 00:09:37.310
So, word is information in the context
data, we've got our f(i) j is 6 and

00:09:38.580 --> 00:09:42.589
our n is 19 so
we have six-nineteenths or 0.32.

00:09:42.589 --> 00:09:44.740
So, that is the joint probability,
information and data.

00:09:46.530 --> 00:09:49.480
Now ,we need to compute
the probability of words and

00:09:49.480 --> 00:09:50.860
compute the probability of context.

00:09:50.860 --> 00:09:55.860
So, the probability of word
we just sum a row over all

00:09:55.860 --> 00:09:57.790
context that that word can occur in.

00:09:57.790 --> 00:10:02.980
So, the word information occurs 11 times.

00:10:02.980 --> 00:10:06.030
Once in this context, plus six, plus four.

00:10:06.030 --> 00:10:10.509
So we have a total of 11 times
over again N of 19, or .58.

00:10:10.509 --> 00:10:11.470
So that's the probability of the word.

00:10:11.470 --> 00:10:14.440
Then we're going to do the same thing for
context.

00:10:14.440 --> 00:10:18.730
We sum over all words that that context
occurs with over those counts and

00:10:18.730 --> 00:10:19.550
normalize.

00:10:19.550 --> 00:10:23.030
So, for the context data,
we have a 1 plus a 6.

00:10:23.030 --> 00:10:25.986
So we have 7/19 or .37.

00:10:28.399 --> 00:10:32.009
So if we do this computation for
all of our examples,

00:10:32.009 --> 00:10:34.230
we'll get this little matrix.

00:10:35.880 --> 00:10:39.290
Here we have the, and
here we have the joint probabilities,

00:10:39.290 --> 00:10:40.480
all the joint probabilities.

00:10:41.480 --> 00:10:44.790
Here we have the marginal,
the probability of the words and

00:10:44.790 --> 00:10:46.150
the probability of the context.

00:10:46.150 --> 00:10:52.390
So again, here's our .37 and
that's the probability of data.

00:10:52.390 --> 00:10:58.763
And Here's our 0.58,

00:10:58.763 --> 00:11:02.479
that's the probability
of the word information.

00:11:02.479 --> 00:11:05.392
And there's our 0.32, okay?

00:11:09.015 --> 00:11:11.240
So now we're ready to compute PMI.

00:11:11.240 --> 00:11:16.190
Recall that PMI is the log based 2
of the joint over the two marginals.

00:11:16.190 --> 00:11:21.028
So, that's going to be our .32
divided by our .37 times .58 and

00:11:21.028 --> 00:11:26.680
then we'll take the log of that, so
that's .58 or .57 using full precision.

00:11:28.200 --> 00:11:31.780
And we can see here,
that if we take this log,

00:11:31.780 --> 00:11:35.160
that we get positive numbers for
computer being linked with digital,

00:11:35.160 --> 00:11:39.310
and information with data, and
pineapple with sugar, and so on.

00:11:39.310 --> 00:11:42.010
And we've replaced any of
the time we got negative numbers,

00:11:42.010 --> 00:11:45.837
since this is positive PMI, we've replaced
the negative numbers with the zeroes.

00:11:48.430 --> 00:11:52.480
Now the problem with PMI is it's
weighted toward infrequent events and

00:11:52.480 --> 00:11:54.360
there is a lovely survey
paper by Turney and

00:11:54.360 --> 00:11:57.290
Pantel that walks through some of
the ways that you can alleviate this.

00:11:57.290 --> 00:12:01.570
But it turns out that very simple methods
like Laplace Smoothing can actually help.

00:12:03.110 --> 00:12:05.930
Let's look at how this
works with Add-2 smoothing.

00:12:05.930 --> 00:12:08.710
So here I've just added
two to all the counts, so

00:12:08.710 --> 00:12:10.250
we just have simple Add-2 smoothing.

00:12:11.580 --> 00:12:16.090
And we computed our probability table,
again with our marginals for

00:12:16.090 --> 00:12:17.230
the context and the word.

00:12:18.490 --> 00:12:21.280
And now what I'm showing you
here is our positive PMI tables,

00:12:21.280 --> 00:12:23.570
here's our original table for
no smoothing.

00:12:23.570 --> 00:12:26.470
And here we are with add two smoothing and

00:12:26.470 --> 00:12:30.310
then what you'll notice is that in
the original table without smoothing.

00:12:30.310 --> 00:12:35.420
The link between apricot and pineapple
in sugar was very high, we get a very

00:12:35.420 --> 00:12:39.090
high PMI despite the fact I don't know if
you remember that the counts were just 1.

00:12:39.090 --> 00:12:42.970
Whereas data for which we had a lot
more evidence, counts of 6 and 4,

00:12:42.970 --> 00:12:45.760
the link between information and
data, or information and

00:12:45.760 --> 00:12:48.220
result, had much lower PMI values.

00:12:48.220 --> 00:12:51.390
Adding two affects the lower counts
more than the higher counts.

00:12:51.390 --> 00:12:55.392
And we can see that the larger counts
haven't changed very much, but

00:12:55.392 --> 00:12:58.704
we've discounted those
excessively high PMI values and

00:12:58.704 --> 00:13:02.431
now we have much more reasonable,
related numbers for our PMI.

00:13:04.278 --> 00:13:08.494
So, that's the first half of our
introduction to distributional similarity,

00:13:08.494 --> 00:13:11.656
and we'll turn in the second half
to how to use cosine metrics

00:13:11.656 --> 00:13:13.640
to actually compute the similarity.

