WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.317
[MUSIC]

00:00:05.317 --> 00:00:07.626
Stanford University
&gt;&gt; Hi, everyone.

00:00:07.626 --> 00:00:12.411
Welcome to the midterm of this session for
CS224N.

00:00:12.411 --> 00:00:17.150
I'm just gonna start
with some announcements.

00:00:17.150 --> 00:00:22.030
So the first one, as I hope you're
aware of, homework 2 is due today.

00:00:22.030 --> 00:00:24.800
And we had an issue with
the submissions script

00:00:24.800 --> 00:00:28.060
that made submissions unavailable for
a couple hours.

00:00:28.060 --> 00:00:31.100
So we apologize for that,
but that is now fixed.

00:00:31.100 --> 00:00:34.576
There's a slight change to the submission
instructions to the initial version.

00:00:34.576 --> 00:00:37.796
And that is we don't want you to
include the trained weights for

00:00:37.796 --> 00:00:41.162
your models, because it turns out,
that takes up a lot of space.

00:00:41.162 --> 00:00:45.510
And we run out of AFS, and
we can't take any more submissions.

00:00:45.510 --> 00:00:47.200
So we apologize for that inconvenience.

00:00:48.200 --> 00:00:50.820
The other thing due today
is the project proposal, so

00:00:50.820 --> 00:00:52.320
these are both due at midnight.

00:00:52.320 --> 00:00:55.852
And we are saying that if you have not yet
found a mentor, but

00:00:55.852 --> 00:01:00.585
you would still like to do a final project
other than the default final project,

00:01:00.585 --> 00:01:03.080
you can still submit a project proposal.

00:01:03.080 --> 00:01:05.697
And say,
that you've not found a mentor yet, and

00:01:05.697 --> 00:01:08.325
we would do our best to find you a mentor,
anyway.

00:01:11.633 --> 00:01:14.620
Now, for a couple of notes on the midterm.

00:01:14.620 --> 00:01:16.690
So the midterm is Tuesday.

00:01:16.690 --> 00:01:18.538
Same time as the regular lecture, but

00:01:18.538 --> 00:01:20.978
I want to point out that
it’s not being held here.

00:01:20.978 --> 00:01:24.250
It’s being held in Memorial Auditorium.

00:01:24.250 --> 00:01:29.363
There is an alternate exam only if you
cannot at all make the February 14th,

00:01:29.363 --> 00:01:31.023
is normal midterm time.

00:01:31.023 --> 00:01:33.440
And if you want to take
the alternative exam,

00:01:33.440 --> 00:01:37.460
or you have to take the alternative exam,
I'll post about that on Piazza.

00:01:37.460 --> 00:01:39.330
We are allowing a single cheat sheet.

00:01:40.580 --> 00:01:45.075
And the material on this midterm
is all the lectures including

00:01:45.075 --> 00:01:46.637
the most recent one.

00:01:46.637 --> 00:01:51.352
The midterm is gonna be a mix of multiple
choice, and true/false questions,

00:01:51.352 --> 00:01:52.924
short answer questions,

00:01:52.924 --> 00:01:57.797
and some longer more involved questions
that may involve radiant computation.

00:01:57.797 --> 00:02:04.367
And for SCPD students,
you have to turn up in person,

00:02:04.367 --> 00:02:09.628
or have an exam monitor pre-registered.

00:02:09.628 --> 00:02:12.930
The rest of this lecture is going
to be just review material to help

00:02:12.930 --> 00:02:14.440
prepare you for the midterm.

00:02:14.440 --> 00:02:18.907
And in particular, we're going to
cover word vector representations,

00:02:18.907 --> 00:02:23.241
neural network basics,
back propagation and gradient computation.

00:02:23.241 --> 00:02:28.241
And we're gonna do some example problems
on the white board for that section,

00:02:28.241 --> 00:02:30.220
RNNs and Dependency Parsing.

00:02:31.430 --> 00:02:34.115
And with that,
I will leave it to our first section,

00:02:34.115 --> 00:02:35.773
Word Vector Representations.

00:03:03.390 --> 00:03:07.320
All right, we can start that again.

00:03:07.320 --> 00:03:08.070
So what are word vectors?

00:03:08.070 --> 00:03:11.010
For every word,
we could come up with a vector that

00:03:11.010 --> 00:03:14.900
encapsulates semantic
information about the word.

00:03:14.900 --> 00:03:17.278
There could be various ways of
interpreting what meaning or

00:03:17.278 --> 00:03:19.080
semantic representation could be.

00:03:19.080 --> 00:03:20.390
And which is why it so

00:03:20.390 --> 00:03:24.870
severely depends on how you actually
go about training your word vectors.

00:03:24.870 --> 00:03:27.170
In this class,
we first covered word2vec and GloVe, and

00:03:27.170 --> 00:03:28.110
we'll be reviewing that now.

00:03:29.820 --> 00:03:34.540
So just to recap, Word2Vec,
the task for it is,

00:03:34.540 --> 00:03:40.000
learn word vectors to encode the
probability of a word given its context.

00:03:41.040 --> 00:03:44.100
Now, consider this example,
our window size is 2 here.

00:03:44.100 --> 00:03:46.328
I understand the Word2Vec model now.

00:03:46.328 --> 00:03:50.220
If you consider the center word to
be Word2Vec, understand the and

00:03:50.220 --> 00:03:52.540
model now become context words.

00:03:52.540 --> 00:03:55.230
And you'll see how to
probabilistically model this now.

00:03:56.860 --> 00:03:59.490
So for each work, we have two vectors.

00:03:59.490 --> 00:04:03.350
The input vector and the output vector,
then for vectors represented by v and

00:04:03.350 --> 00:04:05.030
the output vector by u.

00:04:05.030 --> 00:04:07.850
And we'll see how v and
u are used in the model now.

00:04:09.745 --> 00:04:12.465
So we'll be covering two algorithms here,
Skipgram,

00:04:12.465 --> 00:04:16.265
which predicts the probability of
the context words given the center word.

00:04:16.265 --> 00:04:18.350
And the Continuous Bag-of-Words, or CBOW,

00:04:18.350 --> 00:04:20.965
which predicts the center word
from surrounding context.

00:04:20.965 --> 00:04:24.011
All right, so to recap on Skipgram,

00:04:24.011 --> 00:04:30.910
consider the sentence again where I
understand the Word2Vec model now.

00:04:30.910 --> 00:04:33.650
And in this case,
since the center word is Word2Vec,

00:04:33.650 --> 00:04:36.530
that'll be the only word that's left,
and the context words will be omitted.

00:04:36.530 --> 00:04:41.360
And our job is to actually go about and
assign probably the words there.

00:04:43.540 --> 00:04:45.490
So, how do we go about doing this?

00:04:45.490 --> 00:04:47.600
We first generate a one-hot vector.

00:04:47.600 --> 00:04:51.940
In this case, this vector will have the
same dimensions as the number of words in

00:04:51.940 --> 00:04:57.780
a vocabulary, with one of the index of
the word Word2Vec, and 0 everywhere else.

00:04:57.780 --> 00:05:01.904
Now, we need to use this one-hot vector
to index into over embedding matrix,

00:05:01.904 --> 00:05:05.038
which is capital V,
the input vector in embedding matrix.

00:05:05.038 --> 00:05:10.667
And we looked in a word vector
corresponding to that word.

00:05:10.667 --> 00:05:16.156
We then go and do a doc product
with the output word vectors hue,

00:05:16.156 --> 00:05:19.390
and to generate a score for it.

00:05:19.390 --> 00:05:20.010
So far, so good.

00:05:20.010 --> 00:05:20.700
Any questions?

00:05:22.530 --> 00:05:27.440
So once we get a score,
we get the probabilities by using Softmax.

00:05:27.440 --> 00:05:31.310
And once we have those, we have Softmax
probabilities for every context word.

00:05:31.310 --> 00:05:34.510
And we can then find out which
word actually would go into these

00:05:34.510 --> 00:05:35.390
context windows.

00:05:36.840 --> 00:05:39.100
How do we actually go about training this?

00:05:39.100 --> 00:05:43.780
We assign over a cost, which is either
given by Softmax cross-entropy, or

00:05:43.780 --> 00:05:46.630
the negative sampling loss.

00:05:46.630 --> 00:05:51.617
And as you can see,
the formula takes abstracts or

00:05:51.617 --> 00:05:54.718
the cost function over there.

00:05:54.718 --> 00:06:00.706
So we take the input vector, and we take
the word vector for the context words,

00:06:00.706 --> 00:06:06.010
and we apply these cost functions and
sum them over, okay?

00:06:06.010 --> 00:06:07.749
And our job is to minimize this loss.

00:06:07.749 --> 00:06:11.432
I'm using cost and loss interchangeably,

00:06:11.432 --> 00:06:16.620
as we will do that in the midterm
in the class as well.

00:06:16.620 --> 00:06:17.570
That is Skipgram.

00:06:17.570 --> 00:06:20.165
Now, we move onto Continuous Bag Of Words,
CBOW.

00:06:20.165 --> 00:06:23.300
Now, let's take the previous
sentence again.

00:06:23.300 --> 00:06:25.240
I understand the Word2Vec model now.

00:06:25.240 --> 00:06:28.270
In this case, our job is to
actually predict the context word,

00:06:28.270 --> 00:06:32.060
or the center word, my bad,
given the context words.

00:06:32.060 --> 00:06:36.430
So in this case, our job would be to
actually guess the Word2Vec model here.

00:06:36.430 --> 00:06:40.420
So in this case, as in the previous case
as well, we generate one-hot vectors for

00:06:40.420 --> 00:06:41.630
each of the context words.

00:06:43.000 --> 00:06:43.820
And this would, again,

00:06:43.820 --> 00:06:50.420
be the size of your vocabulary with
one at the index of the word itself.

00:06:50.420 --> 00:06:54.290
And then we look into our
embedding matrix, again, the input

00:06:54.290 --> 00:06:58.190
vector embedding matrix, and obtain
the word vectors for those context words.

00:06:59.370 --> 00:07:02.310
We take the average of those word vectors,

00:07:02.310 --> 00:07:06.880
and then compute the score again by
multiplying with the output vector matrix.

00:07:08.540 --> 00:07:09.583
Now, that we have the scores,

00:07:09.583 --> 00:07:11.804
we can get the probabilities by
computing the Softmax function.

00:07:11.804 --> 00:07:16.650
And now, we have for
each of the words in the vocabulary.

00:07:16.650 --> 00:07:20.528
We have the probability assigned that what
how likely it is to be the center word.

00:07:20.528 --> 00:07:23.050
All clear so far?

00:07:23.050 --> 00:07:26.840
This should be pretty clear since we
also had a live coding session on this.

00:07:28.840 --> 00:07:29.340
Sweet.

00:07:30.630 --> 00:07:35.130
The cost function here is similar to
the one in Skipgram except, in this case,

00:07:35.130 --> 00:07:39.790
it takes in the average of the word
vectors as one of the arguments and

00:07:39.790 --> 00:07:41.361
the center word itself.

00:07:42.520 --> 00:07:46.903
And again,
our job is to minimize the loss here.

00:07:46.903 --> 00:07:47.417
Sweet.

00:07:47.417 --> 00:07:50.628
We didn't do this as a part
of the assignment, but

00:07:50.628 --> 00:07:54.890
GloVe is a pretty famous set
of word vectors, as well.

00:07:54.890 --> 00:07:58.440
Like Word2Vec, it also captures
semantic information about the words.

00:07:58.440 --> 00:07:59.922
But unlike Word2Vec,

00:07:59.922 --> 00:08:04.640
however, it also takes into account
the co-occurence statistics.

00:08:04.640 --> 00:08:07.206
And we'll see what we
refer to just in a second.

00:08:07.206 --> 00:08:12.407
From one of the lines in our review notes,
GloVe consists of a weighted

00:08:12.407 --> 00:08:17.990
least squared model that trains on
global word co-occurrence accounts.

00:08:17.990 --> 00:08:19.590
So what do we mean by co-occurence?

00:08:19.590 --> 00:08:22.950
Consider our corpus to be only
a set of three sentences.

00:08:22.950 --> 00:08:23.930
I like deep learning.

00:08:23.930 --> 00:08:26.020
I like NLP and I enjoy flying.

00:08:26.020 --> 00:08:29.440
Now, you can see that we can come
up with a n by n where n represents

00:08:29.440 --> 00:08:31.950
the number of words in our corpus.

00:08:31.950 --> 00:08:36.484
A matrix where each index
corresponds to whether word GA

00:08:36.484 --> 00:08:40.350
belongs in the context window of word I,
okay?

00:08:42.900 --> 00:08:44.685
This is a symmetric matrix as well.

00:08:44.685 --> 00:08:46.295
So you can, as you can assume.

00:08:46.295 --> 00:08:47.445
The ordering does not matter.

00:08:48.515 --> 00:08:51.405
So let us denote this matrix by x.

00:08:51.405 --> 00:08:54.365
And so if you index into this using -ing,

00:08:54.365 --> 00:08:58.305
that would be the number of times word
j occurs in the context of word i.

00:08:58.305 --> 00:09:02.225
And let us also denote xi to be
the number of times any word

00:09:02.225 --> 00:09:04.425
appears in the context window of a word i.

00:09:07.040 --> 00:09:09.370
Just like in Word2Vec,
we also have two vectors for

00:09:09.370 --> 00:09:12.370
each word here, the input vector and
the output vector.

00:09:12.370 --> 00:09:14.980
And the cost function is
given by the equation here.

00:09:14.980 --> 00:09:17.790
There are a couple of interesting things
about this cost function which we'll cover

00:09:17.790 --> 00:09:18.950
right in a second.

00:09:18.950 --> 00:09:20.810
But a few things to note here,

00:09:22.780 --> 00:09:25.600
we're applying the least squares
as we mentioned earlier.

00:09:27.560 --> 00:09:31.390
We are iterating over every pair of
words in the co-occurrence matrix.

00:09:33.420 --> 00:09:38.630
What's interesting is that since you can
imagine words like d or a, the articles

00:09:38.630 --> 00:09:45.070
as the stock words would have a large xij
value, we need to sort of clip on it.

00:09:45.070 --> 00:09:48.400
And the other option is to
just take the logarithm of it.

00:09:48.400 --> 00:09:52.590
And so which is why you can see there's
a log over the xij over there, okay?

00:09:55.850 --> 00:09:57.960
So any questions about
the cost function here?

00:09:59.810 --> 00:10:01.442
Okay, let's move on then.

00:10:01.442 --> 00:10:06.552
So now, after minimizing this loss for the
GloVe word vectors, they will be having

00:10:06.552 --> 00:10:11.930
V and U, where V represents the output
word vectors and U represents the input.

00:10:11.930 --> 00:10:17.420
And since both of these capture
similar co-occurrence information,

00:10:17.420 --> 00:10:21.370
we can obtain the word vector for
a single word just by summing them up.

00:10:21.370 --> 00:10:25.000
So just U plus V will equal the word
vector for that particular word.

00:10:27.518 --> 00:10:30.470
And we can use that for
all sorts of NLP tasks.

00:10:30.470 --> 00:10:32.090
It's essentially an embedding for
that word.

00:10:34.065 --> 00:10:36.505
So before we move on from
word vector presentations,

00:10:36.505 --> 00:10:39.515
any questions on any of the terminology?

00:10:39.515 --> 00:10:44.135
We got a lot of questions about how does
an embedding defer from a word vector.

00:10:44.135 --> 00:10:48.725
We want to make sure that that
definitions ambiguity is cleared up.

00:10:48.725 --> 00:10:49.225
Yes, please.

00:10:57.121 --> 00:11:01.570
It is a vector, I mean it is a scalar.

00:11:01.570 --> 00:11:03.742
Yes, the question was.

00:11:03.742 --> 00:11:09.430
If Xi is a vector, shouldn't the laws
actually be referring to a scalar?

00:11:09.430 --> 00:11:12.664
Xi is actually a scalar because it's
solely the number of times that word

00:11:12.664 --> 00:11:14.850
appears in the context
window of any other word.

00:11:18.780 --> 00:11:22.775
So Xi is defined as the number of times
any word key appears in the context,

00:11:22.775 --> 00:11:24.005
so it's not a vector.

00:11:24.005 --> 00:11:24.634
Yes.

00:11:28.444 --> 00:11:30.828
Most research papers tend
to use embeddings and

00:11:30.828 --> 00:11:32.860
word vectors pretty interchangeably.

00:11:32.860 --> 00:11:37.335
Basically, you can imagine it's a a vector
that encapsulates semantic information.

00:11:37.335 --> 00:11:41.919
So in terms of analogies, you can
imagine that if you had word vectors for

00:11:41.919 --> 00:11:46.503
king, man, queen, and woman,
then if you subtract king minus queen,

00:11:46.503 --> 00:11:50.515
it should be roughly equal into
man minus woman, for example.

00:11:52.445 --> 00:11:54.185
Sweet, any other questions?

00:11:55.875 --> 00:11:56.375
Yes.

00:11:59.703 --> 00:12:03.924
So those input vectors and output vectors
are solely just differing in the sense

00:12:03.924 --> 00:12:08.710
that, if you have a context word, we are
going to predict the output vector for it.

00:12:08.710 --> 00:12:10.920
So in any case,
you're considering a word solely,

00:12:10.920 --> 00:12:13.860
you're considering only
the output vectors.

00:12:13.860 --> 00:12:17.100
Okay, all right, so
let's cover neural networks.

00:12:17.100 --> 00:12:21.730
This will be roughly quick because we
also have RNNs and LSTMs to cover.

00:12:21.730 --> 00:12:24.020
So coming back to the basics,
you have a data set x and

00:12:24.020 --> 00:12:26.430
y where x is the input, y are the labels.

00:12:26.430 --> 00:12:28.926
And you want to train a 1-hidden
layer neural network on this.

00:12:28.926 --> 00:12:33.577
You do a forward pass given by xW + b,
and then you apply non-linearity or

00:12:33.577 --> 00:12:35.850
a activation function on it.

00:12:35.850 --> 00:12:37.070
You compute the loss,

00:12:37.070 --> 00:12:39.890
you compute the gradients from
that loss using back propagation.

00:12:39.890 --> 00:12:42.030
We'll go into back
propagation with Barack.

00:12:43.260 --> 00:12:45.930
Then we update the weight using
an optimization algorithm like

00:12:45.930 --> 00:12:48.280
saccustic gradient descent, or SGD.

00:12:48.280 --> 00:12:52.900
We do hyperparameter tuning of Dev set,
not on the Test set.

00:12:52.900 --> 00:12:56.830
And then evaluate the neural network
on the Test set itself, all right?

00:12:56.830 --> 00:13:02.437
So this was a very high level of review
of how neural networks have changed.

00:13:02.437 --> 00:13:05.820
Let's go where the activation functions
are the nonlinearities quite quickly.

00:13:05.820 --> 00:13:06.770
So we have the sigmoid function.

00:13:06.770 --> 00:13:10.600
What it does it takes and input and
squashes at between 0 and 1.

00:13:10.600 --> 00:13:12.730
However, this has some problems.

00:13:12.730 --> 00:13:14.750
If the activation function was very large,

00:13:14.750 --> 00:13:18.350
if the activation was very large,
is still always end up being 1?

00:13:18.350 --> 00:13:22.140
Or if the activation is really small,
it's still end up always being 0?

00:13:22.140 --> 00:13:25.330
So you can imagine that a lot of
neurons get saturated by this.

00:13:25.330 --> 00:13:27.660
And the output is not centered as 0.

00:13:27.660 --> 00:13:32.240
This is particularly bad,
because if the output for our sigmoid

00:13:32.240 --> 00:13:37.230
is always a positive number, then the
gradients are always negative or positive.

00:13:37.230 --> 00:13:43.100
And we do not want that, we want
the gradients to be more adaptive, okay?

00:13:43.100 --> 00:13:47.724
Another issue, this is also that takes
the exponent and it's computationally

00:13:47.724 --> 00:13:51.183
expensive, but nothing in here
can run with the map there.

00:13:51.183 --> 00:13:52.443
Okay, then the tan h function,

00:13:52.443 --> 00:13:55.440
[INAUDIBLE] an input squashes
at between negative 1 and 1.

00:13:55.440 --> 00:13:59.080
Here, in this case, the output is centered
at 0 so which is a nice thing, and

00:13:59.080 --> 00:14:01.433
it sort of resolves
the problem that sigmoid had.

00:14:01.433 --> 00:14:06.494
However, similar to a sigmoid, this
also kills the gradients at saturation.

00:14:06.494 --> 00:14:09.090
And which is why tan h
is not as good as well.

00:14:09.090 --> 00:14:09.780
Yes?

00:14:14.193 --> 00:14:18.568
Right, so if your input,
if the output of your node.

00:14:18.568 --> 00:14:23.215
The question was, why is it bad
that the output of our activation

00:14:23.215 --> 00:14:25.720
function is is not centered at 0?

00:14:25.720 --> 00:14:29.490
So if the output of your activation
function is always positive,

00:14:29.490 --> 00:14:34.890
then the gradients are always positive or
always negative,

00:14:34.890 --> 00:14:40.746
because they can only go in one
of the two directions, always.

00:14:40.746 --> 00:14:42.095
Yes.

00:14:42.095 --> 00:14:46.025
They do break the, so we are able to
train networks with this function.

00:14:46.025 --> 00:14:50.525
So it's not like it's the end of the world
in terms of training neutral networks.

00:14:50.525 --> 00:14:54.575
So in this case, what would actually be
better is that you take your input and

00:14:54.575 --> 00:14:56.175
you center it at zero, either way works.

00:15:00.170 --> 00:15:02.625
Moving on to ReLU,
the Rectify Linear Unit,

00:15:02.625 --> 00:15:06.640
which essentially takes some acts
on checks if it's greater than 0.

00:15:06.640 --> 00:15:10.996
If it's greater than 0,
it returns the same value.

00:15:10.996 --> 00:15:13.960
It does not have the problem of
saturation because it's linear.

00:15:13.960 --> 00:15:17.200
It's confidentially cheap,
there are no explain on calculations.

00:15:17.200 --> 00:15:20.679
And it's also empirically known
to converge faster, right?

00:15:20.679 --> 00:15:21.784
Yes?

00:15:25.171 --> 00:15:27.260
Yeah, so that is an error.

00:15:27.260 --> 00:15:31.210
So which is why I have the second point
in the problem says referring to that.

00:15:31.210 --> 00:15:34.980
Yeah, so the problem with this is
that it's again, not centered at 0.

00:15:34.980 --> 00:15:38.536
The larger problem with this, and
it's slightly annoying as well,

00:15:38.536 --> 00:15:42.600
is that if the input is less than 0,
then the ReLU gradient is always 0.

00:15:42.600 --> 00:15:46.710
And this is a problem because once
a neuron dies, it always is dead.

00:15:46.710 --> 00:15:50.505
You can never revive a neuron after
the input becomes less than 0.

00:15:52.910 --> 00:15:53.510
Sad times, I know.

00:15:54.550 --> 00:15:58.220
Anyways, moving on to stochastic gradient
descent or the optimization algorithm.

00:15:58.220 --> 00:16:01.200
In this case, theta represents
our weights or the parameters.

00:16:01.200 --> 00:16:04.870
Alpha is our learning rate and
J is our loss or the cost function.

00:16:04.870 --> 00:16:10.814
And going very technically, SGD update
happens after every training example.

00:16:10.814 --> 00:16:13.928
However, researchers tend
to abuse the notation and

00:16:13.928 --> 00:16:16.500
call minibatch SGD also normal SGD.

00:16:16.500 --> 00:16:19.215
And what minibatch stochastic
gradient descent does,

00:16:19.215 --> 00:16:21.927
is it takes a small batch of
training examples at once and

00:16:21.927 --> 00:16:25.181
averages their loss over before
performing the gradient update.

00:16:28.482 --> 00:16:31.418
Okay, moving on to regularization,
this is a form,

00:16:31.418 --> 00:16:33.710
dropout is a form of regularization.

00:16:33.710 --> 00:16:36.770
And it randomly drops neurons
at forward pass during training.

00:16:36.770 --> 00:16:39.580
While this might seem very
counterintuitive that it should work,

00:16:39.580 --> 00:16:41.640
it does because it prevents overfitting.

00:16:41.640 --> 00:16:43.840
It forces the network
to learn dependencies.

00:16:43.840 --> 00:16:47.080
When you have fewer number of
neurons to learn the same task,

00:16:47.080 --> 00:16:52.240
it forces each neuron to capture more
important information about your data and

00:16:52.240 --> 00:16:54.620
learn more important abstractions.

00:16:54.620 --> 00:16:58.230
So think about dropout as
training an ensemble of networks.

00:16:58.230 --> 00:17:01.740
Every time you run your
network during forward pass,

00:17:01.740 --> 00:17:03.210
a couple of neurons are dropped.

00:17:03.210 --> 00:17:06.660
And as a result, you are getting
a new network almost every time and

00:17:06.660 --> 00:17:09.580
then you're training
an ensemble of networks.

00:17:09.580 --> 00:17:12.913
During test time, make sure that you
turn your dropout also that you again

00:17:12.913 --> 00:17:14.473
take this ensemble of networks and

00:17:14.473 --> 00:17:17.080
then use the shared power
between all of these networks.

00:17:18.770 --> 00:17:20.792
Any questions on dropout?

00:17:20.792 --> 00:17:21.465
Yes.

00:17:27.671 --> 00:17:28.848
So it depends.

00:17:28.848 --> 00:17:29.790
If you make sure that.

00:17:31.310 --> 00:17:34.359
So the question was do you have to
scale your output whenever you run

00:17:34.359 --> 00:17:35.320
dropout at tester?

00:17:35.320 --> 00:17:36.360
And the answer to that is no.

00:17:36.360 --> 00:17:40.579
And just because if during test time
you make sure that your activations

00:17:40.579 --> 00:17:45.238
are already scaled correctly, then you
don't have to do it during test time.

00:17:48.556 --> 00:17:51.030
So, a few training tips and tricks here.

00:17:51.030 --> 00:17:52.690
These are a couple of loss plots.

00:17:52.690 --> 00:17:59.210
So the one in the green represents
a phenomenon when your loss is very noisy.

00:17:59.210 --> 00:18:01.800
And what happens,
you can see the jagged line.

00:18:01.800 --> 00:18:04.770
And in this case,
what must be happening is that since your

00:18:04.770 --> 00:18:09.890
gradient updates are very large,
you are not converging properly.

00:18:09.890 --> 00:18:13.830
You are almost iterating over and
around the local minima,

00:18:13.830 --> 00:18:17.030
which is why it's advisable to lower
your learning rate in that case.

00:18:17.030 --> 00:18:19.241
If it's blue,
you see that there is high potential for

00:18:19.241 --> 00:18:20.887
your network to actually train faster.

00:18:20.887 --> 00:18:24.855
And hence,
you should make your learning rate higher.

00:18:24.855 --> 00:18:25.413
Okay?

00:18:25.413 --> 00:18:26.990
The red line is sort of an ideal one.

00:18:26.990 --> 00:18:29.540
Again, this is something
I just drew up on Paint.

00:18:29.540 --> 00:18:31.858
So it's not something
that's very technical.

00:18:31.858 --> 00:18:32.919
It's just intuition.

00:18:32.919 --> 00:18:33.964
All right.

00:18:33.964 --> 00:18:40.090
So this is something you must have faced
issues with in the last assignment.

00:18:40.090 --> 00:18:45.050
Where if the gap between your training
curve and your dev accuracy is very large,

00:18:45.050 --> 00:18:46.490
that means you're overfitting.

00:18:46.490 --> 00:18:49.450
And in this case, there are a couple
of ways you can actually counter that.

00:18:49.450 --> 00:18:51.680
One is by increasing your
regularization constant.

00:18:51.680 --> 00:18:56.520
This could be by increasing
your dropout rate or

00:18:56.520 --> 00:18:57.688
by increasing the L2 Norm rate, okay?

00:18:59.910 --> 00:19:03.150
And again, this is very
important to stress that do not

00:19:03.150 --> 00:19:07.420
test your model in the test set before
you're done resolving overfitting issue.

00:19:07.420 --> 00:19:10.854
It almost sort of breaks the scientific
pipeline if you test your network on

00:19:10.854 --> 00:19:12.925
the test set and
then tweak your parameters,

00:19:12.925 --> 00:19:15.394
because you're not supposed
to look at the test set.

00:19:23.512 --> 00:19:26.311
So the question was, if you're not
supposed to test them on the test set,

00:19:26.311 --> 00:19:28.580
then what kind of data are you
suppose to test it on?

00:19:28.580 --> 00:19:32.320
And the answer to that is you can
divide your data into three parts.

00:19:32.320 --> 00:19:33.975
Training, dev, and test.

00:19:33.975 --> 00:19:35.180
You train on the train.

00:19:35.180 --> 00:19:37.600
You evaluate regularly on the dev set.

00:19:37.600 --> 00:19:40.600
And then, finally at the very end,
test on the test set, okay?

00:19:40.600 --> 00:19:44.091
With that, I'll leave you to Barak for
Backpropagation and Gradients.

00:19:51.227 --> 00:19:52.090
&gt;&gt; All right, hi everyone.

00:19:52.090 --> 00:19:55.300
It's another bright and
sunny day to do some backpropagation.

00:19:55.300 --> 00:19:56.150
I mean, cold and rainy.

00:19:56.150 --> 00:19:59.870
I think it's also pretty ironic that
last time I spoke to you from here,

00:19:59.870 --> 00:20:02.680
I told you about all of the wonderful
things that TensorFlow can do

00:20:02.680 --> 00:20:06.320
with automatic differentiation, and here
we are again computing gradients by hand.

00:20:06.320 --> 00:20:07.320
But anyway, so

00:20:07.320 --> 00:20:11.240
the main ideas of this part of the review
is to go over some of the intuition and

00:20:11.240 --> 00:20:14.470
the math behind backpropogation,
as well as how to use it in practice.

00:20:14.470 --> 00:20:19.570
Before I begin, I highly urge you to check
out Kevin's notes on gradient computation,

00:20:19.570 --> 00:20:22.480
as well as some of the principles
behind like matrix calculus.

00:20:22.480 --> 00:20:25.310
I think it's very helpful to understand

00:20:25.310 --> 00:20:27.920
what's on those notes to help
you the most with the midterm.

00:20:27.920 --> 00:20:32.770
Okay, so our itinerary is to first
review what backpropagation is,

00:20:32.770 --> 00:20:35.760
then to have a quick chat
about matrix calculus.

00:20:35.760 --> 00:20:40.190
Then, we're gonna talk about how to
compute products of gradients correctly.

00:20:40.190 --> 00:20:43.490
In other words,
when do I transpose my darn symbols?

00:20:43.490 --> 00:20:46.470
And then,
we're going to solve two midterm problems.

00:20:46.470 --> 00:20:51.340
Okay, so the problem statement that we are
looking at is given some function f with

00:20:51.340 --> 00:20:56.702
respect to inputs x and some labels y,
including some parameters theta,

00:20:56.702 --> 00:21:02.300
we wanna compute the gradient of your Loss
with respect to all your parameters theta.

00:21:02.300 --> 00:21:07.510
And what backpropagation is, it is
an algorithm that allows you to compute

00:21:07.510 --> 00:21:13.970
the gradient for some compound function as
a series of local, intermediate gradients.

00:21:13.970 --> 00:21:18.506
So if you have some compound function,
backpropogation is the tool that you have

00:21:18.506 --> 00:21:22.507
that allows you to compute the total
gradient through an application of

00:21:22.507 --> 00:21:26.727
the chain rule to all of the local
intermediate gradients in you function.

00:21:26.727 --> 00:21:29.410
So there are three kind of
parts to backpropagation.

00:21:29.410 --> 00:21:33.350
The first is that you want to identify
what your intermediate functions are, i.e,

00:21:33.350 --> 00:21:34.440
the intermediate variables.

00:21:34.440 --> 00:21:37.520
And this is basically done for
you in the forward propagation stage.

00:21:37.520 --> 00:21:41.429
You then want to know what the local
gradients are of all of the variables

00:21:41.429 --> 00:21:43.264
inside your compound function.

00:21:43.264 --> 00:21:46.975
And then you want to somehow combine those
through some application of the chain rule

00:21:46.975 --> 00:21:48.710
to get your full gradients.

00:21:48.710 --> 00:21:51.280
So the first thing that we need to
talk about is what modularity is and

00:21:51.280 --> 00:21:53.990
let us look at is at
an extremely simple example.

00:21:53.990 --> 00:21:56.817
We have some function of three
scalar variables x, y, z.

00:21:56.817 --> 00:21:59.980
We're going to add x and y and
then multiply that by z.

00:21:59.980 --> 00:22:03.260
Our intermediate variable is q,
which is equal to x + y.

00:22:03.260 --> 00:22:06.190
And our final compound function is q x z.

00:22:06.190 --> 00:22:11.000
So the idea of modularity is to
kind of separate out our smaller

00:22:11.000 --> 00:22:15.830
operations such that each level of our
forward propagation, we know how to

00:22:15.830 --> 00:22:20.320
complete the gradient of the output
with respect to our inputs of interest.

00:22:20.320 --> 00:22:23.650
So the key idea of modularity is
that it allows you separate out

00:22:23.650 --> 00:22:24.920
the operations that you're using.

00:22:24.920 --> 00:22:26.670
So that at each point,

00:22:26.670 --> 00:22:30.220
you're able to calculate the gradient
in some sort of palatable way.

00:22:30.220 --> 00:22:34.450
Okay, so, modularity for a neural network
is basically what we've seen in our

00:22:34.450 --> 00:22:38.510
assignments so far, in terms of
splitting up our compound function

00:22:38.510 --> 00:22:42.480
as a series of all of
our propagation steps.

00:22:42.480 --> 00:22:46.220
So in this example of a two-layer
neural network, we have the loss of

00:22:46.220 --> 00:22:50.050
some cross-entropy of a sigmoid
activation of some linear function

00:22:50.050 --> 00:22:54.405
again applying to linear function over
that and taking the cross-entropy with y.

00:22:54.405 --> 00:22:57.015
So intermediate variables are going to
be all the things you're familiar with.

00:22:57.015 --> 00:22:59.905
It's going to be our hidden layer,
it's going to be the activaion,

00:22:59.905 --> 00:23:04.795
it's going to be our scoring, which is z2
in this example, followed by our Loss.

00:23:04.795 --> 00:23:09.325
So the idea is that at each step of this
forward propagation stage we know how to

00:23:09.325 --> 00:23:12.875
compute the gradient of our output
with respect to our inputs.

00:23:13.930 --> 00:23:17.680
So, let us look at how
the forward propagation and

00:23:17.680 --> 00:23:19.870
the backward propagation
relate to each other.

00:23:19.870 --> 00:23:21.630
On the left,
we have the forward propagation,

00:23:21.630 --> 00:23:26.310
which is where the values from our inputs
are propagated down through our network.

00:23:26.310 --> 00:23:29.440
And we're basically computing
the value of our compound function.

00:23:29.440 --> 00:23:32.330
What is happening in backward
propagation is basically the reverse

00:23:32.330 --> 00:23:33.310
side of the mirror.

00:23:33.310 --> 00:23:34.080
Where at each point,

00:23:34.080 --> 00:23:38.510
we take the gradient at that level
with respect to the variable above it.

00:23:38.510 --> 00:23:42.990
So the last thing that we need to do
to finish off our backpropagation

00:23:42.990 --> 00:23:46.380
Is to find out how we're going to merge
all of these local gradients together,

00:23:46.380 --> 00:23:47.940
to get our total gradient.

00:23:47.940 --> 00:23:50.930
And this is as good a time as ever
to talk about the chain rule.

00:23:50.930 --> 00:23:54.851
So the key intuition behind the chain
rule, is that slopes multiply.

00:23:54.851 --> 00:23:59.623
So if I'm trying to take the derivative
of some sort of compound function,

00:23:59.623 --> 00:24:01.909
f and g, of some input x over that x.

00:24:01.909 --> 00:24:06.411
It's going to equal that derivative
of our total function with respect to

00:24:06.411 --> 00:24:08.019
the intermediate value.

00:24:08.019 --> 00:24:12.025
Times the derivative of
the intermediate value over our inputs.

00:24:12.025 --> 00:24:15.792
And this is a bit of a mathematical
wonder, you might think.

00:24:15.792 --> 00:24:18.281
It's so
beautiful that if you have two functions.

00:24:18.281 --> 00:24:23.204
You can just multiply the slope of
the intermediate by the slope of

00:24:23.204 --> 00:24:26.170
the function that happens after that.

00:24:26.170 --> 00:24:28.995
So this is kind of the key tool that
allows back propagation to work.

00:24:30.465 --> 00:24:32.175
Another useful analogy for

00:24:32.175 --> 00:24:34.915
understanding back propagation
is looking at circuit diagrams.

00:24:34.915 --> 00:24:38.434
So this is going to be the circuit
diagram of our initial simple example.

00:24:38.434 --> 00:24:41.176
We're going to add x and y and
that is our q node over there.

00:24:41.176 --> 00:24:44.550
We're going to multiply
that by our z variable.

00:24:44.550 --> 00:24:45.940
And that gives us an f.

00:24:45.940 --> 00:24:50.530
So the green values in this diagram
represent the forward propagation values.

00:24:50.530 --> 00:24:53.200
And the red values represent
the backward propagation.

00:24:53.200 --> 00:24:54.340
So looking to the right most.

00:24:54.340 --> 00:24:57.520
We start off with an error
value of 1 because trivially,

00:24:57.520 --> 00:25:00.330
the derivative of f with
respect to itself is just 1.

00:25:00.330 --> 00:25:05.099
What is interesting though is if we look
at the error signal that is happening at

00:25:05.099 --> 00:25:05.833
the q node.

00:25:05.833 --> 00:25:08.874
It's currently -4 because
the derivative of F with

00:25:08.874 --> 00:25:11.950
respect to Q is just going to
be the value of Z minus 4.

00:25:11.950 --> 00:25:15.294
We know that the derivative of
Q with respect to X is just 1.

00:25:15.294 --> 00:25:18.359
But what we're going to do to get
the total gradient at position x.

00:25:18.359 --> 00:25:23.028
Is we're going to multiply that
gradient times our error signal

00:25:23.028 --> 00:25:25.100
that is flowing into q.

00:25:25.100 --> 00:25:28.530
And this is I guess the key point
of what we're showing here.

00:25:28.530 --> 00:25:32.550
And we'll see how this kind of

00:25:32.550 --> 00:25:36.520
figure can help us do back propagation
in midterm questions soon.

00:25:37.900 --> 00:25:42.760
Okay, are there any questions so far about
this large overview of back propagation,

00:25:42.760 --> 00:25:44.509
before we start talking
about matrix calculus.

00:25:46.390 --> 00:25:49.270
Great, okay,
moving to some matrix calculus.

00:25:49.270 --> 00:25:55.234
So let us first talk about
derivatives over vectors.

00:25:55.234 --> 00:26:00.130
So the derivative over a vector is going
to be a matrix of partial derivatives.

00:26:00.130 --> 00:26:04.773
Where the derivative of each row is
going to be the derivative of that

00:26:04.773 --> 00:26:09.506
index of the output with respect to
all of the indices of your input x.

00:26:09.506 --> 00:26:12.880
So for the scalar by vector example,
you're going to get a single vector.

00:26:12.880 --> 00:26:17.454
Where each column is going to be the
partial of y by the partial of x at that

00:26:17.454 --> 00:26:19.222
particular column index.

00:26:19.222 --> 00:26:23.023
In the vector by vector case, we're going
to have the matrix of partial derivatives.

00:26:23.023 --> 00:26:27.764
Where each row is the partial
of yi on the ith row by

00:26:27.764 --> 00:26:31.010
all of the indices at position x.

00:26:32.354 --> 00:26:36.480
The case for
derivatives over matrices, question.

00:26:46.167 --> 00:26:50.577
The question was, does the presentation
of the derivatives change with respect to

00:26:50.577 --> 00:26:52.475
whether x and y are column vectors?

00:26:52.475 --> 00:26:56.885
So a potentially complicated
answer to your question is that

00:26:56.885 --> 00:27:00.455
each of these derivatives represents
the Jacobean of that derivative.

00:27:00.455 --> 00:27:04.835
That is a matrix that takes as
input whatever shape your x is and

00:27:04.835 --> 00:27:06.530
outputs the shape of y.

00:27:06.530 --> 00:27:09.280
So if x was a row vector, yes.

00:27:09.280 --> 00:27:11.488
The presentation would
actually be transposed.

00:27:11.488 --> 00:27:13.923
Yes, question.

00:27:18.539 --> 00:27:19.696
So the question was,

00:27:19.696 --> 00:27:23.600
are we going to be using the convention
that x is a row or a column?

00:27:23.600 --> 00:27:25.195
That depends on the question and

00:27:25.195 --> 00:27:29.641
it'll be stated very clearly when you're
taking derivatives in that question, okay.

00:27:29.641 --> 00:27:33.125
So the case for derivatives over
matrices is slightly more complicated.

00:27:33.125 --> 00:27:35.250
Ad it's complicated for this reason.

00:27:35.250 --> 00:27:41.890
You can interpret y as a function.

00:27:41.890 --> 00:27:47.790
If I'm trying to compute my partial of y
over my partial of a where y is a scalar.

00:27:50.230 --> 00:27:56.780
Y is actually going to be a function of
every single element inside your matrix A.

00:28:02.282 --> 00:28:07.730
Okay, so the proper derivative
of partial y over partial

00:28:07.730 --> 00:28:12.390
A is actually going to be an element of,
thanks.

00:28:12.390 --> 00:28:16.027
It is actually going to
be an element of RMN.

00:28:16.027 --> 00:28:20.487
So the true Jacobean derivative
of partial y over partial A is

00:28:20.487 --> 00:28:24.440
actually going to be some
long form vector ofsize MN.

00:28:24.440 --> 00:28:26.734
But this is not such
a good presentation for

00:28:26.734 --> 00:28:29.232
us when we want to do
derivatives of matrices.

00:28:29.232 --> 00:28:33.395
So we actually rearrange that sort
of derivative to be some matrix of

00:28:33.395 --> 00:28:34.375
derivatives.

00:28:34.375 --> 00:28:38.418
Where the partial of y at each index is
going to correspond to the partial of

00:28:38.418 --> 00:28:40.060
the index at that position A.

00:28:40.060 --> 00:28:46.580
The case for derivatives of vectors
over matrices is even more complicated.

00:28:46.580 --> 00:28:51.561
Because the true form derivative of a
derivative over a matrix is actually some

00:28:51.561 --> 00:28:53.910
sort of three-dimensional array.

00:28:53.910 --> 00:28:58.488
Or it's going to be a tensor that is
sort of beyond the scope of what we're

00:28:58.488 --> 00:29:00.040
trying to do here.

00:29:00.040 --> 00:29:02.970
So since we're only
interested in computing

00:29:02.970 --> 00:29:07.080
gradients of our lost scalars
with respect to matrices.

00:29:07.080 --> 00:29:10.010
We can actually hide out that sort of

00:29:10.010 --> 00:29:12.850
strange three-dimensional
array derivative thing.

00:29:12.850 --> 00:29:15.670
Cuz we're going to be computing it
by some error signal from above.

00:29:15.670 --> 00:29:17.570
And let me show you what
that means over here.

00:29:17.570 --> 00:29:23.072
So if we're interested in computing
the derivative of z with respect to A.

00:29:23.072 --> 00:29:26.302
Let us look at what the derivative
would be of that z with respect to

00:29:26.302 --> 00:29:29.080
a single element inside our A matrix.

00:29:29.080 --> 00:29:33.650
So you can think of Aij as
representing the sensitivity

00:29:33.650 --> 00:29:39.250
of the ith index of the output with
respect to the jth index of the input.

00:29:39.250 --> 00:29:42.209
And this is what we're
going to be looking at.

00:29:42.209 --> 00:29:47.283
So the derivative at the ith
position of the dz gradient

00:29:47.283 --> 00:29:52.430
is going to be exactly
the value of xj in our input x.

00:29:52.430 --> 00:29:56.240
Since that is what is modifying the A.

00:29:56.240 --> 00:30:01.020
And this leads to the identity
that the partial of j

00:30:01.020 --> 00:30:05.895
over the partial of Aij is going
to equal the dot of our delta.

00:30:05.895 --> 00:30:10.502
Of our delta gradient flowing
inwards at exactly position i.

00:30:10.502 --> 00:30:14.585
Since that is the only
value that is not 0 in our

00:30:14.585 --> 00:30:18.691
derivative of z over Aij
times the value at xj.

00:30:18.691 --> 00:30:21.410
And this leads to
the identity of dJ by dA.

00:30:21.410 --> 00:30:25.760
So you can somewhat ignore the slide and
just focus on the identity for

00:30:25.760 --> 00:30:27.120
the purposes of the midterm.

00:30:27.120 --> 00:30:31.440
But this might be a bit of
an explanation of what it means to take

00:30:31.440 --> 00:30:32.990
sort of derivatives over matrices.

00:30:34.170 --> 00:30:36.015
Okay, so on this slide I have,
for your reference.

00:30:36.015 --> 00:30:39.755
A list of perhaps the most useful
identities that you'll need for

00:30:39.755 --> 00:30:43.500
computing all of the gradients
that we use in our midterms.

00:30:43.500 --> 00:30:46.260
And I won't explain this now,
but the slides are online.

00:30:46.260 --> 00:30:49.405
So I highly urge you to check these out or
to check them in the notes.

00:30:49.405 --> 00:30:53.580
So are there any questions
before we move onwards?

00:30:56.777 --> 00:30:57.871
Awesome, okay.

00:30:57.871 --> 00:31:00.760
So one thing that you might
notice from the previous slides.

00:31:00.760 --> 00:31:04.874
Is that the nice thing of taking
derivatives of scalars over vectors or

00:31:04.874 --> 00:31:05.580
matrices.

00:31:05.580 --> 00:31:10.021
Is that the shape of our output has
the same dimensions as the shape of our

00:31:10.021 --> 00:31:10.810
inputs.

00:31:10.810 --> 00:31:14.574
Both in the scalar-by-vector case and
the scalar-by-matrix case.

00:31:14.574 --> 00:31:18.780
So what we enforce when we do back
propagation is this shape rule.

00:31:18.780 --> 00:31:21.672
Where when you take the gradients
against a scalar which is what you're

00:31:21.672 --> 00:31:22.839
essentially always doing.

00:31:22.839 --> 00:31:27.560
The gradients at each intermediate step
have the shape of your denominator.

00:31:27.560 --> 00:31:31.911
So if X, whether it's a row vector, column
vector, or matrix, has shape m by n.

00:31:31.911 --> 00:31:38.335
The error signal of d scalar [INAUDIBLE]
loss by dx is equal to the same

00:31:38.335 --> 00:31:45.250
size as our denominator And what this
allows us to do is dimension balancing.

00:31:45.250 --> 00:31:48.259
So this is going to be
the general gradients for

00:31:48.259 --> 00:31:53.198
any sort of matrix multiplication you
do on any gradient question where X and

00:31:53.198 --> 00:31:58.170
W Can be either both matrices, or
X can be a row vector and W a matrix, or

00:31:58.170 --> 00:32:00.886
X can be a matrix and W a column vector,
so this is a general form.

00:32:00.886 --> 00:32:05.816
So if Z is going to be some m times w
matrix, and we represent the error at

00:32:05.816 --> 00:32:09.828
that point as partial Loss
over partial Z as being delta,

00:32:09.828 --> 00:32:14.032
that because of our shape rule,
we know has shape m times w.

00:32:14.032 --> 00:32:18.860
How do we find out what
the dimension of our d Loss by d X?

00:32:18.860 --> 00:32:24.210
Well, if we are looking for
a gradient of shape m times n,

00:32:24.210 --> 00:32:27.240
and we know we're going to be
multiplying our gradient signal by W,

00:32:27.240 --> 00:32:30.290
using our identities
from the previous slide.

00:32:30.290 --> 00:32:35.230
Then we realize that we need to take
the transpose of W so that the W is on

00:32:35.230 --> 00:32:39.760
the left side and the delta is, and
the W is on the right side of the delta.

00:32:39.760 --> 00:32:42.840
So you can kind of solve
these identities by looking

00:32:42.840 --> 00:32:45.340
at the dimensions of all of your terms,
and

00:32:45.340 --> 00:32:50.160
making sure they balance out to give you
the correct dimensions for the gradients.

00:32:50.160 --> 00:32:53.933
By matching the gradients of
your error and the gradient,

00:32:53.933 --> 00:32:58.172
the dimensions of your gradient and
the dimensions of your term.

00:32:58.172 --> 00:33:00.838
Are there any questions about this
dimension and balancing concept?

00:33:06.515 --> 00:33:07.670
The questions was, what is delta?

00:33:07.670 --> 00:33:11.565
Delta is going to be the error
signal at the point of Z.

00:33:12.580 --> 00:33:15.680
So since we're trying to compute the
gradient of the loss with respect to X,

00:33:15.680 --> 00:33:20.030
and the level above that,
the intermediate level above that is Z.

00:33:20.030 --> 00:33:21.870
To apply the chain rule,

00:33:21.870 --> 00:33:25.700
we're going to have delta represent
the derivative up to the point Z.

00:33:25.700 --> 00:33:30.517
And we're going to multiply that as
an application of our chain rule

00:33:30.517 --> 00:33:33.569
with the derivative of
Z with respect to X.

00:33:48.727 --> 00:33:53.740
So the idea whenever we're,
if I'm starting with some Z as

00:33:53.740 --> 00:33:58.752
a matrix multiplication of X and
W, and I want to know what my

00:33:58.752 --> 00:34:05.026
derivative of Loss, With respect to W is.

00:34:05.026 --> 00:34:10.131
Using the chain rule, I know that
that's the derivative of the Loss

00:34:10.131 --> 00:34:14.720
with respect to Z times
the derivative of Z with respect to W.

00:34:14.720 --> 00:34:17.610
So this is going to be our intermediate
variable that we're taking our

00:34:17.610 --> 00:34:19.190
local gradient of.

00:34:19.190 --> 00:34:23.930
And we know what the derivative
of this with respect to this is.

00:34:23.930 --> 00:34:28.610
But this is going to give us
the error signal up to that point.

00:34:28.610 --> 00:34:33.417
The error signal records what
the value of the error is up to

00:34:33.417 --> 00:34:37.844
the point of this intermediate
computation, okay?

00:34:37.844 --> 00:34:42.444
The error signal is kind of what
the gradient is up to the point of your

00:34:42.444 --> 00:34:47.447
equation, of the Loss down to the last
variable that you're looking at,

00:34:47.447 --> 00:34:50.700
which is the output of that expression,
okay.

00:34:53.388 --> 00:34:57.872
All right, so it kind of feels like
dimension balancing is this kinda cheap

00:34:57.872 --> 00:35:01.370
and dirty approach to doing
gradient calculations.

00:35:01.370 --> 00:35:07.370
But they are indeed kind of the most
efficient tool that you have to computing

00:35:07.370 --> 00:35:11.470
gradients quickly in most practical
settings, and especially the midterm.

00:35:11.470 --> 00:35:14.200
But I do encourage you to read
the gradient computation notes

00:35:14.200 --> 00:35:18.640
if you want to understand how
the gradients work from first principle.

00:35:19.660 --> 00:35:22.600
The last thing that I wanna talk
about is how gradients over

00:35:22.600 --> 00:35:24.400
activation functions work.

00:35:24.400 --> 00:35:27.500
So one of the questions that we
fielded frequently in office hours is,

00:35:27.500 --> 00:35:30.740
why is it the case that when you're taking
gradients over an activation function,

00:35:30.740 --> 00:35:34.100
you're doing this element-wise
multiplication or Hadamard product?

00:35:34.100 --> 00:35:37.950
And the answer to that is, if you're
looking at, so the answer to that is

00:35:37.950 --> 00:35:44.310
the activation function is a function
that maps a scalar to another scalar.

00:35:44.310 --> 00:35:50.280
Though we represent z as a vector
as a sigmoid of an entire vector h,

00:35:50.280 --> 00:35:56.140
the sigmoid is actually being applied to
each individual element in that vector,

00:35:56.140 --> 00:35:58.800
which maps the same index in your output.

00:35:58.800 --> 00:36:02.820
So the way we represent the true gradient,
the true derivative,

00:36:02.820 --> 00:36:05.580
is that it's going to
be a diagonal matrix.

00:36:05.580 --> 00:36:09.730
Where each index on your leading
diagonal is going to be the gradient

00:36:11.490 --> 00:36:15.070
of the ith index of your input.

00:36:15.070 --> 00:36:20.040
And multiplying some matrix or
vector by a diagonal matrix

00:36:20.040 --> 00:36:24.390
is equivalent to doing the element-wise
multiplication of a vector itself.

00:36:24.390 --> 00:36:26.320
Which we represent and
the Hadamard product.

00:36:28.644 --> 00:36:31.370
Okay, are there any questions
about what we've discussed so

00:36:31.370 --> 00:36:33.190
far before we get to
some midterm problems?

00:36:34.950 --> 00:36:35.450
Yes,

00:37:18.826 --> 00:37:22.385
If you go back to this slide,
where this is the true, sorry.

00:37:22.385 --> 00:37:25.785
This is the true Jacobian form
of the partial of some y vector,

00:37:25.785 --> 00:37:29.655
with respect to x, you can see that
it's a matrix that takes as input,

00:37:29.655 --> 00:37:32.855
a column vector x, and
maps it to an output y.

00:37:34.368 --> 00:37:38.293
So if the shape of your
Jacobian takes as an input,

00:37:38.293 --> 00:37:43.712
something with shape x and maps to
an output something with a shape y,

00:37:43.712 --> 00:37:48.770
which is exactly what is happening
in the top left box over here.

00:38:15.471 --> 00:38:18.625
Can we talk about that question maybe
after the lecture, so we can move on?

00:38:18.625 --> 00:38:19.750
Cuz we have a few more sections.

00:38:21.650 --> 00:38:25.330
Okay, so let's get through, and
maybe when we actually do some midterm

00:38:25.330 --> 00:38:27.920
problems it'll become more
apparent how it works.

00:38:27.920 --> 00:38:31.190
Okay, so the first question that we're
going to look at is to do with Siamese

00:38:31.190 --> 00:38:32.220
networks.

00:38:32.220 --> 00:38:37.610
And, a Siamese network basically allows
you to compute a similarity metric

00:38:37.610 --> 00:38:41.590
between two inputs x,
between an x1 and some x2 inputs.

00:38:41.590 --> 00:38:45.560
And this allows you to sort of compare
the similarity of two word vectors or

00:38:45.560 --> 00:38:46.480
two sentences.

00:38:47.600 --> 00:38:54.805
So the first thing that I like to do when
we're starting some gradient problem,

00:38:56.891 --> 00:39:02.060
Is to draw a graph of how our
variables are related to each other.

00:39:04.040 --> 00:39:08.218
So we can see is we start off with some J,

00:39:08.218 --> 00:39:12.279
at the top we have our cost function J.

00:39:12.279 --> 00:39:18.960
It takes as input an h1 and
an h2 as two hidden vectors.

00:39:18.960 --> 00:39:23.140
These are activations of z1 and z2.

00:39:23.140 --> 00:39:27.293
And I am defining these variables myself,
so I need to specify what they are.

00:39:27.293 --> 00:39:29.393
And this is going to be, in this problem,

00:39:29.393 --> 00:39:31.795
we're treating our inputs
as column vectors.

00:39:34.796 --> 00:39:38.320
And for
z2 it's going to equal to x2 plus b.

00:39:40.250 --> 00:39:42.710
So we have two inputs, x1 and x2.

00:39:42.710 --> 00:39:46.710
And one interesting thing to notice about
Siamese networks is that the variables,

00:39:46.710 --> 00:39:52.450
sorry the parameters, W and b Are shared
across both of these activations.

00:39:52.450 --> 00:39:55.870
So if we're interested in taking
the derivative of J with respect to W,

00:39:55.870 --> 00:39:59.840
we need to add the derivatives
coming from both branches down.

00:39:59.840 --> 00:40:03.331
So the first thing that I like to do when
I'm computing my gradients, is basically,

00:40:03.331 --> 00:40:06.341
to express this graph, and we'll see
how it's useful to us in a second.

00:40:06.341 --> 00:40:10.883
Okay, can the camera see me, awesome.

00:40:10.883 --> 00:40:13.572
So if we want to start our
gradient computations,

00:40:13.572 --> 00:40:18.030
the first thing we want to do is to
take the derivative with respect to J.

00:40:18.030 --> 00:40:20.650
So let me write out what
my J is going to be.

00:40:20.650 --> 00:40:23.709
It's going to be half of the Hadamard,

00:40:23.709 --> 00:40:28.354
I'm sorry, the Frobenius norm
of h1 minus h2 squared f.

00:40:28.354 --> 00:40:34.510
And we're going to have
a regularization term.

00:40:38.066 --> 00:40:44.310
So this is, I'm first writing out
what my cost function is going to be.

00:40:44.310 --> 00:40:48.350
And I use this graph to kind of inform me
about where I'm taking my derivatives and

00:40:48.350 --> 00:40:49.980
what my error signals are going to be.

00:40:49.980 --> 00:40:52.980
So the first thing I wanna
find out if I wanna compute,

00:40:52.980 --> 00:40:56.690
what is the derivative of J,
with respect to W.

00:40:56.690 --> 00:40:58.390
This is what I'm interested in.

00:40:58.390 --> 00:41:01.020
So the first thing we're going
to do is flow down this branch.

00:41:01.020 --> 00:41:05.090
So we're going to compute
the partial of J with respect to

00:41:07.890 --> 00:41:12.200
the partial of h1, and I'll leave this for
you to show on your own time,

00:41:12.200 --> 00:41:14.370
what the derivative over
the Frobenius norm is.

00:41:14.370 --> 00:41:18.829
But it's basically just going to be
h1- h2, cuz the 2 cancels out and

00:41:18.829 --> 00:41:22.070
you're left with an h1- h2 term.

00:41:22.070 --> 00:41:25.510
Equivalently, the gradient
with respect to h2 is

00:41:25.510 --> 00:41:30.780
just going to be the negative
of that expression, okay?

00:41:30.780 --> 00:41:31.950
So one thing that I like to do,

00:41:31.950 --> 00:41:36.670
is to also record what the dimensions
are of all of my gradients.

00:41:36.670 --> 00:41:39.546
So over here,
since we're dealing with column vectors,

00:41:39.546 --> 00:41:41.552
this is going to remain an m by 1 vector.

00:41:41.552 --> 00:41:44.940
And this is going to be
an m by 1 vector as well.

00:41:44.940 --> 00:41:48.640
And the other thing I'm going to do is
give a name to these error signals.

00:41:48.640 --> 00:41:51.870
This one is going to be delta 1,
and this one is delta 2.

00:41:51.870 --> 00:41:54.770
And what is nice to represent
using this graph is what my

00:41:54.770 --> 00:41:56.820
error signals are flowing down it.

00:41:56.820 --> 00:42:01.506
So over here I've defined delta 1 to be
the partial of J with respect to partial

00:42:01.506 --> 00:42:02.010
of h1.

00:42:02.010 --> 00:42:08.330
And over here we have delta 2 being the
partial of J over the partial of h2, okay?

00:42:08.330 --> 00:42:12.050
So the next thing we are interested in
computing is what is the derivative

00:42:12.050 --> 00:42:13.800
of J with respect to z1?

00:42:13.800 --> 00:42:16.845
So we're going to apply our chain rule.

00:42:16.845 --> 00:42:19.680
We know what the derivative of J with
respect to h1 is, so we only, sorry,

00:42:19.680 --> 00:42:21.805
we want to compute the derivate
of J with respect to z1.

00:42:21.805 --> 00:42:24.119
We know what the derivative of
J with respect to h1 is, so

00:42:24.119 --> 00:42:26.786
we only need to compute the derivative
of h1 with respect to z1.

00:42:26.786 --> 00:42:30.121
So, over here we have a sigmoid
activation function, so

00:42:30.121 --> 00:42:32.925
we just need to compute
the gradient over that.

00:42:32.925 --> 00:42:39.720
So my partial of J over
the partial of z1 is going to

00:42:39.720 --> 00:42:45.990
equal partial of J by partial of h1 times
the partial of h1 by the partial of z1.

00:42:45.990 --> 00:42:53.040
And that is equal to my first error
signal with the Hadamard product of, this

00:42:54.760 --> 00:43:00.020
is getting a bit tight here,
it's going to equal

00:43:00.020 --> 00:43:05.380
my error signal Hadamard
product with h1 times 1- h1.

00:43:05.380 --> 00:43:09.770
Which, as we know from assignment one,
is the derivative of the sigmoid.

00:43:11.210 --> 00:43:12.600
I'm running out of space here.

00:43:12.600 --> 00:43:13.500
Let me move over here.

00:43:13.500 --> 00:43:14.720
I hope it doesn't get shadowed.

00:43:17.044 --> 00:43:22.105
Similarly, the partial of J
with respect to z2 is equal to

00:43:22.105 --> 00:43:28.940
the partial of J by the partial of h2
times the partial of h2 by partial of z2.

00:43:31.180 --> 00:43:38.268
And that's also going to equal our
current error signal flowing down to h2,

00:43:38.268 --> 00:43:43.734
so it's delta 2 Hadamard
product with h2 times 1- h2.

00:43:43.734 --> 00:43:46.160
And we're gonna give this a name.

00:43:46.160 --> 00:43:50.334
This is now going to be delta 4 and
this one is going to be delta 3.

00:43:51.520 --> 00:43:57.020
Okay, so over here we have delta 3 and
here delta 4, okay?

00:44:01.452 --> 00:44:04.954
So now the last thing that we wanna do if
we wanna compute the gradient of J with

00:44:04.954 --> 00:44:13.269
respect to w, This is going to
equal my error signal up to z1,

00:44:13.269 --> 00:44:19.760
so partial of J by partial of z1 times
the partial of z1 by the partial of w.

00:44:20.770 --> 00:44:24.227
It's also going to, and we're going to
add to that what's happening in the error

00:44:24.227 --> 00:44:25.640
signal at the other branch.

00:44:25.640 --> 00:44:30.870
That's partial of J by partial of z2 by
the partial of z2 over the partial of w.

00:44:33.220 --> 00:44:36.400
Plus we don't want to forget about
our regularization experiment.

00:44:36.400 --> 00:44:41.056
The partial of lambda over 2,

00:44:41.056 --> 00:44:46.440
Frobenius norm of w by w, okay.

00:44:46.440 --> 00:44:49.540
And this is going to, now,
we need to do some matrix balancing,

00:44:49.540 --> 00:44:51.240
using what we discussed earlier.

00:44:51.240 --> 00:44:56.240
Since we're taking the derivative
over this matrix product over here.

00:44:56.240 --> 00:45:01.165
So what we need to do is, we want this
final product of each of these things,

00:45:01.165 --> 00:45:02.977
to have the same shape as w.

00:45:02.977 --> 00:45:09.050
So w is an m by n matrix, and
this is just given to you in the problem.

00:45:09.050 --> 00:45:14.640
We know that delta 3 and
delta 4 are m by 1.

00:45:14.640 --> 00:45:15.575
Why?

00:45:15.575 --> 00:45:23.140
Because when we computed our delta 3 and

00:45:23.140 --> 00:45:27.650
delta 4 we just took the elementwise
product of whatever was up here, so

00:45:27.650 --> 00:45:29.890
it's still going to remain m times 1.

00:45:29.890 --> 00:45:34.557
Finally, we know that x1 and
x2 are both n by 1,

00:45:34.557 --> 00:45:38.000
since they are column vectors.

00:45:38.000 --> 00:45:41.350
So if we want to,
we know that this is delta 3 and

00:45:41.350 --> 00:45:45.920
we're going to have to multiply
it by the xs in some way.

00:45:45.920 --> 00:45:48.200
So we see that if we want
our final to be m times n,

00:45:48.200 --> 00:45:52.590
we're gonna have to do this expression
times the transpose of this.

00:45:52.590 --> 00:46:00.650
So this is equal to the delta
3 times w1 transpose.

00:46:00.650 --> 00:46:06.705
Plus delta 2, sorry, delta 4 times, sorry,

00:46:10.414 --> 00:46:15.740
Delta 3 times x1 transpose +
delta 4 times x2 transpose.

00:46:15.740 --> 00:46:18.550
And finally,
we want to add this expression.

00:46:18.550 --> 00:46:21.866
And again, I will leave this for you to
show on your own time that the derivative

00:46:21.866 --> 00:46:24.189
of this Frobenius norm,
the 2 cancels out with this.

00:46:24.189 --> 00:46:28.828
And you're just left with lambda w, okay?

00:46:28.828 --> 00:46:29.389
Yes?

00:46:29.389 --> 00:46:39.389
&gt;&gt; [INAUDIBLE]

00:47:19.721 --> 00:47:20.550
&gt;&gt; So the question was,

00:47:20.550 --> 00:47:22.951
what happens if you run into some
sort of ambiguity over here.

00:47:22.951 --> 00:47:25.914
So the nice thing about when we're,
since, at least for

00:47:25.914 --> 00:47:30.191
computing the neural network models that
we are looking at for the midterm, and.

00:47:30.191 --> 00:47:34.060
The only tractable problems that we're
going to be computing gradients over

00:47:34.060 --> 00:47:37.572
are things like linear transformations and
activations over those,

00:47:37.572 --> 00:47:41.100
which are already the key operations
that you use in neural networks.

00:47:41.100 --> 00:47:46.370
Apart from things like cross entropies and
other losses at the top.

00:47:46.370 --> 00:47:50.740
So in most of those situations you
won't really run into ambiguities.

00:47:50.740 --> 00:47:54.590
And when you do, you just go back
to deriving things using like,

00:47:54.590 --> 00:47:57.900
Traditional matrix calculus,
so you just dig deeper.

00:47:57.900 --> 00:48:01.850
Like if you dug deeper into
how these dimensions work out,

00:48:01.850 --> 00:48:03.440
it should still work out.

00:48:04.560 --> 00:48:08.980
So you just won't find ambiguities
in these types of problems, okay.

00:48:08.980 --> 00:48:11.888
I will quickly run through
my second example.

00:48:11.888 --> 00:48:13.547
Okay.

00:48:20.670 --> 00:48:23.930
Okay, so
the next example that we are looking at.

00:48:25.810 --> 00:48:29.366
Suppose, you wanted to build a word vector
representation that you, and couldn't

00:48:29.366 --> 00:48:32.734
decide whether you wanted to have a hidden
layer that is activated by a sigmoid.

00:48:32.734 --> 00:48:36.700
Or you cannot decide whether you wanted a
hidden layer that is activated by a ReLU.

00:48:36.700 --> 00:48:39.940
So the only thing better than making
a choice is to do both of them.

00:48:39.940 --> 00:48:45.476
So what we're going to do is we're going
to have a network, Where at the top,

00:48:45.476 --> 00:48:53.351
we're going to have a cross entropy over
some prediction y, that takes n from z3.

00:48:57.566 --> 00:48:58.684
So over here,

00:48:58.684 --> 00:49:04.667
we're going to have some input that
flows through both of these variables.

00:49:04.667 --> 00:49:08.250
So here, we have xW1 + b1.

00:49:11.933 --> 00:49:15.547
So we're going to apply a linear
transformation over our input x, and

00:49:15.547 --> 00:49:18.720
then sigmoid activate it to
produce this hidden layer.

00:49:18.720 --> 00:49:21.330
Over here, we're going to have
a different linear transformation with

00:49:21.330 --> 00:49:24.950
two other parameters,
two other matrix and BIOS term.

00:49:24.950 --> 00:49:26.690
But here, we're going to ReLU activate it.

00:49:27.940 --> 00:49:31.045
Then we're going to add our h1 and h2.

00:49:31.045 --> 00:49:33.041
At z3, we're going to,

00:49:38.779 --> 00:49:41.962
Guess our score vector which is z3, and

00:49:41.962 --> 00:49:45.716
then we apply the cross entropy to that,
okay?

00:49:45.716 --> 00:49:46.780
That's your model.

00:49:46.780 --> 00:49:49.470
It's like are the word vector
representation we did in assignment one

00:49:49.470 --> 00:49:53.220
except now, we have two branches
that take the input twice.

00:49:53.220 --> 00:49:55.550
So what we want to compute at least for

00:49:55.550 --> 00:50:01.600
this question is what is the partial
of J with respect to X, okay?

00:50:03.570 --> 00:50:07.379
So the nice thing about this problem
is that we already know how to compute

00:50:07.379 --> 00:50:09.698
the derivative of our
J with respect to z3.

00:50:09.698 --> 00:50:15.070
Since z3 is the input into our cross
entropy, and we did this in our homework.

00:50:15.070 --> 00:50:21.090
So the partial of J with respect to z3
is just going to equal to y hat minus y,

00:50:21.090 --> 00:50:22.110
as we already know.

00:50:22.110 --> 00:50:24.320
And this is going to
be our first r signal.

00:50:24.320 --> 00:50:27.740
So we already know what the errors
from J flowing down to z3.

00:50:27.740 --> 00:50:30.170
And we're calling that delta 1, okay?

00:50:31.410 --> 00:50:35.666
The next thing we see is that
the partial of z3 with respect to h1 and

00:50:35.666 --> 00:50:37.618
h2 are actually equivalent.

00:50:37.618 --> 00:50:41.314
Cuz if we know what the partial of
z3 is with respect to this sum,

00:50:41.314 --> 00:50:45.090
since the gradients of h1 + h2,
with respect to h1 is just 1.

00:50:45.090 --> 00:50:50.300
And similarly, with h2, we can simply
write down the partial of J with respect

00:50:50.300 --> 00:50:54.790
to h1 is equal to the partial of J
with respect to, sorry, partial of h2.

00:51:00.381 --> 00:51:05.908
Which is equal to
the partial of J up to z3

00:51:05.908 --> 00:51:12.909
times the partial of z3
over partial of h1 + h2.

00:51:16.123 --> 00:51:19.650
And what we need to do here is again,
some matrix balancing.

00:51:19.650 --> 00:51:27.460
Since we know that h is
a [1 x m] row vector.

00:51:27.460 --> 00:51:30.580
And I forgot to specify, but
this is actually pretty crucial, and

00:51:30.580 --> 00:51:31.690
this is the next slide.

00:51:31.690 --> 00:51:37.160
I did convert this problem into the exact
same model except using row vectors.

00:51:37.160 --> 00:51:41.000
Just to show you that the way we do
dimension balancing in a row vector

00:51:41.000 --> 00:51:44.740
versus a column vector situation
is basically exactly the same.

00:51:44.740 --> 00:51:48.730
So we have h being 1 times m.

00:51:48.730 --> 00:51:54.958
And our W3 is m times k, is m by k.

00:51:54.958 --> 00:52:00.318
Our delta 1 is also 1 value K.

00:52:00.318 --> 00:52:05.012
So what we can see is that if we wanted to
multiply our error signal from above by

00:52:05.012 --> 00:52:09.562
what we know is going to be the derivative
with respect to h1 + h2, we're

00:52:09.562 --> 00:52:15.270
going to need to transpose our W3, and
multiply our delta 1 by that transpose.

00:52:15.270 --> 00:52:17.710
So going up here,
I'll just write an equal sign.

00:52:17.710 --> 00:52:20.010
This is,
I'm continuing this line over here.

00:52:20.010 --> 00:52:23.444
That is just equal to
delta 1 by W3 transpose.

00:52:23.444 --> 00:52:28.150
And I'm going to call this delta 2.

00:52:28.150 --> 00:52:33.580
And then I'm going to fill in my error
gradient flowing down here, okay?

00:52:36.430 --> 00:52:40.784
Moving over to this side,
if we wanna move down the branches,

00:52:40.784 --> 00:52:45.610
we now wanna compute the derivative
of J with respect to z1 and z2.

00:52:45.610 --> 00:52:51.450
So the partial of J with respect to z1
is equal to the partial of J up to h1.

00:52:51.450 --> 00:52:56.380
There's a partial of h1 up to z1, and
that's just going to equal since this

00:52:56.380 --> 00:52:59.710
is an activation, this is a gradient
over an activation function, we again,

00:52:59.710 --> 00:53:03.630
use our elements y as multiplication,
so that's just equal to delta 2.

00:53:03.630 --> 00:53:06.690
By since on the left branch,
we're doing a sigmoid,

00:53:06.690 --> 00:53:12.490
we're going to have just
h1 times 1 minus h1.

00:53:12.490 --> 00:53:17.244
And on the other side, our partial
of J with respect to partial of z2.

00:53:17.244 --> 00:53:20.674
And I'll just skip writing out,
I won't write out the chain rule for

00:53:20.674 --> 00:53:21.910
the sake of time.

00:53:21.910 --> 00:53:24.750
That's going to equal our delta 2 by

00:53:24.750 --> 00:53:29.100
the gradient of the ReLU
activation function which is 1.

00:53:29.100 --> 00:53:34.270
If our z2 is greater than 0 based
on the value as we it saw earlier.

00:53:34.270 --> 00:53:39.269
And we're going to call this,
delta 3, and this, delta 4.

00:53:40.560 --> 00:53:46.040
So I'm gonna fill in
the deltas in my graph, okay?

00:53:48.405 --> 00:53:50.475
Awesome.
So the last thing we're gonna do, and

00:53:50.475 --> 00:53:52.377
I'm again, running out of space,

00:53:59.181 --> 00:54:02.580
Is to compute the gradients with respect
to x, which is what what we wanna do.

00:54:02.580 --> 00:54:08.691
So the partial of J with respect
to x is going to equal the partial

00:54:08.691 --> 00:54:13.780
of J up to z1 times the partial of z1,
respect to x.

00:54:14.870 --> 00:54:19.909
Plus since we're working with our
derivatives x appears in both branches,

00:54:19.909 --> 00:54:23.922
it's going to equal the partial
of J with the respect to z2,

00:54:23.922 --> 00:54:26.619
partial of z2 with respect to x, okay?

00:54:26.619 --> 00:54:29.692
And what we're gonna do is
matrix bouncing one more,

00:54:29.692 --> 00:54:33.460
since we're taking derivatives
over a linear transformation.

00:54:33.460 --> 00:54:36.430
So we wanna make sure our
dimensions balance out.

00:54:36.430 --> 00:54:41.573
So we know from the problem
statement that x is a row vector.

00:54:41.573 --> 00:54:42.512
So it's [1 x n], okay?

00:54:42.512 --> 00:54:45.616
All right, W1, W2 are by n by m,

00:54:45.616 --> 00:54:51.250
and that's just given from
the problem formulation.

00:54:51.250 --> 00:54:54.126
So, since x is [1 x n] and,

00:55:00.617 --> 00:55:02.169
We're gonna need to compute,

00:55:02.169 --> 00:55:05.940
we're gonna need to multiply our error
signals by this being transposed.

00:55:05.940 --> 00:55:09.660
Since we want the n to appear
on the right hand side.

00:55:09.660 --> 00:55:12.990
And although,
I didn't write over here you can see,

00:55:12.990 --> 00:55:16.839
you can prove to yourself that delta 3 and
delta 4 are 1 by m.

00:55:19.537 --> 00:55:24.090
So this final expression,
and going down here.

00:55:24.090 --> 00:55:26.598
Since we need to transpose this,

00:55:26.598 --> 00:55:30.638
it's going to be the error
gradient up to this point.

00:55:30.638 --> 00:55:37.880
So delta 3 times our W1 transpose,
since W1 appears on the left side,

00:55:37.880 --> 00:55:43.119
plus the error signal delta
4 times W2 transpose.

00:55:43.119 --> 00:55:45.910
Okay, I know this is a little
bit long winded, but

00:55:45.910 --> 00:55:50.361
the general approach that we're using is
to first of all, build the graph of our

00:55:50.361 --> 00:55:55.760
variables Starting from J,
we compute the derivative going down and

00:55:55.760 --> 00:55:59.005
at each point, we mark what the error
signal is up to that point.

00:55:59.005 --> 00:56:03.190
After that, we apply our chain rule,
where we compute the derivative of our

00:56:03.190 --> 00:56:06.160
current variable times our
new thing going down, and

00:56:06.160 --> 00:56:09.200
we multiply that by our error
signal coming from above.

00:56:10.380 --> 00:56:13.610
And we use matrix balancing to
make sure our products work out.

00:56:13.610 --> 00:56:15.157
Are there any questions.

00:56:15.157 --> 00:56:15.657
Yes.

00:56:24.279 --> 00:56:28.142
I would say yes,
since it helps you keep track,

00:56:28.142 --> 00:56:32.983
it helps to keep track of
derivatives up to a certain point.

00:56:32.983 --> 00:56:36.684
And it also helps you if you can
just take my current error signal,

00:56:36.684 --> 00:56:39.057
specify what the dimensions of that are.

00:56:39.057 --> 00:56:42.512
It's very easy to make sure that
your dimensions balance out,

00:56:42.512 --> 00:56:46.140
when you compute my error signal
by what follows afterwards.

00:56:46.140 --> 00:56:48.770
I mean, you don't have to,
but it certainly is a way.

00:56:48.770 --> 00:56:53.080
Of keeping your calculations clean and
organized.

00:56:53.080 --> 00:56:56.190
For the sake of time I'm going
to move on to the next section,

00:56:56.190 --> 00:57:00.100
but we can take all questions
about back log after the lecture.

00:57:00.100 --> 00:57:01.494
So my last slide, and
you can look this up later.

00:57:01.494 --> 00:57:05.815
Your menu for success, I recommend doing
all of them, is to write down your graph,

00:57:05.815 --> 00:57:09.892
compute the derivatives from the top down,
keep track of your error signals,

00:57:09.892 --> 00:57:12.209
and force that the dimensions balance out.

00:57:12.209 --> 00:57:18.599
And yes, that is it, thank you.

00:57:18.599 --> 00:57:23.470
[APPLAUSE]
&gt;&gt; Cool.

00:57:23.470 --> 00:57:24.855
Hi everyone.
So our men are probably some

00:57:24.855 --> 00:57:28.944
of the coolest architectures that
you've learned in this class, and and

00:57:28.944 --> 00:57:32.339
I may have the pleasure unrolling
them a little bit further for you.

00:57:32.339 --> 00:57:33.670
So cool.

00:57:33.670 --> 00:57:34.840
So let's time write in to it,

00:57:34.840 --> 00:57:38.150
we'll also share some material
questions from past exams.

00:57:38.150 --> 00:57:42.790
So here, you now have a distributed
representation of each patient note.

00:57:42.790 --> 00:57:46.300
You assume that a patient's past
medical history is informative

00:57:46.300 --> 00:57:47.950
of their current illness.

00:57:47.950 --> 00:57:50.890
As such, you apply a recurrent neural
network to predict the current

00:57:50.890 --> 00:57:54.750
illness based on a patient's current,
and previous note-vectors.

00:57:54.750 --> 00:57:57.850
You also explain why
a recurrent neural network

00:57:57.850 --> 00:58:00.172
would be better than
a feed-forward network,

00:58:00.172 --> 00:58:04.810
in which your input is summation or
average of past and current note-vectors.

00:58:04.810 --> 00:58:08.142
So you can talk to your neighbors, just
like take a minute to discuss this problem

00:58:08.142 --> 00:58:11.283
and figure out why an RNN would actually
be better than a feed-forward one.

00:58:48.743 --> 00:58:50.216
Great, are there any suggestions?

00:58:50.216 --> 00:58:56.059
Are there any suggestions, to why our RNN
might be better than feed-forward ones?

00:59:18.453 --> 00:59:22.650
I'll try to repeat what you just said,
you can correct me if I'm wrong.

00:59:22.650 --> 00:59:26.448
So you said that the RNNs,
they can do something specific for

00:59:26.448 --> 00:59:30.706
a particular patient, and
while if you sum them up they wouldn't.

00:59:33.709 --> 00:59:35.150
Okay.

00:59:35.150 --> 00:59:36.240
And thanks.

00:59:36.240 --> 00:59:37.421
Are there any other suggestions as well?

01:00:03.367 --> 01:00:07.752
Mm-hm, and so what you just said, and
again, you can correct me if I'm wrong,

01:00:07.752 --> 01:00:12.435
is that the more recent information about
this patient might be more relevant.

01:00:12.435 --> 01:00:14.635
And that is something that
an RNN could capture,

01:00:14.635 --> 01:00:17.210
which a feed network might
not be able to capture.

01:00:17.210 --> 01:00:17.710
Mm-hm.

01:00:29.897 --> 01:00:34.824
Yeah, so what I just said right now is
that, and there is some time dependency in

01:00:34.824 --> 01:00:38.575
this data, since we have
a sequence of a patient notes.

01:00:38.575 --> 01:00:42.975
And if we were to sum them up together,
we lose that temporal information.

01:00:42.975 --> 01:00:44.515
And so these are all very good answers.

01:00:44.515 --> 01:00:47.210
And I do want to comment
just on the first answer,

01:00:47.210 --> 01:00:49.660
I think there might have been
a slight misunderstanding.

01:00:49.660 --> 01:00:52.440
So we are not summing up
across different patients.

01:00:52.440 --> 01:00:57.090
I'm summing across the nodes for
the same patient across time stubs.

01:00:57.090 --> 01:01:00.490
And I think that was probably just like
maybe it wasn't phase super clearly.

01:01:00.490 --> 01:01:03.670
And what the other people said was and
also correct.

01:01:03.670 --> 01:01:08.100
So one thing is that RNNs can take in
more recent information into account.

01:01:08.100 --> 01:01:12.337
And in a more general sense, they can take
in temporal relationships into account,

01:01:12.337 --> 01:01:15.212
which we would ignore by summing up or
arching over them.

01:01:15.212 --> 01:01:16.134
Very nice.

01:01:16.134 --> 01:01:20.578
Cool, so in order for us to see this more,
yes, there's a question?

01:01:31.675 --> 01:01:36.102
The question was on what advantage
would an RNN give us over taking

01:01:36.102 --> 01:01:37.937
a weighted average.

01:01:37.937 --> 01:01:40.475
Often note-vectors, well if you do,

01:01:40.475 --> 01:01:44.215
weighted average we actually had
to come up with the weights for

01:01:44.215 --> 01:01:48.355
on the particular notes, which we
actually don't really know beforehand.

01:01:48.355 --> 01:01:51.265
And the idea of like chaining an RNN here,
is that like

01:01:51.265 --> 01:01:55.030
the RNN might potentially be able to
learn to what extent it should take in.

01:01:55.030 --> 01:01:59.280
Really old notes into account versus
perhaps like more recent ones, so

01:01:59.280 --> 01:02:02.270
we don't want to predefine those weights.

01:02:02.270 --> 01:02:02.770
Does that make sense?

01:02:10.509 --> 01:02:13.879
And we can also talk about
that after class as well.

01:02:13.879 --> 01:02:16.458
Great.
So let us take a really quick look at

01:02:16.458 --> 01:02:17.947
the RNN structure.

01:02:17.947 --> 01:02:20.700
So this is a very simple vanilla RNN.

01:02:20.700 --> 01:02:23.490
They key points are the weights
are shared across timesteps.

01:02:23.490 --> 01:02:25.640
We also call them oftentimes tied.

01:02:25.640 --> 01:02:28.280
And the core of an RNN
is the hidden state, and

01:02:28.280 --> 01:02:31.430
the hidden state depends both on
that previous hidden state, so

01:02:31.430 --> 01:02:36.410
kind of like capture some of that memory
from earlier as well as the new input,

01:02:36.410 --> 01:02:39.930
and that back propagation RNN
happens across all timesteps.

01:02:39.930 --> 01:02:41.832
And you should have seen
that on the assignment two.

01:02:41.832 --> 01:02:45.933
And RNNs because of their architecture
are very suitable for learning

01:02:45.933 --> 01:02:51.021
representations for sequential data,
that has sometimes temporal relationships.

01:02:51.021 --> 01:02:54.499
And the predictions can be
made at every timesteps, so

01:02:54.499 --> 01:02:57.912
you can have a y as can be
seen in this figure over here.

01:02:57.912 --> 01:03:01.487
You can make a prediction every timestep
or you can make can at the end of

01:03:01.487 --> 01:03:04.890
the sequence, depending on whatever
your application might be.

01:03:04.890 --> 01:03:10.455
So in our key, RNN's come to use
a special when we do language modelling.

01:03:10.455 --> 01:03:15.120
So language modelling is the tasks of
computing probability distributions

01:03:15.120 --> 01:03:20.630
over a sequence of words, so that can
be for example seconds or a document.

01:03:20.630 --> 01:03:22.470
And language modeling is very important,

01:03:22.470 --> 01:03:25.959
when we are doing things like speech
recognition, text summariztion and so on.

01:03:25.959 --> 01:03:29.832
So in an RNN, when we want to
use it as a language model,

01:03:29.832 --> 01:03:35.243
would be passing at every time subsidy
inputs Xt, are our word embeddings.

01:03:35.243 --> 01:03:39.866
And you could use word embeddings, for
example, that you produce with word and

01:03:39.866 --> 01:03:41.120
[INAUDIBLE] or glove.

01:03:41.120 --> 01:03:43.300
So you pass it at every timestep.

01:03:43.300 --> 01:03:48.110
And our prediction is just
the next word in the sequence.

01:03:48.110 --> 01:03:53.069
So we can use that as our task
in order to train this RNN.

01:03:53.069 --> 01:03:56.310
And the idea is that the hidden
representation at the end.

01:03:56.310 --> 01:04:00.510
So if you take the hidden representation
at the last time stamp, on our trained

01:04:00.510 --> 01:04:05.610
RNN, capture some of the semantic meaning
of that sentence or sequence of words.

01:04:07.660 --> 01:04:11.955
So that can be, for example, used for
translation, which is something you saw,

01:04:11.955 --> 01:04:14.959
or you saw on Tuesday I think,
which Richard presented.

01:04:14.959 --> 01:04:18.935
So here, this is actually a [INAUDIBLE]
RNN which have an encoding part and

01:04:18.935 --> 01:04:20.270
a decoding part.

01:04:20.270 --> 01:04:22.923
So for the encoding part,
that's your first RNN.

01:04:22.923 --> 01:04:28.030
You feed in the word embeddings for
the German words, so [FOREIGN].

01:04:28.030 --> 01:04:32.491
And then you pass in that hidden
representation which captures the meaning

01:04:32.491 --> 01:04:34.701
of the sentence into the second RNN.

01:04:34.701 --> 01:04:37.098
So the weights are shared
across the first RNN, and

01:04:37.098 --> 01:04:39.010
they're shared within the second RNN.

01:04:40.090 --> 01:04:44.300
And a second RNN is a decoder which
produces the English sentence here.

01:04:44.300 --> 01:04:48.803
So this is one example of using this
type of RNN as a language model for

01:04:48.803 --> 01:04:49.914
a translation.

01:04:49.914 --> 01:04:54.688
Okay, so the problem though with
the vanilla RNNs is that we can see

01:04:54.688 --> 01:04:57.660
the problem vanishing gradients.

01:04:57.660 --> 01:05:00.661
In back propagation,
when we do back propagation RNNs,

01:05:00.661 --> 01:05:03.602
there is a recursive gradient
call on the hidden layer.

01:05:03.602 --> 01:05:07.574
And so that's basically since we define
a hidden layer in terms of its P

01:05:07.574 --> 01:05:08.673
precedent layer.

01:05:08.673 --> 01:05:13.378
And the magnitude of the gradients off
the typical activation functions that

01:05:13.378 --> 01:05:17.860
we'd use, like ton H or sigmoid
are between 0 and 1, which causes, and

01:05:17.860 --> 01:05:21.838
if you multiply a number that's
smaller than 1 multiple times,

01:05:21.838 --> 01:05:25.419
what happens is that the number
will shrink very quickly.

01:05:25.419 --> 01:05:26.973
And if it shrinks very quickly,

01:05:26.973 --> 01:05:30.610
essentially what happens is that you
final, gradient will be close to 0.

01:05:30.610 --> 01:05:32.970
And if your gradients are close to 0,

01:05:32.970 --> 01:05:36.090
then essentially means that you're
not updating your parameters.

01:05:36.090 --> 01:05:39.320
And that also just means that
your RNNs don't really learn.

01:05:39.320 --> 01:05:44.440
So in order to address this issue, this
is like why people have worked on many

01:05:44.440 --> 01:05:50.123
of the different architectures and GRUs,
LSTMs were some of the more popular ones.

01:05:50.123 --> 01:05:50.847
Yes, there's a question?

01:06:11.078 --> 01:06:12.755
Okay, I should also repeat the question.

01:06:12.755 --> 01:06:17.291
So the question was, can we adjust
the vanishing gradient problem by using

01:06:17.291 --> 01:06:21.035
a different activation function,
something like a volume?

01:06:21.035 --> 01:06:22.205
Yeah, okay, sorry.

01:06:22.205 --> 01:06:23.244
And your second part was what?

01:06:37.091 --> 01:06:39.700
Yeah, we can also discuss
this after class, too.

01:06:39.700 --> 01:06:41.625
Yeah, and so good question there.

01:06:41.625 --> 01:06:44.260
Okay, so GRUs and LSTMs to the rescue.

01:06:44.260 --> 01:06:47.600
So I'll talk briefly about GRUs,
Gated Recurrent Units.

01:06:47.600 --> 01:06:49.988
So the addition here is that
we are introducing gate.

01:06:49.988 --> 01:06:52.790
So we have a reset gate,
and an update gate.

01:06:53.890 --> 01:06:58.100
And you can intuitively understand
these gates as they're controlling

01:06:58.100 --> 01:07:00.960
the long term and short term dependencies.

01:07:00.960 --> 01:07:03.784
To what extent we want to
memorize things from the past?

01:07:03.784 --> 01:07:07.660
And to what extent to we just want to
take something from the current input?

01:07:07.660 --> 01:07:10.930
And that might be also something
that you might see or like.

01:07:10.930 --> 01:07:13.350
It's related to what we talked
about with patient notes.

01:07:13.350 --> 01:07:16.560
It's kind of this network might be able
to learn whether I should only look at

01:07:16.560 --> 01:07:19.810
the last patient note, or whether I should
take like the entire history into account?

01:07:21.130 --> 01:07:25.870
So and this is a more visual
representation of a GRU games.

01:07:25.870 --> 01:07:29.296
So you can see that the input,
X team over here,

01:07:29.296 --> 01:07:33.848
is fed into both of these gates,
which are sigmoid functions.

01:07:33.848 --> 01:07:37.975
And it's also fed into this other one,
which calculates H tilde.

01:07:37.975 --> 01:07:42.764
So this is Kind of a first initial
hidden state, but in order to decide

01:07:42.764 --> 01:07:47.553
whether we really want to use it,
we are using the z factor over here,

01:07:47.553 --> 01:07:52.178
to decide whether we just want to
use the previous hidden state, or

01:07:52.178 --> 01:07:58.869
if you want to update it to the H tilde,
And

01:07:58.869 --> 01:08:01.453
this might also be a picture if
you wanted to look at it again.

01:08:01.453 --> 01:08:02.803
It's on our blog, so

01:08:02.803 --> 01:08:07.380
you can just try to figure out how
this information is flowing and there.

01:08:08.630 --> 01:08:13.180
But in order to just get some more
intuitive understanding of how GRU works,

01:08:13.180 --> 01:08:15.750
we are going to do some
exercises together.

01:08:15.750 --> 01:08:16.490
So first,

01:08:16.490 --> 01:08:21.990
try to figure out what the dimensions
are off the ws and the u matrices in here.

01:08:21.990 --> 01:08:25.390
And you can write it in terms of the x,

01:08:25.390 --> 01:08:29.070
dimension of x and the h,
which is the dimension of h.

01:08:29.070 --> 01:08:31.255
Again, you can just take a minute and
try to figure that out.

01:08:43.252 --> 01:08:45.407
And you can also discuss
again with your neighbor.

01:09:04.738 --> 01:09:07.753
Since we are running a little bit out
of time, I'm just gonna go ahead and

01:09:07.753 --> 01:09:10.095
explain how I would approach this problem.

01:09:10.095 --> 01:09:13.525
So first, you know that ht,

01:09:13.525 --> 01:09:18.485
the hidden vector h has
dimensioin h just by definition.

01:09:18.485 --> 01:09:23.720
So I'll write a little bit bigger, okay.

01:09:23.720 --> 01:09:27.700
So this is just by definition,
and we also know that

01:09:28.930 --> 01:09:34.040
ht minus 1 was also has mentioned dh,
which means 1 minus zt, so

01:09:34.040 --> 01:09:39.900
zt itself needs to have to
mention the dh as well, right?

01:09:39.900 --> 01:09:42.520
Since we are doing
a aliments bite operation

01:09:42.520 --> 01:09:43.910
in the fourth equation over here.

01:09:45.130 --> 01:09:48.980
So it also shows us that H tilde,

01:09:48.980 --> 01:09:53.720
suggest by looking at the last equation,
H tilde means half dimension dh as well.

01:09:56.930 --> 01:10:02.330
And now, that we know that
H tilde has dimension dh,

01:10:02.330 --> 01:10:05.500
then we can go into the third
equation over here.

01:10:05.500 --> 01:10:11.480
So we know that the product of W and
xc has to give us something with dh.

01:10:11.480 --> 01:10:14.731
So that tells us that the matrix W,

01:10:17.643 --> 01:10:25.120
Needs to have dimension dh, and dx.

01:10:25.120 --> 01:10:25.853
Is this clear to everyone?

01:10:27.532 --> 01:10:29.810
Cool, I see a couple nods, okay.

01:10:29.810 --> 01:10:34.346
So similarly,
if you look at the matrix u in there,

01:10:34.346 --> 01:10:39.636
we know that like the result of u,
ht minus 1 needs to be dh.

01:10:39.636 --> 01:10:43.280
Like [INAUDIBLE] so percent should be dh.

01:10:43.280 --> 01:10:47.170
And since we're multiplying
it with a hidden vector,

01:10:47.170 --> 01:10:50.210
the second dimension should be dh as well.

01:10:50.210 --> 01:10:53.660
And you can apply a similar approach
to figure out the dimensionality for

01:10:53.660 --> 01:10:55.870
the other matrices as well.

01:10:58.330 --> 01:10:59.920
Cool, and true or false question.

01:10:59.920 --> 01:11:05.332
If the update gate is close to 0, the net
does not update its state significantly.

01:11:11.041 --> 01:11:15.466
Yeah, this one should be pretty obvious
if you look at the fourth equation,

01:11:15.466 --> 01:11:19.836
because in that case, you're essentially
just setting ht to ht minus 1.

01:11:19.836 --> 01:11:25.956
And this also shows that if the network
learns that zt is pretty close to 0,

01:11:25.956 --> 01:11:32.191
that just means that we are essentially
using our input from the first time.

01:11:32.191 --> 01:11:36.692
So essentially, this means that your input
from a long time ago still matters a lot

01:11:36.692 --> 01:11:37.550
in the future.

01:11:39.732 --> 01:11:43.571
Okay another related question, if the
update gate is close to one, and a reset

01:11:43.571 --> 01:11:47.484
gate is close to zero, the net remembers
the passing very well, true or false.

01:11:54.561 --> 01:11:58.300
Yeah this is actually very
similar to the first question,

01:11:58.300 --> 01:12:02.590
so again you can just set them on z
t to 1 r t to 0 in these equations.

01:12:02.590 --> 01:12:07.821
And you'll find out that in this case, h t
would depend very strongly on the input,

01:12:07.821 --> 01:12:12.405
since we're essentially saying that
this term over here becomes zero.

01:12:14.415 --> 01:12:16.861
And we're only using the x t part of it.

01:12:19.278 --> 01:12:22.977
Okay, so Joee will now talk about LSTMs.

01:12:28.342 --> 01:12:31.680
&gt;&gt; So, I’ll just run through
LSTMs really quickly.

01:12:31.680 --> 01:12:35.720
LSTMs are kind of similar to GIUs
except they are a more common model.

01:12:35.720 --> 01:12:39.400
So, instead of just having two gates,
now we have multiple gates.

01:12:39.400 --> 01:12:44.182
One is the input gate, which decides
how much weight should we give to

01:12:44.182 --> 01:12:48.394
the current input or
the current word that we are looking at.

01:12:48.394 --> 01:12:54.575
The ft gate or the forget gate will decide
how much we want to forget our past,

01:12:54.575 --> 01:12:59.230
how much you want to remember the past or
just forget it.

01:12:59.230 --> 01:13:02.197
The new gate ot,
which is a little bit different,

01:13:02.197 --> 01:13:05.850
is how much we wanna expose
our current cell to future.

01:13:05.850 --> 01:13:12.860
So combine the ot from the current,
combined with the ft from the future

01:13:12.860 --> 01:13:17.470
will decide how much all of memory
will be use in any other future node.

01:13:17.470 --> 01:13:21.877
So you can see how adjusting this can
decide like whether you just wanna

01:13:21.877 --> 01:13:25.847
remember the B minus 3 times 3 but
not, T minus 20 minus 1.

01:13:25.847 --> 01:13:28.812
Instead of just going linearly backwards,

01:13:28.812 --> 01:13:32.290
I'm having to remember
everything from the past.

01:13:32.290 --> 01:13:37.139
So, all these gates are computered using
a Sigmoid because that's between 0 and 1.

01:13:37.139 --> 01:13:41.580
And that makes sure that when you
are doing the dot products of, sorry,

01:13:41.580 --> 01:13:46.319
the micro dot products, in the end,
you'll have kind of like a probability

01:13:46.319 --> 01:13:50.857
distribution of not just completely
forgetting, which would be zero.

01:13:50.857 --> 01:13:52.950
Or completely remembering which would one,
but

01:13:52.950 --> 01:13:56.230
a fuzzy possibility of remembering
a little bit or forgetting a little bit.

01:13:57.440 --> 01:13:58.340
Does that make sense?

01:13:59.950 --> 01:14:04.890
Another difference between LSTMs and
GRUs is the ct and ht node, so

01:14:04.890 --> 01:14:07.910
now you don't just have one memory,
you have two memories.

01:14:07.910 --> 01:14:10.215
And these define different things.

01:14:10.215 --> 01:14:14.569
Like the ct would define
exactly what your memory is,

01:14:14.569 --> 01:14:19.209
while your ht defines in your it,
ft, and ot, how much or

01:14:19.209 --> 01:14:23.210
how you want to remember
the ht minus one memory.

01:14:23.210 --> 01:14:27.260
Or how much your memory should
be remembered by the future.

01:14:27.260 --> 01:14:29.710
So instead of just having one,
now we have two.

01:14:31.220 --> 01:14:34.210
These have a certain number
of disadvantages, for

01:14:34.210 --> 01:14:39.790
example there are a lot more parameters
now, which means you need more space.

01:14:39.790 --> 01:14:43.660
Which means you need more learning,
and which also means you need

01:14:43.660 --> 01:14:47.070
much more training data, so
that your model doesn't start warpening.

01:14:47.070 --> 01:14:51.800
And empirically, GRUs and LSTMs have been,
are very close in results.

01:14:51.800 --> 01:14:56.630
So in the end it's a trade off on how much
accuracy you want, compared to how much

01:14:56.630 --> 01:15:01.100
you are willing to use your resources
on training time and training data.

01:15:03.030 --> 01:15:04.480
So here's a quick illustration.

01:15:06.040 --> 01:15:09.750
The lower row corresponds to your ht flow,

01:15:09.750 --> 01:15:12.630
while the upper row
corresponds to your ct flow.

01:15:12.630 --> 01:15:14.154
And both of them are memories and

01:15:14.154 --> 01:15:18.005
they interact with each other which means
they are not completely independent.

01:15:18.005 --> 01:15:23.587
But they represent different
ways in which your future,

01:15:23.587 --> 01:15:28.598
or your next words will be
using your current word.

01:15:28.598 --> 01:15:29.853
Yeah, question?

01:15:36.937 --> 01:15:43.098
So the question was can you give a little
bit of intuition on what is the difference

01:15:43.098 --> 01:15:48.895
between c and h, and so if you see from
the formulae, the edges are present,

01:15:48.895 --> 01:15:54.170
or the edges decide on what your
input gates, your fts or ots will be.

01:15:54.170 --> 01:15:59.358
So they decide how much you want to
remember or forget or expose yourself.

01:15:59.358 --> 01:16:03.894
While your cts are the ones that
are actually being used in the future.

01:16:03.894 --> 01:16:06.027
So they are the ones that
are combining with the gates.

01:16:06.027 --> 01:16:11.205
And so, basically it's like saying your
hts will decide the fuzzy probability

01:16:11.205 --> 01:16:15.530
of how much you want to remember,
and how you want to remember, and

01:16:15.530 --> 01:16:20.900
your ct will kind of decide what you want
to remember from the previous states.

01:16:20.900 --> 01:16:24.310
This is just like illustration,
it's not exactly like that.

01:16:25.720 --> 01:16:28.170
LSTMs might learn something differently.

01:16:28.170 --> 01:16:29.450
Depending on your problem statement.

01:16:29.450 --> 01:16:32.336
But that's like a rough way of
how you can think about it.

01:16:40.345 --> 01:16:42.860
Great, so the question was
will we have the equations.

01:16:42.860 --> 01:16:46.236
And, yes, all the equations for
RINs, or LSTMs or GRTs will be there,

01:16:46.236 --> 01:16:48.225
so that you don't have to remember them.

01:16:48.225 --> 01:16:53.331
Also, you are allowed a cheat sheet, so
to be honest if you wanted to remember it,

01:16:53.331 --> 01:16:56.810
just write it on your cheat sheet,
and you can bring it.

01:16:56.810 --> 01:17:01.513
So we'll just go to the midterm questions
really quickly because we are running out

01:17:01.513 --> 01:17:02.210
of time.

01:17:02.210 --> 01:17:07.012
If xt is the 0 vector then ht = ht -1.

01:17:07.012 --> 01:17:11.054
It is true that ht will depend primarily
on ht -1 but it won't be exactly equal

01:17:11.054 --> 01:17:15.840
because of the non-linearities and
the multiplication with the parameters.

01:17:15.840 --> 01:17:18.679
So that's an embodied difference.

01:17:18.679 --> 01:17:20.919
The second question is if
FD is very small or 0,

01:17:20.919 --> 01:17:24.250
then error will not be back-propagated
to earlier time steps.

01:17:24.250 --> 01:17:28.590
Again, intuitively it feels like because
the FD is 0, they are trying to forget.

01:17:28.590 --> 01:17:33.010
But that is not true, because it and
ct still depend on ht minus 1, and

01:17:33.010 --> 01:17:34.580
hence the error will still back propagate.

01:17:35.870 --> 01:17:38.660
So, one easy way to do it
is just look at the mat,

01:17:38.660 --> 01:17:43.490
see what depends on what, and see how
the rows are actually back-propagating.

01:17:43.490 --> 01:17:49.010
The next question is if either entries ft,
it and ot are negative, and we see

01:17:49.010 --> 01:17:53.990
that the sigmoid activation is used, which
is the range is between zero and one.

01:17:53.990 --> 01:17:56.510
And so they're always non-negative.

01:17:56.510 --> 01:17:58.810
And, as I said, that just
represents the fuzzy probability.

01:17:59.930 --> 01:18:02.115
Which brings me to the next question,

01:18:02.115 --> 01:18:05.095
can they be viewed as
probability distributions.

01:18:05.095 --> 01:18:08.660
And the problem with viewing them as
probability distributions is that they do

01:18:08.660 --> 01:18:11.790
not sum to one,
like any probability distribution should.

01:18:11.790 --> 01:18:14.706
So these are applied
independently of the element and

01:18:14.706 --> 01:18:18.430
thus they have no requirement
of having to sum to one.

01:18:18.430 --> 01:18:20.620
So, this is just like
an intuition of probability, but

01:18:20.620 --> 01:18:21.770
it's not exactly probability.

01:18:24.220 --> 01:18:25.480
Does that make sense to everyone?

01:18:25.480 --> 01:18:25.980
Any questions?

01:18:28.406 --> 01:18:29.010
Okay.

01:18:29.010 --> 01:18:31.640
So, very quickly we'll
go dependency parsing.

01:18:33.220 --> 01:18:36.112
So, these are two views
of linguistic structures,

01:18:36.112 --> 01:18:40.795
we haven't seen which was constituency
structure, we'll be seeing it for later.

01:18:40.795 --> 01:18:45.119
But basically it uses a kind
of c f g model where you

01:18:45.119 --> 01:18:49.771
decide which phrase is
broken down into what words.

01:18:49.771 --> 01:18:52.633
What we are seeing is
dependency structure,

01:18:52.633 --> 01:18:57.076
where we decide what word depends on
what other word in the sentence and

01:18:57.076 --> 01:19:00.714
how they depend on each other so
what modifiers are used.

01:19:00.714 --> 01:19:03.690
Just some properties
about dependency parsing.

01:19:03.690 --> 01:19:07.440
One, it's a binary asymmetric
relation with words.

01:19:07.440 --> 01:19:10.810
It's binary, which means two words
are related to each other, no more and

01:19:10.810 --> 01:19:11.600
no less.

01:19:11.600 --> 01:19:14.160
It's asymmetric, which means one
word depends on the other, but

01:19:14.160 --> 01:19:15.950
not the other way around.

01:19:15.950 --> 01:19:18.640
In this case,
the dependent depends on the head.

01:19:18.640 --> 01:19:20.800
Which are the terms that
we'll generally be using.

01:19:20.800 --> 01:19:23.770
And the arrows when you're drawing
the dependency tree will go from the head

01:19:23.770 --> 01:19:24.480
to the dependent.

01:19:25.950 --> 01:19:29.320
They usually form a connected,
acyclic, single-head tree.

01:19:29.320 --> 01:19:33.120
And to make sure that this happens,
we also add a fake root node so

01:19:33.120 --> 01:19:37.450
that one of the words will always
have the head as a root node.

01:19:37.450 --> 01:19:40.870
And this also makes sure
that every word has a head.

01:19:40.870 --> 01:19:43.090
And none of the words are just standing.

01:19:43.090 --> 01:19:45.840
And in the rare cases where
you have a disconnected tree,

01:19:45.840 --> 01:19:49.290
the root known make sure that all of
them are connected into a single tree.

01:19:51.190 --> 01:19:51.910
Any questions?

01:19:54.060 --> 01:19:54.580
Okay.

01:19:54.580 --> 01:19:57.790
So we saw two different
types of dependency parsing.

01:19:57.790 --> 01:20:01.940
One was the greedy deterministic
transition based parsing.

01:20:01.940 --> 01:20:04.700
We've just seen this in
the assignment right now so

01:20:04.700 --> 01:20:07.520
I am sure it's all fresh in your memories.

01:20:07.520 --> 01:20:11.390
You have a stack, a buffer,
and a dependency list.

01:20:11.390 --> 01:20:17.180
Initially the buffer will be full of
the words in your sentence, in order.

01:20:17.180 --> 01:20:19.380
So that will act like a queue structure.

01:20:19.380 --> 01:20:21.350
Your stack will act like
a stack structure so

01:20:21.350 --> 01:20:24.380
that you pop from the top of this chart.

01:20:24.380 --> 01:20:26.820
And whenever you do this shift transition,

01:20:26.820 --> 01:20:31.060
you take something from the top of
the queue and put it onto the stack.

01:20:31.060 --> 01:20:34.510
The left toll will take the top
two elements from the stack, and

01:20:34.510 --> 01:20:38.110
make an arrow from the first
element to the second element, and

01:20:38.110 --> 01:20:39.150
just remove the second element.

01:20:40.560 --> 01:20:42.640
And this can be seen
from the example here,

01:20:42.640 --> 01:20:47.750
which you can just scroll through
the slides and go like decide.

01:20:47.750 --> 01:20:50.350
Now, how do we decide
which transition to use?

01:20:50.350 --> 01:20:54.670
This is generally done by some kind of
a classifier, you could use multiclass as

01:20:54.670 --> 01:20:59.730
SVMs or any other kind of machine
learning classifiers that you know of.

01:20:59.730 --> 01:21:03.260
The features that any machine
learning involves need, so

01:21:03.260 --> 01:21:06.900
here we'll use the features, and
you can always add more features.

01:21:06.900 --> 01:21:09.410
But the typical features
that are generally used

01:21:09.410 --> 01:21:14.160
is the top of the word on the stack,
the first word in the buffer, and

01:21:14.160 --> 01:21:16.830
maybe a lookahead on what
words are gonna come next.

01:21:16.830 --> 01:21:20.460
And also the dependence of
the current word in the stack.

01:21:20.460 --> 01:21:21.820
And now that we have all the words,

01:21:21.820 --> 01:21:23.820
we also use the parts of
speech of all these words.

01:21:23.820 --> 01:21:28.320
So generally we know that adjectives and
nouns already likely to be dependents or

01:21:28.320 --> 01:21:32.550
connector, so that there's something
that you would want to use as a feature.

01:21:32.550 --> 01:21:35.210
Yeah, those are kind of what
dependencies that we have

01:21:35.210 --> 01:21:36.090
already figured out until now.

01:21:37.440 --> 01:21:42.830
And using all of these, we try to get what
kind of transition we want to do next.

01:21:42.830 --> 01:21:46.530
And define the valuation metric
that we use is either UAs,

01:21:46.530 --> 01:21:49.720
if you're not typing the dependency
from one word to another.

01:21:49.720 --> 01:21:53.660
Or LAS, which is the liberal attachment
school which types their attachment from

01:21:53.660 --> 01:21:58.430
one word to another, and these
are basically kind of like accuracies.

01:21:58.430 --> 01:21:59.740
You can think of them as accuracies.

01:22:02.380 --> 01:22:03.440
Okay.

01:22:03.440 --> 01:22:06.740
So one thing that we saw
was projectivity and

01:22:06.740 --> 01:22:11.160
how we can handle cases where
the non-projectivity comes in.

01:22:11.160 --> 01:22:14.310
And what projectivity
means is that there are no

01:22:14.310 --> 01:22:17.830
arrows that cross each other when
you put them in a horizontal line.

01:22:18.860 --> 01:22:23.410
And why this would be a problem, is if
you go back to this kind of a parsing

01:22:23.410 --> 01:22:29.180
mechanism, here the from and who will
never be next to each other on a stack.

01:22:29.180 --> 01:22:30.430
And thus, left arc or

01:22:30.430 --> 01:22:35.340
right arc will never be able to get
the dependency between from and who.

01:22:35.340 --> 01:22:39.090
And, so
how do we handle something like this?

01:22:39.090 --> 01:22:41.450
One simple thing would be,
you just declare defeat.

01:22:41.450 --> 01:22:44.500
You just say that okay,
these are really, really rare cases.

01:22:44.500 --> 01:22:47.590
And if you don't care about
your accuracy as much and

01:22:47.590 --> 01:22:51.240
you don't want to complicate your
model a lot, then you can just

01:22:51.240 --> 01:22:55.340
leave them as it is and let them
decrease your accuracy a little bit.

01:22:55.340 --> 01:22:58.550
If you want to handle them, one of
them would be to use a post resistor.

01:22:58.550 --> 01:23:01.982
So you go through the entire
parsing mechanism and in the end,

01:23:01.982 --> 01:23:05.612
you use some kind of a post-processor
to identify which ones have

01:23:05.612 --> 01:23:08.760
been parsed strongly and
try to dissolve them.

01:23:08.760 --> 01:23:13.780
This can be done using multiple methods
like classifiers or other things.

01:23:13.780 --> 01:23:17.461
The last one is you use a completely
different parsing mechanism or

01:23:17.461 --> 01:23:20.961
you make slight modifications
to the ones that already exist.

01:23:20.961 --> 01:23:25.198
So in the greedy transition based parsing,
you could add a transition,

01:23:25.198 --> 01:23:29.852
let's say swap where you just swap the
elements that are already in the stacks so

01:23:29.852 --> 01:23:33.324
that you bring the element that
is at the bottom to the top and

01:23:33.324 --> 01:23:36.270
then you can either lift right down to it.

01:23:36.270 --> 01:23:40.070
There are other more complicated ways of
doing it, and those can also be done.

01:23:42.870 --> 01:23:45.420
Finally, we have the neural
dependency parsing.

01:23:46.780 --> 01:23:50.660
So one problem with really the domestic
parsing that we saw was the features were

01:23:50.660 --> 01:23:56.350
all one-hot vectors, where it was
either a word or a part of speech.

01:23:56.350 --> 01:23:59.170
And we have seen a lot of problems
with one-hot vectors before.

01:23:59.170 --> 01:24:02.650
One is that, you don't have the semantic
representation of the word.

01:24:02.650 --> 01:24:08.160
So if two words are actually similar
in either a grammatical sense or

01:24:08.160 --> 01:24:13.940
meaning wise, you don't understand digest
because of the one-hot vector notation.

01:24:13.940 --> 01:24:15.220
And an easy way to do some,

01:24:15.220 --> 01:24:18.150
to solve something like that
is using an embedding matrix.

01:24:18.150 --> 01:24:21.883
And here, we'll not only be using
embedded matrix for the words but

01:24:21.883 --> 01:24:24.779
also the parts of speech and
the dependency labels.

01:24:24.779 --> 01:24:29.782
And all of these are then added into one
feature stack or a feature drawer and

01:24:29.782 --> 01:24:34.560
they are put into a black box which
would be any unit that you want to use.

01:24:34.560 --> 01:24:37.470
For example, here you are using
a hidden layer and a soft max layer.

01:24:37.470 --> 01:24:40.823
But that black box can be any
neural network and then finally,

01:24:40.823 --> 01:24:44.385
you use classification to decide
what transition you want to do.

01:24:46.207 --> 01:24:48.375
Any questions on dependency parsing?

01:24:48.375 --> 01:24:51.387
And I know this was a little quick but
we are running out of time so.

01:24:53.503 --> 01:24:57.538
Okay, so there are just a couple
of acknowledgements and

01:24:57.538 --> 01:24:59.479
all the best for your exam.

01:24:59.479 --> 01:25:01.080
&gt;&gt; [APPLAUSE]

