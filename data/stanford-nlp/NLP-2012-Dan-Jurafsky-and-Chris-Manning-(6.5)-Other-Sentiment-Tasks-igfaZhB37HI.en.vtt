WEBVTT
Kind: captions
Language: en

00:00:00.430 --> 00:00:04.490
There are other sentiment tasks than just
simple positive or negative polarity.

00:00:05.950 --> 00:00:10.910
One important one has to do with finding
the sentiment of an individual sentence,

00:00:10.910 --> 00:00:13.180
which we're going to want to do for
finding aspects or

00:00:13.180 --> 00:00:15.770
attributes that we talked about earlier.

00:00:15.770 --> 00:00:17.920
So that's the finding
the target of the sentiment,

00:00:17.920 --> 00:00:20.990
what it is that we're talking
with the sentiment about.

00:00:20.990 --> 00:00:24.670
So here's a sentence, the food was
great but the service was awful.

00:00:24.670 --> 00:00:27.090
This whole sentence doesn't
really have a sentiment.

00:00:27.090 --> 00:00:33.010
It has a positive sentiment about food and
a negative sentiment about service.

00:00:33.010 --> 00:00:37.530
So we'd like to be able to deal with this
kind of micro sentiment where we have

00:00:37.530 --> 00:00:39.230
a sentiment about one attribute and

00:00:39.230 --> 00:00:41.900
a different sentiment about
a different attribute or aspect.

00:00:43.880 --> 00:00:46.780
So how are we going to find the aspect or

00:00:46.780 --> 00:00:49.730
attribute, or sometimes it's
called target of a sentiment?

00:00:50.840 --> 00:00:55.153
And I'll show you an example that,
a way of doing things that

00:00:55.153 --> 00:01:00.390
was developed both by Hu and
Liu and by Blair-Goldensohn, et al.

00:01:00.390 --> 00:01:04.880
And the idea is we first look
through all the reviews, and

00:01:04.880 --> 00:01:07.490
we find every highly frequent phrase.

00:01:07.490 --> 00:01:12.850
So let's say that the word fish tacos
occurs a lot in this particular

00:01:12.850 --> 00:01:19.280
restaurant reviews of this restaurant, and
then we mark down, we have filters that

00:01:19.280 --> 00:01:24.470
say we prefer highly frequent phrases if
they occur right after a sentiment word.

00:01:24.470 --> 00:01:29.390
So maybe we very often see the phrase
great followed by fish tacos or

00:01:29.390 --> 00:01:31.370
terrible followed by fish tacos.

00:01:31.370 --> 00:01:34.850
That suggests that fish
tacos is an aspect,

00:01:34.850 --> 00:01:38.730
a likely aspect, of a,
let's say a Tacoria.

00:01:38.730 --> 00:01:43.670
So if we see great food a lot,
we see food occurring very often

00:01:43.670 --> 00:01:48.280
in a review, and we see the word great and
terrible and awful before it a lot,

00:01:48.280 --> 00:01:52.130
we know it's probably an aspect that
people are applying sentiment for.

00:01:52.130 --> 00:01:56.575
And this can be used automatically to
find sentiment for lots of things.

00:01:56.575 --> 00:02:01.305
So Blair-Goldensohn, et al used it for
example, for casino, reviews of casinos,

00:02:01.305 --> 00:02:05.215
to find words like casino,
buffet, pool, resort, and

00:02:05.215 --> 00:02:09.105
beds are the kind of things people
comment about in reviews on casinos.

00:02:09.105 --> 00:02:13.315
For a barber you might talk
about the experience or

00:02:13.315 --> 00:02:16.290
how good a job it was or
facts about the haircut.

00:02:16.290 --> 00:02:21.150
For a department store you might talk
about sales, or different departments, or

00:02:21.150 --> 00:02:22.780
the selection of the store,
things like that.

00:02:24.220 --> 00:02:26.648
So we can automatically find the aspect.

00:02:29.428 --> 00:02:30.962
In other cases though,

00:02:30.962 --> 00:02:35.480
the word that describes the aspect
may not be in the sentence.

00:02:35.480 --> 00:02:39.608
And so for some sentiment topics like
restaurants or hotels, it's pretty well

00:02:39.608 --> 00:02:43.438
understood what the aspects are that
people care about for restaurants.

00:02:43.438 --> 00:02:48.400
For restaurants they tend to care
about food, decor, service, value and

00:02:48.400 --> 00:02:52.914
so for cases where we know in advance
what the frequent aspects are.

00:02:52.914 --> 00:02:56.669
Here, we can take a small
corpus of restaurant reviews,

00:02:56.669 --> 00:02:58.320
and we can hand label it.

00:02:58.320 --> 00:03:01.120
We'll just mark every sentence.

00:03:01.120 --> 00:03:03.540
Does this sentence talk about the food?

00:03:03.540 --> 00:03:05.784
Does it talk about the decor,
or is it none of the above?

00:03:05.784 --> 00:03:08.360
And then we can just build a classifier,

00:03:08.360 --> 00:03:12.472
which given a sentence assigns
an aspect to the sentence.

00:03:12.472 --> 00:03:15.345
Is this sentence about food,
is the sentence about decor,

00:03:15.345 --> 00:03:17.018
sentence about service and so on.

00:03:17.018 --> 00:03:20.731
And it could be a sentence or
we could assign that with phrases or

00:03:20.731 --> 00:03:25.298
maybe with clauses, with pieces of
sentences, which ever we'd like to do.

00:03:27.378 --> 00:03:32.060
So two ways of finding
aspect of a sentiment.

00:03:32.060 --> 00:03:36.591
We can automatically find frequent
phrases, so we can find phrases.

00:03:39.911 --> 00:03:44.717
And then we can build up a set of phrases
that occur frequently, and decide those

00:03:44.717 --> 00:03:49.655
are good phrases that we'd like to know
about for this particular product.

00:03:49.655 --> 00:03:54.295
Or, we can decide the aspects in advance,

00:03:54.295 --> 00:03:59.155
aspects come in advance, like we
know what they are for restaurants.

00:03:59.155 --> 00:04:03.487
And then our job is just to build a little
classifier to find them in the reviews so

00:04:03.487 --> 00:04:07.578
we can decide if what a person said
about the food was positive or negative.

00:04:09.618 --> 00:04:14.515
So putting this all together this is the
Blair-Goldensohn algorithm, from Google we

00:04:14.515 --> 00:04:19.009
have a set of reviews, we're going to
extract a bunch of sentences or phrases,

00:04:19.009 --> 00:04:22.363
so here's our extracted sentences or
phrases, and now for

00:04:22.363 --> 00:04:26.220
each one of them we're going to
run our sentiment classifier.

00:04:26.220 --> 00:04:30.170
Is this phrase positive,
negative, or maybe it's neutral?

00:04:30.170 --> 00:04:32.790
Okay?
And then for the ones that have sentiment

00:04:32.790 --> 00:04:36.290
in them, the ones that are positive or
negative, so now we have sentiments and

00:04:36.290 --> 00:04:40.420
phrases with sentiment, plus, minus,
minus, plus, plus, minus, and so on.

00:04:40.420 --> 00:04:42.990
We're going to extract
the aspects of these.

00:04:42.990 --> 00:04:44.850
So is this about food?

00:04:44.850 --> 00:04:48.220
Is it about decor?

00:04:48.220 --> 00:04:49.430
What's the sentence about?

00:04:49.430 --> 00:04:52.460
So now we have plus food, and

00:04:52.460 --> 00:04:57.768
now we have minus decor,
minus service and so on.

00:04:57.768 --> 00:05:01.660
For each sentence or phrase we have
both a sentiment and an aspect.

00:05:01.660 --> 00:05:05.370
Now we aggregate these together and
produce a final summary.

00:05:05.370 --> 00:05:10.060
And here's an example of what you
might get as the final summary.

00:05:10.060 --> 00:05:14.730
So, maybe for a hotel for rooms, we might
get this sentence, the room was clean and

00:05:14.730 --> 00:05:17.660
everything worked fine,
positive sentiment.

00:05:17.660 --> 00:05:19.870
Another positive sentiment.

00:05:19.870 --> 00:05:22.090
Here's an example of negative sentiment.

00:05:22.090 --> 00:05:26.080
So we first figure out that
worst is a negative sentiment.

00:05:26.080 --> 00:05:30.450
And then we might extract from
somewhere in here that there's some

00:05:30.450 --> 00:05:32.180
mention of rooms.

00:05:32.180 --> 00:05:36.804
And similarly for service,
we might see service

00:05:36.804 --> 00:05:41.663
gives new meaning to slow,
and that's negative.

00:05:46.103 --> 00:05:50.156
We made some simplifications in
the baseline method we gave for

00:05:50.156 --> 00:05:52.070
sentiment classification.

00:05:53.390 --> 00:05:57.690
We assumed that the classes, positive and
negative occurred with equal frequency.

00:05:57.690 --> 00:06:02.290
And that's of course not usually true
in the real world and it turns out that

00:06:02.290 --> 00:06:06.970
when the classes are not balanced,
we can't use accuracies as an evaluation.

00:06:06.970 --> 00:06:12.720
And the F-score can deal well and when
evaluating classes where there's many more

00:06:12.720 --> 00:06:16.970
positive than negative or maybe there's
many more negative than positive reviews.

00:06:16.970 --> 00:06:22.230
It turns out that this, if there's
a severe imbalance in class frequencies,

00:06:22.230 --> 00:06:24.760
that can actually degrade
the classifier performance.

00:06:24.760 --> 00:06:27.610
And there are two standard
things we do to deal with that.

00:06:27.610 --> 00:06:30.400
One is we just resampled before we train.

00:06:30.400 --> 00:06:35.210
So for example,
if one class has a million reviews and

00:06:35.210 --> 00:06:41.090
one class has 10,000 reviews,
then we might just dance down sample and

00:06:41.090 --> 00:06:46.370
take only ten to the fourth of these
reviews to match with these reviews.

00:06:48.230 --> 00:06:50.970
Instead of resampling we can
use cost sensitive learning.

00:06:50.970 --> 00:06:54.971
Cost sensitive learning,
we actually change the classifier and

00:06:54.971 --> 00:06:59.698
we tell the classifier even though you've
seen a lot of this frequent thing,

00:06:59.698 --> 00:07:03.134
penalize it for
misclassifying the really rare thing.

00:07:03.134 --> 00:07:06.700
And so that'll force it to focus
a little more on the rarer

00:07:06.700 --> 00:07:09.340
things than on the very frequent things.

00:07:09.340 --> 00:07:12.260
So two things you can do to deal with

00:07:12.260 --> 00:07:16.269
the imbalanced frequency problem
that often occurs in real classes.

00:07:18.560 --> 00:07:23.287
We also in our baseline algorithm
made the simplifying assumption that

00:07:23.287 --> 00:07:27.500
sentiment was a binary problem,
positive or negative.

00:07:27.500 --> 00:07:32.080
So how do we deal with five stars,
or seven stars, or ten stars?

00:07:32.080 --> 00:07:33.450
There was two ways we can do this.

00:07:33.450 --> 00:07:37.080
We can map onto a binary class,
we can say things that

00:07:37.080 --> 00:07:41.000
are more than 3.5 stars are positive or
less than 3.5 are negative.

00:07:41.000 --> 00:07:44.160
Or we can take the average of all the
data, we can Z score, we can do various

00:07:44.160 --> 00:07:49.570
ways of drawing a boundary between
positive things and negative things.

00:07:49.570 --> 00:07:53.910
Or we can actually attack the one to seven

00:07:53.910 --> 00:07:58.730
classification task directly by
using linear, ordinal regression, or

00:07:58.730 --> 00:08:02.190
specialized models like metric
labeling that's used by Pang and Lee.

00:08:02.190 --> 00:08:07.080
So again, we can either downsample and
just do binary classification or

00:08:07.080 --> 00:08:11.880
we can use a more advanced method of
classification that lets us predict

00:08:11.880 --> 00:08:14.570
an ordinal value or a real valued number.

00:08:16.680 --> 00:08:23.770
So in summary, sentiment generally modeled
as a classification task, polarity,

00:08:23.770 --> 00:08:29.500
often with binary, less often with some
kind of ordinal or linear valued label.

00:08:30.850 --> 00:08:33.910
And negation,
a very important feature to use.

00:08:35.170 --> 00:08:39.470
For lots of tasks, using all the words
in naive bayes seems to work well.

00:08:39.470 --> 00:08:42.600
For other tasks,
using subsets of words may help.

00:08:42.600 --> 00:08:46.999
And we can either used hand-built
polarity lexicons, or for tasks for

00:08:46.999 --> 00:08:50.959
which the polarity lexicons seem
inappropriate, we can induce

00:08:50.959 --> 00:08:55.594
lexicons using semi-supervised
learning from some hand-built seeds.

00:08:55.594 --> 00:09:00.900
Now remember that sentiment is really just
one of many kinds of affective states.

00:09:00.900 --> 00:09:03.850
It's a kind of attitude classification,
but

00:09:03.850 --> 00:09:07.780
there's lots of other kinds of affective
state classification that come up and

00:09:07.780 --> 00:09:11.980
they are computational problems that
are similar to sentiment analysis.

00:09:11.980 --> 00:09:14.400
So to look at some of them for emotions,

00:09:14.400 --> 00:09:17.835
we might want to detect annoyed
callers to some dialogue systems,

00:09:17.835 --> 00:09:22.070
so it's detecting the emotion of
annoyance, or detect confused or

00:09:22.070 --> 00:09:26.800
frustrated students in an online tutorial,
versus confident students.

00:09:26.800 --> 00:09:30.160
For longer moods,
we want to find traumatized or

00:09:30.160 --> 00:09:33.960
depressed people like
writers of some blog or text.

00:09:35.000 --> 00:09:38.740
In conversations we might want to detect
if someone's friendly or unfriendly.

00:09:39.850 --> 00:09:43.970
In very long term personality traits
we might want to detect extroverts or

00:09:43.970 --> 00:09:46.930
introverts for building dialogue
systems that can communicate

00:09:46.930 --> 00:09:48.420
better with extroverts or introverts.

00:09:48.420 --> 00:09:51.010
And there's lots of research
actually on all of these tasks.

00:09:52.700 --> 00:09:56.710
We'll just show you one of them which is
a task we've worked on here at Stanford

00:09:56.710 --> 00:09:58.180
which is detection of friendliness.

00:09:59.410 --> 00:10:06.070
So friendly speakers, it turns out,
use collaborative conversational style so

00:10:06.070 --> 00:10:10.210
we've built classifiers that look at
features like how often someone laughs or

00:10:10.210 --> 00:10:14.140
how often they use negative emotional
words or how frequently they use phrases

00:10:14.140 --> 00:10:20.090
like, that's too bad, that indicate
sympathy or agreement, I think so too.

00:10:20.090 --> 00:10:24.050
So the more friendly someone is, the more
likely they are to use sympathetic words,

00:10:24.050 --> 00:10:29.360
or agreement, or the less likely they are
to use a hedge like, kind of or sort of.

00:10:29.360 --> 00:10:33.500
And we've shown that these classifiers
help and we've looked at the task of

00:10:33.500 --> 00:10:37.280
detecting how friendly someone is, or
how flirtatious someone is in speed dates.

00:10:37.280 --> 00:10:42.270
And we found that these kind of features
help in detecting friendliness.

00:10:44.300 --> 00:10:50.960
So simple lexical features like words and
phrases, and where we have speech.

00:10:50.960 --> 00:10:56.360
Speech features like prosody or pitch, all
of these can help in detecting sentiment,

00:10:56.360 --> 00:11:01.599
simple polarity, and much more rich,
effective meaning of all sorts of kinds.

