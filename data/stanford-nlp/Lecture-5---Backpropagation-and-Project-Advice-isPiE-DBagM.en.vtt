WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.839
[MUSIC]

00:00:04.839 --> 00:00:08.390
&gt;&gt; Stanford University.

00:00:08.390 --> 00:00:11.990
&gt;&gt; About the full back
propagation algorithm.

00:00:11.990 --> 00:00:17.240
And I promise you as the last bit of
super heavy math- After that you can have

00:00:17.240 --> 00:00:22.730
a warm fuzzy feeling around most of the
state of the art deep learning techniques.

00:00:22.730 --> 00:00:24.860
Both for natural language processing and
even a little bit for

00:00:24.860 --> 00:00:26.400
computer vision a lot of other places.

00:00:26.400 --> 00:00:32.290
So, I know this can be a lot if you're not
super familiar with multivariate calculus.

00:00:32.290 --> 00:00:36.710
And so, I'll actually describe
backprop in four different ways today.

00:00:36.710 --> 00:00:41.320
And hopefully bring most
of you into the backprop

00:00:41.320 --> 00:00:46.870
the group of people who know
back propagation really well.

00:00:46.870 --> 00:00:49.930
So 4 different descriptions of
essentially the same thing.

00:00:49.930 --> 00:00:55.800
But hopefully, some will resonate
more with some folks than others.

00:00:55.800 --> 00:00:58.980
And so to show you that afterwards
we can have a lot more fun.

00:00:58.980 --> 00:01:02.260
Well, actually then the second sort of,
well, not quite half, but

00:01:02.260 --> 00:01:05.440
maybe the last third,
talk about the projects and

00:01:05.440 --> 00:01:07.630
encourage you to get
started on the project.

00:01:07.630 --> 00:01:10.780
Give you some advice on what the projects

00:01:10.780 --> 00:01:15.510
will likely entail if you choose to do
a project instead of the last problem set.

00:01:15.510 --> 00:01:18.810
Maybe one small hint for problem set one.

00:01:19.840 --> 00:01:23.570
Again, it's super important to understand
the math and dimensionality, and if you do

00:01:23.570 --> 00:01:27.610
that on paper, and then you still have
some trouble in the implementation.

00:01:27.610 --> 00:01:31.970
It can be very helpful to
essentially set break points and

00:01:31.970 --> 00:01:36.500
then print out the shape of all the
various derivatives you may be computing,

00:01:36.500 --> 00:01:40.050
and that can help you in your
debugging process a little bit.

00:01:41.640 --> 00:01:44.960
All right, are there any questions around
organization, problem sets, one, no?

00:01:46.230 --> 00:01:49.470
All right, my project,

00:01:49.470 --> 00:01:51.990
my office hours going to be a half
hour after the class ends today.

00:01:51.990 --> 00:01:57.510
So if you have project questions,
I'll be there after the class.

00:01:59.280 --> 00:02:02.650
All right, let's go to explanation
number 1 for back propagation.

00:02:02.650 --> 00:02:06.310
And again, just to motivate you of
why we want to go through this.

00:02:06.310 --> 00:02:10.540
Why do I torture some of you
with all these derivatives?

00:02:10.540 --> 00:02:14.190
It is really important to have an actual
understanding of the math behind

00:02:14.190 --> 00:02:15.840
most of deep learning.

00:02:15.840 --> 00:02:20.640
And in many cases, in the future, you will
kind of abstract the way backpropagation.

00:02:20.640 --> 00:02:23.950
You'll just kind of assume it
works based on a framework,

00:02:23.950 --> 00:02:25.890
software package that you might use.

00:02:25.890 --> 00:02:29.630
But that sometimes leads you to not
understand why your model might not

00:02:29.630 --> 00:02:30.710
be working, right?

00:02:30.710 --> 00:02:33.390
In theory,
you say it's just abstracted away,

00:02:33.390 --> 00:02:35.050
I don't have to worry about it anymore.

00:02:35.050 --> 00:02:41.270
But really in practice, in the
optimization you might run into problems.

00:02:41.270 --> 00:02:43.500
And if you don't understand
the actual back propagation,

00:02:43.500 --> 00:02:45.230
you don't know why you
will have these problems.

00:02:45.230 --> 00:02:50.090
And so in addition to that, we kinda wanna
prepare you to not just be a user of

00:02:50.090 --> 00:02:53.240
deep learning, but
maybe even eventually do research.

00:02:53.240 --> 00:02:57.820
In this field and maybe think of and
implement and be very,

00:02:57.820 --> 00:03:00.480
very good at debugging
completely new kinds of models.

00:03:00.480 --> 00:03:04.640
And you'll observe that depending on which
software package you'll use in the future,

00:03:04.640 --> 00:03:08.530
not everything is de facto supported
in some of these frameworks.

00:03:08.530 --> 00:03:12.775
So if you want to create a completely new
model that's sort of outside the convex

00:03:12.775 --> 00:03:15.770
Known things, you will need to implement,
the forward and

00:03:15.770 --> 00:03:20.650
the backward propagation for a new
sub-module that you might have invented.

00:03:20.650 --> 00:03:24.760
So, you just have to trust me
a little bit in why it's useful.

00:03:24.760 --> 00:03:26.390
Hopefully this helps.

00:03:27.390 --> 00:03:31.700
So last time we ended with
this kind of neural network,

00:03:31.700 --> 00:03:34.240
where we had a single hidden layer.

00:03:34.240 --> 00:03:38.150
And we derive all the gradients for

00:03:38.150 --> 00:03:41.780
all the different parameters of this
model, namely the word vectors here.

00:03:41.780 --> 00:03:47.330
The W, the weight matrix for
our single hidden layer and

00:03:47.330 --> 00:03:51.900
the U for the simple linear layer here.

00:03:51.900 --> 00:03:57.060
And we defined this objective function and
we ended up writing,

00:03:57.060 --> 00:04:01.320
for instance, one such derivative here
fully out where we have the indicator

00:04:01.320 --> 00:04:03.830
function whether we're in this regime or
if it's zero.

00:04:03.830 --> 00:04:07.055
And if it's above zero,
then this was the derivative.

00:04:07.055 --> 00:04:11.710
In here, I just rewrote the same
derivative twice, essentially showing you

00:04:11.710 --> 00:04:16.550
that instead of having to recompute,
this if you had basically

00:04:18.060 --> 00:04:21.650
stored during forward propagation
the activations of this,

00:04:21.650 --> 00:04:27.440
this is exactly the same thing,
so f(Wx + b) we defined

00:04:27.440 --> 00:04:31.550
as our hidden activation, a, then you
could reuse those to compute derivatives.

00:04:34.330 --> 00:04:39.130
Alright, now we're going to take it up
a notch and add an additional hidden

00:04:39.130 --> 00:04:44.080
layer to that exact same model, and it's
the same kind of layer but in out that

00:04:44.080 --> 00:04:48.800
we have 2, we have to be very careful here
about our superscript which will indicate.

00:04:48.800 --> 00:04:50.150
The layers that we're in.

00:04:51.220 --> 00:04:52.980
So it's the same kind
of window definition,

00:04:52.980 --> 00:04:57.970
we'll go over corpus, we'll select
samples for our positive class and

00:04:57.970 --> 00:05:02.200
everything that doesn't for instance have
an entity will be our negative class.

00:05:02.200 --> 00:05:06.430
Everything else is the same, but
we're adding one hidden layer to this.

00:05:07.920 --> 00:05:09.830
And so, let's go through the definition.

00:05:09.830 --> 00:05:15.710
We'll define x here, our Windows and
our word vectors that we concatenated,

00:05:15.710 --> 00:05:19.140
as our first activations,
our first hidden layer.

00:05:19.140 --> 00:05:23.850
And now to compute to intermediate
representation for our second layer,

00:05:23.850 --> 00:05:28.660
just a linear part of that,
we basically have here W1.

00:05:28.660 --> 00:05:30.820
A superscript matrix times x plus b1.

00:05:30.820 --> 00:05:35.790
And then to compute the activations A,
superscript two of that will apply

00:05:35.790 --> 00:05:39.680
the element-wise nonlinearities,
such as the sigmoid function.

00:05:39.680 --> 00:05:45.420
All right, and
then we'll define this here as z3,

00:05:45.420 --> 00:05:49.780
same idea but this could potentially have
different dimensionalities, w1, w2 for

00:05:49.780 --> 00:05:52.280
instance don't have to have
exactly the same dimensionality.

00:05:53.280 --> 00:05:55.400
Do the same thing again,
element wise nonlinearity and

00:05:55.400 --> 00:05:57.380
we have the same linear layer at the top.

00:05:57.380 --> 00:06:02.582
All right, are there any questions
around the definition of this here?

00:06:02.582 --> 00:06:05.420
&gt;&gt; [INAUDIBLE]
&gt;&gt; The question is do those two element

00:06:05.420 --> 00:06:06.870
wise functions here have to be the same?

00:06:06.870 --> 00:06:08.350
And the answer is they do not.

00:06:08.350 --> 00:06:12.710
And if fact this is something
that you could cross-validate and

00:06:12.710 --> 00:06:15.260
try as different
hyper-parameters of the model.

00:06:16.510 --> 00:06:18.990
We so far have only introduced
to you the sigmoid.

00:06:18.990 --> 00:06:21.600
So let's assume for now, it's the same.

00:06:21.600 --> 00:06:25.616
But in a later lecture, I think next week,
we'll describe a lot of other

00:06:25.616 --> 00:06:30.640
kinds of non-linearities.

00:06:30.640 --> 00:06:31.741
Yeah.

00:06:35.348 --> 00:06:37.410
How do you choose which
of these functions.

00:06:37.410 --> 00:06:40.100
We'll go into all of that once
we know that we have which or

00:06:40.100 --> 00:06:41.120
what the options are.

00:06:41.120 --> 00:06:44.140
The best answer usually is,
you let your data speak for

00:06:44.140 --> 00:06:48.800
yourself and you run experiments
with a lot of different options.

00:06:48.800 --> 00:06:51.380
Once you do that, after a while you gain,
again, certain intuitions.

00:06:51.380 --> 00:06:53.640
And you don't have to redo it every time.

00:06:53.640 --> 00:06:56.810
Especially if you have ten layers, you
don't wanna go through the cross-product

00:06:56.810 --> 00:06:58.990
of five different nonlinearities.

00:06:58.990 --> 00:07:00.940
And then all the different variations.

00:07:00.940 --> 00:07:04.020
Usually, you get diminishing returns for
some of those type of parameters.

00:07:04.020 --> 00:07:04.520
&gt;&gt; Question.

00:07:05.520 --> 00:07:10.743
&gt;&gt; Yeah?
[INAUDIBLE]

00:07:16.418 --> 00:07:17.140
&gt;&gt; Sorry, I didn't hear you.

00:07:19.915 --> 00:07:22.268
Right.

00:07:22.268 --> 00:07:28.387
So the question

00:07:28.387 --> 00:07:34.820
is,could we put the b into the w?

00:07:34.820 --> 00:07:35.720
And if that's confusing,

00:07:35.720 --> 00:07:41.380
you could essentially assume
that b is this biased term here.

00:07:41.380 --> 00:07:44.230
Is another element of this W matrix,

00:07:44.230 --> 00:07:49.138
if we add a single one to every
activation that we have here.

00:07:49.138 --> 00:07:54.010
So, if a two frame,

00:07:54.010 --> 00:07:56.920
since we just added one here, then we
could get rid of this bias term and

00:07:56.920 --> 00:08:01.869
we'd have an additional Row or
column depending on what we have in W.

00:08:01.869 --> 00:08:05.254
So yes, we could fold b into W
to simplify the notation, but

00:08:05.254 --> 00:08:09.955
then as we're taking derivatives we want
to keep everything separate and clear.

00:08:09.955 --> 00:08:12.830
And you'll usually back propagate
through these activations,

00:08:12.830 --> 00:08:14.875
whereas you don't back
propagate through b.

00:08:14.875 --> 00:08:17.236
So for this math,
it's better to keep them separate.

00:08:17.236 --> 00:08:18.735
Yeah.

00:08:22.068 --> 00:08:26.245
So U transpose is our last,
the question is what's U transposed?

00:08:26.245 --> 00:08:29.811
And U transpose is our last layer,
if you will.

00:08:29.811 --> 00:08:34.738
But there's no non-linearity with it,
and it's just a single vector.

00:08:34.738 --> 00:08:39.260
And so because by default here in
the notation of the class we assume these

00:08:39.260 --> 00:08:40.604
are column vectors.

00:08:40.604 --> 00:08:43.320
We transpose it so
that we have a simple inner product.

00:08:43.320 --> 00:08:46.770
So it's just another set of
parameters that will score

00:08:46.770 --> 00:08:50.110
the final activations to be
high if they're a named entity,

00:08:50.110 --> 00:08:53.790
if there's a named entity of
the center of this window.

00:08:53.790 --> 00:08:55.250
And scored low if not.

00:08:58.565 --> 00:08:59.240
That's correct.

00:08:59.240 --> 00:09:01.430
It is just a score that
we're trying to maximize and

00:09:01.430 --> 00:09:04.640
we compute that final score
with this inner product.

00:09:04.640 --> 00:09:07.598
And so these activations
are now something that we

00:09:07.598 --> 00:09:11.480
compute in this pretty complex
neural network function, yeah.

00:09:20.021 --> 00:09:22.013
Here everything is a column vector,
that's correct.

00:09:22.013 --> 00:09:24.375
All the x's are column vectors.

00:09:34.576 --> 00:09:35.825
So the question is,

00:09:35.825 --> 00:09:40.720
is there a particular reason of why we
chose a linear layer as the last layer?

00:09:40.720 --> 00:09:43.415
And the answer here is to
simplify the math a little bit.

00:09:43.415 --> 00:09:48.266
And because and to introduce to you
another kind of objective function,

00:09:48.266 --> 00:09:54.130
not everything has to be normalized and
basically summed to 1 as probabilities.

00:09:54.130 --> 00:09:58.306
If you just care about finding one
thing versus a lot of other things,

00:09:58.306 --> 00:10:02.927
like I just want to find named entities
that are locations as center words.

00:10:02.927 --> 00:10:05.018
And if it's high, then that's likely one.

00:10:05.018 --> 00:10:08.860
And if it's low,
then it's likely not a center location.

00:10:08.860 --> 00:10:10.040
Then that's all you need to do.

00:10:10.040 --> 00:10:15.084
And in some sense, you could add here
a sigmoid after this, and then call it

00:10:15.084 --> 00:10:20.311
a probability, and then use standard
cross entropy loss to, in your model.

00:10:20.311 --> 00:10:23.331
There's no reason of why
you shouldn't do it.

00:10:23.331 --> 00:10:28.613
That's something you have to do in the
problem sets, trying to combine, we derive

00:10:28.613 --> 00:10:33.302
that, we help you derive the softmax and
cross entropy pair optimization.

00:10:33.302 --> 00:10:37.706
And then we going through this and
hopefully you can combine the two and

00:10:37.706 --> 00:10:39.700
you'll see how both work.

00:10:39.700 --> 00:10:42.493
But it's essentially
a modeling decision and

00:10:42.493 --> 00:10:44.933
it's not wrong to apply a sigmoid here.

00:10:44.933 --> 00:10:47.280
And then call this a probability,
instead of a score.

00:10:49.385 --> 00:10:53.914
All right, so now,
we have this two layer neural network, and

00:10:53.914 --> 00:10:59.820
we essentially did most of the work
already to derive the final things here.

00:10:59.820 --> 00:11:04.370
We already knew how to
derive our U gradients.

00:11:04.370 --> 00:11:08.510
And what used to be just
W is not W superscript 2,

00:11:08.510 --> 00:11:11.670
but just because we add the superscript
all the math is the same.

00:11:12.680 --> 00:11:15.960
So here, same derivation that we did for

00:11:15.960 --> 00:11:20.280
W, it's just now sitting
on a2 instead of on just a.

00:11:20.280 --> 00:11:25.730
And so what we did here, basically,
follows directly to what we now have.

00:11:25.730 --> 00:11:30.470
It's the same thing, but we now have
to be careful to add these superscripts

00:11:30.470 --> 00:11:32.700
depending on where we
are in the neural network.

00:11:34.640 --> 00:11:39.310
And we'll have the same definition
here when we multiply Ui and

00:11:39.310 --> 00:11:43.960
f prime of zi superscript 3, we'll
just call that delta superscript 3 and

00:11:43.960 --> 00:11:45.450
subscript i, for the ith element.

00:11:45.450 --> 00:11:50.140
And this is going to give us the partial
derivative with respect to Wij,

00:11:50.140 --> 00:11:52.100
the i jth element of the W matrix.

00:11:53.460 --> 00:11:55.970
So this one we've already
derived in all its glory.

00:11:55.970 --> 00:12:01.290
I'm just putting here again
with the right superscripts.

00:12:03.050 --> 00:12:07.030
Now, the total function
that we have is this one.

00:12:07.030 --> 00:12:12.000
And, again, we have here this same
derivative, I just copied it over.

00:12:12.000 --> 00:12:16.110
And in matrix notation, we have to
find this as the outer product here.

00:12:16.110 --> 00:12:20.130
That would give us the cross product,
all the pairs of i and

00:12:20.130 --> 00:12:23.720
j to have the full
gradient of the W2 matrix.

00:12:25.010 --> 00:12:28.990
So this one was exactly as before, except
that we now add the superscript a2 here.

00:12:30.090 --> 00:12:32.048
Now, in terms of the notation,

00:12:32.048 --> 00:12:35.393
we defined this delta i in
terms of all these elements.

00:12:35.393 --> 00:12:39.010
And these are basically,
if you think about it, two vectors.

00:12:39.010 --> 00:12:42.010
This Ui we could write as the full U.

00:12:42.010 --> 00:12:44.090
All the elements of the u vector.

00:12:44.090 --> 00:12:48.140
And f prime of zi we could
write as f prime of z3 where

00:12:48.140 --> 00:12:52.000
we basically drop the index and assume
this is just one vector of a bunch of

00:12:52.000 --> 00:12:57.670
element wise applications of this gradient
function of this derivative here.

00:12:57.670 --> 00:13:01.400
So we'll introduce now this notation
which will come in very handy.

00:13:01.400 --> 00:13:05.135
And we call the Hadamard product or
element-wise product.

00:13:05.135 --> 00:13:07.465
Sometimes you'll see it as little circles.

00:13:07.465 --> 00:13:10.970
Sometimes it's a circle with a cross or
with a dot inside.

00:13:10.970 --> 00:13:12.933
Whenever you see these
in backprop derivatives,

00:13:12.933 --> 00:13:14.390
it's usually means the same thing.

00:13:14.390 --> 00:13:19.329
Which we just element-wise multiply all
the elements of the two vectors with one

00:13:19.329 --> 00:13:19.990
another.

00:13:21.440 --> 00:13:25.120
So this is how we'll define
from now on this delta,

00:13:25.120 --> 00:13:28.850
the air signal that's
coming in at this layer.

00:13:28.850 --> 00:13:31.970
So the last missing piece for
back propagation and

00:13:31.970 --> 00:13:37.980
to understand it is essentially
the gradient with respect to W1,

00:13:37.980 --> 00:13:41.570
the second layer now,
that we're moving through.

00:13:44.092 --> 00:13:48.500
Any questions around the Hadamard product,
the outer product from the W?

00:13:52.062 --> 00:13:52.616
Yeah?

00:13:59.539 --> 00:14:00.416
It is no longer what?

00:14:05.310 --> 00:14:06.210
Sorry it's.

00:14:08.285 --> 00:14:10.236
Associated, so yes.

00:14:10.236 --> 00:14:14.524
So the question is once you use the
Hadamard product, how is this related to

00:14:14.524 --> 00:14:18.260
the matrix multiplication here or
the vector, outer product?

00:14:18.260 --> 00:14:21.770
And so
you basically first have to compute this.

00:14:21.770 --> 00:14:24.350
And then you have the full
delta definition.

00:14:24.350 --> 00:14:30.436
And then you can multiply these and
outer product to get the gradient.

00:14:30.436 --> 00:14:31.186
Yeah.

00:14:38.230 --> 00:14:42.813
Sure, so the question is, could you
assume that these are diagonal matrices?

00:14:42.813 --> 00:14:45.680
And yes, it's this kind of the same thing.

00:14:45.680 --> 00:14:50.170
But in terms of the multiplication
you have to then make sure your

00:14:50.170 --> 00:14:54.940
diagonal matrix is efficiently implemented
when it's multiplied with another vector.

00:14:54.940 --> 00:14:58.572
And as you write this out,
if this is confusing,

00:14:58.572 --> 00:15:02.389
write out what it means to
have a matrix times this.

00:15:02.389 --> 00:15:04.830
And what if this is
just a diagonal matrix?

00:15:04.830 --> 00:15:09.349
And what do you get versus just
multiplying each of these elements with

00:15:09.349 --> 00:15:10.275
one another?

00:15:10.275 --> 00:15:13.092
So just write out the definitions
of the matrix product and

00:15:13.092 --> 00:15:15.864
then you'll observe that you
could think of it this way.

00:15:15.864 --> 00:15:19.539
But then really this f prime here,

00:15:19.539 --> 00:15:24.863
is just as a single vector
why apply the derivative

00:15:24.863 --> 00:15:31.232
function to a bunch of zeros,
in this case, zero, two so.

00:15:31.232 --> 00:15:37.140
All right, so the last missing piece,
W1, the gradient for it.

00:15:37.140 --> 00:15:40.915
And so the main thing we have to figure
out now is what's the bottom layer's

00:15:40.915 --> 00:15:44.385
error message delta 2 that's coming in?

00:15:44.385 --> 00:15:47.205
And I'm not going to go
through all the indices again.

00:15:47.205 --> 00:15:49.235
It would take a while and
it's kind of repetitive.

00:15:49.235 --> 00:15:52.615
And it's very, very similar to what
we've done in the last lecture.

00:15:52.615 --> 00:15:56.750
But essentially,
we had already arrived at this expression.

00:15:56.750 --> 00:15:59.790
As the next lower update.

00:15:59.790 --> 00:16:03.250
And in our previous model,
we would just arrive,

00:16:03.250 --> 00:16:07.310
that would be the signal that
arrives at the word vectors.

00:16:07.310 --> 00:16:12.860
So our final word vector
update was defined this way.

00:16:12.860 --> 00:16:15.753
And what we now basically
have to do is once more,

00:16:15.753 --> 00:16:20.670
just apply the chain rule because instead
of having coming up at the word vectors.

00:16:20.670 --> 00:16:23.170
Instead, we're actually
coming up at another layer.

00:16:25.710 --> 00:16:29.770
So basically, you can kind of call
it a local gradient also, but

00:16:29.770 --> 00:16:35.880
it's when you multiply whatever error
signal comes from the top layer,

00:16:35.880 --> 00:16:40.440
you multiply that with your local error
signal, in this case, f prime here.

00:16:40.440 --> 00:16:47.000
Then together, you'll get the update for
either the weights that are at that layer,

00:16:47.000 --> 00:16:51.860
or the intermediate term for
the gradient for lower layers.

00:16:51.860 --> 00:16:53.720
So that's what we mean by our signal.

00:16:53.720 --> 00:16:57.081
And it might help in the next definition,

00:16:57.081 --> 00:17:02.760
it might give you a better explanation
of this in backprop number two.

00:17:02.760 --> 00:17:04.571
All right, so almost there.

00:17:04.571 --> 00:17:06.870
Basically, we apply the chain rule again.

00:17:06.870 --> 00:17:10.750
And if the chain rule for such a complex
function is maybe less intuitive,

00:17:10.750 --> 00:17:13.520
so one thing that helped
me many years ago,

00:17:13.520 --> 00:17:18.810
is to essentially assume all of these
are scalars, just single variable.

00:17:18.810 --> 00:17:23.110
And then derive all this,
assuming it just, U, W,

00:17:23.110 --> 00:17:25.940
and x are all just single numbers.

00:17:25.940 --> 00:17:30.200
And then derive it,
that will help you gain some intuition.

00:17:30.200 --> 00:17:35.650
And then you'll observe in the end that
the final delta 2 is essentially similar

00:17:35.650 --> 00:17:41.520
to what we had derived in a very detailed
way, which is W2 transposed times delta 3,

00:17:41.520 --> 00:17:47.500
and then Hadamard product times f
prime of Z2 which is that layer here.

00:17:49.170 --> 00:17:53.770
And this is basically it,
if you understand these two equations, and

00:17:53.770 --> 00:17:58.360
you feel you can derive them now,
then you will know all the updates for

00:17:58.360 --> 00:18:00.680
all standard multilayer neural networks.

00:18:00.680 --> 00:18:05.010
You will, in the end,
always arrive at these two equations.

00:18:05.010 --> 00:18:09.849
And that is, if you wanna
compute the error signal that's

00:18:09.849 --> 00:18:14.686
coming into a new layer,
then you'll have some form of W,

00:18:14.686 --> 00:18:21.143
of the high layer transposed times
the error signal that's coming in there.

00:18:21.143 --> 00:18:28.810
Hadamard product with element y's
derivatives here of f in the f prime.

00:18:28.810 --> 00:18:33.640
And in the final update for each W,
will always be this outer product

00:18:33.640 --> 00:18:37.450
of delta error signal times
the activation at that layer.

00:18:39.390 --> 00:18:43.410
And here, I include also our
standard regularization term.

00:18:46.590 --> 00:18:51.880
And you can even describe the top and
bottom layers this way.

00:18:51.880 --> 00:18:53.980
And then lead to word vectors and
the linear layer, but

00:18:53.980 --> 00:18:56.090
they just have a very simple delta.

00:18:57.280 --> 00:19:01.452
All right, now, for some of you, just like
all right, now I understand everything,

00:19:01.452 --> 00:19:04.426
and it's great that I fully
understand back propagation.

00:19:04.426 --> 00:19:07.890
But judging from Piazza and

00:19:07.890 --> 00:19:12.890
just from previous years, it's also
quite a lot to wrap your head around.

00:19:12.890 --> 00:19:17.344
And so I will go through three
additional explanations now,

00:19:17.344 --> 00:19:19.623
of this exact same algorithm.

00:19:19.623 --> 00:19:24.045
And there, we're going through much
simpler functions not full neural

00:19:24.045 --> 00:19:27.100
networks, but
much simpler kinds of functions.

00:19:27.100 --> 00:19:31.915
But maybe for some, it will help to wrap
their heads around sort of the general

00:19:31.915 --> 00:19:36.380
idea of these error signals through
these simpler kinds of functions.

00:19:36.380 --> 00:19:40.220
So instead of having a crazy neural
network with lots of matrices and

00:19:40.220 --> 00:19:42.108
hidden layers.

00:19:42.108 --> 00:19:45.367
We'll just kinda look at
a simple function like this, and

00:19:45.367 --> 00:19:47.860
we'll arrive at a similar kind of idea.

00:19:47.860 --> 00:19:52.555
Namely, recursively applying and
computing these error signals or

00:19:52.555 --> 00:19:56.690
local gradients as we
move through a network.

00:19:56.690 --> 00:20:01.540
Now, the networks in this idea seen
function as circuits are going to be much,

00:20:01.540 --> 00:20:02.905
much simpler.

00:20:02.905 --> 00:20:08.410
And these are examples from another
lecture on Git learning for

00:20:08.410 --> 00:20:09.920
convolutional neural networks and

00:20:09.920 --> 00:20:15.350
computer vision, and we're basically
copying here some of their slides.

00:20:15.350 --> 00:20:22.499
And so let's take, for example, this very
simple function, f of three variables.

00:20:22.499 --> 00:20:26.230
And this simple function
is just x plus y times z.

00:20:27.720 --> 00:20:32.410
And let's assume we start with
some random initial values for

00:20:32.410 --> 00:20:35.960
x, y, and z from which we start and
wanna compute derivatives.

00:20:37.040 --> 00:20:41.840
Now, just as before with a complex neural
network, we can define intermediate

00:20:41.840 --> 00:20:45.800
terms but now, the intermediate
terms are very, very simple.

00:20:45.800 --> 00:20:49.713
So we'll just take q, for instance,
and we define q as x plus y,

00:20:49.713 --> 00:20:51.320
this local computation.

00:20:51.320 --> 00:20:56.044
And now,
we can look at the partial derivatives

00:20:56.044 --> 00:21:00.780
here of q with respect to x and
with respect to y.

00:21:01.840 --> 00:21:05.080
They're very simple,
it's just addition, right, just one.

00:21:05.080 --> 00:21:07.950
And we can also define f now,
in terms of q times z,

00:21:07.950 --> 00:21:12.840
where we use our intermediately
defined function here.

00:21:12.840 --> 00:21:17.080
And here, we're kind of simplifying q, it
should be q is a function of x and y, but

00:21:17.080 --> 00:21:18.530
we just drop that.

00:21:20.110 --> 00:21:25.948
And we can also define our partials of f,
our overall function with respect to q.

00:21:25.948 --> 00:21:29.030
Now, again, to connect that
to what we looked at before.

00:21:29.030 --> 00:21:33.665
F could be our lost function, x, y, z
could be parameters of this and we wanna,

00:21:33.665 --> 00:21:36.310
for instance, minimize our lost function.

00:21:37.550 --> 00:21:43.490
So now, what we want is, we want the final
updates to update these variables.

00:21:44.750 --> 00:21:47.418
So we'll start with at the very top.

00:21:47.418 --> 00:21:54.490
Just a df by df which is just 1,
so it's not much there.

00:21:54.490 --> 00:21:57.540
We usually start with that.

00:21:57.540 --> 00:22:02.260
And now, we want to update and
learn how do we update our z vectors?

00:22:02.260 --> 00:22:08.240
So we look at dfdz, and what is that?

00:22:08.240 --> 00:22:16.996
Well, we wrote down here all our different
derivatives, so df by dz is just q.

00:22:16.996 --> 00:22:21.628
And we define q as x + y, and x and

00:22:21.628 --> 00:22:24.671
y is minus 2 and 5.

00:22:24.671 --> 00:22:28.323
And so the gradient or
the partial derivative here is just 3.

00:22:30.834 --> 00:22:36.385
All right, so far we're just very
simple q times derivative z, that's it.

00:22:36.385 --> 00:22:41.578
All right, now,
we can move also through this circuit.

00:22:41.578 --> 00:22:44.805
And are there questions around just
the description of this circuit,

00:22:44.805 --> 00:22:46.728
of this function in terms of the circuit?

00:22:51.607 --> 00:22:59.323
All right, so now, let's look at the dfdq
which is the element here of the circuit,

00:22:59.323 --> 00:23:04.525
this node in the circuit
description of this function.

00:23:04.525 --> 00:23:10.820
Now, the dfdq is again, quite simple
we already wrote it right here.

00:23:10.820 --> 00:23:14.657
It's just z, and z is just minus 4.

00:23:14.657 --> 00:23:18.253
But now, the chain rule,
we have to multiply and

00:23:18.253 --> 00:23:22.260
this is essentially a delta
kind of error message.

00:23:22.260 --> 00:23:26.390
We multiply what we have from
the higher node in the circuit, but

00:23:26.390 --> 00:23:28.619
that's in this case, is just 1.

00:23:28.619 --> 00:23:33.878
And so the overall is just z times 1,
and z is minus 4, z is minus 4.

00:23:33.878 --> 00:23:38.586
And now,
we're going to move through this plus

00:23:38.586 --> 00:23:43.540
node to compute the next
lower derivatives here.

00:23:43.540 --> 00:23:49.591
And this is, we end up at the final
nodes here, the final leaf nodes if you

00:23:49.591 --> 00:23:54.668
will of this tree structure,
and we wanna compute the dfdy.

00:23:54.668 --> 00:24:00.428
Now dfdy,
We basically wanna use the chain rule,

00:24:00.428 --> 00:24:05.617
and we're going to multiply what
we have in the previous one, dfdq,

00:24:05.617 --> 00:24:11.699
which is the error signal coming from here
times dqdy, which is the local error,

00:24:11.699 --> 00:24:16.529
the local gradient,
it's not really the full gradient right,

00:24:16.529 --> 00:24:19.873
this is the local part
of the gradient dqdy.

00:24:19.873 --> 00:24:24.345
So we multiply these two terms,
the dfdq we wrote down here,

00:24:24.345 --> 00:24:28.740
that says z minus 4, times dqdy,
as you wrote down here.

00:24:28.740 --> 00:24:32.360
It's just one, so
minus 4 times 1, we got minus 4.

00:24:34.610 --> 00:24:37.627
And we can do the same thing for
x, again, apply the chain rule.

00:24:37.627 --> 00:24:43.648
All right, so in general, in this way of
seeing all these functions as circuits,

00:24:43.648 --> 00:24:46.964
we basically always have
some kind of input so

00:24:46.964 --> 00:24:52.290
each node in the circuit and
we compute some kind of output.

00:24:52.290 --> 00:24:56.060
And what's important is we can
compute our local gradients here

00:24:56.060 --> 00:24:59.050
directly during the forward propagation.

00:24:59.050 --> 00:25:03.030
We don't need to know this
local part of the gradient.

00:25:03.030 --> 00:25:04.870
We don't need to know what's up before.

00:25:04.870 --> 00:25:07.650
But in general, we will run this forward.

00:25:07.650 --> 00:25:10.028
We'll around some of these values.

00:25:10.028 --> 00:25:14.920
And then in back propagation,
we get the gradient signals from any

00:25:14.920 --> 00:25:20.430
element upstream from each of
these nodes in the circuits.

00:25:20.430 --> 00:25:23.873
And essentially then,
use the chain rule and

00:25:23.873 --> 00:25:27.413
multiply all of these
to compute the updates.

00:25:27.413 --> 00:25:32.002
All right, any questions around
the definition of the circuits for

00:25:32.002 --> 00:25:33.380
simple functions?

00:25:33.380 --> 00:25:35.710
It's very hard to take this
kind of abstraction, and

00:25:35.710 --> 00:25:37.705
then get all the way to this full update.

00:25:37.705 --> 00:25:41.995
Therefore, a full near
layer neural network, but

00:25:41.995 --> 00:25:48.594
it's very good to gain intuition of
what's really going on, on a high level.

00:25:56.347 --> 00:25:57.718
&gt;&gt; All right, so now,

00:25:57.718 --> 00:26:02.664
let's go through a little more complex
example of what this looks like.

00:26:02.664 --> 00:26:07.446
And I think of at the end of that you
kind of gain some good intuition of how

00:26:07.446 --> 00:26:10.319
we basically do forward propagation, and

00:26:10.319 --> 00:26:15.850
recursively call these kinds of
circuits to compute the full update.

00:26:15.850 --> 00:26:20.400
So here,
we have a little bit more of a complex

00:26:20.400 --> 00:26:24.680
function namely actually our sigmoid
function that we had before.

00:26:24.680 --> 00:26:26.260
Usually when we have our sigmoid function,

00:26:26.260 --> 00:26:29.100
this was one activation
of one hidden layer.

00:26:30.190 --> 00:26:34.635
In most cases, x was our input and
w were the weights.

00:26:34.635 --> 00:26:39.239
So we defined this already, and now,
let's assume we just want to compute

00:26:39.239 --> 00:26:43.421
the partial derivatives with respect
to all the elements, w and x.

00:26:43.421 --> 00:26:47.661
And let's assume x and w are just,
x is two-dimensional, and

00:26:47.661 --> 00:26:49.530
w is three-dimensional.

00:26:49.530 --> 00:26:53.200
And we have here the bias term
as just an extra element of w.

00:26:54.440 --> 00:26:59.050
So now, if you take this whole function,
we're gonna now compute or

00:26:59.050 --> 00:27:00.590
define this as a circuit.

00:27:00.590 --> 00:27:05.802
That one description that's the most
detailed description of this function

00:27:05.802 --> 00:27:10.372
as a circuit would look like this,
where you basically recursively

00:27:10.372 --> 00:27:15.276
divide this function into all
the separate actions that you might take.

00:27:15.276 --> 00:27:17.641
And you can compute gradients and

00:27:17.641 --> 00:27:21.704
the local gradients at each
note in this kind of circuit.

00:27:21.704 --> 00:27:26.893
So the last operation to compute
the final output f here of this function,

00:27:26.893 --> 00:27:29.080
is 1 over whatever is in here.

00:27:29.080 --> 00:27:34.248
And so that's our last element of
the circuit, and from the bottom it

00:27:34.248 --> 00:27:40.132
starts with multiplying these two numbers,
multiplying these two numbers,

00:27:40.132 --> 00:27:45.057
and then adding to their summation
this w2 install, all right?

00:27:45.057 --> 00:27:48.387
Are there any questions around
the description of the circuit?

00:27:54.293 --> 00:27:59.477
All right, so now, let's assume we
start with these simple numbers here,

00:27:59.477 --> 00:28:05.650
so w2, w0 starts at 2, x0 starts at minus
1, minus 3, minus 2, and minus 3 here.

00:28:06.840 --> 00:28:10.561
So we just move forward through
the circuit to compute our forward

00:28:10.561 --> 00:28:11.920
propagation, right?

00:28:11.920 --> 00:28:18.500
So this is a relatively simple
concatenation of functions.

00:28:18.500 --> 00:28:21.970
And now, we wanna compute all our partial

00:28:21.970 --> 00:28:26.260
derivatives with respect to all
these different elements here.

00:28:26.260 --> 00:28:31.290
So we'll now go backwards and recursively
backwards through this circuit, and

00:28:31.290 --> 00:28:34.080
apply the chain rule every time.

00:28:34.080 --> 00:28:39.290
So let's start the final value to the
forward propagation numbers here in green,

00:28:39.290 --> 00:28:43.490
at the top the final
value of this is 0.73.

00:28:43.490 --> 00:28:51.080
And again, the first delta derivative of
just the function with itself, is just 1.

00:28:51.080 --> 00:28:56.330
And now, we hit this node in a circuit,
and we want to now compute

00:28:56.330 --> 00:28:59.110
the derivative of this function,
and the function's 1 over x.

00:28:59.110 --> 00:29:02.688
And so the derivative is
just minus 1 over x squared.

00:29:02.688 --> 00:29:07.968
x is 1.73, and so we basically compute

00:29:07.968 --> 00:29:15.311
minus 1 divided by 1.37,
sorry, 1.37 squared.

00:29:15.311 --> 00:29:19.753
And then we multiply using a chain rule,

00:29:19.753 --> 00:29:26.688
the gradient signal here from
the top that goes into this node.

00:29:26.688 --> 00:29:31.861
So now, you multiple these two,
and you get the number minus 0.53.

00:29:35.686 --> 00:29:42.454
Now, we're moved to the next node,
so this node here,

00:29:42.454 --> 00:29:47.494
we just sum up a constant
with the value x,

00:29:47.494 --> 00:29:52.390
and so the derivative of that is just 1.

00:29:52.390 --> 00:29:57.130
So we multiply, use the chain rule,
multiply these two elements,

00:29:57.130 --> 00:30:01.840
the error signal or
gradient signal from the top as it moves

00:30:01.840 --> 00:30:07.440
through this element of the circuit,
which is just minus 0.3 times 1 so

00:30:07.440 --> 00:30:12.342
we get again minus 0.53, sorry.

00:30:12.342 --> 00:30:14.260
Now, we move through the exponent.

00:30:14.260 --> 00:30:15.690
It's a little more interesting.

00:30:15.690 --> 00:30:19.580
So here, derivative of e to
the x is just e to the x.

00:30:19.580 --> 00:30:25.000
And we have the incoming value
which is minus 1, so that's our x.

00:30:26.140 --> 00:30:30.487
So we have e to the minus
1 times minus 0.53,

00:30:30.487 --> 00:30:35.693
the gradient signal from
the higher node in this circuit.

00:30:39.000 --> 00:30:43.212
And we basically continue like this for
a while, and

00:30:43.212 --> 00:30:47.917
compute the same for plus,
similar to this plus and so on.

00:30:47.917 --> 00:30:50.785
And that the end, we arrive right here.

00:30:50.785 --> 00:30:55.350
And our error signal is 0.2, and
we have this multiplication here.

00:30:55.350 --> 00:31:00.394
And we know in multiplication, the partial

00:31:00.394 --> 00:31:06.811
of w0 times x0 partial with
respect to x0 is just w0.

00:31:06.811 --> 00:31:14.548
And so we multiply 0.2 times the value
here which is 2 and we get 0.4.

00:31:14.548 --> 00:31:19.718
And now, we have an update for
this parameter after we've moved

00:31:19.718 --> 00:31:24.996
recursively through the circuit
all the way to where it was used.

00:31:27.600 --> 00:31:31.537
And this is essentially the same thing
that we've done for the very complex

00:31:31.537 --> 00:31:35.425
neural network, but sort of one step
at a time for a very simple function.

00:31:41.033 --> 00:31:45.887
Any questions around this
sort of circuit description

00:31:45.887 --> 00:31:49.268
of the same back propagation at year.

00:31:49.268 --> 00:31:54.400
Namely reusing the derivatives,
multiplying local error signals

00:31:54.400 --> 00:31:59.749
with the global error signals from
higher Layers, where here, the layer

00:31:59.749 --> 00:32:03.209
definition's a bit stretched, it's very,
very simple kinds of operations.

00:32:06.620 --> 00:32:07.218
Yeah?

00:32:12.645 --> 00:32:16.934
That's right, so here,
each time the sort of gradient,

00:32:16.934 --> 00:32:22.550
the local gradient times the global or
above higher layer gradient signal.

00:32:22.550 --> 00:32:24.940
When you multiply them,
you get an actual gradient.

00:32:24.940 --> 00:32:26.975
So they're not really gradients, right,

00:32:26.975 --> 00:32:29.453
they're sort of intermediate
values of a gradient.

00:32:32.874 --> 00:32:33.643
Yep.

00:32:53.706 --> 00:32:57.824
So the question is we're
using this kind of circuit

00:32:57.824 --> 00:33:03.105
interpretation to compute derivatives and
that's correct.

00:33:04.125 --> 00:33:08.310
If you were to just do
standard math on this equation

00:33:08.310 --> 00:33:11.070
you would end up with something
that looks exactly like this.

00:33:11.070 --> 00:33:14.650
And you would also have
similar kinds of numbers.

00:33:14.650 --> 00:33:18.630
But we're making it a little
more complicated, in some ways,

00:33:18.630 --> 00:33:23.480
to compute the derivatives here,
of each of the elements of this function.

00:33:23.480 --> 00:33:27.540
We're kind of push the chain rule to its

00:33:27.540 --> 00:33:32.650
maximum by defining every single
operation as a separate function.

00:33:32.650 --> 00:33:35.650
And then computing gradients at
every single separate function.

00:33:35.650 --> 00:33:40.950
And when you do that, even for this kind
of simple function, you usually wouldn't

00:33:40.950 --> 00:33:45.080
write out this complex thing and take
a derivative with respect to this node,

00:33:45.080 --> 00:33:47.378
which is just plus,
cuz we all know how to do that.

00:33:47.378 --> 00:33:49.650
And usually we just move
through this very quickly but

00:33:49.650 --> 00:33:54.810
the circuit definitions can help you
understand the idea that at each node what

00:33:54.810 --> 00:33:59.870
you end up getting is the local gradient
times the gradient signal from the top.

00:34:01.710 --> 00:34:06.310
So in the end you get the exact same
updates as if you had just taken

00:34:06.310 --> 00:34:08.300
the derivatives using
the chain rule like this.

00:34:09.620 --> 00:34:14.810
And in fact, the definition of the circuit
can be arbitrary too and sometimes

00:34:14.810 --> 00:34:21.090
it's a lot more work to write out all the
different sub components of a function.

00:34:21.090 --> 00:34:23.850
So for instance,
we know if we just described

00:34:24.910 --> 00:34:29.920
sigma of x as our sigmoid function, we
could kind of combine all these different

00:34:29.920 --> 00:34:35.760
elements of the circuit as
just one node in the circuit.

00:34:35.760 --> 00:34:40.300
And we know, with this one little
trick here, the derivative

00:34:40.300 --> 00:34:44.880
of sigma x with respect to x can actually
be described in terms of sigma x.

00:34:44.880 --> 00:34:48.380
So we don't need to do any extra
computation like we did internally here,

00:34:48.380 --> 00:34:50.580
take another exponent and so on.

00:34:50.580 --> 00:34:55.482
We actually can just know, well if that
was our value here of sigma x then

00:34:55.482 --> 00:35:00.479
the derivative that will come out here
is just 1- sigma x times sigma x.

00:35:00.479 --> 00:35:04.752
And so we could, in theory, also define
our circuit differently, and in fact

00:35:04.752 --> 00:35:09.382
the circuits we eventually define are this
whole thing is one neural network layer.

00:35:09.382 --> 00:35:14.094
And internally we know exactly the kinds
of messages that pass through such

00:35:14.094 --> 00:35:18.980
a layer, or the error signals, or
again, elements of the final gradients.

00:35:20.080 --> 00:35:20.580
Yeah?

00:35:26.314 --> 00:35:28.270
That's a good question, sorry, yes.

00:35:28.270 --> 00:35:30.350
So the question is, we're talking
about back propagation here, and

00:35:30.350 --> 00:35:31.300
what is forward propagation?

00:35:31.300 --> 00:35:34.485
Yeah, forward propagation just means
computing the value of your overall

00:35:34.485 --> 00:35:34.997
function.

00:35:37.828 --> 00:35:42.039
The relationship between the two is
forward propagation is what you compute,

00:35:42.039 --> 00:35:46.330
what you do at test time, to compute
the final output of your function.

00:35:46.330 --> 00:35:52.200
So, you want the probability for
this node to be a location, or for this

00:35:52.200 --> 00:35:56.550
word to be a location, you'd do forward
propagation to compute that probability.

00:35:56.550 --> 00:36:00.294
And the you do backward propagation to
compute the gradients if you wanna train

00:36:00.294 --> 00:36:03.256
and update your model if you have
a training data set and so on.

00:36:07.457 --> 00:36:11.830
That's right, the red numbers here at
the bottom are all the partial derivatives

00:36:11.830 --> 00:36:14.148
with respect to each of these parameters.

00:36:14.148 --> 00:36:19.872
And here all the intermediate values
that we use as that gradient flows

00:36:19.872 --> 00:36:26.769
through the circuit to the parameters that
we might wanna update, great question.

00:36:30.972 --> 00:36:31.829
All right, so

00:36:31.829 --> 00:36:37.330
essentially we recursively applied the
chain rule as we moved through this graph.

00:36:37.330 --> 00:36:40.480
And we end up with a similar
kind of intuition,

00:36:42.410 --> 00:36:47.125
as we did with the same,
with just using math and

00:36:47.125 --> 00:36:51.980
multivariate calculus, to arrive at these
final gradients, to update our parameters.

00:36:53.010 --> 00:36:56.032
All right,
any questions around the circuit?

00:36:56.032 --> 00:36:59.725
Interpretation of back propagation, yeah.

00:37:03.642 --> 00:37:09.031
So here w2 is our bias term,
it doesn't depend on the values of x,

00:37:09.031 --> 00:37:13.140
we just add it, and
w2 down here in the circuit.

00:37:13.140 --> 00:37:18.997
So that is the last element we add
after adding these two multiplications.

00:37:24.206 --> 00:37:27.781
All right, so,
now if that was too simple and

00:37:27.781 --> 00:37:31.264
you wanna get a little
bit high level again,

00:37:31.264 --> 00:37:36.460
you can essentially think of these
circuits also as flow graphs.

00:37:36.460 --> 00:37:41.714
And circuit is the terminology that
Andrej Karpathy used in 231 and

00:37:41.714 --> 00:37:46.701
Yoshua Bengio, for instance,
another very famous researcher in

00:37:46.701 --> 00:37:51.687
deep learning uses the terminology
of flow graphs, but, again,

00:37:51.687 --> 00:37:54.560
we have the very similar kind of idea.

00:37:54.560 --> 00:37:56.231
You start with some input x,

00:37:56.231 --> 00:37:59.653
you do forward propagation to
compute some kind of value.

00:37:59.653 --> 00:38:04.516
You go through some intermediate variables
y, and then, in back propagation,

00:38:04.516 --> 00:38:09.309
you compute your gradients going backwards
in the reverse order to what you've

00:38:09.309 --> 00:38:11.470
done during forward propagation.

00:38:13.534 --> 00:38:18.123
And so this is if you just have one
intermediate value now if x, and

00:38:18.123 --> 00:38:23.464
this is something else important to
know it for the circuits it's the same,

00:38:23.464 --> 00:38:27.135
if x modifies two paths in
your flow graph you end up,

00:38:27.135 --> 00:38:30.930
based on the multiple variable Chain Rule.

00:38:30.930 --> 00:38:37.160
You have to sum up the local air signals
for both from both of the paths.

00:38:37.160 --> 00:38:40.880
And in general,
again you move backwards through them.

00:38:40.880 --> 00:38:46.040
So usually as long as you have some
kind of directed basically graph or

00:38:46.040 --> 00:38:49.870
tree structure,
you can always compute these flows and

00:38:49.870 --> 00:38:53.660
these elements of your gradient.

00:38:55.030 --> 00:39:00.040
And in general, if x goes through multiple
different elements in your flow graph,

00:39:00.040 --> 00:39:03.690
you just sum up all the partials this way.

00:39:05.550 --> 00:39:10.165
And so this is another interpretation much
more high level without defining exactly

00:39:10.165 --> 00:39:13.098
what kinds of computation
you have here at each node.

00:39:13.098 --> 00:39:17.201
But in general you can define
these kind of flow graphs and

00:39:17.201 --> 00:39:20.722
each node is some kind
of computational result.

00:39:20.722 --> 00:39:23.915
And each arc here is some kind
of dependency, so you need,

00:39:23.915 --> 00:39:26.680
in order to compute this, you needed this.

00:39:26.680 --> 00:39:29.854
And you can define more complex
things where you have so

00:39:29.854 --> 00:39:34.822
called short circuit connections, we'll
define those much later in the class, but

00:39:34.822 --> 00:39:37.720
in general,
you move forward through your node.

00:39:37.720 --> 00:39:41.625
So this is a more realistic example
where we may have some input x,

00:39:41.625 --> 00:39:45.893
we have some probability, or
sorry some class y for our train data set.

00:39:45.893 --> 00:39:50.814
And in forward propagation,
we'll move these through a sigmoid neural

00:39:50.814 --> 00:39:54.050
network layer here such
as h is just sigma of Vx.

00:39:54.050 --> 00:39:56.720
We dropped here The bias term.

00:39:56.720 --> 00:40:01.640
And so, you can also describe your
v as part of this flow graph.

00:40:01.640 --> 00:40:06.150
You move through a next layer, and
then you may have a softmax layer here,

00:40:06.150 --> 00:40:09.690
similar to the one that you
derived in problem set one.

00:40:09.690 --> 00:40:12.420
And then you have your
negative log likelihood, and

00:40:12.420 --> 00:40:18.100
you compute that final cost function for
this pair xy, for this training element.

00:40:18.100 --> 00:40:21.597
And then back propagation again,
you move backwards through the flow graph.

00:40:21.597 --> 00:40:26.871
And you update your parameters as
you move through the flow graph.

00:40:35.548 --> 00:40:40.294
Now, before I go through the last and
final explanation, the good news is you

00:40:40.294 --> 00:40:44.250
won't actually have to do that for
very complex neural networks.

00:40:44.250 --> 00:40:45.790
It would be close to impossible for

00:40:45.790 --> 00:40:50.020
the kinds of large complex neural
networks to do this by hand.

00:40:50.020 --> 00:40:52.640
Many years ago, when I had started my PhD,

00:40:52.640 --> 00:40:55.660
there weren't any software packages
with automatic differentiation.

00:40:55.660 --> 00:40:57.210
So you did have to do that.

00:40:57.210 --> 00:40:59.610
And it slowed us down a little bit.

00:40:59.610 --> 00:41:03.370
But, nowadays,
you can essentially automatically

00:41:03.370 --> 00:41:07.580
infer your back propagation updates
based on the forward propagation.

00:41:07.580 --> 00:41:10.230
It's a completely deterministic process,
and

00:41:10.230 --> 00:41:14.470
so can use symbolic expressions for
your forward prop.

00:41:14.470 --> 00:41:19.030
And then have algorithms automatically
determine your gradient, right?

00:41:19.030 --> 00:41:21.070
The gradients always exist for
these kinds of functions.

00:41:22.150 --> 00:41:25.140
And so that will allow us
to much faster prototyping.

00:41:25.140 --> 00:41:29.280
And you'll get introduced
next week to a tensor flow,

00:41:29.280 --> 00:41:34.250
which is one such package that essentially
takes all these headaches away from you.

00:41:34.250 --> 00:41:35.973
But with this knowledge,

00:41:35.973 --> 00:41:40.842
you'll actually know what's going on
under the hood of these packages.

00:41:40.842 --> 00:41:44.530
All right, any question around the flow
graph interpretation of back propagation?

00:41:45.950 --> 00:41:46.450
Yes?

00:41:51.209 --> 00:41:53.180
It's actually in closed form.

00:41:53.180 --> 00:41:54.580
Yeah, it's not numerically solved.

00:41:54.580 --> 00:41:59.225
So sorry, the question was, the automatic
differentiation, is it numeric or

00:41:59.225 --> 00:41:59.931
symbolic?

00:41:59.931 --> 00:42:00.710
It's usually symbolic.

00:42:03.310 --> 00:42:07.320
All right, now, for the last and
final explanation of the same idea.

00:42:08.800 --> 00:42:14.690
But combining the idea of the flow graph
with the math that you've seen before,

00:42:14.690 --> 00:42:17.030
and hopefully that will help.

00:42:17.030 --> 00:42:21.840
So, let's bring back this complex
two layer neural network.

00:42:21.840 --> 00:42:27.110
Now, how can we describe this
at a much simplified kind of

00:42:27.110 --> 00:42:32.240
flow graph or circuit where we can combine
in a lot of different elements instead of

00:42:32.240 --> 00:42:37.680
writing every multiplication, summation,
exponent, negation, and so and out?

00:42:37.680 --> 00:42:40.540
This is the kind of flow
graph that kind of yeah,

00:42:40.540 --> 00:42:42.880
kind of combines these two worlds.

00:42:42.880 --> 00:42:46.340
So we assumed here we had our delta

00:42:47.980 --> 00:42:52.380
error signal coming from
the simple score that we have.

00:42:52.380 --> 00:42:57.071
And let's say that our final, we want all
the updates, essentially, to W(2) and

00:42:57.071 --> 00:42:57.669
W(1).

00:42:57.669 --> 00:43:04.580
Now W(2), as we move through this
linear score, the delta doesn't change.

00:43:04.580 --> 00:43:11.116
And so the update that we get for W(2)
here is just this outer product again.

00:43:11.116 --> 00:43:17.017
And that's kind of, as we move through
this very high level flow graph,

00:43:17.017 --> 00:43:23.820
we basically now update W(2) once we get
the error message from the layer above.

00:43:24.900 --> 00:43:27.642
Now, as we move through W(2),

00:43:27.642 --> 00:43:32.844
this kind of circuit will essentially
just multiply the affine,

00:43:32.844 --> 00:43:39.844
like as we move through this simple affine
transformation this matrix vector product,

00:43:39.844 --> 00:43:45.356
we're just required to transpose
the forward propagation matrix.

00:43:45.356 --> 00:43:50.435
And we arrived why this is before,
but this is kind of the interpretation

00:43:50.435 --> 00:43:55.780
of this flow graph in terms of a complex
and large realistic neural network.

00:43:56.930 --> 00:44:00.557
And so notice also that
the dimensions here line up perfectly.

00:44:00.557 --> 00:44:07.856
So the output here, we multiply this delta
that has the dimensionality of the output.

00:44:07.856 --> 00:44:12.834
With the transpose, we get exactly
the dimensionality of the input of this W.

00:44:12.834 --> 00:44:14.040
So it's quite intuitive, right?

00:44:14.040 --> 00:44:19.340
You have the linear transformation,
affine transformation through this W

00:44:19.340 --> 00:44:24.230
as you move backwards to this W,
you just multiply it with its transpose.

00:44:24.230 --> 00:44:29.613
And now, we are hitting this
element wise nonlinearity.

00:44:29.613 --> 00:44:33.332
And so as we update the next delta,

00:44:33.332 --> 00:44:38.979
we essentially have also
an element wise derivative

00:44:38.979 --> 00:44:44.270
here of each of the elements
of this activation.

00:44:44.270 --> 00:44:47.942
So as we're moving our error vector,
error signal, or

00:44:47.942 --> 00:44:53.104
global parts of the gradient through
these point-wise nonlinearities, we need

00:44:53.104 --> 00:44:58.989
to apply point-wise multiplications with
the local gradients of the non-linearity.

00:45:01.110 --> 00:45:04.777
And now we have this delta
that's arrived at W(1).

00:45:04.777 --> 00:45:08.837
And so W1 we can now compute
the final gradient with respect to

00:45:08.837 --> 00:45:13.317
W(1) as just the delta again times
the activation of the previous layer,

00:45:13.317 --> 00:45:17.020
which is a(1) and
we have this outer product.

00:45:17.020 --> 00:45:20.890
So this is combining the different
interpretations that we've learned.

00:45:21.910 --> 00:45:26.060
We arrived through this through
just multivariate calculus.

00:45:26.060 --> 00:45:31.060
And now this is the flow graph or
circuit interpretation of what's going on.

00:45:32.840 --> 00:45:33.393
Yes?

00:45:39.080 --> 00:45:42.894
&gt;&gt; If I mean point-wise non linearity,
I mean coordinate wise, yes,

00:45:42.894 --> 00:45:43.962
they are the same.

00:45:43.962 --> 00:45:48.589
So, whenever we write f(z) here, and

00:45:48.589 --> 00:45:53.487
z was a vector of z1, z2, for instance,

00:45:53.487 --> 00:45:57.858
then we meant f(z1) and f(z2).

00:45:57.858 --> 00:46:00.748
And the same is true if
we write it like this.

00:46:02.360 --> 00:46:05.167
And look at the partial derivatives.

00:46:05.167 --> 00:46:06.064
Yeah?
&gt;&gt; I know.

00:46:16.168 --> 00:46:19.539
&gt;&gt; That's just, from matrix [INAUDIBLE].

00:46:28.926 --> 00:46:32.581
It is, yes, so the question is the delta
here the same as in the definition of

00:46:32.581 --> 00:46:34.080
the two layer neural network?

00:46:34.080 --> 00:46:35.190
And it is, yeah.

00:46:35.190 --> 00:46:37.158
So this delta here is this and

00:46:37.158 --> 00:46:41.518
you notice here that it's the same
thing that we wrote before.

00:46:41.518 --> 00:46:44.455
We have W(2) transpose times delta(3).

00:46:44.455 --> 00:46:49.846
And then you have the Hadamard product
with the element-wise derivatives here.

00:46:56.574 --> 00:46:59.370
All right, congratulations!

00:46:59.370 --> 00:47:00.200
You've done it.
So

00:47:00.200 --> 00:47:05.220
now, understand the inner workings of
most deep learning models out there.

00:47:05.220 --> 00:47:06.950
And this was literally
the hardest part of the class.

00:47:06.950 --> 00:47:10.520
I think it's gonna go all uphill
from here for many of you.

00:47:10.520 --> 00:47:15.690
And everything from now on is really
just more matrix multiplications and

00:47:15.690 --> 00:47:17.380
this kind of back propagation.

00:47:17.380 --> 00:47:21.320
It's really 90% of the state of
the art models out there right now and

00:47:21.320 --> 00:47:23.920
top new papers that
are coming out this year.

00:47:23.920 --> 00:47:26.582
You now can have a warm, fuzzy feeling,

00:47:26.582 --> 00:47:30.508
as you look through the forward
propagation definitions.

00:47:30.508 --> 00:47:37.996
All right, with that, let's have a little
intermission and look at a paper.

00:47:37.996 --> 00:47:40.021
Take it away.
&gt;&gt; Hi everyone.

00:47:40.021 --> 00:47:44.565
So yeah, so let's take a break from neural
networks, and let's talk about this

00:47:44.565 --> 00:47:48.698
paper which came out from Facebook ARV
search just this past summer.

00:47:48.698 --> 00:47:53.931
So text classification is
a really important topic in NLP.

00:47:53.931 --> 00:47:57.514
Given a piece of text, we may wanna say,
is this a positive sentiment or

00:47:57.514 --> 00:48:00.160
does it have negative sentiment?

00:48:00.160 --> 00:48:03.498
Is this spam or ham, or
did JK Rowling actually write this?

00:48:03.498 --> 00:48:07.229
And so
this one's particular from a website and

00:48:07.229 --> 00:48:11.527
it's basing [COUGH] an example
of sentiment analysis.

00:48:11.527 --> 00:48:16.653
And so if you recall from your
problem set in problem four.

00:48:16.653 --> 00:48:21.612
An easy way to featurize a sentence
is to just average out all the word

00:48:21.612 --> 00:48:23.580
vectors in a sentence.

00:48:23.580 --> 00:48:27.510
And that's basically what
the model from this paper does.

00:48:27.510 --> 00:48:30.930
And so they use really low
dimensional word vectors.

00:48:30.930 --> 00:48:34.230
Take the average of them, kind of you
know you lose the ordering of it and

00:48:34.230 --> 00:48:39.650
then you get this low dimensional text
vector which represents the sentence.

00:48:39.650 --> 00:48:44.864
In order to kind of get some of
the ordering back, they also use n-grams.

00:48:44.864 --> 00:48:49.471
And so now that we have the text vector
that's kind of like in the hidden layer.

00:48:49.471 --> 00:48:52.802
We then feed it through a linear
classifier which uses softmax compute

00:48:52.802 --> 00:48:55.240
the probability over all
the predictive classes.

00:48:56.490 --> 00:48:59.521
The hidden representation is also
shared by all the classifiers for

00:48:59.521 --> 00:49:00.969
all the different categories.

00:49:00.969 --> 00:49:05.937
Which helps the classifier use information
about words learned from one category for

00:49:05.937 --> 00:49:07.110
another category.

00:49:09.052 --> 00:49:12.010
And so
will look a little bit more familiar

00:49:12.010 --> 00:49:15.300
to you now that you guys have gone
through all the costs and whatnot.

00:49:15.300 --> 00:49:19.310
So we minimize the negative flaws
likelihood over all the classes, and

00:49:19.310 --> 00:49:22.535
the model's trying to using
stochastic gradient descent and

00:49:22.535 --> 00:49:26.055
a linear decaying learning rate.

00:49:26.055 --> 00:49:29.695
Another thing that makes it really fast
is the use of the hierarchical softmax.

00:49:29.695 --> 00:49:32.995
And so by using this, the classes
are organized in like this tree kind of

00:49:32.995 --> 00:49:34.818
fashion instead of just like in a list.

00:49:34.818 --> 00:49:39.193
And so this also helps with the timing, so

00:49:39.193 --> 00:49:43.940
we go from linear time
to logarithmic time.

00:49:43.940 --> 00:49:47.701
Because also the costs are organized
in terms of how frequent they are.

00:49:47.701 --> 00:49:50.812
So in case, we have maybe like a lot
of class, but less of one class.

00:49:50.812 --> 00:49:54.395
This helps kind of balance that out so
NLP is really hot right now.

00:49:54.395 --> 00:49:59.714
So in here the depth is much smaller,
so we can access that cost a lot faster.

00:49:59.714 --> 00:50:02.741
But maybe for some less popular topics,
I just made some up here,

00:50:02.741 --> 00:50:04.404
that's not actually my opinion.

00:50:04.404 --> 00:50:10.271
But they have a much deeper depth
because they are much more infrequent.

00:50:10.271 --> 00:50:13.166
And so especially in this day and age when
we're really crazy about neural networks,

00:50:13.166 --> 00:50:15.313
the question is like how well
does this stack up against them?

00:50:15.313 --> 00:50:18.835
Because it uses a linear classifier, it
doesn't really have all those layers for

00:50:18.835 --> 00:50:20.270
neural network.

00:50:20.270 --> 00:50:23.530
And as it turns out,
this actually performs really well.

00:50:23.530 --> 00:50:28.445
It's not only really fast, but it performs
just as well if not sometimes better than

00:50:28.445 --> 00:50:30.060
neural networks which is pretty crazy.

00:50:31.230 --> 00:50:32.309
And so just a quick summary.

00:50:32.309 --> 00:50:36.963
FastText, which is what they call their
model is often on par with deep learning

00:50:36.963 --> 00:50:37.872
classifiers.

00:50:37.872 --> 00:50:40.118
It takes seconds to train,
instead of days,

00:50:40.118 --> 00:50:43.972
thanks to their use of low dimensional
word vectors in the hierarchical softmax.

00:50:43.972 --> 00:50:48.055
And another side bit, is that it can also
learn vector representations of words in

00:50:48.055 --> 00:50:51.624
different languages,
with performs even better than word2vec.

00:50:51.624 --> 00:50:52.250
Thank you.

00:50:52.250 --> 00:50:59.483
&gt;&gt; [APPLAUSE]
&gt;&gt; All right, and you know what's awesome?

00:50:59.483 --> 00:51:02.823
Like this kind of equation you could
totally derive all the gradients now too.

00:51:02.823 --> 00:51:07.499
&gt;&gt; [LAUGH]
&gt;&gt; Just another day in the office.

00:51:07.499 --> 00:51:11.932
All right, so class project.

00:51:11.932 --> 00:51:18.554
This is for many, the most lasting and
fun part of the class.

00:51:18.554 --> 00:51:22.219
But some people also don't
have a research agenda or

00:51:22.219 --> 00:51:27.310
some kind of interesting data set,
so you don't have to do the project.

00:51:28.610 --> 00:51:33.506
If you do a project,
we want you to have a mandatory mentor.

00:51:33.506 --> 00:51:39.867
The mentors that are pre-approved are all
the PhD students, and Chris and me.

00:51:39.867 --> 00:51:43.148
So we wanna really give
you good advice and

00:51:43.148 --> 00:51:46.725
we want you to meet your
mentors frequently.

00:51:46.725 --> 00:51:50.302
So think I'll have 25, Chris has 25, and

00:51:50.302 --> 00:51:54.906
then I guess each of the PhD TAs
also has at most 25 groups.

00:51:54.906 --> 00:51:56.986
It's a very large class.

00:51:56.986 --> 00:52:00.500
But yeah, so
basically your class projects,

00:52:00.500 --> 00:52:04.747
if you do decide to do it,
is 30% of your final grade.

00:52:04.747 --> 00:52:09.476
And sometimes real paper
submissions come out from these.

00:52:09.476 --> 00:52:11.044
It's really exciting, you get to travel.

00:52:11.044 --> 00:52:14.208
You get probably paid,
depending on who you're working with.

00:52:14.208 --> 00:52:17.568
If you're a grad student and
you write a paper,

00:52:17.568 --> 00:52:20.181
to go to some fun places in the world.

00:52:20.181 --> 00:52:24.330
And something that's really helpful for
people's careers.

00:52:24.330 --> 00:52:25.822
Sometimes these papers,

00:52:25.822 --> 00:52:30.035
people get contacted from various
companies once we put these papers up.

00:52:30.035 --> 00:52:31.737
If you do a really good job,

00:52:31.737 --> 00:52:35.664
it can have really lasting impact
on the kinda work that you do.

00:52:35.664 --> 00:52:39.527
So on the choice of doing assignment four,
the final project.

00:52:39.527 --> 00:52:41.905
We don't wanna force you
to do the final project,

00:52:41.905 --> 00:52:45.371
cuz some people just wanna learn
the concepts and then move on with life.

00:52:45.371 --> 00:52:49.341
And it can be a little painful to
try to come up with something.

00:52:49.341 --> 00:52:51.627
So there is a final project, and

00:52:51.627 --> 00:52:56.128
we will ask you to sort of define
your project with your mentor.

00:52:56.128 --> 00:52:57.752
And then we might encourage you or

00:52:57.752 --> 00:53:00.481
discourage you from moving
forward with that project.

00:53:00.481 --> 00:53:04.230
Some projects might be too large in
scope or too small in scope, and so on.

00:53:04.230 --> 00:53:10.506
And so do check with the TAs of whether
the project is the right thing for you.

00:53:10.506 --> 00:53:15.750
If you do a project, and if you decide
to do it you really have to start early.

00:53:15.750 --> 00:53:20.346
Ideally you will start meeting me today,
or

00:53:20.346 --> 00:53:26.074
latest like next week or
two weeks and or the other TAs.

00:53:26.074 --> 00:53:32.953
We write out a lot of the sort of
organizational things on the website.

00:53:32.953 --> 00:53:35.220
So let's look at the website really quick.

00:53:35.220 --> 00:53:37.689
It's now linked from our main page.

00:53:37.689 --> 00:53:41.605
So you can get a couple of different
ideas from these top conferences.

00:53:41.605 --> 00:53:45.416
So one project idea and
we'll go into that a little bit later,

00:53:45.416 --> 00:53:49.227
is to take one of these newest
papers from the various groups or

00:53:49.227 --> 00:53:53.470
various conferences and
just try to replicate the results.

00:53:53.470 --> 00:53:57.802
You will notice that despite having in
theory, everything written in the paper,

00:53:57.802 --> 00:54:01.022
if it's a nontrivial model
there's a lot of subtle detail.

00:54:01.022 --> 00:54:04.466
And it's hard to squeeze all of
those details in eight pages.

00:54:04.466 --> 00:54:08.653
So usually the maximum page them in so
replicating sometimes,

00:54:08.653 --> 00:54:13.083
this paper is sufficient enough for
most papers in most projects.

00:54:16.107 --> 00:54:21.553
So here, here's some very concrete
papers that you can look at and

00:54:21.553 --> 00:54:23.894
to get adheres from others.

00:54:23.894 --> 00:54:29.846
And what's kind of interesting and
new these days, this is by no means and

00:54:29.846 --> 00:54:35.280
exclusive list,
there a lot more other interesting papers.

00:54:35.280 --> 00:54:39.431
So again here there sort of pre
proofed mentors for projects.

00:54:39.431 --> 00:54:42.234
You'll have to contact
us through office hours.

00:54:42.234 --> 00:54:47.277
And if you do a project
in your project proposal,

00:54:47.277 --> 00:54:51.341
you have to write out who the mentor is.

00:54:51.341 --> 00:54:55.168
A lot of other mentors, we'll
actually list probably next week now.

00:54:55.168 --> 00:54:59.074
A list of potential projects that
are coming from people who spend all their

00:54:59.074 --> 00:55:02.300
time thinking about deep learning and NLP.

00:55:02.300 --> 00:55:06.341
So if you don't have an idea, but you
really do wanna do some interesting novel

00:55:06.341 --> 00:55:09.232
research project,
we'll post that link internally.

00:55:09.232 --> 00:55:13.410
So that not the whole world sees it,
but only the students in this class.

00:55:13.410 --> 00:55:16.647
Cuz sometimes, the PhD students
have some interesting novel idea.

00:55:16.647 --> 00:55:20.112
They don't want it to get scooped and
have some other researchers do that idea,

00:55:20.112 --> 00:55:22.607
but they do wanna collaborate
with students and youths.

00:55:22.607 --> 00:55:27.897
So we'll keep those
ideas under wraps here.

00:55:27.897 --> 00:55:29.274
So yeah, this is your project proposal.

00:55:29.274 --> 00:55:32.119
You have to define all these things, and

00:55:32.119 --> 00:55:36.890
we'll go through that now in
some of the details here.

00:55:36.890 --> 00:55:39.557
And then you have a final submission,
you have to write a report.

00:55:39.557 --> 00:55:42.571
And then we'll also have
a poster presentation,

00:55:42.571 --> 00:55:45.957
where all the projects
are basically being described.

00:55:45.957 --> 00:55:49.518
You'll have to print a little poster,
and we'll walk around.

00:55:49.518 --> 00:55:50.413
It's usually quite fun.

00:55:50.413 --> 00:55:55.858
Maybe we'll even come up with a prize for
best poster, and best paper, and so on.

00:55:55.858 --> 00:55:59.550
All right, so
these are the organizational, Tips.

00:55:59.550 --> 00:56:04.340
Posters and projects by the way
I have maximum of three people.

00:56:04.340 --> 00:56:08.830
If you have some insanely,
well thought out plan,

00:56:08.830 --> 00:56:10.830
we may make an exception and go to four.

00:56:10.830 --> 00:56:12.920
But the standard default is three.

00:56:12.920 --> 00:56:17.540
So the exception kind of has to be
mailed to the TAs or Aston Piazza.

00:56:20.530 --> 00:56:23.520
Any questions around the organizational
aspects of the project?

00:56:24.720 --> 00:56:25.370
Groups.

00:56:25.370 --> 00:56:28.060
You can do groups of one, two, or three.

00:56:28.060 --> 00:56:29.520
So it doesn't have to be three.

00:56:29.520 --> 00:56:33.150
The bigger your group,
the more we expect from the project.

00:56:33.150 --> 00:56:38.980
And you have to also write out exactly
what each person in the project has done.

00:56:38.980 --> 00:56:47.300
You can actually use any kind of open
source library and code that you want.

00:56:47.300 --> 00:56:49.690
It's just a realistic research project.

00:56:49.690 --> 00:56:54.100
But if you just take Kaldi,
which is a speech recognition system, and

00:56:54.100 --> 00:56:55.470
you say I did speech recognition.

00:56:55.470 --> 00:56:57.900
And then really all you did
was download the package and

00:56:57.900 --> 00:57:00.730
run it, then that's not very impressive.

00:57:00.730 --> 00:57:05.500
So the more you use,
the more you also have to be careful and

00:57:05.500 --> 00:57:09.230
say exactly what parts
you actually implemented.

00:57:11.030 --> 00:57:14.550
And in the code,
you also have to submit your code, so

00:57:14.550 --> 00:57:18.718
that we understand what you've done and
the results are real.

00:57:30.368 --> 00:57:34.860
So this year we do want
some language in there.

00:57:34.860 --> 00:57:36.620
Some natural human language.

00:57:38.270 --> 00:57:39.600
Last year I was a little more open.

00:57:39.600 --> 00:57:41.470
It could be the language of music and
so on now.

00:57:41.470 --> 00:57:42.990
But this year it's [INAUDIBLE].

00:57:42.990 --> 00:57:46.948
So we've got to have some
natural language in there, yeah.

00:57:48.984 --> 00:57:51.876
But other than that,
that can be done quite easily so

00:57:51.876 --> 00:57:55.760
we'll go through the types of
projects you might want to do.

00:57:55.760 --> 00:57:59.430
And if you have a more theoretically
inclined project where you

00:57:59.430 --> 00:58:04.380
really are just faking out some clever
way of doing a sarcastic ready to sent or

00:58:04.380 --> 00:58:07.100
using different kinds of
optimization functions.

00:58:07.100 --> 00:58:10.390
An optimizers that we'll talk
about leading the class to

00:58:10.390 --> 00:58:13.300
then as long as you at least
applied it in one experiment

00:58:13.300 --> 00:58:17.270
to a natural language processing data set
that would still be a pretty cool project.

00:58:18.870 --> 00:58:21.520
So you can also apply
it to genomics data and

00:58:21.520 --> 00:58:24.890
to text data if you wanna have
a little bit of that flavor.

00:58:24.890 --> 00:58:28.414
But there is gonna be at least one
experiment where you apply it to a text

00:58:28.414 --> 00:58:28.967
data set.

00:58:32.183 --> 00:58:37.701
All right, so now let's walk through the
different kinds of projects that you might

00:58:37.701 --> 00:58:42.920
wanna consider, and what might be entailed
in such project to give you an idea.

00:58:45.070 --> 00:58:48.517
Unless there are any other questions
around the organization of the projects,

00:58:48.517 --> 00:58:49.450
deadlines and so on.

00:58:51.491 --> 00:58:54.575
So, let's start with
the kind of simplest and

00:58:54.575 --> 00:58:59.700
all the other ones are sort of bonuses
on top of that simple kind of project.

00:58:59.700 --> 00:59:05.810
And this is actually, I think generally,
good advice, not just for a class project,

00:59:05.810 --> 00:59:10.210
but in general, how to apply a deep
learning algorithm to any kind of problem,

00:59:10.210 --> 00:59:13.620
whether in academia or
in industry, or elsewhere.

00:59:13.620 --> 00:59:16.720
So, let's assume you want to

00:59:17.960 --> 00:59:21.150
apply an existing neural
network to an existing task.

00:59:22.370 --> 00:59:25.290
So in our case, for instance,
let's take summarization.

00:59:26.320 --> 00:59:29.470
So you want to be able to
take a long document and

00:59:29.470 --> 00:59:32.380
summarize into a short paragraph.

00:59:32.380 --> 00:59:33.210
Let's say that was your goal.

00:59:34.540 --> 00:59:39.360
Now step one, after you define your task,
is you have to define your dataset.

00:59:39.360 --> 00:59:44.120
And that is actually, sadly,
in many cases in both industry and

00:59:44.120 --> 00:59:48.420
in academia,
an incredibly time intensive problem.

00:59:48.420 --> 00:59:52.200
And so, the simplest solution
to that is you just search for

00:59:52.200 --> 00:59:54.120
an existing academic dataset.

00:59:54.120 --> 00:59:58.220
There's some people who've
worked in summarization before.

00:59:58.220 --> 01:00:01.110
The nice thing is if you use
an existing data set, for instance,

01:00:01.110 --> 01:00:05.450
from the Document Understanding
Conference, DUC here, then other people

01:00:05.450 --> 01:00:08.770
have already applied some algorithms
to it, you'll have some base lines,

01:00:08.770 --> 01:00:13.980
you know what kind of metric or evaluation
is reasonable versus close to random.

01:00:13.980 --> 01:00:16.060
And so on,
cuz sometimes that's not always obvious.

01:00:16.060 --> 01:00:19.020
We don't always us just accuracy for
instance.

01:00:19.020 --> 01:00:23.200
So in that case, using an existing
academic data set gets rid

01:00:24.470 --> 01:00:28.590
of a lot of complexity.

01:00:28.590 --> 01:00:34.300
However, it is really fun if you actually
come up with your own kind of dataset too.

01:00:34.300 --> 01:00:38.810
So maybe you're really excited about food,
and you want to prowl Yelp, or

01:00:38.810 --> 01:00:43.750
use a Yelp dataset for restaurant review,
or something like that.

01:00:43.750 --> 01:00:46.780
So, however,
when you do decide to do that,

01:00:46.780 --> 01:00:51.070
you definitely have to check in with your
mentor, or with Chris and me, and others.

01:00:52.350 --> 01:00:56.250
Because I sadly have seen several projects

01:00:56.250 --> 01:00:58.900
in the last couple of years where
people have this amazing idea.

01:00:58.900 --> 01:01:00.580
I'm excited, they're excited.

01:01:00.580 --> 01:01:05.910
And then they spent 80% of the time
on their project on a web crawler

01:01:05.910 --> 01:01:08.870
getting not blocked from IP addresses,

01:01:08.870 --> 01:01:13.465
writing multiple IP addresses,
having multiple machines, and crawling.

01:01:13.465 --> 01:01:16.255
And so on, then they realize,
all right, it's super noisy.

01:01:16.255 --> 01:01:19.155
Sometimes it's just the document
they were hoping to get and

01:01:19.155 --> 01:01:20.075
crawl, it's just a 404 page.

01:01:20.075 --> 01:01:21.875
And now they've filtered that.

01:01:21.875 --> 01:01:25.215
And then they realize HTML,
and they filter that.

01:01:25.215 --> 01:01:25.925
And before you know it,

01:01:25.925 --> 01:01:29.670
it's like, they have like three more days
left to do any deep learning for NLP.

01:01:31.090 --> 01:01:35.150
And so, it has happened before so
don't fall into that trap.

01:01:35.150 --> 01:01:41.810
If you do decide to do that, check with us
and try to, before the milestone deadline.

01:01:41.810 --> 01:01:45.740
For sure have the data set ready so
you can actually do deep learning for NLP,

01:01:45.740 --> 01:01:50.330
cuz sadly we just can't give you a good
grade for a deep learning for NLP class if

01:01:50.330 --> 01:01:54.590
you spend 95% of your time writing a web
crawler and explaining your data set.

01:01:54.590 --> 01:01:59.440
So in this case, for instance, you might
say all right, I want to use Wikipedia.

01:01:59.440 --> 01:02:01.430
Wikipedia slightly easier to crawl.

01:02:01.430 --> 01:02:05.270
You can actually download sort of
already pre-crawled versions of it.

01:02:05.270 --> 01:02:08.040
Maybe you want to say my intro paragraph

01:02:08.040 --> 01:02:11.450
is the summary of the whole
rest of the article.

01:02:11.450 --> 01:02:15.350
Not completely crazy to
make that assumption, but

01:02:15.350 --> 01:02:17.985
really you can be creative in this part.

01:02:17.985 --> 01:02:21.555
You can try to connect it to your
own research or your own job if your

01:02:21.555 --> 01:02:25.425
a [INAUDIBLE] student, or
just any kind of interest that you have.

01:02:25.425 --> 01:02:29.085
Song lyrics come up from time
to time it's really fun NLP

01:02:29.085 --> 01:02:32.115
combine with language of music
with natural language and so on.

01:02:32.115 --> 01:02:36.565
So you can be creative here, and
we kind of value a little bit of

01:02:36.565 --> 01:02:39.985
the creativity this is like a task of
data set we had never seen before and

01:02:39.985 --> 01:02:44.300
you actually gain some interesting
Linguistic insights or something.

01:02:44.300 --> 01:02:47.620
That is the cool part of the project,
right.

01:02:47.620 --> 01:02:49.405
Any questions around defining a data set?

01:02:53.762 --> 01:02:59.290
All right, so
then you wanna define your metric.

01:02:59.290 --> 01:03:00.940
This is also super important.

01:03:02.260 --> 01:03:05.390
For instance, you have maybe
have crawled your dataset and

01:03:05.390 --> 01:03:10.110
let's say you did something simpler like
restaurant star rating classification.

01:03:10.110 --> 01:03:14.050
This is a review and I want to
classify if this a four star review or

01:03:14.050 --> 01:03:17.790
a one star review or a two or three.

01:03:17.790 --> 01:03:24.464
And now you may have a class
distribution where this is one star,

01:03:24.464 --> 01:03:31.523
this is two stars, three and four,
and now the majority are three.

01:03:31.523 --> 01:03:33.574
Maybe that you troll kind of funny and

01:03:33.574 --> 01:03:37.060
so really most of the reviews
are three star reviews.

01:03:37.060 --> 01:03:42.226
So this is just like number
of reviews per star category.

01:03:42.226 --> 01:03:49.550
And maybe 90% of the things you
called are in the third class.

01:03:49.550 --> 01:03:52.510
And then you write your report, you're
super excited, it was a new data set,

01:03:52.510 --> 01:03:54.460
you did well, you crawled it quickly.

01:03:54.460 --> 01:03:57.100
And then all you give us
is an accuracy metric, so

01:03:57.100 --> 01:04:00.390
accuracy is total correct
divided by total.

01:04:00.390 --> 01:04:02.738
And now, let's say your accuracy is 90%.

01:04:02.738 --> 01:04:08.580
It's 90% accurate, 90% of the cases
gives you the ride star rating.

01:04:08.580 --> 01:04:11.600
Sadly, it just always gives three.

01:04:11.600 --> 01:04:14.830
It never gives any other result.

01:04:14.830 --> 01:04:18.011
You're essentially overfit
to your dataset and

01:04:18.011 --> 01:04:21.123
your evaluation metric
was completely bogus.

01:04:21.123 --> 01:04:24.161
It's hard to know whether they basically
could have implemented a one line

01:04:24.161 --> 01:04:27.630
algorithm that's just as accurate as yours
which is just, no matter what the input,

01:04:27.630 --> 01:04:28.270
return three.

01:04:29.480 --> 01:04:33.780
So hard to give a good grade on that and
it's a very tricky trap to fall into.

01:04:33.780 --> 01:04:38.518
I see it all the time in industry and
for young researchers and so on.

01:04:38.518 --> 01:04:41.141
So in this case, you should've used,

01:04:41.141 --> 01:04:45.118
does anybody know what kind
of metric you should've used?

01:04:45.118 --> 01:04:46.087
F1, that's right.

01:04:46.087 --> 01:04:49.852
So, and we'll go through some of
these as we go through the class but

01:04:49.852 --> 01:04:52.640
it's very important to
define your metric well.

01:04:53.650 --> 01:04:56.290
Now, for something as tricky as
summarization, this isn't where you're

01:04:56.290 --> 01:04:58.900
really just like, this is the class,
this is the final answer.

01:04:58.900 --> 01:05:04.228
You have to actually either extract or
generate a longer sequence.

01:05:04.228 --> 01:05:07.328
And there are a lot of different
kinds of metrics you can use.

01:05:07.328 --> 01:05:12.998
BLEU's n-gram overlap or Rouge share
which is a Recall-Oriented Understudy for

01:05:12.998 --> 01:05:18.182
Gisting Evaluation which essentially
is just a metric to weigh differently

01:05:18.182 --> 01:05:23.779
how many n-grams are correctly overlapping
between a human generated summary.

01:05:23.779 --> 01:05:26.727
For instance,
your Wikipedia paragraph number one, and

01:05:26.727 --> 01:05:28.780
whatever output your algorithm gives.

01:05:30.420 --> 01:05:32.405
So, Rouge is the official metric for

01:05:32.405 --> 01:05:36.762
summarization in different sub-communities
and NOP have their own metrics and

01:05:36.762 --> 01:05:39.850
it's important that you know
what you're optimizing.

01:05:39.850 --> 01:05:44.371
So, the machine translation, for
instance, you might use BLEU scores,

01:05:44.371 --> 01:05:48.486
BLEU scores are essentially also
a type of n-gram overlap metric.

01:05:48.486 --> 01:05:50.570
If you have a skewed data set,
you wanna use F1.

01:05:50.570 --> 01:05:52.690
And in some cases,
you can just use accuracy.

01:05:54.150 --> 01:05:57.080
And this is generally useful
even if you're in industry and

01:05:57.080 --> 01:05:59.610
later in life, you always wanna
know what metric you're optimizing.

01:05:59.610 --> 01:06:04.040
It's hard to do well if you don't know
the metric that you're optimizing for,

01:06:04.040 --> 01:06:05.926
both in life and deep learning projects.

01:06:05.926 --> 01:06:10.140
All right so,
let's say you defined your metric now,

01:06:10.140 --> 01:06:11.960
you need to split your dataset.

01:06:11.960 --> 01:06:14.480
And it's also very important step and

01:06:14.480 --> 01:06:20.550
it's also something that you can
easily make sort of honest mistakes.

01:06:20.550 --> 01:06:25.370
Again, in advantage of taking pre-existing
academic dataset is that in many cases,

01:06:25.370 --> 01:06:28.500
it's already pre-split but not always.

01:06:29.690 --> 01:06:31.800
And you don't wanna look at your

01:06:32.820 --> 01:06:36.710
final test split until around
1 week before the deadline.

01:06:36.710 --> 01:06:41.028
So, let's say you have downloaded
a lot of different articles and

01:06:41.028 --> 01:06:45.740
now you basically have 100% of
some articles you wanna summarize.

01:06:45.740 --> 01:06:49.436
And normal split would be take 80% for
training,

01:06:49.436 --> 01:06:53.900
you take 10% for your validation and
your development.

01:06:53.900 --> 01:06:57.116
So, oftentimes this is called
the validation split, or

01:06:57.116 --> 01:07:00.741
the development split, or
dev split, or various other terms.

01:07:00.741 --> 01:07:02.890
And 10% for your final test split.

01:07:02.890 --> 01:07:08.510
And so, the final one, you ideally get a
sense of how your algorithm would work in

01:07:08.510 --> 01:07:13.540
real life, on data you've never
seen before, you didn't try to chew

01:07:13.540 --> 01:07:18.191
on your model like, how many layers should
I use, how wide should each layer be?

01:07:18.191 --> 01:07:22.848
You'll try a lot of these things,
we'll describe these in the future.

01:07:22.848 --> 01:07:30.168
But it's very important to correctly split
and why do I make such a fuss about that?

01:07:30.168 --> 01:07:33.090
Well, there too, you might make mistakes.

01:07:33.090 --> 01:07:36.114
So let's say,
you have unused text and let's say,

01:07:36.114 --> 01:07:40.649
you crawled it in such a way there's a lot
of mistakes that you can make if you try

01:07:40.649 --> 01:07:45.152
to predict the soft market for instance,
don't do that, it doesn't work.

01:07:45.152 --> 01:07:51.030
But in many cases, you might say,
or there some temporal sequence.

01:07:51.030 --> 01:07:56.520
And now, you basically have all your
dataset and the perfect thing to do

01:07:56.520 --> 01:08:02.240
is actually do it like this, you take 80%
of let's say, month January to May or

01:08:02.240 --> 01:08:06.210
something and then,
your final test split is from November.

01:08:06.210 --> 01:08:07.930
That way you know there's no overlap.

01:08:09.000 --> 01:08:13.060
But maybe you made a mistake and
you said well, I crawled it this way, but

01:08:13.060 --> 01:08:15.130
now I'm just randomly sample.

01:08:15.130 --> 01:08:18.110
So, as sample an article from here,
and one from here, and one from here.

01:08:18.110 --> 01:08:22.220
And then the random sample goes
to the 80% of my training data.

01:08:22.220 --> 01:08:26.815
And now, the test data and the development
data might actually have some overlap.

01:08:26.815 --> 01:08:32.500
Cuz if you're depending on how you
chose your dataset maybe the another

01:08:32.500 --> 01:08:37.920
article which just like a slight addition,
like some update to an emerging story.

01:08:37.920 --> 01:08:40.743
And now the summary is
almost exact same but

01:08:40.743 --> 01:08:43.654
the input document just
changed a tiny bit.

01:08:43.654 --> 01:08:50.256
And you have one article in your training
set and another one in your test set.

01:08:50.256 --> 01:08:53.952
But the test set article is really only
one extra paragraph on an emerging story

01:08:53.952 --> 01:08:55.868
and the rest is exactly the same.

01:08:55.868 --> 01:08:58.710
So now you have an overlap of your
training and your testing data.

01:08:59.980 --> 01:09:01.810
And so in general,

01:09:01.810 --> 01:09:07.180
if this is your training data and
this should be your test data.

01:09:07.180 --> 01:09:09.280
It should be not overlapping at all.

01:09:10.770 --> 01:09:14.657
And whenever you do really well, you run
your first experiment and you get 90 F1.

01:09:14.657 --> 01:09:19.016
And things look just too good to be
true sadly in many cases they are and

01:09:19.016 --> 01:09:23.979
you made some mistake where maybe your
test set had some overlap for instance,

01:09:23.979 --> 01:09:25.650
with your training data.

01:09:27.070 --> 01:09:31.650
It's very important to be a little
paranoid about that when your first couple

01:09:31.650 --> 01:09:34.830
of experiments turn out just
to be too good to be true.

01:09:34.830 --> 01:09:39.721
That can mean either your training,
your task is too simple,

01:09:39.721 --> 01:09:44.624
or you made a mistake in splitting and
defining your dataset.

01:09:46.080 --> 01:09:49.528
All right, any questions around defining
a metric or your dataset, yeah?

01:10:00.975 --> 01:10:04.700
So, if we split it temporally, wouldn't
we learn a different distribution?

01:10:04.700 --> 01:10:08.097
That is correct,
we would learn a different distribution,

01:10:08.097 --> 01:10:09.772
these are non-stationary.

01:10:09.772 --> 01:10:14.322
And that is kinda true for a lot of texts,
but if you, ideally, when you built

01:10:14.322 --> 01:10:18.940
a deep learning system for an LP you
want it to built it so that it's robust.

01:10:18.940 --> 01:10:21.429
It's robust to sum such changes over time.

01:10:21.429 --> 01:10:25.277
And you wanna make sure that when
you run it in a real world setting,

01:10:25.277 --> 01:10:29.612
on something you've never seen before,
you've shipped your software,

01:10:29.612 --> 01:10:32.635
it's doing something, it will still work.

01:10:32.635 --> 01:10:35.155
And this was the most realistic way

01:10:35.155 --> 01:10:37.329
to capture how well it
would work in real life.

01:10:44.400 --> 01:10:47.961
Would it be appropriate to run both
experiments as in both where you subsample

01:10:47.961 --> 01:10:51.020
randomly, and
then you subsample temporally for your?

01:10:51.020 --> 01:10:56.029
You could do that, and the intuitive
thing that is likely going to happen

01:10:56.029 --> 01:11:01.035
is if you sample randomly from all
over the place, then you will probably

01:11:01.035 --> 01:11:05.492
do better than if you have this
sort of more strict kind of split.

01:11:05.492 --> 01:11:11.020
But running an additional experiment will
rarely ever get you points subtracted.

01:11:11.020 --> 01:11:16.097
You can always run more experiments,
and we're trying really

01:11:16.097 --> 01:11:21.481
hard to help you get computing
infrastructure and Cloud compute.

01:11:21.481 --> 01:11:24.850
So you don't feel restricted with
the number of experiments you run.

01:11:30.145 --> 01:11:34.065
All right, now, number 5,
establish a baseline.

01:11:34.065 --> 01:11:37.475
So, you basically wanna implement
the simplest model first.

01:11:37.475 --> 01:11:41.871
This could just be a very simple logistic
regression on unigrams or bigrams.

01:11:41.871 --> 01:11:45.776
Then, compute your metrics on your train
data and your development data, so

01:11:45.776 --> 01:11:49.615
you understand whether you're
overfitting or underfitting.

01:11:49.615 --> 01:11:54.260
If, for instance, you're training Metric.

01:11:54.260 --> 01:11:57.450
Let's say your loss is very,
very low on training.

01:11:57.450 --> 01:12:01.220
You do very well on training, but
you don't do very well on testing,

01:12:01.220 --> 01:12:03.310
then you're in an over fitting regime.

01:12:03.310 --> 01:12:07.130
If you do very well on training and well
on testing, you're done, you're happy.

01:12:07.130 --> 01:12:11.756
But if your training loss can't be lower,
so you're not even doing well on your

01:12:11.756 --> 01:12:16.070
training, that often means your
model is not powerful enough.

01:12:16.070 --> 01:12:19.610
So it's very important to compute
both the metrics on your training and

01:12:19.610 --> 01:12:20.418
your development split.

01:12:20.418 --> 01:12:24.230
And then, and this is something
we value a lot in this class too.

01:12:24.230 --> 01:12:27.560
And it's something very important for
you in both research and

01:12:27.560 --> 01:12:31.110
industries like you wanna analyze your
errors carefully for that baseline.

01:12:32.200 --> 01:12:36.510
And if the metrics are amazing and
there are no errors, you're done,.

01:12:36.510 --> 01:12:37.740
Probably a problem was too easy and

01:12:37.740 --> 01:12:41.000
you may wanna restart unless it's really
a valuable problem for the world.

01:12:41.000 --> 01:12:45.480
And then maybe you can just really
describe it carefully and you're done too.

01:12:45.480 --> 01:12:50.207
All right, now, any questions
around establishing your baseline?

01:12:50.207 --> 01:12:52.960
It is very important to not just go in and
add lots of bells and

01:12:52.960 --> 01:12:56.542
whistles that you'll learn about in
the next couple of weeks in this class and

01:12:56.542 --> 01:12:58.650
create this monster of a model.

01:12:58.650 --> 01:13:00.621
You want to start with something simple,

01:13:00.621 --> 01:13:04.400
sanity check, make sure you didn't
make mistakes in splitting your data.

01:13:04.400 --> 01:13:06.180
You have the right kind of metric.

01:13:06.180 --> 01:13:11.400
And in many cases, it's a good indicator
for how successful your final project

01:13:11.400 --> 01:13:17.540
is if you can get this baseline
In the first half of the quarter.

01:13:17.540 --> 01:13:22.800
Cuz that means you figured out a lot
of these potential issues here.

01:13:22.800 --> 01:13:24.940
And you kind of have your right data set.

01:13:24.940 --> 01:13:28.200
You know what the metric is, you know what
you're optimizing, and everything is good.

01:13:28.200 --> 01:13:30.785
So try to get to this point
as quickly as possible.

01:13:30.785 --> 01:13:33.101
Cuz that is also not as interesting, and

01:13:33.101 --> 01:13:36.450
you can't really use that much
knowledge from the class.

01:13:36.450 --> 01:13:39.170
Now then it gets more interesting.

01:13:39.170 --> 01:13:42.530
And now you can implement some
existing neural network model that

01:13:42.530 --> 01:13:44.390
we taught you in class.

01:13:44.390 --> 01:13:48.080
For instance, this Window-based model if
your task is named entity recognition.

01:13:48.080 --> 01:13:51.390
You can compute your metric
again on your train AND dev set.

01:13:51.390 --> 01:13:56.680
Hopefully you'll see some interesting
patterns such as usually train

01:13:56.680 --> 01:14:02.800
neural nets is quite easy in a sense
that we lower the loss very well.

01:14:02.800 --> 01:14:05.920
And then we might not generalize
as well in the development set.

01:14:05.920 --> 01:14:10.000
And then you'll play around
with regularization techniques.

01:14:10.000 --> 01:14:13.570
And don't worry if some of the stuff
I'm saying now is kind of confusing.

01:14:13.570 --> 01:14:14.710
If you want to do this,

01:14:14.710 --> 01:14:17.380
we'll walk you through that as we're
mentoring you through the project.

01:14:17.380 --> 01:14:23.158
And that's why each project has to
have an assigned mentor that we trust.

01:14:23.158 --> 01:14:26.585
All right, then you analyze your
output and your errors again.

01:14:26.585 --> 01:14:27.985
Very important, be close to your data.

01:14:27.985 --> 01:14:32.465
You can't give too many
examples usually ever.

01:14:32.465 --> 01:14:34.255
And this is kind of the minimum bar for
this class.

01:14:34.255 --> 01:14:35.685
So if you've done this well and

01:14:35.685 --> 01:14:40.890
there's an interesting dataset, then
your project is kind of in a safe haven.

01:14:40.890 --> 01:14:44.800
Now again it's very important
to be close to your data.

01:14:44.800 --> 01:14:46.750
Once you have a metric and
everything looks good,

01:14:46.750 --> 01:14:51.170
we still want you to visualize the kind
of data, even if it's a known data set.

01:14:51.170 --> 01:14:54.530
We wanted you to visualize it,
collect summary statistics.

01:14:54.530 --> 01:14:58.450
It's always good to know the distribution
if you have different kinds of classes.

01:14:58.450 --> 01:15:02.385
You want to, again very important, look
at the errors that your model is making.

01:15:02.385 --> 01:15:05.290
Cuz that can also give you
intuitions of what kinds of

01:15:05.290 --> 01:15:08.760
patterns can your deep learning
algorithm not capture.

01:15:08.760 --> 01:15:11.210
Maybe you need to add
a memory component or

01:15:11.210 --> 01:15:16.010
maybe you need to have longer temporal
kind of dependencies and so on.

01:15:16.010 --> 01:15:19.040
Those things you can only figure out
if you're close to your data and

01:15:19.040 --> 01:15:22.030
you look at the errors that your
baseline models are making.

01:15:23.970 --> 01:15:26.790
And then we want you to analyze
also different hyperparameters.

01:15:26.790 --> 01:15:28.560
A lot of these models
have lots of choices.

01:15:28.560 --> 01:15:30.760
Did we add the sigmoid to that score or
not?

01:15:30.760 --> 01:15:34.020
Is the second layer 100 dimensional or
200 dimensional?

01:15:34.020 --> 01:15:38.138
Should we use 50 dimensional word vectors
or 1,000 dimensional word vectors?

01:15:38.138 --> 01:15:40.400
There are a lot of choices that you make.

01:15:40.400 --> 01:15:44.800
And it's really good in your first
couple projects to try more and

01:15:44.800 --> 01:15:46.910
gain that intuition yourself.

01:15:46.910 --> 01:15:50.560
And sometimes, if you're running
out of time, and only so much, so

01:15:50.560 --> 01:15:54.350
many experiments you can run, we can help
you, and use our intuition to guide you,.

01:15:54.350 --> 01:15:57.360
But it's best if you do
that a little bit yourself.

01:15:57.360 --> 01:16:01.870
And once you've done all of that, now you
can try different model variants, and

01:16:01.870 --> 01:16:04.350
you'll soon see a lot of
these kinds of options.

01:16:04.350 --> 01:16:07.240
We'll talk through all
of them in the class.

01:16:08.920 --> 01:16:11.780
So now another

01:16:11.780 --> 01:16:15.830
kind of class project is you actually
wanna implement a new fancy model.

01:16:15.830 --> 01:16:19.650
Those are the kinds of things that
will put you into potentially writing

01:16:19.650 --> 01:16:22.710
an academic paper, peer review,
and at a conference, and so on.

01:16:23.890 --> 01:16:27.390
The tricky bit of that is you kinda have
to do all the other steps that I just

01:16:27.390 --> 01:16:28.900
described first.

01:16:28.900 --> 01:16:32.360
And then, on top of that,
you know the errors that you're making.

01:16:32.360 --> 01:16:36.640
And now you can gain some intuition of
why the existing models are flawed.

01:16:36.640 --> 01:16:39.190
And you come up with your own new model.

01:16:40.480 --> 01:16:45.770
If you do that, you really wanna be in
close contact with your mentor and some

01:16:45.770 --> 01:16:50.000
researchers, unless you're a researcher
yourself, and you earned your PhD.

01:16:50.000 --> 01:16:53.260
But even then,
you should chat with us from the class.

01:16:53.260 --> 01:16:56.450
You want to basically try to set up

01:16:56.450 --> 01:16:58.540
an infrastructure such that
you can iterate quickly.

01:16:58.540 --> 01:17:04.250
You're like, maybe I should add this new
layer type to this part of my model.

01:17:04.250 --> 01:17:07.850
You want to be able to quickly iterate and
see if that helps or not.

01:17:07.850 --> 01:17:09.020
So it's important and

01:17:09.020 --> 01:17:13.150
actually require a fair amount of software
engineering skills to set up efficient

01:17:13.150 --> 01:17:17.980
experimental frameworks that
allow you to collect results.

01:17:17.980 --> 01:17:21.000
And again you want to start with
simple models and then go to more and

01:17:21.000 --> 01:17:21.890
more complex ones.

01:17:21.890 --> 01:17:25.308
So for instance, in summarization you
might start with something super simple

01:17:25.308 --> 01:17:27.815
like just average all your
word vectors in the paragraph.

01:17:27.815 --> 01:17:31.470
And then do a greedy search of
generating one word at a time.

01:17:31.470 --> 01:17:36.110
Or even greedily searching for
just snippets from the existing

01:17:38.570 --> 01:17:42.940
article in Wikipedia and
you're just copying certain snippets over.

01:17:44.190 --> 01:17:47.195
And then stretch goal is something more
advanced would be lets you actually

01:17:47.195 --> 01:17:48.440
generate that whole summary.

01:17:48.440 --> 01:17:51.310
And so here are a couple of project ideas.

01:17:51.310 --> 01:17:56.600
But, again, we'll post the whole
list of them with potential mentors

01:17:56.600 --> 01:18:01.730
from the NOP group and the vision group
and various other groups inside Stanford.

01:18:01.730 --> 01:18:03.440
Sentiment is also a fun data set.

01:18:03.440 --> 01:18:06.250
You can look at this URL here for

01:18:06.250 --> 01:18:09.497
one of the preexisting data sets
that a lot of people have worked on.

01:18:09.497 --> 01:18:12.800
All right, so
next week we'll look at some fun and

01:18:12.800 --> 01:18:15.120
fundamental linguistic tasks
like syntactic parsing.

01:18:15.120 --> 01:18:18.920
And then you'll learn TensorFlow and
have some great tools under your belt.

01:18:18.920 --> 01:18:19.420
Thank you.

