WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.758
[MUSIC]

00:00:04.758 --> 00:00:06.679
Stanford University.

00:00:08.915 --> 00:00:10.290
It's getting real today.

00:00:10.290 --> 00:00:15.592
So, let's talk about a little
bit of the overview today.

00:00:15.592 --> 00:00:20.370
So, we'll really get you into
the background for classification.

00:00:20.370 --> 00:00:24.030
And then, we'll do some interesting things
with updating these word vectors that we

00:00:24.030 --> 00:00:26.090
so far have learned in
an unsupervised way.

00:00:26.090 --> 00:00:29.380
We'll update them with some real
supervision signals such as sentiment and

00:00:29.380 --> 00:00:30.590
other things.

00:00:30.590 --> 00:00:33.882
Then, we'll look at the first real
model that is actually useful and

00:00:33.882 --> 00:00:35.488
you might wanna use in practice.

00:00:35.488 --> 00:00:39.847
Well, other than, of course, the word
vectors, but one sort of downstream task

00:00:39.847 --> 00:00:44.207
which is window classification and we'll
really also clear up some of the confusion

00:00:44.207 --> 00:00:48.480
around the cross entropy error and
how it connects with the softmax.

00:00:48.480 --> 00:00:54.825
And then, we'll introduce the famous
neural network, our most basic LEGO block

00:00:54.825 --> 00:01:00.575
that we may start to call deep to get
to the actual title of this class.

00:01:00.575 --> 00:01:01.653
Deep learning in NLP.
And then,

00:01:01.653 --> 00:01:05.905
we'll actually introduce another loss
function, the max margin loss and

00:01:05.905 --> 00:01:09.285
take our first steps into
the direction of backprop.

00:01:09.285 --> 00:01:14.380
So, this lecture will be,
I think very helpful for problem set one.

00:01:14.380 --> 00:01:17.169
We'll go into a lot of the math
that you'll need probably for

00:01:17.169 --> 00:01:18.598
number two in the problem set.

00:01:18.598 --> 00:01:23.628
So, I hope it'll be very useful and
I'm excited for you cuz at the end of this

00:01:23.628 --> 00:01:28.990
lecture, you'll feel hopefully a lot
better about the magic of deep learning.

00:01:30.020 --> 00:01:34.973
All right, are there any
organizational questions around

00:01:34.973 --> 00:01:40.270
problem sets or
programming sessions with the TAs?

00:01:40.270 --> 00:01:41.810
No, we're all good?

00:01:41.810 --> 00:01:44.968
Awesome, thanks to the TAs for
clearing up everything.

00:01:44.968 --> 00:01:50.020
Cool, so let's be very careful about
our notation today because that is one

00:01:50.020 --> 00:01:52.820
of the main things that
a lot of people trip up over

00:01:52.820 --> 00:01:55.260
as we go through very complex
chain-rules and so on.

00:01:55.260 --> 00:01:58.230
So, let's start at the beginning and
say, all right,

00:01:58.230 --> 00:02:01.670
we have usually a training dataset
of some input X and some output Y.

00:02:02.670 --> 00:02:07.110
X could be in the simplest case, words
in isolation, just a single word vector.

00:02:07.110 --> 00:02:10.160
It's not something you would
usually do in practice.

00:02:10.160 --> 00:02:13.010
But it'll be easy for
us to learn that way.

00:02:13.010 --> 00:02:15.550
So we'll start with that but then,
we'll move to context windows today.

00:02:15.550 --> 00:02:20.435
And then eventually, we'll use the same
basic building blocks that we introduce

00:02:20.435 --> 00:02:25.410
today for sentences and documents and
then complex interactions for everything.

00:02:26.430 --> 00:02:30.264
Now, the output in the simplest
case it's just a single label.

00:02:30.264 --> 00:02:33.945
It's just a positive or
a negative kind of sentence.

00:02:33.945 --> 00:02:37.728
It could be the named entities of
certain words in their context.

00:02:37.728 --> 00:02:41.040
It can also be other words, so
in machine translation, for instance,

00:02:41.040 --> 00:02:44.244
you might wanna output eventually
a sequence of other words as our yi and

00:02:44.244 --> 00:02:46.625
we'll get to that in a couple weeks.

00:02:46.625 --> 00:02:50.537
And, yeah, basically they have multiword
sequences as potential outputs.

00:02:50.537 --> 00:02:54.429
All right, so what is the intuition for
classification?

00:02:54.429 --> 00:02:58.817
In the standard machine learning case,
so not yet the deep learning world,

00:02:58.817 --> 00:03:02.605
we usually just, for
something as simple logistic regression,

00:03:02.605 --> 00:03:07.170
basically want to define and learn
a simple decision boundary where we say

00:03:07.170 --> 00:03:12.490
everything to the left of this or
in one direction is in one class and

00:03:12.490 --> 00:03:14.940
the other one,
all the other things in the other class.

00:03:14.940 --> 00:03:19.998
And so, in general machine learning,
we assume our inputs,

00:03:19.998 --> 00:03:23.559
the Xs are kinda fixed,
they're just set and

00:03:23.559 --> 00:03:28.545
we'll only train the W parameter,
which is our softmax weights.

00:03:28.545 --> 00:03:34.120
So, we'll compute the probability of Y,
given the input X with this kind of input.

00:03:34.120 --> 00:03:37.792
And so, one notational comment here is for

00:03:37.792 --> 00:03:42.586
the whole dataset,
we often subscript with i but then,

00:03:42.586 --> 00:03:49.040
when I drop the i we're just looking
at a single example of x and y.

00:03:49.040 --> 00:03:52.260
Eventually, we're going to overload
at the subscript a little bit and

00:03:52.260 --> 00:03:56.220
look at the indices of certain vector so,
if you get confused,

00:03:56.220 --> 00:03:57.810
just raise your hand and ask.

00:03:57.810 --> 00:03:59.630
I'll try to make it clear
which one is which.

00:04:01.610 --> 00:04:04.220
Now, let's dive into the softmax.

00:04:04.220 --> 00:04:09.380
We mentioned it before but we wanna really
carefully define and recall the notation

00:04:09.380 --> 00:04:14.660
here cuz we'll go and take derivatives
with respect to all of these parameters.

00:04:14.660 --> 00:04:21.079
So, we can tease apart two steps here for
computing this probability of y given x.

00:04:21.079 --> 00:04:25.890
The first thing is, we'll take the y'th
row of W and multiply that row with x.

00:04:25.890 --> 00:04:31.867
And so again this notation here,
when we have Wy.

00:04:31.867 --> 00:04:37.190
And that means we'll have,
we're taking the y'th row of this matrix.

00:04:37.190 --> 00:04:39.650
And then, multiplying it here with x.

00:04:42.200 --> 00:04:48.120
Now if we do that multiple times for
all c from one to our classes.

00:04:48.120 --> 00:04:53.050
So let's say, this is 1, 2, 3,
the 4th row and multiply each of these.

00:04:53.050 --> 00:04:55.020
So then we get four numbers here.

00:04:55.020 --> 00:04:57.003
And these are unnormalized scores.

00:04:59.287 --> 00:05:03.599
And then, we'll basically,
pipe this vector through the softmax to

00:05:03.599 --> 00:05:06.971
compute a probability
distribution that sums to one.

00:05:08.958 --> 00:05:11.430
All right, that's our step one.

00:05:11.430 --> 00:05:13.610
Any questions around that?

00:05:13.610 --> 00:05:15.637
Cuz it's just gonna keep
on going from here.

00:05:17.438 --> 00:05:19.270
All right, great.

00:05:19.270 --> 00:05:24.510
And, I get that sometimes
in general from previous

00:05:24.510 --> 00:05:29.450
sort of surveys, it seems to be that
15% of the class are usually bored

00:05:29.450 --> 00:05:32.690
when we go through all of these,
like all of these derivatives.

00:05:32.690 --> 00:05:36.980
15% are super overwhelmed and then the
majority of people are like, okay, it's

00:05:36.980 --> 00:05:39.810
a good speed, I'm learning something, I'm
getting it, and you're making progress.

00:05:39.810 --> 00:05:44.714
So, sorry for the 30% for
whom this is too slow or too fast.

00:05:44.714 --> 00:05:47.900
You can probably just skim
through the lecture slides or

00:05:47.900 --> 00:05:50.620
speed it up if you're watching online.

00:05:50.620 --> 00:05:53.564
If you're super familiar with taking
super complex derivatives and

00:05:53.564 --> 00:05:56.976
if it's a little overwhelming, then
definitely come to all the office hours.

00:05:56.976 --> 00:05:59.614
We have an awesome set of
TAs who will help you.

00:05:59.614 --> 00:06:04.715
All right, now we,
let's look at a single example of an x and

00:06:04.715 --> 00:06:06.825
y that we wanna predict.

00:06:07.825 --> 00:06:12.015
In general, we want our model to
essentially maximize the probability

00:06:12.015 --> 00:06:12.999
of the correct class.

00:06:12.999 --> 00:06:19.815
We wanted to output the right class at the
end by taking the argmax of that output.

00:06:19.815 --> 00:06:23.195
And maximizing probability is the same
as maximizing log probability,

00:06:23.195 --> 00:06:27.040
it's the same as minimizing the negative
of that log probability and

00:06:27.040 --> 00:06:29.810
that is often our objective function.

00:06:29.810 --> 00:06:33.500
So, why do we call this
the cross-entropy error?

00:06:33.500 --> 00:06:39.280
Well, we can define the cross-entropy
in the abstract in general as follows.

00:06:39.280 --> 00:06:42.461
So let's assume we have
the ground truth or gold or

00:06:42.461 --> 00:06:47.630
target probability distribution,
we use those three terms interchangeably.

00:06:47.630 --> 00:06:51.810
Basically, what the ideal target
in our training dataset, the y and

00:06:51.810 --> 00:06:56.826
we'll assume that, that is one at
the right class and zero everywhere else.

00:06:56.826 --> 00:07:01.820
So if we have for instance, five
classes here and it's the center class.

00:07:01.820 --> 00:07:04.470
Its the third class and this would be one
and all the other, numbers would be zero.

00:07:05.580 --> 00:07:09.243
So, if we define this as p
in our computed probability,

00:07:09.243 --> 00:07:13.570
that our softmax outputs as q then we
would define here the cross-entropy

00:07:13.570 --> 00:07:16.161
is basically this sum
over all the classes.

00:07:17.871 --> 00:07:21.718
And in our case, p here is just
one-hot vector that's really only 1 in

00:07:21.718 --> 00:07:24.090
one location and 0 everywhere else.

00:07:24.090 --> 00:07:28.020
So, all these other terms
are basically gone.

00:07:28.020 --> 00:07:30.195
And we end up with just log of q and

00:07:30.195 --> 00:07:34.559
that's exactly the log of what
our softmax outputs, all right?

00:07:34.559 --> 00:07:39.980
And then, there are some nice connections
to Kullback-Leibler divergence and so on.

00:07:39.980 --> 00:07:42.155
I used to talk about it but
we don't have that much time today.

00:07:42.155 --> 00:07:45.116
So and you can also if you're
familiar of this in stats,

00:07:45.116 --> 00:07:49.718
you can see this as trying to minimize the
Kullback-Leibler divergence between these

00:07:49.718 --> 00:07:50.894
two distributions.

00:07:50.894 --> 00:07:55.377
But really, this is all you need to
know for the purpose of this class.

00:07:55.377 --> 00:07:59.500
So this is for
one element of your training data set.

00:07:59.500 --> 00:08:03.030
Now, of course, in general,
you have lots of training examples.

00:08:03.030 --> 00:08:06.980
So we have our overall objective
function we often denote with J,

00:08:06.980 --> 00:08:09.530
over all our parameters theta.

00:08:09.530 --> 00:08:14.990
And we basically sum these negative log
probabilities of the correct classes

00:08:14.990 --> 00:08:18.620
that we index here, a sub-index with yi.

00:08:18.620 --> 00:08:21.910
And basically we want to
minimize this whole sum.

00:08:23.070 --> 00:08:25.400
So that's our cross-entropy error
that we're trying to minimize, and

00:08:25.400 --> 00:08:30.510
we'll take lots of derivatives off in
a lot of the next couple of hours.

00:08:32.500 --> 00:08:35.550
All right, any questions so far?

00:08:35.550 --> 00:08:40.549
So this is the general ML case where
we assume our inputs here are fixed.

00:08:43.037 --> 00:08:44.463
Yes, it's a single number.

00:08:51.810 --> 00:08:56.424
So we are not multiplying a vector here,
so p(c) is the probability for that class,

00:08:56.424 --> 00:08:58.388
so that's one single number.

00:08:58.388 --> 00:09:01.435
Great question.

00:09:01.435 --> 00:09:05.810
So the cross entropy, a single number,
our main objective that we're trying

00:09:05.810 --> 00:09:08.330
to minimize, or
our error that we're trying to minimize.

00:09:09.490 --> 00:09:13.420
Now, whenever you write
this F subscript Y here,

00:09:13.420 --> 00:09:18.640
we don't want to forget that F is really
also a function of X, our inputs, right?

00:09:18.640 --> 00:09:20.630
It's sort of an intermediate step and
it's very important for

00:09:20.630 --> 00:09:24.060
us to play around with this notation.

00:09:24.060 --> 00:09:29.440
So we can also rewrite this as W y,
that row,

00:09:29.440 --> 00:09:31.660
times x, and
we can write out that whole sum.

00:09:31.660 --> 00:09:35.880
And that can often be helpful as you are
trying to take derivatives of one element

00:09:35.880 --> 00:09:41.830
at a time to eventually see the bigger
picture of the whole matrix notation.

00:09:43.272 --> 00:09:46.780
All right, so often we'll write f here
in terms of this matrix notation.

00:09:46.780 --> 00:09:49.910
So this is our f, this is our W,
and this is our x.

00:09:49.910 --> 00:09:55.692
So just standard matrix
multiplication with a vector.

00:09:55.692 --> 00:09:59.190
All right, now most of the time we'll
just talk about this first part of

00:09:59.190 --> 00:10:02.390
the objective function but
it's a bit of a simplification because in

00:10:02.390 --> 00:10:06.820
all your real applications you will
also have this regularization term here.

00:10:07.990 --> 00:10:10.350
As part of your overall
objective function.

00:10:10.350 --> 00:10:13.060
And in many cases,
this theta here for instance,

00:10:13.060 --> 00:10:17.390
if it's the W matrix of our
standard logistic regression,

00:10:17.390 --> 00:10:21.190
we'll essentially just try this
part of the objective function.

00:10:21.190 --> 00:10:25.120
We'll try to encourage the model to keep
all the weights as small as possible and

00:10:25.120 --> 00:10:26.630
as close as possible to zero.

00:10:28.780 --> 00:10:33.230
You can kind of assume if you want as
a Bayesian that you can have a prior,

00:10:33.230 --> 00:10:38.020
a Gaussian distributed prior that says
ideally all these are small numbers.

00:10:38.020 --> 00:10:40.530
Often times if you don't have
this regularization term

00:10:40.530 --> 00:10:43.350
your numbers will blow up and
it will start to overfit more and more.

00:10:43.350 --> 00:10:47.830
And in fact, this kind of plot is
something that you will very often see

00:10:47.830 --> 00:10:50.810
in your projects and
even in the problem sets.

00:10:50.810 --> 00:10:55.090
And when I took my very first statistical
learning class, the professor said,

00:10:55.090 --> 00:10:56.980
this is the number one plot to remember.

00:10:56.980 --> 00:10:58.830
So, I don't know if it's that important,
but it is very,

00:10:58.830 --> 00:11:01.360
very important for all our applications.

00:11:01.360 --> 00:11:03.770
And it's basically a pretty abstract plot.

00:11:03.770 --> 00:11:07.240
You can think of the x-axis as
a variety of different things.

00:11:07.240 --> 00:11:09.260
For instance, how powerful your model is.

00:11:09.260 --> 00:11:12.500
How many deep layers you'll have or
how many parameters you'll have.

00:11:12.500 --> 00:11:15.200
Or how many dimensions
each word vector has.

00:11:15.200 --> 00:11:18.450
Or how long you trained a model for.

00:11:18.450 --> 00:11:22.780
You'll see the same kind of pattern
with a lot of different, x-axis and

00:11:22.780 --> 00:11:26.600
then the y-axis here is
essentially your error.

00:11:26.600 --> 00:11:30.410
Or your objective function that you're
trying to optimize and minimize.

00:11:30.410 --> 00:11:33.750
And what you often observe is,
the more powerful your model gets,

00:11:33.750 --> 00:11:39.010
the better you are on
lowering your training error,

00:11:39.010 --> 00:11:42.636
the better you can fit these x-i,
y-i pairs.

00:11:42.636 --> 00:11:47.050
But at some point you'll actually start
to over-fit, and then your test error, or

00:11:47.050 --> 00:11:50.950
your validation or
development set error, will go up again.

00:11:50.950 --> 00:11:55.730
We'll go into a little bit more details
on how to avoid all of that throughout

00:11:55.730 --> 00:11:57.780
this course and
in the project advice and so on.

00:11:57.780 --> 00:12:02.460
But this is a pretty fundamental thing and
just keep in mind that for a lot of

00:12:02.460 --> 00:12:07.030
the implementations, and your projects you
will want this regularization parameter.

00:12:07.030 --> 00:12:10.220
But really it's the same one for
almost all the objective functions so

00:12:10.220 --> 00:12:14.450
we're going to chop it and mostly
focus on actually fitting our dataset.

00:12:15.590 --> 00:12:17.485
All right,
any questions around regularization?

00:12:26.562 --> 00:12:31.447
So basically, you can think of
this in terms of if you really

00:12:31.447 --> 00:12:36.432
care about one specific number,
then you can adjust all your

00:12:36.432 --> 00:12:42.750
parameters such that it will exactly
go to those different points.

00:12:42.750 --> 00:12:47.650
And if you force it to not do that,
it will kind of be a little smoother.

00:12:47.650 --> 00:12:50.690
And be less likely to fit
exactly those points and

00:12:50.690 --> 00:12:53.410
hence often generalize slightly better.

00:12:53.410 --> 00:12:56.623
And we'll go through a couple of examples
of what this will look like soon.

00:12:59.292 --> 00:13:03.296
All right, now as I mentioned
in general machine learning,

00:13:03.296 --> 00:13:08.410
we'll only optimize the W here,
the parameters of our Softmax classifier.

00:13:10.112 --> 00:13:13.464
And hence our updates and
gradients will only be pretty small,

00:13:13.464 --> 00:13:16.825
so in many cases we only have you
know a handful of classes and

00:13:16.825 --> 00:13:21.601
maybe our word vectors are hundred so if
we have three classes and 100 dimensional

00:13:21.601 --> 00:13:25.733
word vectors we're trying to classify,
we'd only have 300 parameters.

00:13:27.837 --> 00:13:31.760
Now, in deep learning,
we have these amazing word vectors.

00:13:31.760 --> 00:13:35.600
And we actually will want to
learn not just the Softmax but

00:13:35.600 --> 00:13:37.090
also the word vectors.

00:13:37.090 --> 00:13:41.210
We can back propagate into them and
we'll talk about how to do that today.

00:13:41.210 --> 00:13:43.510
Hint, it's going to be taking derivatives.

00:13:44.570 --> 00:13:47.870
But the problem is when we update
word vectors, conceptually

00:13:47.870 --> 00:13:51.830
as you are thinking through this, you
have to realize this is very, very large.

00:13:51.830 --> 00:13:54.500
And now all of the sudden have a very
large set of parameters, right?

00:13:54.500 --> 00:13:57.788
Let's say your word vectors
are 300 dimensional you have,

00:13:57.788 --> 00:14:00.780
you know 10,000 words in your vocabulary.

00:14:00.780 --> 00:14:05.320
All of the sudden you have an immensely
large set of parameters so

00:14:05.320 --> 00:14:10.280
on this kind of plot you're going
to be very likely to overfit.

00:14:10.280 --> 00:14:14.600
And so before we dive into all this
optimization, I want you to get

00:14:14.600 --> 00:14:17.590
a little bit of an intuition of what
it means to update word vectors.

00:14:17.590 --> 00:14:20.180
So let's go through a very simple example

00:14:20.180 --> 00:14:22.880
where we might want to
classify single words.

00:14:22.880 --> 00:14:25.200
Again, it's not something
we'll do very often, but

00:14:25.200 --> 00:14:28.090
let's say you want to classify single
words as positive or negative.

00:14:29.220 --> 00:14:33.400
And let's say in our training data set we
have the word TV and telly and say you

00:14:33.400 --> 00:14:36.990
know this is movie reviews and if you
say this movie is better suited for TV.

00:14:36.990 --> 00:14:40.160
It's not a very positive thing to say
about a movie that's just coming out into

00:14:40.160 --> 00:14:41.420
movie theaters.

00:14:41.420 --> 00:14:44.860
And so we would assume that
in the beginning telly, TV,

00:14:44.860 --> 00:14:48.240
and television are actually all
close by in the vector space.

00:14:49.320 --> 00:14:54.080
We learn something with word2vec or
glove vectors and we train these word

00:14:54.080 --> 00:14:57.800
vectors on a very, very large corpus and
it learned all these three words appear

00:14:57.800 --> 00:15:01.910
often in a similar context, so
they are close by in the vector space.

00:15:01.910 --> 00:15:06.020
And now we're going to train but,
our smaller sentiment data set

00:15:07.200 --> 00:15:11.800
only includes in the training set, the X-i
Y-i as TV and telly and not television.

00:15:14.035 --> 00:15:17.115
So now what happens as we
train these word vectors?

00:15:17.115 --> 00:15:19.465
Well, they will start to move around.

00:15:19.465 --> 00:15:24.675
We'll project sentiment into them and
so you now might see telly and

00:15:24.675 --> 00:15:29.585
TV, that's a British dataset, so like to
move somewhere else into the vector space.

00:15:29.585 --> 00:15:32.229
But television actually stays
where it was in the beginning.

00:15:33.660 --> 00:15:36.220
And now when we want to test it,

00:15:36.220 --> 00:15:41.420
we would actually now misclassify this
word because it's never been moved.

00:15:41.420 --> 00:15:42.470
And so what does that mean?

00:15:42.470 --> 00:15:45.350
The take home message here will be that

00:15:45.350 --> 00:15:48.598
if you have only a very
small training dataset.

00:15:48.598 --> 00:15:52.664
That will allow you especially with these
deep models to overfit very quickly,

00:15:52.664 --> 00:15:55.110
you do not want to train
your word vectors.

00:15:55.110 --> 00:15:59.240
You want to keep them fixed,
you pre-trained them with nice Glove or

00:15:59.240 --> 00:16:01.430
word2vec models on a very large corpus or

00:16:01.430 --> 00:16:05.580
you just downloaded them from the cloud
website and you want to keep them fixed,

00:16:05.580 --> 00:16:08.860
cuz otherwise you will
not generalize as well.

00:16:08.860 --> 00:16:13.690
However, if you have a very large dataset
it may be better to train them in a way

00:16:13.690 --> 00:16:16.420
we're going to describe in
the next couple of slides.

00:16:16.420 --> 00:16:18.400
So, an example for
where you do that is, for instance,

00:16:18.400 --> 00:16:23.090
machine translation where you might have
many hundreds of Megabytes or Gigabytes of

00:16:23.090 --> 00:16:28.090
training data and you don't really need to
do much with the word vectors other than

00:16:28.090 --> 00:16:30.970
initialize them randomly, and then train
them as part of your overall objective.

00:16:30.970 --> 00:16:32.740
All right,

00:16:32.740 --> 00:16:36.380
any questions around generalization
capabilities of word vectors?

00:16:41.780 --> 00:16:45.380
All right, it might still be
magical how we're training this, so

00:16:45.380 --> 00:16:47.410
that's what we're gonna describe now.

00:16:48.990 --> 00:16:51.510
So, we rarely ever really
classify single words.

00:16:51.510 --> 00:16:55.210
Really what we wanna do is
classify words in their context.

00:16:55.210 --> 00:16:58.170
And there are a lot of fun and
interesting.

00:16:58.170 --> 00:17:01.860
Issues that arise in context really
that's where language begins and

00:17:01.860 --> 00:17:05.260
grammar and
the connection to meaning and so on.

00:17:05.260 --> 00:17:09.260
So here, a couple of fun examples of
where context is really necessary.

00:17:09.260 --> 00:17:13.130
So for instance, we have some words
that actually auto-antonyms, so

00:17:13.130 --> 00:17:14.780
they mean their own opposite.

00:17:14.780 --> 00:17:18.790
So for instance to sanction can
mean to permit or to punish.

00:17:18.790 --> 00:17:23.070
And it really depends on the context for
you to understand which one is meant, or

00:17:23.070 --> 00:17:26.580
to seed can mean to place seeds or
to remove seeds.

00:17:26.580 --> 00:17:29.920
So without the context, we wouldn't really
understand the meaning of these words.

00:17:31.140 --> 00:17:35.930
And in one of the examples that you'll see
a lot, which is named entity recognition,

00:17:35.930 --> 00:17:39.070
let's say we wanna find locations or
people names,

00:17:39.070 --> 00:17:41.480
we wanna identify is this the location or
not.

00:17:41.480 --> 00:17:46.690
You may also have things like Paris, which
could be Paris in France or Paris Hilton.

00:17:46.690 --> 00:17:48.390
And you might have Paris
staying in Paris and

00:17:48.390 --> 00:17:51.310
you still wanna understand
which one is which.

00:17:51.310 --> 00:17:55.330
Or if you wanna use deep learning for
financial trading and you see Hathaway,

00:17:55.330 --> 00:17:59.810
you wanna make sure that if it's just a
positive movie review from Anne Hathaway.

00:17:59.810 --> 00:18:03.000
You're not all the sudden buying
stocks from Berkshire Hathaway, right?

00:18:03.000 --> 00:18:05.660
And so,
there are a lot of issues that are fun and

00:18:05.660 --> 00:18:08.700
interesting and
complex that arise in context.

00:18:08.700 --> 00:18:13.400
And so, let's now carefully walk
through this first useful model,

00:18:13.400 --> 00:18:14.890
which is Window classification.

00:18:15.940 --> 00:18:20.420
So, we'll use as our first motivating
example here 4-class named

00:18:20.420 --> 00:18:24.460
entity recognition, where we basically
wanna identify a person or location or

00:18:24.460 --> 00:18:29.210
organization or none of the above for
every single word in a large corpus.

00:18:30.750 --> 00:18:33.230
And there are lots of different
possibilities that exist.

00:18:33.230 --> 00:18:35.670
But we'll basically look
at the following model.

00:18:35.670 --> 00:18:37.170
Which is actually quite
a reasonable model.

00:18:37.170 --> 00:18:39.660
And also one that started in 2008.

00:18:39.660 --> 00:18:44.190
So the first beginning by Collobert and
Weston, a great paper,

00:18:44.190 --> 00:18:48.740
to do the first kind of useful state
of the art Text classification and

00:18:48.740 --> 00:18:50.700
word classification context.

00:18:50.700 --> 00:18:55.940
So, what we wanna do is basically train a
softmax classifier by assigning a label to

00:18:55.940 --> 00:19:01.100
the center word and then concatenating all
the words in a window around that word.

00:19:01.100 --> 00:19:07.530
So, let's take for example this
subphrase here from a longer sentence.

00:19:07.530 --> 00:19:11.230
We basically wanna classify
the center word here which is Paris,

00:19:11.230 --> 00:19:13.580
in the context of this window.

00:19:13.580 --> 00:19:15.930
And we'll define the window length as 2.

00:19:15.930 --> 00:19:17.256
2 being 2 words to the left and

00:19:17.256 --> 00:19:20.675
2 words to the right of the current center
word that we're trying to classify.

00:19:23.375 --> 00:19:27.602
All right, so what we will do
is we'll define our new x for

00:19:27.602 --> 00:19:33.750
this whole window as the concatenation
of these five word vectors.

00:19:35.340 --> 00:19:38.820
And just in general

00:19:38.820 --> 00:19:42.194
throughout all of this lecture all my
vectors are going to be column vectors.

00:19:43.215 --> 00:19:46.995
Sadly in number two of the problem set,
they're row vectors.

00:19:46.995 --> 00:19:47.755
Sorry for that.

00:19:49.335 --> 00:19:54.415
Eventually, all these programming
frameworks they're actually row-wise first

00:19:54.415 --> 00:19:59.615
and so it's faster in the low-level
optimization to use row vectors.

00:19:59.615 --> 00:20:02.355
For a lot of the math it's actually I find
it simpler to think of them as column

00:20:02.355 --> 00:20:03.400
vectors so.

00:20:03.400 --> 00:20:07.650
We're very clear in the problem set but
don't get tripped up on that.

00:20:07.650 --> 00:20:13.750
So basically, we'll define this here as
one five D dimensional column vector.

00:20:13.750 --> 00:20:16.350
So, we have T dimensional word vectors,
we have five of them and

00:20:16.350 --> 00:20:20.160
we stack them up in one column, all right.

00:20:20.160 --> 00:20:25.270
Now, the simplest window classifier that
we could think of is to now just put

00:20:25.270 --> 00:20:30.920
the softmax on top of this
concatenation of five word vectors and

00:20:30.920 --> 00:20:33.710
we'll define this, our x here.

00:20:33.710 --> 00:20:37.090
Our inputs is just the x of the entire
window for this concatenation.

00:20:37.090 --> 00:20:39.960
And we have the softmax on top of that.

00:20:39.960 --> 00:20:42.720
And so, this is the same
notation that we used before.

00:20:42.720 --> 00:20:47.480
We're introducing here y hat,
with sadly the subscript y for

00:20:47.480 --> 00:20:48.840
the correct current class.

00:20:51.100 --> 00:20:54.435
It's tough, I went through [LAUGH] several
iterations, it's tough to have like

00:20:54.435 --> 00:20:57.590
prefect notation that works
through the entire lecture always.

00:20:57.590 --> 00:20:59.160
But you'll see why soon.

00:21:00.530 --> 00:21:04.790
So, our overall objective here is,
again, this whole sum over all

00:21:04.790 --> 00:21:10.020
these probabilities that we have,
or negative log of those.

00:21:10.020 --> 00:21:13.460
So now, the question is, how do we
update these word vectors x here?

00:21:13.460 --> 00:21:17.290
One x is a window, and
x is now deep inside the softmax.

00:21:18.760 --> 00:21:21.830
All right, well, the short answer
is we'll take a lot of derivatives.

00:21:21.830 --> 00:21:25.370
But the long answer is, you're gonna have
to do that a lot in problem set one and

00:21:25.370 --> 00:21:26.200
maybe in the midterm.

00:21:26.200 --> 00:21:31.090
So, let's be a little more helpful, and
actually go through some of the steps and

00:21:31.090 --> 00:21:32.320
give you some hints.

00:21:32.320 --> 00:21:35.230
So some of this, you'll actually
have to do in your problem set, so

00:21:35.230 --> 00:21:37.650
I'm not gonna go through all the details.

00:21:37.650 --> 00:21:42.270
But I'll give you a couple of hints
along the way and then you can know if

00:21:42.270 --> 00:21:46.060
you're hitting those and then you'll
see if you're on the right track.

00:21:46.060 --> 00:21:50.150
So, step one, always very
carefully define your variables,

00:21:50.150 --> 00:21:51.680
their dimensionality and everything.

00:21:51.680 --> 00:21:56.980
So, y hat will define as the softmax
probability of the vector.

00:21:56.980 --> 00:22:00.580
So, the normalized scores or
the probabilities for

00:22:00.580 --> 00:22:02.100
all the different classes that we have.

00:22:02.100 --> 00:22:04.410
So, in our case we have four.

00:22:05.920 --> 00:22:07.690
Then we have the target distribution.

00:22:07.690 --> 00:22:11.200
Again, that will be a one hot
vector where it's all zeroes except

00:22:11.200 --> 00:22:15.990
at the ground truth index of the class y,
where it's one.

00:22:15.990 --> 00:22:18.800
And we'll define our f
here as f of x again,

00:22:18.800 --> 00:22:20.430
which is this matrix multiplication.

00:22:20.430 --> 00:22:24.010
Which is going to be a C dimensional
vector where capital C is the number of

00:22:24.010 --> 00:22:28.240
classes that we have, all right.

00:22:28.240 --> 00:22:29.540
So, that was step one.

00:22:29.540 --> 00:22:32.660
Carefully define all of your variables and
keep track of their dimensionality.

00:22:32.660 --> 00:22:36.920
It's very easy when you implement this and
you multiply two things, and

00:22:36.920 --> 00:22:40.480
they have wrong dimensionality, and
you can't actually legally multiply them,

00:22:40.480 --> 00:22:41.270
you know you have a bug.

00:22:41.270 --> 00:22:43.500
And you can do this also
in a lot of your equations.

00:22:43.500 --> 00:22:44.650
You'd be surprised.

00:22:44.650 --> 00:22:46.840
In the midterm, you're nervous.

00:22:46.840 --> 00:22:48.820
But maybe at the end you have some time.

00:22:48.820 --> 00:22:51.990
And you could totally grade it
by yourself in the first pass,

00:22:51.990 --> 00:22:54.990
by just making sure that all your
dimensionality of your matrix and

00:22:54.990 --> 00:22:58.010
vector multiplications are correct.

00:22:58.010 --> 00:23:01.960
All right, the second tip is the chain
rule, we went over this before, but

00:23:01.960 --> 00:23:04.950
I heard there's a little bit of
confusion still in the office hours.

00:23:04.950 --> 00:23:09.160
So, let's define this carefully for
a simple example and then we'll go and

00:23:09.160 --> 00:23:12.340
give you a couple more hints also for
more complex example.

00:23:12.340 --> 00:23:15.770
So again, if you have something
very simple, such as a function y,

00:23:15.770 --> 00:23:20.760
which you can defined here as f of u and
u can be defined as g of

00:23:20.760 --> 00:23:25.740
x as in the whole function, y of x,
can be described as f of g of x,

00:23:25.740 --> 00:23:30.630
then you would basically multiply dy,
u times the udx.

00:23:30.630 --> 00:23:34.080
And so very concretely here,
this is sort of high school level,

00:23:34.080 --> 00:23:39.320
but we'll define it properly in
order to show the chain rule.

00:23:39.320 --> 00:23:42.570
So here,
you can basically define u as g(x),

00:23:42.570 --> 00:23:46.378
which is just the inside in
the parentheses here, so x cubed + 7.

00:23:46.378 --> 00:23:49.920
It can have y as a function of f(u),

00:23:49.920 --> 00:23:54.510
where we use 5 times u,
just replacing the inside definition here.

00:23:54.510 --> 00:23:57.750
So it's very simple,
just replacing things.

00:23:57.750 --> 00:24:00.700
And now, we can take the derivative
with respect to u and

00:24:00.700 --> 00:24:04.000
we can take the derivative
with respect to x(u).

00:24:04.000 --> 00:24:07.510
And then we just multiply these two terms,
and we plug in u again.

00:24:08.660 --> 00:24:12.110
So in that sense, we all know,
in theory, the chain rule.

00:24:12.110 --> 00:24:14.230
But, now we're gonna have the softmax, and

00:24:14.230 --> 00:24:16.890
we're gonna have lots of matrices and
so on.

00:24:16.890 --> 00:24:19.810
So, we have to be very,
very careful about our notation.

00:24:20.840 --> 00:24:22.890
And we also have to be
careful about understanding,

00:24:22.890 --> 00:24:28.280
which parameters appear inside
what other higher level elements.

00:24:28.280 --> 00:24:30.630
So, f for instance is a function of x.

00:24:30.630 --> 00:24:34.440
So, if you're trying to take
a derivative with respect to x,

00:24:34.440 --> 00:24:38.440
of this overall soft max you're gonna have
to sum over all of the different classes

00:24:38.440 --> 00:24:39.874
inside which x appears.

00:24:41.265 --> 00:24:43.845
And you'll see here,
this first application, but

00:24:43.845 --> 00:24:48.745
not just of fy again this is just
a subscript the y element of the effector

00:24:48.745 --> 00:24:53.205
which is the function of x, but
also multiply it then here by this.

00:24:54.420 --> 00:24:59.560
So, when you write this out,
another tip that can be helpful is for

00:24:59.560 --> 00:25:03.470
this softmax part of he derivative
is to actually think of two cases.

00:25:03.470 --> 00:25:05.489
One where c = y, the correct class,

00:25:05.489 --> 00:25:09.890
and one where it's basically all
the other incorrect classes.

00:25:09.890 --> 00:25:14.200
And as you write this out,
you will observe and

00:25:14.200 --> 00:25:15.920
come up with something like this.

00:25:15.920 --> 00:25:19.210
So, don't just write that as your thing
you have to put in your problems,

00:25:19.210 --> 00:25:21.690
the steps on how to get there.

00:25:21.690 --> 00:25:25.980
Bur, basically at some point you
observe this kinda pattern when you now

00:25:25.980 --> 00:25:29.650
try to look at all the derivatives
with respect to all the elements of f.

00:25:30.910 --> 00:25:33.590
And now,
when you have this you realize ,okay at

00:25:33.590 --> 00:25:35.840
the correct class we're
actually subtracting one here,

00:25:35.840 --> 00:25:38.760
and all the incorrect classes,
you will not do anything.

00:25:40.140 --> 00:25:42.270
Now, the problem is when
you implement this,

00:25:42.270 --> 00:25:44.375
it kind of looks like
a bunch of if statements.

00:25:44.375 --> 00:25:47.110
If y equals the correct class for

00:25:47.110 --> 00:25:51.360
my training set, then, subtract 1,
that's not gonna be very efficient.

00:25:51.360 --> 00:25:54.576
Also, you're gonna go insane if you try
to actually write down equations for

00:25:54.576 --> 00:25:56.714
more complex neural network
architectures ever.

00:25:56.714 --> 00:26:01.110
And so, instead, what we wanna do is
always try to vectorize a lot of our

00:26:01.110 --> 00:26:03.840
notation, as well as our implementation.

00:26:05.110 --> 00:26:07.430
And so, what this means here,
in this case,

00:26:07.430 --> 00:26:10.720
is you can actually observe that,
well, this 1 is exactly 1,

00:26:10.720 --> 00:26:15.570
where t, our hot to target distribution,
also happens to be 1.

00:26:15.570 --> 00:26:20.605
And so, what you're gonna wanna do,
is basically

00:26:20.605 --> 00:26:27.200
describe this as y(hat)- t, so
it's the same thing as this.

00:26:27.200 --> 00:26:28.970
And don't worry if you don't
understand how we got there,

00:26:28.970 --> 00:26:31.290
cuz that's part of your problem set.

00:26:31.290 --> 00:26:32.910
You have to, at some point,

00:26:32.910 --> 00:26:36.360
see this equation while you're
taking those derivatives.

00:26:36.360 --> 00:26:41.690
And now, the very first baby step towards
back-propagation is actually to define

00:26:41.690 --> 00:26:46.100
this term, in terms of a simpler single
variable and we'll call this delta.

00:26:47.350 --> 00:26:50.742
We'll get good, we'll become good friends
with deltas because they are sort of our

00:26:50.742 --> 00:26:51.422
error signals.

00:26:51.422 --> 00:26:55.460
Now, the last couple of tips.

00:26:55.460 --> 00:26:56.730
Tip number six.

00:26:56.730 --> 00:27:01.000
When you start with this chain rule, you
might want to sometimes use explicit sums,

00:27:01.000 --> 00:27:03.860
before and
look at all the partial derivatives.

00:27:03.860 --> 00:27:06.560
And if you do that a couple of times
at some point you see a pattern, and

00:27:06.560 --> 00:27:11.360
then you try to think of how to
extrapolate from those patterns of

00:27:11.360 --> 00:27:16.010
single partial derivatives,
into vector and matrix notation.

00:27:16.010 --> 00:27:21.700
So, for example,
you'll see something like this here,

00:27:21.700 --> 00:27:25.215
in at some point in your derivation.

00:27:25.215 --> 00:27:31.420
S,o the overall derivative with respect to
x of our overall objective function for

00:27:31.420 --> 00:27:37.270
one element, for one element from our
training set x and y is this sum.

00:27:37.270 --> 00:27:39.670
And it turns out when you
think about this for a while,

00:27:39.670 --> 00:27:44.840
you take here this row vector but
then you transpose it,

00:27:44.840 --> 00:27:49.240
and becomes an inner product, well if you
do that multiple times for all the C's and

00:27:49.240 --> 00:27:53.703
you wanna get in the end a whole vector
out, it turns out you can actually just

00:27:53.703 --> 00:27:58.320
re-write the sum as W
transpose* the delta.

00:27:58.320 --> 00:28:03.208
So, this is one error signal here
that we got from our softmax,

00:28:03.208 --> 00:28:08.200
and we multiply the transpose of
our softmax weights with this.

00:28:08.200 --> 00:28:10.010
And again,
if some of these are not clear and

00:28:10.010 --> 00:28:11.910
you're confused,
write them out into full sum,

00:28:11.910 --> 00:28:16.160
and then you'll see that it's really
just re-write this in vector notation.

00:28:16.160 --> 00:28:22.700
All right, now what is the dimensionality
of the window vector gradient?

00:28:22.700 --> 00:28:28.080
So in the end, we have this derivative
of the overall cost here for

00:28:28.080 --> 00:28:30.150
one element of our training
set with respect to x.

00:28:30.150 --> 00:28:30.960
But x is a window.

00:28:32.130 --> 00:28:36.010
All right, so
each say we have a window of five words.

00:28:36.010 --> 00:28:39.710
And each word is d-dimensional.

00:28:39.710 --> 00:28:43.841
Now, what should be the dimensionality
of this derivative of this gradient?

00:28:55.660 --> 00:28:58.360
That's right,
it's five times the dimensionality.

00:28:58.360 --> 00:29:02.490
And that's another really good way, and
one of the reasons we make you implement

00:29:02.490 --> 00:29:07.870
this from scratch, if you have any kinda
parameter, and you have a gradient for

00:29:07.870 --> 00:29:11.340
that parameter, and they're not the same
dimensionality, you'll also know you

00:29:11.340 --> 00:29:15.570
screwed up and there's some mistake or
bug in either your code or your map.

00:29:15.570 --> 00:29:18.680
So, it's very simple debugging skill.

00:29:19.780 --> 00:29:22.560
And way to check your own equations.

00:29:22.560 --> 00:29:26.750
So, the final derivative with respect
to this window is now this five

00:29:26.750 --> 00:29:29.850
vector because we had five d-dimensional
vectors that we concatenated.

00:29:29.850 --> 00:29:32.865
Now, of course the tricky bit is,

00:29:32.865 --> 00:29:35.825
you actually wanna update your word
vectors and not the whole window, right?

00:29:35.825 --> 00:29:38.445
The window is just this
intermediate step also.

00:29:38.445 --> 00:29:40.965
So really, what you wanna do is update and

00:29:40.965 --> 00:29:44.475
take derivatives with respect to each
of the elements of your word vectors.

00:29:45.535 --> 00:29:51.035
And so it turns out, very simply,
that can be done by just splitting

00:29:51.035 --> 00:29:56.650
that error that you've got on the gradient
overall, at the whole window and that's

00:29:56.650 --> 00:30:02.500
just basically the concatenation of the
reduced of all the different word vectors.

00:30:02.500 --> 00:30:06.446
And those you can use to update your word
vectors, as you train the whole system.

00:30:06.446 --> 00:30:10.590
All right, any questions?

00:30:19.024 --> 00:30:19.902
Is there a mathematical what?

00:30:19.902 --> 00:30:24.664
Is there a mathematical notation for
the word vector t,

00:30:24.664 --> 00:30:27.690
other than it's just variable t?

00:30:27.690 --> 00:30:29.960
Or that seems like a fine notation.

00:30:31.350 --> 00:30:34.500
You can see this as a probability
distribution, that is very peaked.

00:30:36.080 --> 00:30:39.400
&gt;&gt; Yeah.
&gt;&gt; That's all, there's nothing else to it.

00:30:39.400 --> 00:30:41.900
Just a single vector with all zeroes,
except in one location.

00:30:41.900 --> 00:30:44.370
&gt;&gt; So I'll just write that down?

00:30:44.370 --> 00:30:45.370
&gt;&gt; You can write that up, yeah.

00:30:45.370 --> 00:30:49.060
You can always just write out and
it's also something very important.

00:30:49.060 --> 00:30:54.630
You always wanna define everything, so
that you make sure that the TAs know that

00:30:54.630 --> 00:30:57.950
you're thinking about the right thing,
as you're writing out your derivatives,

00:30:57.950 --> 00:31:00.470
you write out the dimensionality,
you define them properly,

00:31:00.470 --> 00:31:03.460
you can use dot, dot,
dot if it's a larger dimensional vector.

00:31:03.460 --> 00:31:12.330
You can just define t as your
target distribution [INAUDIBLE]

00:31:12.330 --> 00:31:13.200
&gt;&gt; The question is,

00:31:13.200 --> 00:31:15.010
do we still have two vectors for
each word?

00:31:15.010 --> 00:31:16.620
Great question, no.

00:31:16.620 --> 00:31:21.100
We essentially, when we did glove and
word2vec, and had these two u's and v's,

00:31:21.100 --> 00:31:25.510
for all subsequent lectures from now on,
we'll just assume we have the sum of u and

00:31:25.510 --> 00:31:27.600
v and that's our single vector x,
for each word.

00:31:34.310 --> 00:31:36.900
So, the question is does this gradient
appear in lots of other windows

00:31:36.900 --> 00:31:37.620
and it does.

00:31:37.620 --> 00:31:39.620
So, if you, the answer is yes.

00:31:39.620 --> 00:31:44.820
If you have the word "in," that vector
here and the gradients will appear

00:31:44.820 --> 00:31:49.690
in all the windows that have
the word "in" inside of them.

00:31:49.690 --> 00:31:51.130
And same with museums and so on.

00:31:51.130 --> 00:31:54.507
And so as you do stochastic gradient
descent you look at one window at a time,

00:31:54.507 --> 00:31:57.682
you update it, then you go to the next
window, you update it and so on.

00:31:57.682 --> 00:31:59.218
Great questions.

00:32:01.663 --> 00:32:03.060
All right.

00:32:04.294 --> 00:32:10.144
Now, let's look at how we update
these concatenated word vectors.

00:32:10.144 --> 00:32:12.797
So basically, as we're training this,
if we train it for

00:32:12.797 --> 00:32:16.427
instance with sentiment we'll push all
the positive words in one direction and

00:32:16.427 --> 00:32:18.460
the other words in other direction.

00:32:18.460 --> 00:32:22.590
If we train it, for
named entity recognition and

00:32:22.590 --> 00:32:26.930
eventually our model can learn that seeing
something like in as the word just before

00:32:26.930 --> 00:32:30.670
the center word, would be indicative for
that center word to be a location.

00:32:32.720 --> 00:32:36.480
So now what's missing for
training this full window model?

00:32:36.480 --> 00:32:41.580
Well mainly the gradient of J with
respect to the softmax weights W.

00:32:43.280 --> 00:32:45.897
And so
we basically will take similar steps.

00:32:45.897 --> 00:32:48.757
We'll write down all the partial
derivatives with respect to Wij

00:32:48.757 --> 00:32:49.485
first and so on.

00:32:49.485 --> 00:32:53.060
And then we have our full gradient for
this entire model.

00:32:53.060 --> 00:32:55.395
And again, this will be very sparse, and

00:32:55.395 --> 00:33:00.292
you're gonna wanna have some clever ways
of implementing these word vector updates.

00:33:00.292 --> 00:33:05.741
So you don't send a bunch of zeros
around at every single window,

00:33:05.741 --> 00:33:09.319
Cuz each window will
only have a few words.

00:33:09.319 --> 00:33:13.819
So in fact, it's so important for
your code in the problem set to think

00:33:13.819 --> 00:33:17.102
carefully through your
matrix implementations,

00:33:17.102 --> 00:33:20.555
that it's worth to spend two or
three slides on this.

00:33:20.555 --> 00:33:25.720
So there are essentially two very
expensive operations in the softmax.

00:33:25.720 --> 00:33:28.245
The matrix multiplication and
the exponent.

00:33:28.245 --> 00:33:34.092
Actually later in the lecture, we'll
find a way to deal with the exponent.

00:33:34.092 --> 00:33:40.440
But the matrix multiplication can also
be implemented much more efficiently.

00:33:40.440 --> 00:33:43.516
So you might be tempted in the beginning
to think this is probability for

00:33:43.516 --> 00:33:45.740
this class and
this is the probability for that class.

00:33:45.740 --> 00:33:49.257
And so implemented a for
loop of all my different classes and

00:33:49.257 --> 00:33:53.631
then I'll take derivatives or
matrix multiplications one row at a time.

00:33:53.631 --> 00:33:57.790
And that is going to be very,
very inefficient.

00:33:57.790 --> 00:34:02.260
So let's go through some very simple
Python code here to show you what I mean.

00:34:02.260 --> 00:34:05.654
So essentially,
always looping over these word vectors

00:34:05.654 --> 00:34:09.380
instead of concatenating
everything into one large matrix.

00:34:09.380 --> 00:34:13.680
And then multiplying these is
always going to be more efficient.

00:34:13.680 --> 00:34:18.879
So let's assume we have 500
windows that we want to classify,

00:34:18.879 --> 00:34:23.900
and let's assume each window
has a dimensionality of 300.

00:34:23.900 --> 00:34:26.594
These are reasonable numbers, and

00:34:26.594 --> 00:34:30.465
let's assume we have five
classes in our softmax.

00:34:30.465 --> 00:34:34.610
And so at some point during
the computation, we now have two options.

00:34:34.610 --> 00:34:36.166
So W here are weights for the softmax.

00:34:36.166 --> 00:34:40.686
It's gonna be C many rows and
d many columns.

00:34:40.686 --> 00:34:44.131
Now the word vectors here that
you concatenated for each window.

00:34:44.131 --> 00:34:48.168
We can either have the list of
a bunch of separate word vectors,

00:34:48.168 --> 00:34:52.350
or we can have one large matrix
that's going to be d times n.

00:34:52.350 --> 00:34:55.622
So d many rows and n many windows.

00:34:55.622 --> 00:35:01.260
So we have 500 windows, so
we have 500 columns here in this 1 matrix.

00:35:01.260 --> 00:35:07.416
And now essentially, we can multiply
the W here for each vector separately,

00:35:07.416 --> 00:35:11.786
or we can do this one matrix
multiplication entirely.

00:35:11.786 --> 00:35:16.890
And you literally have
a 12x speed difference.

00:35:16.890 --> 00:35:19.791
And sadly with these larger models,
one iteration or

00:35:19.791 --> 00:35:24.250
something might take a day, eventually for
more complex models large data sets.

00:35:24.250 --> 00:35:26.842
So the difference is between
literally 12 days or

00:35:26.842 --> 00:35:30.230
1 day of you iterating and
making your deadlines and everything.

00:35:31.260 --> 00:35:35.155
So it's super important,
and now sometimes people

00:35:35.155 --> 00:35:39.855
are tripped up by what does it
mean to multiply and do this here.

00:35:39.855 --> 00:35:44.534
Essentially, it's the same
thing that we've done here for

00:35:44.534 --> 00:35:49.625
one softmax, but
what we did is we actually concatenated.

00:35:51.727 --> 00:35:55.647
A lot of different input vectors x, and so

00:35:55.647 --> 00:36:01.950
we'll get a lot of different
unnormalized scores out at the end.

00:36:01.950 --> 00:36:04.620
And then we can tease them apart again for
them.

00:36:04.620 --> 00:36:10.241
So you have here, c times t dimensional
matrix for the d dimensional input.

00:36:10.241 --> 00:36:13.003
So using the same notation, yeah,

00:36:13.003 --> 00:36:18.724
dimensional of each window times d times
n matrix to get a c times n matrix.

00:36:18.724 --> 00:36:24.172
So these are all
the probabilities here for

00:36:24.172 --> 00:36:28.111
your N many training samples.

00:36:30.920 --> 00:36:31.486
Any questions around that?

00:36:31.486 --> 00:36:38.099
So it's super important, all your code
will be way too slow if you don't do this.

00:36:38.099 --> 00:36:42.738
And so
this is very much an implementation trick.

00:36:42.738 --> 00:36:44.775
And so in most of the equations,

00:36:44.775 --> 00:36:49.912
we're not gonna actually go there cuz
that makes everything more complicated.

00:36:49.912 --> 00:36:53.329
And the equations look at only
a singular example at a time, but

00:36:53.329 --> 00:36:56.374
in the end you're gonna wanna
vectorize all your code.

00:37:00.286 --> 00:37:04.412
Yeah, matrices are your friend,
use them as much as you can.

00:37:04.412 --> 00:37:09.110
Also in many cases, especially for
this problem set where you really

00:37:09.110 --> 00:37:13.978
understand the nuts and bolts of how
to train and optimize your models.

00:37:13.978 --> 00:37:17.150
You will come across a lot
of different choices.

00:37:17.150 --> 00:37:19.396
It's like,
I could implement it this way or that way.

00:37:19.396 --> 00:37:22.058
And you can go to your TA and ask,
should I implement this way or that way?

00:37:22.058 --> 00:37:28.200
But you can also just use time it
as your magic Python and just let,

00:37:28.200 --> 00:37:33.908
make a very informed decision and
gain intuition yourself.

00:37:33.908 --> 00:37:37.565
And just basically wanna
speed test a lot of different

00:37:37.565 --> 00:37:41.070
options that you have in
your code a lot of the time.

00:37:43.210 --> 00:37:47.878
All right, so
this is was just a pure softmax,

00:37:47.878 --> 00:37:52.683
and now the softmax alone
is not play powerful.

00:37:52.683 --> 00:37:56.805
Because it really only gets with this
linear decision boundaries in your

00:37:56.805 --> 00:37:57.827
original space.

00:37:57.827 --> 00:38:01.820
If you have very, very little
training data that could be okay, and

00:38:01.820 --> 00:38:06.255
you kind of used a not so powerful model
almost as an abstract regularizer.

00:38:06.255 --> 00:38:08.895
But with more data,
it's actually quite limiting.

00:38:08.895 --> 00:38:13.483
So if we have here a bunch of words and
we don't wanna update our word vectors,

00:38:13.483 --> 00:38:18.230
softmax would only give us this linear
decision boundary which is kind of lame.

00:38:18.230 --> 00:38:21.263
And it would be way better if we could

00:38:21.263 --> 00:38:25.840
correctly classify these
points here as well.

00:38:25.840 --> 00:38:30.630
And so basically, this is one of the many
motivations for using neural networks.

00:38:30.630 --> 00:38:35.135
Cuz neural networks will give us much
more complex decision boundaries and

00:38:35.135 --> 00:38:39.080
allow us to fit much more complex
functions to our training data.

00:38:40.690 --> 00:38:42.142
And you could be snarky and

00:38:42.142 --> 00:38:45.511
actually rename neural networks
which sounds really cool.

00:38:45.511 --> 00:38:46.939
It's just general function approximators.

00:38:46.939 --> 00:38:53.690
Just wouldn't have quite the same ring to
it, but it's essentially what they are.

00:38:53.690 --> 00:38:58.201
So let's define how we get from
the symbol of logistic regression to

00:38:58.201 --> 00:39:01.700
a neural network and beyond,
and deep neural nets.

00:39:01.700 --> 00:39:03.707
So let's demystify the whole
thing by starting,

00:39:03.707 --> 00:39:05.388
defining again some of the terminology.

00:39:05.388 --> 00:39:10.530
And we can have more fun with the math,
and then one and a half lectures from now.

00:39:10.530 --> 00:39:12.073
We can just basically use
all of these Lego blocks.

00:39:12.073 --> 00:39:15.220
So bear with me,
this is going to be tough.

00:39:15.220 --> 00:39:19.528
And try to concentrate and
ask questions if you have any,

00:39:19.528 --> 00:39:25.744
cuz we'll keep building now a pretty
awesome large model that's really useful.

00:39:25.744 --> 00:39:30.248
So we'll have inputs, we'll have
a bias unit, we'll have an activation

00:39:30.248 --> 00:39:34.690
function and output for each single
neuron in our larger neuron network.

00:39:36.930 --> 00:39:39.931
So let's define a single neuron first.

00:39:39.931 --> 00:39:44.450
Basically, you can see it as
a binary logistic regression unit.

00:39:44.450 --> 00:39:47.082
We're going to have inside,

00:39:47.082 --> 00:39:52.160
again a set of weights that we
have in a product with our input.

00:39:52.160 --> 00:39:55.030
So we have the input x
here to this neuron.

00:39:55.030 --> 00:39:56.692
And in the end,
we're going to add a bias term.

00:39:56.692 --> 00:39:58.912
So we have an always on feature, and

00:39:58.912 --> 00:40:02.828
that kind of defines how likely
should this neuron fire.

00:40:02.828 --> 00:40:07.440
And by firing, I mean have a very
high probability that's close to one.

00:40:07.440 --> 00:40:08.690
For being on.

00:40:08.690 --> 00:40:14.630
And f here is always, from now on,
going to be this element wise function.

00:40:14.630 --> 00:40:20.693
In our case here the sigmoid that just
squashes whatever this sum gives us in our

00:40:20.693 --> 00:40:26.305
product plus the bias term and basically
just squashes it to be between 0 and 1.

00:40:26.305 --> 00:40:28.780
All right, so this is the definition
of the single neuron.

00:40:30.810 --> 00:40:34.750
Now if we feed a vector of inputs through
all this different little logistic

00:40:34.750 --> 00:40:38.450
regression functions and
neurons, we get this output.

00:40:38.450 --> 00:40:43.530
And now the main difference between
just predicting directly a softmax and

00:40:43.530 --> 00:40:45.090
standard machine learning and

00:40:45.090 --> 00:40:50.530
deep learning is that we'll actually not
force this to give directly the output.

00:40:50.530 --> 00:40:54.780
But they will themselves be inputs to yet
another neuron.

00:40:56.800 --> 00:41:01.470
And it's a loss function on top of that
neuron such as cross entropy that will

00:41:01.470 --> 00:41:06.260
now govern what these
intermediate hidden neurons.

00:41:06.260 --> 00:41:09.670
Or in the hidden layer what they
will actually try to achieve.

00:41:09.670 --> 00:41:13.320
And the model can decide itself
what it should represent,

00:41:13.320 --> 00:41:17.890
how it should transform this input
inside these hidden units here

00:41:17.890 --> 00:41:22.225
in order to give us a lower
error at the final output.

00:41:23.970 --> 00:41:27.780
And it's really just this
concatenation of these hidden neurons,

00:41:27.780 --> 00:41:30.480
these little binary
logistic regression units

00:41:30.480 --> 00:41:34.810
that will allow us to build very
deep neural network architectures.

00:41:37.280 --> 00:41:43.060
Now again, for sanity's sake, we're
going to have to use matrix notation cuz

00:41:43.060 --> 00:41:48.030
all of this can be very simply described
in terms of matrix multiplication.

00:41:48.030 --> 00:41:52.240
So a1 here is where going to be the final

00:41:52.240 --> 00:41:56.410
activation of the first neuron,
a2 in second neuron and so on.

00:41:56.410 --> 00:42:00.770
So instead of writing out the inner
product here, or writing even this

00:42:00.770 --> 00:42:06.010
as an inner product plus the bias term
we're going to use matrix notation.

00:42:06.010 --> 00:42:10.620
And it's very important now to pay
attention to this intermediate variables

00:42:10.620 --> 00:42:12.610
that we'll define because
we'll see these over and

00:42:12.610 --> 00:42:16.520
over again as we use a chain
rule to take derivatives.

00:42:17.870 --> 00:42:24.260
So we'll define z here as W
times x plus the bias vector.

00:42:24.260 --> 00:42:28.730
So we'll basically have here as
many bias terms and this vector has

00:42:28.730 --> 00:42:33.180
the same dimensionality as the number
of neurons that we have in this layer.

00:42:34.760 --> 00:42:39.620
And W will have number of rows for
the number of neurons that we have

00:42:39.620 --> 00:42:44.330
times number of columns for
the input dimensionality of x.

00:42:44.330 --> 00:42:47.280
And then, whenever we write a of f(z),

00:42:47.280 --> 00:42:50.570
what that means here is that we'll
actually apply f element wise.

00:42:52.380 --> 00:42:59.099
So f(z) when z is a vector is just f(z1),
f(z2) and f(z3).

00:42:59.099 --> 00:43:03.510
And now you might ask, well, why do we
have all this added complexity here

00:43:03.510 --> 00:43:07.090
with this sigmoid function.

00:43:07.090 --> 00:43:10.580
Later on we can actually have other
kinds of so called non linearities.

00:43:10.580 --> 00:43:13.890
This f function and
it turns out that if we don't have

00:43:13.890 --> 00:43:17.250
the non-linearities in between and
we will just stack a couple of

00:43:17.250 --> 00:43:20.980
this linear layers together it wouldn't
add a very different function.

00:43:20.980 --> 00:43:24.870
In fact it would be continuing to
just be a single linear function.

00:43:25.950 --> 00:43:29.580
And intuitively as you
have more hidden neurons,

00:43:29.580 --> 00:43:31.960
you can fit more and
more complex functions.

00:43:31.960 --> 00:43:34.930
So this is like a decision boundary
in a three dimensional space,

00:43:34.930 --> 00:43:37.710
you can think of it also in
terms of simple regression.

00:43:37.710 --> 00:43:39.730
If you had just a single hidden neuron,

00:43:39.730 --> 00:43:42.428
you kinda see here almost
an inverted sigmoid.

00:43:42.428 --> 00:43:46.454
If you have three hidden neurons,
you could fit this kind of more complex

00:43:46.454 --> 00:43:50.481
functions and with ten neurons,
each neuron can start to essentially,

00:43:50.481 --> 00:43:53.862
over fit and try to be very good
at fitting exactly one point.

00:43:56.584 --> 00:44:00.658
All right, now let's revisit our
single window classifier and

00:44:00.658 --> 00:44:05.562
instead of slapping a softmax directly
onto the word vectors we're now going

00:44:05.562 --> 00:44:10.960
to have an intermediate hidden layer
between the word vectors and the output.

00:44:10.960 --> 00:44:15.820
And that's when we really start to
gain an accuracy and expressive power.

00:44:17.380 --> 00:44:21.040
So let's define a single
layer neural network.

00:44:24.190 --> 00:44:26.900
We have our input x that will be again,

00:44:26.900 --> 00:44:31.160
our window, the concatenation
of multiple word vectors.

00:44:31.160 --> 00:44:36.050
We'll define z and we'll define a as
element wise on the areas a and z.

00:44:37.580 --> 00:44:43.220
And now, we can use this
neural activation vector a as

00:44:43.220 --> 00:44:47.540
input to our final classification layer.

00:44:48.660 --> 00:44:51.660
The default that we've had so
far was the softmax, but

00:44:51.660 --> 00:44:53.100
let's not rederive the softmax.

00:44:53.100 --> 00:44:56.220
We've done it multiple times now,
you'll do it again in a problem set and

00:44:56.220 --> 00:44:59.010
introduce an even simpler one and

00:44:59.010 --> 00:45:03.680
walk through all the glory details
of that simple classifier.

00:45:03.680 --> 00:45:06.760
And that will be a simple,
unnormalized score.

00:45:06.760 --> 00:45:12.002
And this case here, this will
essentially be the right mechanism for

00:45:12.002 --> 00:45:13.930
various simple binary
classification problems,

00:45:13.930 --> 00:45:17.929
where you don't even care that much
about this probability z is 0.8.

00:45:17.929 --> 00:45:21.530
You really just cares like, is it one,
is it in this class, or is it not?

00:45:23.170 --> 00:45:28.830
And so we'll define the objective function
for this new output layer in a second.

00:45:28.830 --> 00:45:31.400
Well, let's first understand
the feed-forward process.

00:45:31.400 --> 00:45:35.680
And well feed-forward process is what you
will end up using a test time and for

00:45:35.680 --> 00:45:38.735
each element also in training
before you can take derivative.

00:45:38.735 --> 00:45:43.640
Always be feed-forward and
then backward to take the derivatives.

00:45:43.640 --> 00:45:45.980
So what we wanna do here is for

00:45:45.980 --> 00:45:50.860
example, take basically each window and
then score it.

00:45:50.860 --> 00:45:54.870
And say if the score is high we want to
train the model such that it would assign

00:45:54.870 --> 00:46:01.110
high scores to windows where the center
word is a named entity location.

00:46:01.110 --> 00:46:06.610
Such as Paris, or London, or Germany,
or Stanford, or something like that.

00:46:06.610 --> 00:46:09.145
Now we will often use and

00:46:09.145 --> 00:46:14.560
you'll see a in a lot of papers this kind
of graph, so it's good to get used to it.

00:46:14.560 --> 00:46:17.480
There are various other kinds,
and we'll try to introduce them

00:46:17.480 --> 00:46:20.750
slowly throughout the lecture but
this is the most common one.

00:46:20.750 --> 00:46:26.420
So we'll define bottom up,
what each of these layers will do and

00:46:26.420 --> 00:46:30.360
then we'll take the derivatives and
learn how to optimize it.

00:46:31.800 --> 00:46:35.820
Now x window here is the concatenation
of all our word vectors.

00:46:35.820 --> 00:46:39.110
So let's hear, and
I'll ask you a question in a second,

00:46:39.110 --> 00:46:42.900
let's try to figure out the dimensionality
here of all our parameters so that you're,

00:46:42.900 --> 00:46:44.950
I know you're with me.

00:46:44.950 --> 00:46:49.300
So let's say each of our word
vectors here is four dimensional and

00:46:49.300 --> 00:46:52.600
we have five of these word vectors in
each window that are concatenated.

00:46:52.600 --> 00:46:56.110
So x is a 20 dimensional vector.

00:46:57.290 --> 00:46:59.570
And again,
we'll define it as column vectors.

00:47:00.590 --> 00:47:03.900
And then lets say we have
in our first hidden layer,

00:47:03.900 --> 00:47:05.930
lets say we have eight units here.

00:47:05.930 --> 00:47:11.368
So you want an eight unit hidden layer
as our intermediate representation.

00:47:11.368 --> 00:47:15.774
And then our final scores just
again a simple single number.

00:47:15.774 --> 00:47:21.600
Now what's the dimensionality
of our W given what I just said?

00:47:21.600 --> 00:47:25.393
20 dimensional input, eight hidden units.

00:47:31.618 --> 00:47:33.490
20 rows and eight columns.

00:47:34.980 --> 00:47:36.810
We have one more transfer,
[LAUGH] that's right.

00:47:38.140 --> 00:47:41.590
So it's going to be eight rows and
20 columns, right?

00:47:41.590 --> 00:47:44.720
And you can always
whenever you're unsure and

00:47:44.720 --> 00:47:48.830
you have something like this then
this will have some n times d.

00:47:48.830 --> 00:47:53.820
And then multiply this and then this
will have, this will always be d,

00:47:53.820 --> 00:47:55.910
and so these two always
have to be the same, right?

00:47:55.910 --> 00:48:00.140
So all right, now

00:48:00.140 --> 00:48:03.580
what's the main intuition behind this
extra layer, especially for NLP?

00:48:03.580 --> 00:48:06.380
Well, that will allow
us to learn non-linear

00:48:06.380 --> 00:48:08.640
interactions between these
different input words.

00:48:08.640 --> 00:48:12.590
Whereas before, we could only say
well if in appears in this location,

00:48:12.590 --> 00:48:17.820
always increase the probability
that the next word is a location.

00:48:17.820 --> 00:48:23.590
Now we can learn things and patterns like,
if in is in the second position, increase

00:48:23.590 --> 00:48:28.480
the probability of this being the location
only if museum is also the first vector.

00:48:28.480 --> 00:48:31.770
So we can learn interactions
between these different inputs.

00:48:31.770 --> 00:48:34.535
And now we'll eventually make
our model more accurate.

00:48:43.528 --> 00:48:44.170
Great question.

00:48:44.170 --> 00:48:45.710
So do I have a second W there.

00:48:45.710 --> 00:48:50.370
So the second layer here the scores
are unnormalized, so it'll just be U and

00:48:50.370 --> 00:48:54.810
because we just have a single U, this will
just be a single column vector and we'll

00:48:54.810 --> 00:48:59.480
transpose that to get our inner product
to get a single number out for the score.

00:48:59.480 --> 00:49:01.942
Sorry, yeah, so the question was
do we have a second W vector.

00:49:04.068 --> 00:49:07.092
So yeah, that's in some
sense our second matrix, but

00:49:07.092 --> 00:49:11.713
because we only have one hidden neuron in
that layer, we only need a single vector.

00:49:16.575 --> 00:49:17.386
Wonderful.

00:49:17.386 --> 00:49:21.120
All right, so,
now let's define the max-margin loss.

00:49:21.120 --> 00:49:26.210
It's actually a super powerful loss
function often is even more robust

00:49:26.210 --> 00:49:31.310
than the cross entropy error in softmax,
and is quite powerful and useful.

00:49:31.310 --> 00:49:35.640
So let's define here two examples.

00:49:35.640 --> 00:49:40.260
Basically, you want to give
a high score to windows,

00:49:40.260 --> 00:49:42.730
where the center word is a location.

00:49:42.730 --> 00:49:46.132
And we wanna give low scores to corrupt or

00:49:46.132 --> 00:49:50.770
incorrect windows where the center
word is not a named entity location.

00:49:50.770 --> 00:49:54.450
So museum is technically a location,
but it's not a named entity location.

00:49:55.790 --> 00:49:56.889
And so the idea for

00:49:56.889 --> 00:50:01.876
this training objective of max-margin is
to essentially try to make the score of

00:50:01.876 --> 00:50:07.660
the true windows larger than the ones of
the corrupt windows smaller or lower.

00:50:07.660 --> 00:50:08.640
Until they're good enough.

00:50:08.640 --> 00:50:14.710
And we define good enough as being
different by the value of one.

00:50:14.710 --> 00:50:16.020
And this one here is a margin.

00:50:16.020 --> 00:50:18.190
You can often see it as
a hyperparameter too and

00:50:18.190 --> 00:50:22.540
set it to m and try different ones but
in many cases one works fine.

00:50:22.540 --> 00:50:25.480
This is continuous and
we'll be able to use SGD.

00:50:25.480 --> 00:50:32.270
So now what's the intuition behind the
softmax, sorry the max-margin loss here?

00:50:32.270 --> 00:50:35.320
If you have for
instance a very simple data set and

00:50:35.320 --> 00:50:38.390
you have here a couple
of training samples.

00:50:38.390 --> 00:50:43.503
And here you have the other class c,
what a standard

00:50:43.503 --> 00:50:50.300
softmax may give you is a decision
boundary that looks like this.

00:50:50.300 --> 00:50:52.330
It's like perfectly separates the two.

00:50:52.330 --> 00:50:54.110
It's a very simple training example.

00:50:54.110 --> 00:50:56.808
Most standard softmax
classifiers will be able to

00:50:56.808 --> 00:50:59.000
perfectly separate these two classes.

00:50:59.000 --> 00:51:01.320
And again, this is just for
illustration in two dimensions.

00:51:01.320 --> 00:51:03.680
These are much higher
dimensional problems and so on.

00:51:03.680 --> 00:51:06.180
But a lot of the intuition
carries through.

00:51:06.180 --> 00:51:09.510
So now here we have our decision
boundary and this is the softmax.

00:51:09.510 --> 00:51:12.280
Now, the problem is maybe that
was your training data set.

00:51:12.280 --> 00:51:17.440
But your test set, actually,
might include some other ones that

00:51:17.440 --> 00:51:22.430
are quite similar to those stuff you saw
at training, but a little different.

00:51:22.430 --> 00:51:25.920
And now this kind of decision
boundary is not very robust.

00:51:27.370 --> 00:51:32.510
In contrast to this, what the max margin

00:51:32.510 --> 00:51:37.680
loss will attempt to do is to
try to increase the margin

00:51:37.680 --> 00:51:42.750
between the closest points
of your training data set.

00:51:42.750 --> 00:51:48.290
So if you have a couple of points here and
you have different points here.

00:51:48.290 --> 00:51:52.400
We'll try to maximize the distance between

00:51:52.400 --> 00:51:56.900
the closest points here, and
essentially be more robust.

00:51:56.900 --> 00:52:00.860
So then if at test time you have some
things that are kinda similar, but

00:52:00.860 --> 00:52:05.670
not quite there, you're more likely
to also correctly classify them.

00:52:05.670 --> 00:52:08.560
So it's a really great lost or
objective function.

00:52:10.370 --> 00:52:15.210
Now in our case here when we say a sc for
one corrupt window.

00:52:15.210 --> 00:52:18.400
In many cases in practice we're
actually going to have a sum over

00:52:18.400 --> 00:52:19.600
multiple of these.

00:52:19.600 --> 00:52:23.390
And you can think of this similar to the
skip-gram model where we sample randomly

00:52:23.390 --> 00:52:25.480
a couple of corrupt examples.

00:52:25.480 --> 00:52:28.290
So you really only need for
this kind of training

00:52:28.290 --> 00:52:31.490
a bunch of true examples of this
is a location in this context.

00:52:32.540 --> 00:52:35.550
And then all the other windows
where you don't have that

00:52:35.550 --> 00:52:38.290
as your training data are essentially
part of your negative class.

00:52:40.930 --> 00:52:43.680
All right, any questions around
the max-margin objective function?

00:52:43.680 --> 00:52:46.123
We're gonna take a lot of
derivatives of it now.

00:52:57.427 --> 00:53:00.020
That's right, is the corrupt
window just a negative class?

00:53:00.020 --> 00:53:01.530
Yes, that's exactly right.

00:53:02.640 --> 00:53:07.033
So you can think of any other window that
doesn't have as its center location just

00:53:07.033 --> 00:53:08.142
as the other class.

00:53:11.433 --> 00:53:14.790
All right, now how do we optimize this?

00:53:14.790 --> 00:53:19.610
We're going to take very similar steps to
what we've done with cross entropy, but

00:53:19.610 --> 00:53:25.160
now we actually have this hidden layer and
we'll take our second to last step towards

00:53:25.160 --> 00:53:29.740
the full back-propagation algorithm
which we'll cover in the next lecture.

00:53:29.740 --> 00:53:34.095
So let's assume our cost
J here is larger than 0.

00:53:35.380 --> 00:53:36.930
So what does that mean?

00:53:36.930 --> 00:53:41.080
In the very beginning you will initialize
all your parameters here again.

00:53:41.080 --> 00:53:44.140
Either randomly or maybe you'll initialize
your word vectors to be reasonable.

00:53:44.140 --> 00:53:48.970
But they're not gonna be quite perfect at
learning in this context in the window

00:53:48.970 --> 00:53:50.770
what is location and what isn't.

00:53:50.770 --> 00:53:55.827
And so in the beginning all your scores
are likely going to be low cuz all

00:53:55.827 --> 00:54:01.590
our parameters, U and W and b have been
initialized to small, random numbers.

00:54:01.590 --> 00:54:06.288
And so I'm unlikely going to be great
at distinguishing the window with

00:54:06.288 --> 00:54:10.108
a correct location at center
versus one that is corrupt.

00:54:10.108 --> 00:54:15.440
And so basically,
we will be in this regime.

00:54:15.440 --> 00:54:19.070
After a while of training, eventually
you're gonna get better and better.

00:54:19.070 --> 00:54:21.480
And then intuitively
if your score here for

00:54:21.480 --> 00:54:26.790
instance of the good window is five and
one of the corrupt is just two,

00:54:26.790 --> 00:54:30.530
then you'll see 1- 5 + 2 is less than 0 so

00:54:30.530 --> 00:54:34.645
you just basically have 0
loss on those elements.

00:54:34.645 --> 00:54:39.600
And that's another great property of
this objective function which is over

00:54:39.600 --> 00:54:44.220
time you can start ignoring more and more
of your training set cuz it's good enough.

00:54:44.220 --> 00:54:48.617
It will assign 0 cost as in 0

00:54:48.617 --> 00:54:54.180
error to these examples and so
you can start to focus on your objective

00:54:54.180 --> 00:54:59.310
function only on the things that the model
still has trouble to distinguish.

00:54:59.310 --> 00:55:03.739
All right, so let's in the very
beginning assume most of our examples

00:55:03.739 --> 00:55:06.151
will J will be larger than 0 for them.

00:55:06.151 --> 00:55:09.303
And so what we're gonna have to do now
is take derivatives with respect to

00:55:09.303 --> 00:55:10.760
all the parameters of our model.

00:55:12.660 --> 00:55:13.700
And so what are those?

00:55:13.700 --> 00:55:17.310
Those are U, W, b and our word vectors x.

00:55:17.310 --> 00:55:22.450
So we always start from the top and then
we go down because we'll start to reuse

00:55:22.450 --> 00:55:26.870
different elements and just the simple
combination of taking derivatives and

00:55:26.870 --> 00:55:30.830
reusing variables is going to
lead us to back propagation.

00:55:30.830 --> 00:55:32.950
So derivative of s with respect to U.

00:55:32.950 --> 00:55:34.970
Well, what was s?

00:55:34.970 --> 00:55:38.890
s was just u transpose times a and so

00:55:38.890 --> 00:55:41.850
we all know that derivative
of that is just a.

00:55:41.850 --> 00:55:47.810
So that was easy, first element,
first derivative super straight forward.

00:55:47.810 --> 00:55:51.070
Now it's important when we
take the next derivative

00:55:51.070 --> 00:55:53.500
to also be aware of all our definitions.

00:55:53.500 --> 00:55:57.990
How we define these functions that
we're taking derivatives off.

00:55:57.990 --> 00:56:04.199
So s is basically U transpose a,
a was f(z) and z was just Wx + b.

00:56:04.199 --> 00:56:06.510
All right,
it's very important to just keep track.

00:56:06.510 --> 00:56:07.710
That's like almost 80% of the work.

00:56:09.590 --> 00:56:12.671
Now, let's take
the derivative like I said,

00:56:12.671 --> 00:56:16.547
first partial of only one
element of W to gain intuitions.

00:56:16.547 --> 00:56:23.428
And then we can put it back together and
have a more complex matrix notation.

00:56:23.428 --> 00:56:28.196
So we'll observe for
Wij that it will actually only appear

00:56:28.196 --> 00:56:32.420
in the ith activation of our hidden layer.

00:56:32.420 --> 00:56:37.109
So for example, let's say we have a very
simple input with a three dimensional x.

00:56:37.109 --> 00:56:43.059
And we have two hidden units,
and this one final score U.

00:56:43.059 --> 00:56:48.056
Then we'll observe that if we take
the derivative with respect to W23.

00:56:48.056 --> 00:56:52.005
So the second row and
the third column of W,

00:56:52.005 --> 00:56:55.862
well that actually only is needed in a2.

00:56:55.862 --> 00:56:59.366
You can compute a1 without using W23.

00:57:00.580 --> 00:57:01.580
So what does that mean?

00:57:01.580 --> 00:57:05.540
That means if we take
the derivative of weight Wij,

00:57:05.540 --> 00:57:11.160
we really only need to look at
the ith element of the vector a.

00:57:11.160 --> 00:57:13.495
And hence, we don't need to look
at this whole inner product.

00:57:16.085 --> 00:57:17.805
So what's the next step?

00:57:17.805 --> 00:57:21.886
Well as we're taking derivatives with W,
we need to be again aware of where does W

00:57:21.886 --> 00:57:25.145
appear and all the other parameters
are essentially constant.

00:57:25.145 --> 00:57:29.012
So U here is not something
we're taking a derivative off.

00:57:29.012 --> 00:57:32.525
So what we can do is just take it out,
just as like a single number, right.

00:57:32.525 --> 00:57:36.460
We'll just get it outside,
put the derivative inside here.

00:57:36.460 --> 00:57:41.950
And now, we just need to very
carefully define our ai.

00:57:41.950 --> 00:57:45.249
So a subscript i, so
that's where Wij appears.

00:57:45.249 --> 00:57:50.661
Now, ai was this function,
and we defined it as f of zi.

00:57:50.661 --> 00:57:54.591
So why don't we just
write this carefully out,

00:57:54.591 --> 00:57:59.111
and now this is first application
of the chain rule with

00:57:59.111 --> 00:58:04.137
derivative of ai with respect to zi,
and then zi with respect to Wij.

00:58:04.137 --> 00:58:07.503
So this is single application
of the chain rule.

00:58:14.263 --> 00:58:18.882
And then end of it it looks kind of
overwhelming, but each step is very clear.

00:58:18.882 --> 00:58:23.848
And each step is simple, we're really
writing out all the glory details.

00:58:23.848 --> 00:58:29.450
So application of the chain rule,
now we're going to define ai.

00:58:29.450 --> 00:58:36.451
Well ai is just f of zi, and f was just an
element y function on a single number zi.

00:58:36.451 --> 00:58:40.433
So we can just rewrite ai with
its definition of f of zi,

00:58:40.433 --> 00:58:43.388
and we keep this one intact, all right?

00:58:43.388 --> 00:58:48.591
And now derivative of f,
we can just for now assume is f prime.

00:58:48.591 --> 00:58:50.585
Just a single number, take derivative.

00:58:50.585 --> 00:58:52.378
We'll just define this as f prime for now.

00:58:52.378 --> 00:58:57.310
It's also just a single number,
so no harm done.

00:58:57.310 --> 00:58:59.532
Now we're still in this part here,

00:58:59.532 --> 00:59:03.992
where we basically wanna take
the derivative of zi with respect to Wij.

00:59:03.992 --> 00:59:08.063
Well let's define what zi was,
zi was just here.

00:59:08.063 --> 00:59:15.666
The W of the ith row times x
plus the ith element of b.

00:59:15.666 --> 00:59:18.436
So let's just replace zi
with it's definition.

00:59:20.883 --> 00:59:21.967
Any questions so far?

00:59:31.099 --> 00:59:34.813
All right,

00:59:34.813 --> 00:59:38.902
good or not?

00:59:38.902 --> 00:59:43.672
So we have our f prime and
we have now the derivative

00:59:43.672 --> 00:59:48.680
with respect to Wij of just
this inner product here.

00:59:48.680 --> 00:59:51.781
And we can again,
very carefully write out well,

00:59:51.781 --> 00:59:56.070
the inner product is just this
row times this column vector.

00:59:56.070 --> 01:00:00.560
That's just the sum, and now when we
take the derivative with respect to Wij,

01:00:02.040 --> 01:00:04.213
all the other Ws are constants.

01:00:04.213 --> 01:00:09.250
They fall out, and so
basically it's only the xk,

01:00:09.250 --> 01:00:15.223
the only one that actually appears
in the sum with Wij is xj and

01:00:15.223 --> 01:00:19.341
so basically this derivative is just Xj.

01:00:22.138 --> 01:00:27.147
All right, so now we have this
whole expressions of just taking

01:00:27.147 --> 01:00:33.399
carefully chain rule multiplications
definitions of all our terms and so on.

01:00:33.399 --> 01:00:38.887
And now basically, what we're gonna want
to do is simplify this a little bit,

01:00:38.887 --> 01:00:42.500
cuz we might want to
reuse different parts.

01:00:42.500 --> 01:00:49.300
And so we can define, this first term here
actually happens to only use subindices i.

01:00:50.390 --> 01:00:52.537
And it doesn't use any other subindex.

01:00:52.537 --> 01:00:57.132
So we'll just define Uif prime of zi for

01:00:57.132 --> 01:01:00.899
all the different is as delta i.

01:01:00.899 --> 01:01:06.708
At first notational simplicity and
xj is our local input signal.

01:01:06.708 --> 01:01:09.421
And one thing that's very helpful for

01:01:09.421 --> 01:01:15.113
you to do is actually look at also the
derivative of the logistic function here.

01:01:15.113 --> 01:01:20.420
Which can be very conveniently computed
in terms of the original values.

01:01:20.420 --> 01:01:22.847
And remember f of z here, or

01:01:22.847 --> 01:01:27.609
f of zi of each element is
always just a single number.

01:01:27.609 --> 01:01:30.088
And we've already computed it
during forward propagation.

01:01:30.088 --> 01:01:38.411
So we wanna ideally use hidden activation
functions that are very fast to compute.

01:01:38.411 --> 01:01:41.729
And here, we don't need to compute
another exponent or anything.

01:01:41.729 --> 01:01:45.347
We're not gonna recompute f of zi cuz
we already did that in the forward

01:01:45.347 --> 01:01:46.400
propagation step.

01:01:48.380 --> 01:01:49.280
All right,

01:01:49.280 --> 01:01:54.781
now we have the partial derivative
here with respect to one element of W.

01:01:54.781 --> 01:01:58.979
But of course, we wanna have the whole
gradient for the whole matrix.

01:01:58.979 --> 01:02:04.318
So now the question is,
with the definitions of this delta i for

01:02:04.318 --> 01:02:08.952
all the different elements of
i of this matrix and xj for

01:02:08.952 --> 01:02:12.507
all the different elements of the input.

01:02:12.507 --> 01:02:17.954
What would be a good way of trying to
combine all of these different elements

01:02:17.954 --> 01:02:23.155
to get a single gradient for the whole
matrix W, if we have two vectors.

01:02:29.245 --> 01:02:30.359
That's right.

01:02:30.359 --> 01:02:35.753
So essentially, we can use delta
times x transpose, namely the outer

01:02:35.753 --> 01:02:41.342
product to get all the combinations
of all elements i and all elements j.

01:02:41.342 --> 01:02:44.488
And so this again might seem
like a little bit like magic.

01:02:44.488 --> 01:02:48.209
But if you just think again of
the definition of the outer product here.

01:02:48.209 --> 01:02:53.108
And you write it out in terms of all
the indices, you'll see that turns out

01:02:53.108 --> 01:02:57.650
to be exactly what we would want in
one very nice, very simple equation.

01:03:00.660 --> 01:03:06.028
So we can kind of think of this delta
term actually as the responsibility of

01:03:06.028 --> 01:03:11.929
the error signal that's now arriving from
our overall loss into this layer of W.

01:03:11.929 --> 01:03:15.987
And that will eventually
lead us to flow graphs.

01:03:15.987 --> 01:03:19.237
And that will eventually lead us to you
not having to actually go through all this

01:03:19.237 --> 01:03:20.948
misery of taking all these derivatives.

01:03:20.948 --> 01:03:23.706
And being able to abstract it
away with software packages.

01:03:23.706 --> 01:03:26.939
But this is really the nuts and
bolts of how this works, yeah?

01:03:37.260 --> 01:03:42.635
Yeah, the question is, this outer product
will get all the elements of i and j?

01:03:42.635 --> 01:03:43.243
And that's right.

01:03:43.243 --> 01:03:47.438
So when we have delta times x transposed.

01:03:47.438 --> 01:03:53.168
Then now we have basically here,
x is usually this vector.

01:03:53.168 --> 01:03:57.961
So now let's take the right notation.

01:03:57.961 --> 01:04:03.200
So we wanna have derivative
with respect to W.

01:04:03.200 --> 01:04:10.552
W was a, 2x3 dimension matrix for
example, 2x3.

01:04:19.440 --> 01:04:21.077
We should be very careful of our notation.

01:04:24.007 --> 01:04:25.690
2x3.

01:04:25.690 --> 01:04:31.120
So now,
the derivative of j with respect to our w

01:04:31.120 --> 01:04:34.940
has to, in the end, also be a 2x3 matrix.

01:04:34.940 --> 01:04:41.340
And if we have delta times x transposed,
then that means we'll have

01:04:41.340 --> 01:04:45.905
to have a two-dimensional delta, which is
exactly the dimensions that are coming in.

01:04:45.905 --> 01:04:49.103
[INAUDIBLE] Signal that I
mentions that we have for

01:04:49.103 --> 01:04:51.762
the number of hidden units that we have.

01:04:51.762 --> 01:04:56.883
Times this one dimensional,
basically row vector times

01:04:56.883 --> 01:05:02.127
xt which is a 1 x 3 dimensional
vector that we transpose.

01:05:02.127 --> 01:05:05.633
And so, what does that mean?

01:05:05.633 --> 01:05:10.210
Well, that's basically multiplying now,
standard matrix multiplication.

01:05:10.210 --> 01:05:14.525
You should write that.

01:05:14.525 --> 01:05:20.647
So now the last term that we haven't
taken derivatives of off the [INAUDIBLE],

01:05:20.647 --> 01:05:24.710
is our bi and
it'll eventually be very similar.

01:05:24.710 --> 01:05:26.850
We're going to go through it.

01:05:26.850 --> 01:05:30.988
We can pull Ui out, we're going to
take f prime, assume that's the same.

01:05:30.988 --> 01:05:33.360
So now, this is our delta i.

01:05:33.360 --> 01:05:35.044
We'll observe something very similar.

01:05:35.044 --> 01:05:37.410
These are very similar steps for bi.

01:05:37.410 --> 01:05:40.093
But in the end, we're going to
just end up with this term and

01:05:40.093 --> 01:05:41.444
that's just going to be one.

01:05:41.444 --> 01:05:45.387
And so,
the derivative of our bi element here,

01:05:45.387 --> 01:05:50.415
is just delta i and we can again
use all the elements of delta,

01:05:50.415 --> 01:05:54.370
to have the entire gradient for
the update of b.

01:05:58.959 --> 01:06:00.250
Any questions?

01:06:04.705 --> 01:06:10.230
Excellent, so this is essentially,
almost back-propagation.

01:06:10.230 --> 01:06:13.750
Weve so far only taken derivatives and
using the chain rule.

01:06:13.750 --> 01:06:15.490
And first thing, when I went through this,

01:06:15.490 --> 01:06:20.440
this is like a lot of the magic of deep
learning, is just becoming a lot clear.

01:06:20.440 --> 01:06:24.650
Weve just taken derivatives, we have
an objective function and then we update

01:06:24.650 --> 01:06:28.030
based on our derivatives, all
the parameters of these large functions.

01:06:28.030 --> 01:06:32.610
Now the main remaining trick, is to re-use
derivatives that we've computed for

01:06:32.610 --> 01:06:36.550
the higher layers in computing
derivatives for the lower layers.

01:06:36.550 --> 01:06:39.999
It's very much an efficiency trick.

01:06:39.999 --> 01:06:43.660
You could not use it and it would
just be very, very inefficient to do.

01:06:43.660 --> 01:06:46.687
But this is the main insight

01:06:46.687 --> 01:06:51.468
of why we re-named taking
derivatives as back propagation.

01:06:51.468 --> 01:06:55.050
So what is the last derivatives
that we need to take?

01:06:55.050 --> 01:06:57.640
For this model, well again,
it's in terms of our word vectors.

01:06:59.370 --> 01:07:01.796
So let's go through all of those.

01:07:01.796 --> 01:07:06.680
Basically, we'll have to take the
derivative of the score with respect to

01:07:06.680 --> 01:07:09.480
every single element of our word vectors.

01:07:09.480 --> 01:07:12.420
Where again, we concatenated all
of them into a single window.

01:07:14.000 --> 01:07:15.830
And now, the problem here is that

01:07:16.980 --> 01:07:21.140
each word vector actually
appears in both of these terms.

01:07:21.140 --> 01:07:27.470
And both hidden units use all of
the elements of the input here.

01:07:27.470 --> 01:07:30.760
So we can't just look at a single element.

01:07:30.760 --> 01:07:37.090
We'll really have to sum over, both of the
activation units in the simple case here,

01:07:37.090 --> 01:07:41.030
where we just have two hidden units and
three dimensional inputs.

01:07:41.030 --> 01:07:44.590
Keeps it a little simpler,
and there's less notation.

01:07:44.590 --> 01:07:47.380
So then, we basically start with this.

01:07:47.380 --> 01:07:52.770
I have to take derivatives with
respect to both of the activations.

01:07:52.770 --> 01:07:55.267
And now, we're just going to go
through similar kinds of steps.

01:07:55.267 --> 01:07:56.212
We have s.

01:07:56.212 --> 01:07:59.966
We defined s as u transpose
times our activation.

01:07:59.966 --> 01:08:08.190
That was just Ui then ai
was just f of w and so on.

01:08:08.190 --> 01:08:12.335
Now, what we'll observe as we're going
through all these similar steps again is

01:08:12.335 --> 01:08:19.760
that, we'll actually see the same
term here reused from before.

01:08:19.760 --> 01:08:23.610
It's Ui x F prime of Zi.

01:08:25.040 --> 01:08:27.210
This is exactly the same.

01:08:27.210 --> 01:08:28.650
That we've seen here.

01:08:28.650 --> 01:08:32.100
F prime of Zi.

01:08:32.100 --> 01:08:34.360
And what that means is,
we can reuse that same delta.

01:08:35.400 --> 01:08:38.280
And that's really one of the big insights.

01:08:38.280 --> 01:08:42.460
Fairly trivial but very exciting,
cuz it makes it a lot faster.

01:08:42.460 --> 01:08:43.670
But, what's still different now,

01:08:43.670 --> 01:08:46.340
is that of course we have to take
the the derivative with respect.

01:08:46.340 --> 01:08:50.830
To each of these, to this inner product
here in Xj, where we basically dumped

01:08:50.830 --> 01:08:55.670
the bias term, cuz that's just a constant,
when we were taking this derivative.

01:08:55.670 --> 01:08:59.384
And so, this one here again,
Xj is just inner product,

01:08:59.384 --> 01:09:03.732
it's the jth element of this matrix
W that's the relevant one for

01:09:03.732 --> 01:09:07.068
this inner product,
let me take the derivative.

01:09:07.068 --> 01:09:12.358
So now we have this sum here, and
now comes again this tricky bit of trying

01:09:12.358 --> 01:09:17.767
to simplify this sum into something
simpler in terms of matrix products.

01:09:17.767 --> 01:09:22.833
And again, the reason we're getting
towards back propagation is that we're

01:09:22.833 --> 01:09:27.910
reusing here these previous error signals,
and elements of the derivative.

01:09:29.190 --> 01:09:33.490
Now, the simplest, the first thing we'll
observe here as we're doing this sum, is

01:09:33.490 --> 01:09:39.890
that sum is actually also a simple inner
product, where we now take the jth column.

01:09:39.890 --> 01:09:43.810
So this again, this dot notation
when the dot is after the first, and

01:09:43.810 --> 01:09:46.010
next we take the row,
here we take the column.

01:09:46.010 --> 01:09:47.250
So it's a column vector.

01:09:47.250 --> 01:09:48.800
But then of course we transpose it, so

01:09:48.800 --> 01:09:52.178
it's a simple inner product for
getting us a single number.

01:09:52.178 --> 01:09:58.773
Just the derivative of this element of
the word vectors and the word window.

01:09:58.773 --> 01:09:59.273
Yes.

01:10:13.000 --> 01:10:13.745
Great question.

01:10:13.745 --> 01:10:16.205
So once we have the derivatives for

01:10:16.205 --> 01:10:19.900
all these different variables, what's
the sequence in which we update them, and

01:10:19.900 --> 01:10:22.525
there's really no sequence we
update them all in parallel.

01:10:22.525 --> 01:10:26.990
We just take one step in all the elements
that we now had a variable in or

01:10:26.990 --> 01:10:29.390
have seen that parameter in.

01:10:29.390 --> 01:10:33.030
And the complexity there,
is in standard machine learning you'll see

01:10:33.030 --> 01:10:35.040
in many models just like
standard logistic regression,

01:10:35.040 --> 01:10:38.460
you see all your parameters like
your W in all the examples.

01:10:38.460 --> 01:10:42.450
And ours, it's a little more complex,
because most words you won't see in

01:10:42.450 --> 01:10:47.220
a specific window and so, you only update
the words that you see in that window.

01:10:47.220 --> 01:10:51.780
And if you assumed all the other ones,
you'd just have very, very large,

01:10:51.780 --> 01:10:55.910
quite sparse updates, and that's not
very RAM efficient, great question.

01:10:57.362 --> 01:11:00.944
So now we have this simple
multiplication here and

01:11:00.944 --> 01:11:03.680
the sum is just is just inner product.

01:11:03.680 --> 01:11:06.860
So far so simple, and we have our D
dimension vector which I mentioned,

01:11:06.860 --> 01:11:07.767
is two dimensions.

01:11:07.767 --> 01:11:10.150
We have the sum over two elements.

01:11:10.150 --> 01:11:11.650
So, so far so good.

01:11:11.650 --> 01:11:17.040
Now, really, we would like to get the full
gradient here with respect to all

01:11:18.190 --> 01:11:23.070
XJs for J equals one to three and
its simple case, or

01:11:23.070 --> 01:11:28.570
five D if we have a five
word large window.

01:11:28.570 --> 01:11:34.678
So now the question is, how do we
combine this single element here.

01:11:34.678 --> 01:11:41.765
Into a vector that eventually gives us all
the different gradients for all the xij.

01:11:41.765 --> 01:11:48.830
And j equals 1 to however long our window
is Is anybody follow along this closely?

01:11:54.282 --> 01:11:55.310
That's right.

01:11:55.310 --> 01:11:56.570
W transposed delta.

01:11:56.570 --> 01:11:58.290
Well done.

01:11:58.290 --> 01:12:04.750
So basically our final derivative and
final gradient here for.

01:12:04.750 --> 01:12:10.280
Our score s with respect to the entire
window, is just W transpose times delta.

01:12:10.280 --> 01:12:15.230
Super simple very fast to implement,
I can easily think about how to vectorize

01:12:15.230 --> 01:12:19.550
this again by concatenating multiple
deltas from multiple Windows and so on.

01:12:19.550 --> 01:12:23.170
And it can be very efficiently,
like implemented and derived.

01:12:24.220 --> 01:12:27.820
All right, now the error message is
delta that arrives at this hidden layer,

01:12:27.820 --> 01:12:31.710
has of course the same dimensionality as
its hidden layer because we're updating

01:12:31.710 --> 01:12:32.440
all the windows.

01:12:32.440 --> 01:12:36.431
And now from the previous slides we
also know that when we update a window,

01:12:36.431 --> 01:12:40.100
it really means we now cut up that final

01:12:40.100 --> 01:12:44.660
gradient here into the different chunks
for each specific word in that window,

01:12:44.660 --> 01:12:48.520
and that's how we update our
first large neural network.

01:12:48.520 --> 01:12:51.550
So let's put all of this together again.

01:12:52.590 --> 01:12:58.120
So, our full objective function
here was this max and I started

01:12:58.120 --> 01:13:02.660
out with saying let's assume it's larger
than zero so you have this identity here.

01:13:02.660 --> 01:13:05.920
So this is simple indicator function if.

01:13:05.920 --> 01:13:09.310
The indication is true,
then it's one and if not, it's zero.

01:13:09.310 --> 01:13:14.100
And then you can essentially
ignore that pair of correct

01:13:15.170 --> 01:13:18.950
and corrupt windows x and
xc, respectively.

01:13:18.950 --> 01:13:21.400
So our final gradient when we have

01:13:21.400 --> 01:13:26.830
these kinds of max margin functions
is essentially implemented this way.

01:13:26.830 --> 01:13:32.373
And we can very efficiently
multiply all of this stuff.

01:13:35.111 --> 01:13:36.501
All right.

01:13:41.415 --> 01:13:43.690
So this is just that, this is not right.

01:13:43.690 --> 01:13:46.580
This is our [INAUDIBLE] But you still
have to take the derivative here,

01:13:46.580 --> 01:13:50.828
but basically this indicator function is
the main novelty that we haven't seen yet.

01:13:50.828 --> 01:13:54.047
All right.

01:13:54.047 --> 01:13:55.522
Yeah.

01:14:00.683 --> 01:14:10.683
&gt;&gt; [INAUDIBLE]

01:14:35.468 --> 01:14:36.261
&gt;&gt; Yeah, it's a long question.

01:14:36.261 --> 01:14:42.400
The gist of the question is how to we make
sure we don't get stuck in local optima.

01:14:42.400 --> 01:14:46.860
And you've kinda answered it a little
bit already which is indeed because of

01:14:46.860 --> 01:14:50.860
the stochasticity you keep making updates
anyway it's very hard to get stuck.

01:14:50.860 --> 01:14:55.420
In fact, the smaller your,
the more stochastic you are,

01:14:55.420 --> 01:14:59.010
as in the fewer windows you look at
each time you want to make an update,

01:14:59.010 --> 01:15:00.820
the less likely you're getting stuck.

01:15:00.820 --> 01:15:04.110
If you had tried to get through all the
windows and then make one gigantic update,

01:15:04.110 --> 01:15:08.020
so it's actually very inefficient and
much more likely to get you stuck.

01:15:08.020 --> 01:15:11.640
And then the other observation
that it's just slowly coming

01:15:11.640 --> 01:15:14.420
through some of the theory that
we couldn't get into this class.

01:15:15.570 --> 01:15:19.200
Is that it turns out a lot of the local
optima are actually pretty good.

01:15:19.200 --> 01:15:20.200
And in many cases,

01:15:20.200 --> 01:15:23.672
not even that far away from what you
might think the global optima would be.

01:15:24.710 --> 01:15:29.030
Also, you'll observe a lot of times,
and we'll go through this in some

01:15:29.030 --> 01:15:32.830
of the project advice in many cases,
you can actually perfectly fit.

01:15:32.830 --> 01:15:34.720
We have a powerful enough
neural network model.

01:15:34.720 --> 01:15:39.210
You can often perfectly fit your input and
your training dataset.

01:15:39.210 --> 01:15:42.960
And you'll actually, eventually spend
most of your time thinking about how to

01:15:42.960 --> 01:15:47.760
regularize your models better and often,
at least, even more stochasticity.

01:15:47.760 --> 01:15:49.800
We'll get through some of those.

01:15:49.800 --> 01:15:50.630
But yeah, good question.

01:15:50.630 --> 01:15:55.290
Yeah, in the end, we just have all
these updates and it's all very simple.

01:15:55.290 --> 01:15:56.440
All right, so let's summarize.

01:15:56.440 --> 01:15:58.860
This was a pretty epic lecture.

01:15:58.860 --> 01:16:01.240
Well done for sticking through it.

01:16:01.240 --> 01:16:06.170
Congrats again, this was our super
useful basic components lecture.

01:16:06.170 --> 01:16:09.100
And now this window model is actually
really the first one that you

01:16:09.100 --> 01:16:12.040
might observe and practice and
you might actually want to implement.

01:16:12.040 --> 01:16:13.490
In a real life setting.

01:16:13.490 --> 01:16:14.360
So to recap,

01:16:14.360 --> 01:16:18.860
we've learned word vector training,
we learned how to combine Windows.

01:16:18.860 --> 01:16:21.010
We have the softmax and
the cross entropy error and

01:16:21.010 --> 01:16:23.000
we went through some of the details there.

01:16:23.000 --> 01:16:26.540
Have the scores and the max margin loss,
and we have the neural network, and it's

01:16:26.540 --> 01:16:31.100
really these two steps here that you have
to combine differently for problem set.

01:16:31.100 --> 01:16:33.820
Number one and
especially number two in that.

01:16:33.820 --> 01:16:36.080
So, we just have one more
math heavy lecture and

01:16:36.080 --> 01:16:39.750
after that we can have fun and
combine all these things together.

01:16:39.750 --> 01:16:40.250
Thanks.

