WEBVTT
Kind: captions
Language: en

00:00:00.900 --> 00:00:04.890
In this lecture we'll introduce
the topic of text classification and

00:00:04.890 --> 00:00:07.820
the naive Bayes algorithm which
is one of the most important

00:00:07.820 --> 00:00:10.210
ways of doing text classification.

00:00:10.210 --> 00:00:14.110
Let's begin by looking at some examples
of text classification applications.

00:00:16.220 --> 00:00:18.530
Here, I've shown an email that I
actually received the other day.

00:00:20.250 --> 00:00:22.406
How do I know that this e-mail is spam?

00:00:24.520 --> 00:00:28.600
Take a look at the mail and think of some
features you might automatically extract

00:00:28.600 --> 00:00:31.008
from this e-mail that
tells you that its spam.

00:00:33.094 --> 00:00:37.150
You might notice the word great,
a misspelling of great, so

00:00:37.150 --> 00:00:39.380
we have a typo here.

00:00:39.380 --> 00:00:43.610
Maybe you might notice important notice,
and maybe an exclamation point.

00:00:43.610 --> 00:00:46.410
It's pretty rare that universities
put exclamation points in their

00:00:46.410 --> 00:00:47.130
subject headers.

00:00:48.130 --> 00:00:52.190
You might notice that there's no Dan here.

00:00:52.190 --> 00:00:53.850
It's not addressed to me in particular.

00:00:53.850 --> 00:00:57.112
We have undisclosed recipients,
and there's no particular address.

00:00:57.112 --> 00:00:59.500
And the URL's a little funny here.

00:00:59.500 --> 00:01:00.994
That's not a Stanford URL.

00:01:03.804 --> 00:01:05.360
Maybe the word exciting.

00:01:06.961 --> 00:01:11.410
Each of these features can be combined in
a classifier to give us some evidence that

00:01:11.410 --> 00:01:12.895
we've got a piece of spam.

00:01:17.620 --> 00:01:22.720
Another important text classification
task is authorship attribution.

00:01:22.720 --> 00:01:25.510
How do I know which author
wrote which piece of text?

00:01:26.980 --> 00:01:29.930
One of the most famous examples
of authorship attribution

00:01:29.930 --> 00:01:33.230
is the famous anonymous essays
called The Federalist Papers

00:01:33.230 --> 00:01:35.780
that were written at the beginning
of the history of our country

00:01:35.780 --> 00:01:39.930
in part to convince the state of New York
to ratify the early constitution.

00:01:39.930 --> 00:01:43.570
And three authors wrote various
numbers of the letters.

00:01:43.570 --> 00:01:46.334
But 12 of the letters,
it wasn't clear which author wrote.

00:01:46.334 --> 00:01:52.244
And in 1963, the And

00:01:52.244 --> 00:01:57.619
in 1963, Mosteller and Wallace showed that
Bayesian methods were able to distinguish

00:01:57.619 --> 00:02:02.520
which letters were written by Madison and
which letters were written by Hamilton.

00:02:03.780 --> 00:02:06.400
And the Bayesian methods that
they used in 1963 gave rise

00:02:06.400 --> 00:02:08.850
to the naive Bayes method that we're
going to be talking about today.

00:02:11.160 --> 00:02:14.700
Another text classification
task is gender identification,

00:02:14.700 --> 00:02:16.650
determining if an author is male or
female.

00:02:18.550 --> 00:02:22.640
Recent research in gender
identification has shown that

00:02:22.640 --> 00:02:25.220
we can look at the number of pronouns and

00:02:25.220 --> 00:02:30.060
other features, the number of determiners,
the number of noun phrases,

00:02:30.060 --> 00:02:34.530
are subtlety indicative of the difference
between male and female writers.

00:02:36.140 --> 00:02:39.184
Female writers tend to use more pronouns,
and

00:02:39.184 --> 00:02:44.111
male writers tend to use more facts and
determiners in their noun phrases.

00:02:44.111 --> 00:02:49.371
And you can see from that,
that here we have a lot of pronouns.

00:02:53.817 --> 00:03:00.640
And here we have a lot of determiners and
factual sentences with copula verbs.

00:03:02.340 --> 00:03:07.291
And so, you might determine that this is,
in fact, a male, and this is a female.

00:03:07.291 --> 00:03:08.196
And that would be correct.

00:03:08.196 --> 00:03:12.538
This is the author Margaret Drabble,
and this is the author Anthony Grey.

00:03:15.498 --> 00:03:20.490
Another text classification
task is sentiment analysis.

00:03:20.490 --> 00:03:25.690
And one of the classic sentiment analysis
tasks is movie review identification.

00:03:25.690 --> 00:03:27.540
Given a review, whether it's a movie or

00:03:27.540 --> 00:03:31.640
a product, can I tell whether this
review is positive or negative?

00:03:31.640 --> 00:03:35.070
And although I'm going to show you an
example here for movies, this could apply

00:03:35.070 --> 00:03:39.620
to any product review for any product or
service that you might find on the web.

00:03:39.620 --> 00:03:42.398
And this is actually a very
important commercial application.

00:03:43.825 --> 00:03:48.381
So, suppose we saw a review that
said unbelievably disappointing.

00:03:48.381 --> 00:03:50.920
Well, that's clearly a negative review.

00:03:50.920 --> 00:03:55.137
How about full of zany characters and
richly applied satire?

00:03:55.137 --> 00:03:55.770
Positive.

00:03:55.770 --> 00:03:59.520
How about this is the greatest
screwball comedy ever filmed?

00:03:59.520 --> 00:04:03.300
We've got words like greatest,
or greatest ever.

00:04:03.300 --> 00:04:04.610
That's very positive.

00:04:05.880 --> 00:04:09.348
How about it was pathetic, the worst
part about it was the boxing scenes.

00:04:09.348 --> 00:04:13.938
Here we've got evidence like pathetic and
worst, and so on to tell us that this is,

00:04:13.938 --> 00:04:15.589
in fact, a negative review.

00:04:18.909 --> 00:04:21.208
Text classification often

00:04:24.331 --> 00:04:28.030
We also apply text classification
to scientific articles.

00:04:28.030 --> 00:04:31.020
For example,
deciding what the topic of a particular

00:04:31.020 --> 00:04:33.450
article in a database
like MEDLINE might be.

00:04:35.080 --> 00:04:39.680
For example, we might have to decide, in
automatically indexing an article, which

00:04:39.680 --> 00:04:45.510
of various subjects, antagonist, or blood
supply, or drug therapy, or epidemiology,

00:04:45.510 --> 00:04:49.550
apply to any particular article that's
written, that's in our database.

00:04:51.510 --> 00:04:56.650
So, in summary, text classification
is the task of assigning any kind of

00:04:56.650 --> 00:04:59.520
topic category to any piece of text, and

00:04:59.520 --> 00:05:03.840
that could be subject categories in
some kind of an online database.

00:05:03.840 --> 00:05:05.530
It can be detecting spam.

00:05:05.530 --> 00:05:07.840
It can be choosing an author
from a set of authors.

00:05:07.840 --> 00:05:09.640
Choosing their gender,
or maybe it's their age.

00:05:09.640 --> 00:05:11.830
You want to find young writers or
old writers.

00:05:11.830 --> 00:05:17.350
Telling if a text was written in one
language versus another language.

00:05:17.350 --> 00:05:19.810
And the important commercial
application of sentiment analysis.

00:05:19.810 --> 00:05:23.135
All of these are examples
of text classification.

00:05:26.768 --> 00:05:30.030
Let's define the task
of text classification.

00:05:30.030 --> 00:05:33.200
We have as input a document, d.

00:05:33.200 --> 00:05:40.414
And then a fixed set of classes, a set c,
with j classes c1, c2, up til cj.

00:05:40.414 --> 00:05:42.640
And our goal, given this document and

00:05:42.640 --> 00:05:47.630
this set of classes, is to predict
a class c from that set of classes.

00:05:47.630 --> 00:05:51.590
Or our job is to take a document and
assign a class to that document.

00:05:53.784 --> 00:05:54.880
How do we do this?

00:05:56.160 --> 00:06:00.420
The simplest possible text classification
method is to use hand written rules.

00:06:01.500 --> 00:06:06.246
So, for example, if we're doing spam
detection we might just have a list of

00:06:06.246 --> 00:06:11.920
bad e-mail addresses, a black list,
that these people are probably spammers.

00:06:11.920 --> 00:06:16.520
Or we might look for
phrases like millions of dollars or

00:06:16.520 --> 00:06:20.370
you have been selected, which are good
indications that we have spam.

00:06:20.370 --> 00:06:23.210
And if these rules are carefully
refined by an expert,

00:06:23.210 --> 00:06:26.460
you can get high accuracy
from hand written rules.

00:06:26.460 --> 00:06:29.608
But in general, building and
maintaining these rules is expensive.

00:06:29.608 --> 00:06:34.213
So although hand-coded rules are often
used as part of a system of text

00:06:34.213 --> 00:06:39.055
classification, we generally combine
that with an important method for

00:06:39.055 --> 00:06:40.396
machine learning.

00:06:42.021 --> 00:06:44.810
This method is supervised
machine learning.

00:06:44.810 --> 00:06:50.790
So, in supervised machine learning, we
have a document, d, just as we did before.

00:06:50.790 --> 00:06:53.300
And a fixed set of classes,
as we did before.

00:06:53.300 --> 00:06:55.220
But we need one more thing now.

00:06:55.220 --> 00:06:59.960
We need a training set of some documents
that have been hand-labeled for

00:06:59.960 --> 00:07:00.560
their class.

00:07:00.560 --> 00:07:03.860
So we have, for document one,
we know that it's in class one.

00:07:03.860 --> 00:07:06.090
For document two,
it's in some other class.

00:07:06.090 --> 00:07:09.577
Maybe for document m, we have a label for
the class of document m.

00:07:09.577 --> 00:07:13.550
So given the document,
the set of classes, and

00:07:13.550 --> 00:07:18.175
the training set of hand-labeled
documents, the goal of machine learning

00:07:18.175 --> 00:07:22.647
is to produce a classifier, and we'll be
using gamma to refer to the classifier.

00:07:22.647 --> 00:07:26.815
And gamma's a function that, given
a new document, will give us the class.

00:07:26.815 --> 00:07:30.128
So given a set of training
labels of documents and classes,

00:07:30.128 --> 00:07:33.387
we'll learn a classifier that
maps a document to a class.

00:07:35.465 --> 00:07:39.500
There's lot's of kinds of
machine learning classifiers.

00:07:39.500 --> 00:07:42.260
We're going to talk today
about Naive Bayes, but

00:07:42.260 --> 00:07:46.500
we'll look later in the course we'll
talk about Logistical regression, and

00:07:46.500 --> 00:07:50.410
we'll touch on other kinds of classifiers
like Support Vector machines also called

00:07:50.410 --> 00:07:54.069
SVMs, K-nearest Neighbors,
lot of other classifiers.

00:07:55.210 --> 00:07:57.730
No matter which classifier we use,

00:07:57.730 --> 00:08:02.540
the task of text classification
is to take a document, its text,

00:08:02.540 --> 00:08:07.820
other kinds of features, and extract
features that represent the document and

00:08:07.820 --> 00:08:11.230
build a classifier that can tell us
which class the document belongs to.

