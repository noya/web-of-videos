WEBVTT
Kind: captions
Language: en

00:00:01.265 --> 00:00:03.880
How do we estimate these
n-gram probabilities?

00:00:05.770 --> 00:00:07.630
Let's look at bigram probabilities.

00:00:07.630 --> 00:00:12.191
The maximum likelihood estimate for
a bigram probability, the probability of

00:00:12.191 --> 00:00:16.423
a word i given the previous word i
minus 1, we just estimate by counting.

00:00:16.423 --> 00:00:20.015
We count how many times did word i
minus 1 and i occur together, and

00:00:20.015 --> 00:00:23.680
divide it by how many times
word i minus 1 occurs.

00:00:23.680 --> 00:00:26.320
So it's like saying, of all the times
that we saw word i minus 1,

00:00:26.320 --> 00:00:28.650
how many times was it followed by word i?

00:00:30.920 --> 00:00:35.360
And we'll use the notation count sometimes
and sometimes for simplification,

00:00:35.360 --> 00:00:38.870
we'll just refer to a c,
we'll use c for count.

00:00:38.870 --> 00:00:45.230
So the joint count of word i minus 1 and
i divided by the count of word i minus 1.

00:00:45.230 --> 00:00:49.070
So let's walk through an example.

00:00:49.070 --> 00:00:54.008
So we have again our equation, the
probability of word i given word i minus

00:00:54.008 --> 00:00:58.229
1 is, the maximum likely estimate is just,
I'll write mle,

00:00:58.229 --> 00:01:03.670
estimated by the maximum likelihood
estimator, as this count over this count.

00:01:03.670 --> 00:01:08.740
And let's make up a corpus, so here's our
simple corpus borrowed from Dr. Seuss.

00:01:08.740 --> 00:01:12.390
So we have three sentences,
each one starts with a special token,

00:01:12.390 --> 00:01:15.890
start of sentence and ends with
a special token, end of sentence.

00:01:15.890 --> 00:01:19.072
And they're very short sentences,
I am Sam, Sam I am,

00:01:19.072 --> 00:01:20.916
I do not like green eggs and ham.

00:01:20.916 --> 00:01:25.250
And let's compute some language model
probabilities from this small corpus.

00:01:26.720 --> 00:01:33.273
So first, probability of i
given the start symbol, so

00:01:33.273 --> 00:01:39.683
that's computed as count of start symbol,
comma i,

00:01:39.683 --> 00:01:45.111
make that big I,
over count of start symbol.

00:01:45.111 --> 00:01:50.318
So I follows the start symbol twice,
one, two,

00:01:50.318 --> 00:01:57.320
and the start symbol occurs three times,
one, two, three.

00:01:57.320 --> 00:02:01.444
So that probability is two-thirds or
0.67, so

00:02:01.444 --> 00:02:04.828
that's the probability of I given start.

00:02:04.828 --> 00:02:08.221
And you can see examples of lots of
different probabilities here, so for

00:02:08.221 --> 00:02:10.920
example, let's just pick
another one at random.

00:02:10.920 --> 00:02:15.937
The probability of Sam, given the word am,

00:02:15.937 --> 00:02:20.010
how many times does am, Sam occur.

00:02:20.010 --> 00:02:25.350
It occurs once.

00:02:25.350 --> 00:02:29.252
So that's one.

00:02:29.252 --> 00:02:33.490
And the denominator is how
many times does am occur.

00:02:35.020 --> 00:02:36.090
And that occurs twice.

00:02:37.530 --> 00:02:39.230
So that's going to be 1 over 2.

00:02:39.230 --> 00:02:42.110
So there's our probability
of Sam given am and so on.

00:02:44.110 --> 00:02:48.130
So let's look at a larger corpus in
order to get some more realistic counts.

00:02:48.130 --> 00:02:52.430
And the corpus that we're using here
was collected from a dialog system

00:02:52.430 --> 00:02:56.110
that answered questions about restaurants
in the city of Berkeley, California.

00:02:56.110 --> 00:02:59.120
And here's the kind of sentences
that were in this corpus.

00:02:59.120 --> 00:03:01.845
Can you tell me about any good
Cantonese restaurants close by or

00:03:01.845 --> 00:03:06.790
mid-priced Thai food is what I'm looking
for, tell me about Chez Panisse.

00:03:06.790 --> 00:03:08.529
All right, here's some sentences,

00:03:08.529 --> 00:03:11.310
and let's compute some anagrams
based on these sentences.

00:03:13.210 --> 00:03:17.640
So first, let's start with the raw
bigram counts from this small corpus of

00:03:17.640 --> 00:03:19.000
just under 10,000 sentences.

00:03:20.170 --> 00:03:23.380
So what I'm showing you here
is a bigram count table.

00:03:23.380 --> 00:03:28.210
So here's the word i is followed
by the word i five times.

00:03:28.210 --> 00:03:32.280
The word i is followed by
the word want 827 times.

00:03:32.280 --> 00:03:36.150
The word want is followed
by the word to 608 times.

00:03:36.150 --> 00:03:39.030
The word to is followed by
the word eat 686 times.

00:03:39.030 --> 00:03:41.260
And we've just put some
sample words up here.

00:03:41.260 --> 00:03:46.210
I picked I want to eat Chinese food
lunch spend just to show you some

00:03:46.210 --> 00:03:48.930
words that might occur together in
a sentence and some other words.

00:03:48.930 --> 00:03:53.922
And you can see, a lot of these words,
a lot of these probabilities, are zeros,

00:03:53.922 --> 00:03:58.402
a lot of these counts, I'm sorry,
are zeros, because it just happened

00:03:58.402 --> 00:04:02.096
in this small data set that want
was never followed by want.

00:04:02.096 --> 00:04:08.345
So that's this 0 here or Chinese was
never followed by the word to here, okay.

00:04:09.810 --> 00:04:12.520
So in order to turn these
counts into probabilities,

00:04:12.520 --> 00:04:16.800
all we have to do is
normalize by a unigram count.

00:04:16.800 --> 00:04:22.466
Because remember,
the probability of a word i given the word

00:04:22.466 --> 00:04:29.710
i minus 1 is the count of word i minus 1
word i over the count of word i minus 1.

00:04:29.710 --> 00:04:33.831
So we need to divide these join counts
of the two words by the count of

00:04:33.831 --> 00:04:35.090
the previous word.

00:04:37.090 --> 00:04:40.360
Here's the unigram counts we're going to
need to compute those probabilities.

00:04:40.360 --> 00:04:43.842
So here's the count of i, it's 2533.

00:04:43.842 --> 00:04:46.082
Here's the count of eat, it's 746.

00:04:46.082 --> 00:04:50.270
And using the equation,
we can now compute the bigram probability.

00:04:50.270 --> 00:04:53.879
The probability for
example, of to given want,

00:04:53.879 --> 00:04:59.257
how likely given that the previous word
was want that the next word is to.

00:04:59.257 --> 00:05:01.670
And it's pretty likely, for example, 0.66.

00:05:03.720 --> 00:05:07.668
But notice that things with counts
of 0 still have probabilities of 0,

00:05:07.668 --> 00:05:09.460
lots of things have zeros here.

00:05:11.790 --> 00:05:15.570
So now that we've computed all
of our bigram probabilities,

00:05:15.570 --> 00:05:18.650
we can now estimate the probability
of a sentence, that's our goal for

00:05:18.650 --> 00:05:24.710
language modeling, simply by multiplying
together all the component probabilities.

00:05:24.710 --> 00:05:27.910
So the probability of start of sentence,
I want English food,

00:05:27.910 --> 00:05:32.110
end of sentence is the probability of
I given start times the probability

00:05:32.110 --> 00:05:36.200
of want given I times the probability of
English given want, food given english,

00:05:36.200 --> 00:05:37.610
start given food, and so on.

00:05:39.240 --> 00:05:43.040
What kinds of knowledge are expressed
by these bigram probabilities?

00:05:44.460 --> 00:05:47.220
Why, for example, is the probability of

00:05:47.220 --> 00:05:50.420
English given want lower than
the probability of Chinese given want?

00:05:51.580 --> 00:05:55.350
Well, probably that's because
Chinese food is more popular and

00:05:55.350 --> 00:05:57.160
more people are going to ask about it.

00:05:57.160 --> 00:06:00.240
And so wanting Chinese is more
likely than wanting English.

00:06:00.240 --> 00:06:03.890
So that's a fact about the world,
it's a fact about cuisines, not so

00:06:03.890 --> 00:06:05.550
much a fact about English.

00:06:05.550 --> 00:06:08.570
What about the probability of
to given want being so high?

00:06:09.830 --> 00:06:11.050
Now, that's a fact about grammar,

00:06:11.050 --> 00:06:15.590
that's a fact that the verb want in
English requires an infinitive after it.

00:06:15.590 --> 00:06:20.190
So want has infinitives, and that's
a grammatical fact, so that's grammar.

00:06:26.190 --> 00:06:30.246
And what about the probability that
want given that the previous word is

00:06:30.246 --> 00:06:31.000
spend is 0?

00:06:32.298 --> 00:06:35.260
So that 0 seems to be caused
by grammatical facts,

00:06:35.260 --> 00:06:37.970
spend want is two verbs in a row.

00:06:37.970 --> 00:06:41.890
That kind of verb doesn't seem to be
grammatically possible in English.

00:06:41.890 --> 00:06:47.080
So that 0 is caused by
a grammatical disallowing.

00:06:47.080 --> 00:06:47.800
How about this 0?

00:06:50.010 --> 00:06:56.360
Why is the probably of
food following to 0?

00:06:56.360 --> 00:07:00.520
Now, here, this 0 is a contingent 0.

00:07:00.520 --> 00:07:03.130
You could imagine a sentence
that has to food in it.

00:07:03.130 --> 00:07:05.580
I'd like you to stop and
think about a sentence like that yourself.

00:07:07.620 --> 00:07:11.883
Good, it just happens that such a sentence
never occurred in the training data.

00:07:11.883 --> 00:07:15.554
So this is a contingent 0,
where this is a structural 0.

00:07:15.554 --> 00:07:17.000
All right, let's move on.

00:07:20.160 --> 00:07:21.320
In practice,

00:07:21.320 --> 00:07:25.230
we don't keep these probabilities
in the form of probabilities.

00:07:25.230 --> 00:07:27.846
In fact, we keep them in
the form of log probabilities.

00:07:27.846 --> 00:07:30.290
And there are two reasons for this.

00:07:30.290 --> 00:07:32.100
One is, we can avoid underflow.

00:07:32.100 --> 00:07:34.110
If you think about it,
if you have a very long sentence and

00:07:34.110 --> 00:07:38.550
you're multiplying together 20 or
30 or 40 little tiny probabilities,

00:07:38.550 --> 00:07:42.480
each of which is a very small number
less than zero, when you multiply so

00:07:42.480 --> 00:07:45.160
many small numbers,
you get a very small number that often

00:07:45.160 --> 00:07:48.570
ends up with arithmetic
underflow in the computation.

00:07:48.570 --> 00:07:50.190
And so
we want to avoid this kind of underflow,

00:07:50.190 --> 00:07:53.860
and it turns out that adding is
faster than multiplying anyway.

00:07:53.860 --> 00:07:56.670
So again,
instead of multiplying four probabilities,

00:07:56.670 --> 00:07:59.252
we'll in general just add
four log probabilities.

00:07:59.252 --> 00:08:01.850
And we're going to store our
language models in logs.

00:08:04.230 --> 00:08:07.532
There are a number of publicly
available language modeling toolkits.

00:08:07.532 --> 00:08:12.948
One of them, SRILM.

00:08:12.948 --> 00:08:17.360
And so you can download SRILM.

00:08:17.360 --> 00:08:22.987
Another publicly available resource
is the Google N-Gram Release.

00:08:22.987 --> 00:08:25.506
So this has been out for
over five years now.

00:08:25.506 --> 00:08:28.663
Google released a trillion word corpus,

00:08:28.663 --> 00:08:33.770
over a trillion five-grams,
and 13 million unique words.

00:08:33.770 --> 00:08:38.480
So a huge data set which you
can download and use for

00:08:38.480 --> 00:08:40.489
any kind of n-gram applications
you'd like to use.

00:08:42.190 --> 00:08:45.890
So here's an example, some of the data
from the Google N-Gram Release.

00:08:45.890 --> 00:08:51.054
Some four-gram counts for
four-grams beginning with serve as the and

00:08:51.054 --> 00:08:55.420
words beginning with I am, so
this is a very big corpus.

00:08:55.420 --> 00:08:59.680
And you can see that serve as
the indication occurred 72 times on this

00:08:59.680 --> 00:09:00.690
web corpus.

00:09:00.690 --> 00:09:04.167
And you have more information
on the Google web corpus there.

00:09:07.390 --> 00:09:11.548
Another publicly available corpus
is the Google Book N-gram corpus,

00:09:11.548 --> 00:09:12.802
let's look at that.

00:09:16.994 --> 00:09:23.510
So this corpus lets you plot
counts of words in Google Books.

00:09:26.100 --> 00:09:28.810
And a number of corpora are available for
American English,

00:09:28.810 --> 00:09:31.630
British English, Chinese, French, and

00:09:31.630 --> 00:09:34.570
German, and various kinds of corpora
which can all be downloaded.

