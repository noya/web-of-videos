WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.842
[MUSIC]

00:00:04.842 --> 00:00:07.303
Stanford University.

00:00:07.303 --> 00:00:11.351
&gt;&gt; Network, there's actually a whole
class, I think next quarter,

00:00:11.351 --> 00:00:14.590
on just networks for computer vision.

00:00:14.590 --> 00:00:17.110
Where they've really
changed the entire field.

00:00:17.110 --> 00:00:19.310
In NLP, they've had some impact but
not as much,

00:00:19.310 --> 00:00:22.610
which is why we don't have
the entire lecture on just CNNs.

00:00:22.610 --> 00:00:24.500
But they are an interesting model family.

00:00:24.500 --> 00:00:30.670
They are paralyzable, they're very good
on GPUs, and so we'll sort of look into

00:00:30.670 --> 00:00:34.840
them in detail today, understand hopefully
by the end why they're so useful.

00:00:34.840 --> 00:00:40.170
And fast to implement on GPUs but really
also give you at least some intuition,

00:00:40.170 --> 00:00:44.170
to be honest there's much less intuition
behind some of these very advanced CNN

00:00:44.170 --> 00:00:49.780
architectures compared to even some of the
recurrent networks and LSTMs that we had.

00:00:49.780 --> 00:00:54.200
So we'll actually start today with
a mini tutorial of Azure and GPUs,

00:00:54.200 --> 00:01:00.390
we wanna encourage you all to really get
started on that as soon as possible.

00:01:00.390 --> 00:01:05.616
Also, thanks everybody for filling out
the survey I think this one is one

00:01:05.616 --> 00:01:11.714
of the important take away messages, which
is overall what do you think of the pace.

00:01:11.714 --> 00:01:16.240
We're very happy to see that the majority
are quite happy with the pace.

00:01:16.240 --> 00:01:20.830
It's kind of impossible with such
a large class to not be too fast and

00:01:20.830 --> 00:01:26.420
not too slow for 100% of everybody, since
people have vastly different backgrounds.

00:01:26.420 --> 00:01:30.780
Very sorry for
the little less than a third I think, for

00:01:30.780 --> 00:01:35.290
whom it's too fast,
I hope today will be, not quite as fast.

00:01:36.480 --> 00:01:41.520
And hopefully, in office hours and so
on we can make up for some of that.

00:01:41.520 --> 00:01:45.230
So we'll talk about a couple
of different CNN variants.

00:01:45.230 --> 00:01:50.540
We'll have a fun research highlight on
character-aware neural language models.

00:01:50.540 --> 00:01:53.320
And then, we'll actually also look
a little bit into tips and tricks that

00:01:53.320 --> 00:01:59.020
are slightly more practical, and you'll
observe that these practical details and

00:01:59.020 --> 00:02:03.740
tricks actually making this particular CNN
architecture work are super important and

00:02:03.740 --> 00:02:08.750
without it you really lose 10% or
so of accuracy.

00:02:08.750 --> 00:02:10.760
Look at it a little critically.

00:02:10.760 --> 00:02:14.387
At some of the evaluations that
are going on in the field, and

00:02:14.387 --> 00:02:18.444
then I will compare a couple of
different models which will lead us to

00:02:18.444 --> 00:02:22.856
a very new model called the
quasi-recurrent neural network for treaty,

00:02:22.856 --> 00:02:25.440
which just came out
a couple of months ago.

00:02:25.440 --> 00:02:28.940
With that, I’ll do one organization
slide before we go onto Azure.

00:02:28.940 --> 00:02:33.540
So project advice office hours, I would
really encourage everybody who’s doing

00:02:33.540 --> 00:02:37.490
a project to now come to project
advice office hours every week.

00:02:37.490 --> 00:02:40.820
I’ve asked groups that I’m
mentoring personally to also

00:02:40.820 --> 00:02:42.360
As a server requirement.

00:02:42.360 --> 00:02:45.280
Not all the groups were
able to come every week.

00:02:45.280 --> 00:02:46.930
I encourage you all to come.

00:02:46.930 --> 00:02:51.674
I am keeping track of
whether you're there.

00:02:51.674 --> 00:02:56.619
So also for everybody who basically is
still undecided whether they should

00:02:56.619 --> 00:03:01.508
move on with their project, you'll
see kind of where PA4 folks should be

00:03:01.508 --> 00:03:06.113
at in the next week or so, where you
have to have run some baselines on

00:03:06.113 --> 00:03:09.901
your data set by now if you're
doing your final project.

00:03:09.901 --> 00:03:14.266
If you don't even have your dataset
ready yet, you can't even run a simple,

00:03:14.266 --> 00:03:19.236
let's say, bag of vectors, kinda baseline,
it's starting to be really worrisome,

00:03:19.236 --> 00:03:22.900
so definitely make sure you
start running your experiments.

00:03:22.900 --> 00:03:26.410
Some simple things, baselines, could be
just any, just could be your regressions.

00:03:26.410 --> 00:03:28.128
You download some code somewhere, but

00:03:28.128 --> 00:03:30.323
you need to make sure you
have your data set ready.

00:03:30.323 --> 00:03:33.340
Otherwise, it'll be too late.

00:03:33.340 --> 00:03:36.313
And for
PA4 folks we actually enforce that with

00:03:36.313 --> 00:03:40.563
a little additional deadline just
to make sure you're really all.

00:03:40.563 --> 00:03:44.391
Going to be able to run it cuz this is
not one of those things that you can

00:03:44.391 --> 00:03:48.087
cram really hard and you work 10x and
so you make 10x to progress

00:03:48.087 --> 00:03:52.530
because your experiments will take a day
to run and so you run for one day.

00:03:52.530 --> 00:03:54.380
Turns out at the end you had a bug and

00:03:54.380 --> 00:03:56.990
then the deadline was there and
you have nothing.

00:03:56.990 --> 00:03:58.830
So it happens every year.

00:03:58.830 --> 00:04:01.920
And we really want to make sure
it doesn't happen this year even

00:04:01.920 --> 00:04:02.950
though we're a bigger class.

00:04:02.950 --> 00:04:05.780
So we'll talk about that soon.

00:04:05.780 --> 00:04:10.289
Also, in terms of positive motivation,
there's actually going to be a really

00:04:10.289 --> 00:04:13.455
awesome poster session that
we're putting together,

00:04:13.455 --> 00:04:16.618
we have corporate sponsors
that give us some money, and

00:04:16.618 --> 00:04:20.560
they will allow us to basically give
out price, have prices for you.

00:04:20.560 --> 00:04:24.394
We'll make it public, so
hopefully a lot of folks will show up and

00:04:24.394 --> 00:04:25.940
check out your research.

00:04:25.940 --> 00:04:30.513
It's a lot of excitement both from various
companies, VCs, if you have a really

00:04:30.513 --> 00:04:35.104
awesome poster, who knows, at the end you
may have some funding for your start up.

00:04:35.104 --> 00:04:40.186
And we'll have food also, very nice
catering, so should be really fun poster

00:04:40.186 --> 00:04:45.200
session, so hopefully you can be very
excited about that and your projects.

00:04:45.200 --> 00:04:45.700
Yeah?

00:04:47.450 --> 00:04:48.765
Will there be enough food for everybody?

00:04:48.765 --> 00:04:51.430
&gt;&gt; [LAUGH]
&gt;&gt; It’s a good question.

00:04:51.430 --> 00:04:52.750
We’ll spend thousands and

00:04:52.750 --> 00:04:56.400
thousands of dollars on food we hope
there will be enough food for everybody.

00:04:56.400 --> 00:04:58.350
Schein is organizing it she's nodding.

00:04:58.350 --> 00:04:58.850
Yes.

00:05:03.737 --> 00:05:07.490
Any other organizational questions
around the Poster Areas Project.

00:05:10.214 --> 00:05:14.043
All right,
then take it away on the GPU side.

00:05:14.043 --> 00:05:15.635
&gt;&gt; [INAUDIBLE].

00:05:15.635 --> 00:05:17.950
&gt;&gt; Nope, you're good.

00:05:17.950 --> 00:05:18.840
&gt;&gt; All right, everyone.

00:05:18.840 --> 00:05:21.390
This is just intended to be a short
public service announcement basically

00:05:21.390 --> 00:05:25.310
about how to get started with Azure and
why you should get started with Azure.

00:05:26.520 --> 00:05:30.562
By now, every team should have received
an email to at least one of your team

00:05:30.562 --> 00:05:35.439
members, probably to your Stanford, one of
your Stanford emails, and you'll have this

00:05:35.439 --> 00:05:39.885
message which is basically an invitation
to join our CS224N subscription.

00:05:39.885 --> 00:05:43.871
And using following the instructions
of this email you should sign up for

00:05:43.871 --> 00:05:45.188
basically GPU access.

00:05:45.188 --> 00:05:50.456
So far only 161 people have signed up or
teams have signed up out of the 311,

00:05:50.456 --> 00:05:54.717
and essentially we want this number
to increase because everyone

00:05:54.717 --> 00:05:59.540
should be using GPUs for
reasons that we'll cover very shortly.

00:05:59.540 --> 00:06:02.050
And if you have any issues signing up,

00:06:02.050 --> 00:06:07.160
then please report the problems
that you have to Piazza post 1830,

00:06:07.160 --> 00:06:12.470
which has the form, also screenshotted
there and we'll help you, essentially,

00:06:12.470 --> 00:06:16.580
through any of the problems that you
have with their subscriptions, cool.

00:06:16.580 --> 00:06:19.120
So then, more important question that
we're gonna go over is why should you

00:06:19.120 --> 00:06:20.700
really care about the GPUs.

00:06:20.700 --> 00:06:24.990
Well, first, yesterday we actually
announced the milestone for

00:06:24.990 --> 00:06:27.160
the final project and the homework.

00:06:27.160 --> 00:06:29.230
It's intended to be something
very quick and easy,

00:06:29.230 --> 00:06:30.880
just a paragraph of what you've done.

00:06:30.880 --> 00:06:35.250
But we expect you to have used I always
experimented with running some code on

00:06:35.250 --> 00:06:36.810
a GPU like that and

00:06:36.810 --> 00:06:41.890
this will be worth essentially 2% of your
final grade just if you do it or not.

00:06:41.890 --> 00:06:45.940
But really down there the better reason
of why you should be using GPU's

00:06:45.940 --> 00:06:50.900
is GPU's will train your models much, much
faster over a much, much larger data set.

00:06:52.180 --> 00:06:54.900
And specifically,
Microsoft has offered us,

00:06:54.900 --> 00:06:58.910
I think 311 MB6 instances
on their 0 cloud.

00:06:58.910 --> 00:07:02.957
These use Tesla GPU's, M60,
if you're interested in the model.

00:07:02.957 --> 00:07:06.575
The specifications are,
they have a huge number of CUDA cores,

00:07:06.575 --> 00:07:11.600
a huge amount of graphics memory, and
they cost a huge amount of money each.

00:07:11.600 --> 00:07:12.690
You also get a nice CPU,

00:07:12.690 --> 00:07:16.640
as well as a lot of system memory,
to go along with your Instance.

00:07:16.640 --> 00:07:19.930
And the key takeaway here
is that these ar not your

00:07:19.930 --> 00:07:22.290
average hardware that you
have in your local machine.

00:07:23.420 --> 00:07:26.390
There's gonna be way more power, in terms
of the CPU, in terms of the GPU, and

00:07:26.390 --> 00:07:32.420
in terms of well, even for the gaming
whatever hardware you have at home.

00:07:32.420 --> 00:07:35.330
And the speed-ups will be 10 to 20,
maybe even 100,

00:07:35.330 --> 00:07:36.849
depending on the libraries
that you're running.

00:07:38.170 --> 00:07:42.627
So in conclusion, please do get started
on Azure as soon as possible, fill out

00:07:42.627 --> 00:07:46.542
the form if you run into subscription
issues, come to office hours or

00:07:46.542 --> 00:07:51.831
file support tickets if you have technical
problems such as not being able to etc.

00:07:51.831 --> 00:07:57.019
And then, also see our step-by-step guide
to just get started with the process.

00:07:57.019 --> 00:07:59.260
For Homework 4,

00:07:59.260 --> 00:08:02.730
The full assignment handout will go
over essentially all the details.

00:08:02.730 --> 00:08:05.728
But decent models will
take a long time to train.

00:08:05.728 --> 00:08:09.850
They'll take one hour plus per epoch,
even on a strong GPU,

00:08:09.850 --> 00:08:12.050
such as the previously described ones.

00:08:12.050 --> 00:08:14.980
If you don't deal with a GPU,
you'll be spending a week, basically,

00:08:14.980 --> 00:08:16.520
just to train a baseline model.

00:08:17.970 --> 00:08:23.950
And for the final project, if your project
is sufficiently challenging that needs,

00:08:23.950 --> 00:08:28.110
well, you have enough data or your problem
is sufficiently challenging, you really do

00:08:28.110 --> 00:08:31.920
want to use a GPU if you want to
receive a good score in this class.

00:08:31.920 --> 00:08:33.410
And that would be all.

00:08:33.410 --> 00:08:37.298
&gt;&gt; Cool.
Thanks, James.

00:08:37.298 --> 00:08:40.823
And by decent model he also means
decent implementations, so if your

00:08:40.823 --> 00:08:44.860
implementations isn't super well-optimized
it will take you even longer.

00:08:44.860 --> 00:08:49.180
So again, not something you can cram on
in the last couple of days of the class.

00:08:50.510 --> 00:08:53.895
All right, any questions about Azure or
[INAUDIBLE]?

00:08:59.672 --> 00:09:02.880
What if we're not in the same group
between homework three and four?

00:09:21.310 --> 00:09:25.947
So recurrent neural networks were
pretty awesome and are pretty awesome

00:09:25.947 --> 00:09:30.673
actually and a lot of times the default
model, but they have some issues.

00:09:30.673 --> 00:09:36.150
Namely, they can't really
capture phrases in isolation.

00:09:36.150 --> 00:09:41.530
They can really only capture a phrase
given it's left side context.

00:09:41.530 --> 00:09:42.630
So what do we mean by this?

00:09:43.690 --> 00:09:49.040
If I want to have just a representation
of my birth, in this whole sentence,

00:09:49.040 --> 00:09:52.890
well recurrent network will
always go from left to right.

00:09:54.080 --> 00:09:58.450
And so, that phrase vector up there
isn't going to just capture my birth,

00:09:58.450 --> 00:09:59.950
it will also capture the country of.

00:10:00.980 --> 00:10:03.780
And so, sometimes when you have
simple classification problems

00:10:03.780 --> 00:10:07.040
you might actually just want to
identify that there's a certain word or

00:10:07.040 --> 00:10:10.830
phrase in that over all document and
just try to give

00:10:10.830 --> 00:10:15.330
the fact that that phrase exist in your
overall document to somewhere higher up in

00:10:15.330 --> 00:10:18.220
the final classifier that
actually needs to classify this.

00:10:18.220 --> 00:10:20.920
But here,
you will always go from left to right or

00:10:20.920 --> 00:10:24.370
even if you have a bidirectional one,
you go from right to left.

00:10:24.370 --> 00:10:27.220
But then you have the same problem,
but on the other side.

00:10:27.220 --> 00:10:29.340
Namely, the intermediate,

00:10:29.340 --> 00:10:32.660
the words in the center of
a longer document might get lost.

00:10:32.660 --> 00:10:36.040
You really have to keep track
of them through every iteration.

00:10:36.040 --> 00:10:38.910
And, of course, if you're using
LSTMs are better at doing that,

00:10:38.910 --> 00:10:43.210
they're better able to say,
don't turn on the forget gate,

00:10:43.210 --> 00:10:46.390
keep some things around, keep certain
units on when you see something.

00:10:46.390 --> 00:10:49.380
But it requires a lot of the model
to be able to do that perfectly.

00:10:51.480 --> 00:10:55.890
And so, in many of the cases you will see
your classifiers only at the very end,

00:10:55.890 --> 00:11:00.340
once it has read the whole sentence and
that is not the issue cuz now,

00:11:00.340 --> 00:11:02.680
again, the grading has
to flow through this.

00:11:02.680 --> 00:11:06.870
And despite all the [INAUDIBLE] and LSTM,
it's even hard for them to keep very

00:11:06.870 --> 00:11:12.020
complex kinds of relationships alive
over many, many different time steps.

00:11:13.700 --> 00:11:20.240
So that's one issue with RNNs
that CNNs are trying to resolve.

00:11:20.240 --> 00:11:25.290
Now, the main idea here is instead of
computing a single representation of

00:11:25.290 --> 00:11:30.341
vector at every time step that captures
basically the context on the left so

00:11:30.341 --> 00:11:33.787
far what if we could just
compute a phrase vector for

00:11:33.787 --> 00:11:37.258
every single phrase that
we have in this sentence.

00:11:37.258 --> 00:11:41.641
So if we have here the phrase a country
of my birth, we might compute

00:11:41.641 --> 00:11:46.574
in the very first step of this kinds
of convolutional networks if vector for

00:11:46.574 --> 00:11:50.220
the country just this
two words in isolation.

00:11:50.220 --> 00:11:56.540
Just country of my birth so
basically compute a vector for

00:11:56.540 --> 00:11:59.780
all the by grams in the sentence and
then another one maybe for

00:11:59.780 --> 00:12:04.450
all the trigrams,
country of my birth, the country.

00:12:04.450 --> 00:12:07.685
And then, for all the fourgrams,
the country of my birth.

00:12:08.780 --> 00:12:11.083
So hoping that if this was, for instance,

00:12:11.083 --> 00:12:15.300
sentiment classification, that one of
these said, not very good, for instance.

00:12:15.300 --> 00:12:20.410
And then, if we captured that vector and
we kind of will try to eventually.

00:12:20.410 --> 00:12:24.260
Handle and push that vector all
the way to a softmax through some

00:12:24.260 --> 00:12:26.230
other forms that I'll describe soon.

00:12:26.230 --> 00:12:29.110
But that is basically the idea
of the very first layer

00:12:29.110 --> 00:12:31.110
of a convolutional network for NLP.

00:12:32.160 --> 00:12:35.340
And this will basically compute

00:12:35.340 --> 00:12:38.580
these phrase vectors regardless of
whether that is a grammatical phrase.

00:12:38.580 --> 00:12:43.640
So we know from parsing, for instance,
certain phrases like a country

00:12:43.640 --> 00:12:48.612
of is not really a proper noun phrase,
it's sort of an odd,

00:12:48.612 --> 00:12:54.330
ungrammatical chunk but this motto
really doesn't care linguistic or

00:12:54.330 --> 00:12:57.780
cognitive possibility in any
kind of way for language.

00:12:57.780 --> 00:13:02.710
And so, people don't read sentences
that way, but you might be

00:13:02.710 --> 00:13:06.900
able to eventually compute several of
these representations in parallel.

00:13:06.900 --> 00:13:08.880
And that's going to be a big advantage.

00:13:09.970 --> 00:13:13.002
So once we compute all these vectors,

00:13:13.002 --> 00:13:17.849
we'll group them after, but
we'll get to that in a second.

00:13:17.849 --> 00:13:20.838
So you might ask,
what is convolution, anyway?

00:13:20.838 --> 00:13:26.880
And so, here is a very simple definition
for any convolutional operator.

00:13:26.880 --> 00:13:30.805
So let's look at the simplest
case of a 1d discrete convolution

00:13:31.835 --> 00:13:37.795
of a filter over another function,
at a specific point in time.

00:13:37.795 --> 00:13:41.245
You'll basically have a filter size,
here M, and

00:13:41.245 --> 00:13:47.035
you'll basically multiply just a filter
at different locations of this input.

00:13:47.035 --> 00:13:52.402
And so, in computer vision, that will
help us to extract very meaningful

00:13:52.402 --> 00:13:57.959
features such as edges from an image and
eventually more complex features.

00:13:57.959 --> 00:14:02.724
And for 2d example, which you'll observe
a lot in computer vision, we have this

00:14:02.724 --> 00:14:07.421
really great animation here from the
Stanford Unsupervised Feature Learning and

00:14:07.421 --> 00:14:09.680
Deep Learning wiki page.

00:14:09.680 --> 00:14:12.526
So imagine you had an image
that you see here in green.

00:14:12.526 --> 00:14:15.190
And that image, let's say, is only binary.

00:14:15.190 --> 00:14:20.455
The first row of this image is 1, 1,
1, 0, 0 and the second row of pixels

00:14:20.455 --> 00:14:25.727
of this binary image is 0, 1, 1, 1,
0 and so on, and you have a filter.

00:14:25.727 --> 00:14:31.535
And this filter here has number that
you'll see in the small red font here, and

00:14:31.535 --> 00:14:37.175
I’ll turn the animation off for a second
so we can look at it without moving.

00:14:37.175 --> 00:14:44.126
Now, the filter here is basically 1,
0, 1, 0, 1, 0, 1, 0, 1.

00:14:44.126 --> 00:14:48.125
And now every time step
of the convolution,

00:14:48.125 --> 00:14:55.598
we’re going to multiply the numbers of
the filter with the numbers off the image.

00:14:55.598 --> 00:15:00.093
We multiply, again,
the red numbers from the filter with the,

00:15:00.093 --> 00:15:03.840
images, with the image values and
that will result,

00:15:03.840 --> 00:15:07.435
basically multiply all of
them then we sum them up.

00:15:07.435 --> 00:15:12.303
So very simple in our product if we were
to vectorize these three by three blocks

00:15:12.303 --> 00:15:16.877
into, and nine dimensional vector and
we just have a simple inner product

00:15:16.877 --> 00:15:21.910
between those two vectors or we just
multiply them here and then sum them up.

00:15:21.910 --> 00:15:27.381
So one times one plus one times zero plus
one times one and so on will sum to four.

00:15:27.381 --> 00:15:31.664
And we'll basically move
this filter one time step,

00:15:31.664 --> 00:15:34.620
one pixel at a time across the image.

00:15:37.640 --> 00:15:42.040
So let's look again, this looks like
basically multiply all the numbers and

00:15:42.040 --> 00:15:43.120
then sum them up.

00:15:43.120 --> 00:15:47.400
And then, we'll move one down, and
again move from left to right.

00:15:49.320 --> 00:15:49.950
Any questions, yeah?

00:15:56.106 --> 00:15:57.210
That's a great question.

00:15:57.210 --> 00:15:59.930
What would be the equivalent
of a pixel in LP and

00:15:59.930 --> 00:16:03.170
yes, you're exactly right,
it will be a word vector.

00:16:03.170 --> 00:16:04.075
Before I jump there,

00:16:04.075 --> 00:16:07.607
are there any more questions about the
general definition of convolution, yeah?

00:16:10.423 --> 00:16:12.575
How do we decide on the convolution?

00:16:13.767 --> 00:16:16.490
So how do we decide what matrix it is?

00:16:16.490 --> 00:16:19.160
The matrix of the convolution
of filter here,

00:16:19.160 --> 00:16:21.230
these red numbers are actually
going to be learned.

00:16:21.230 --> 00:16:25.270
So you have an input and then you do
back propagation through a network,

00:16:25.270 --> 00:16:28.320
we'll get to eventually it'll have
the same kind of cross entropy error,

00:16:28.320 --> 00:16:29.800
that we have for all the other ones.

00:16:29.800 --> 00:16:31.100
It'll have a softmax and

00:16:31.100 --> 00:16:34.640
we're going to basically back propagate
through this entire architecture, and

00:16:34.640 --> 00:16:38.880
then we'll actually update the weights
here in this particular example in red.

00:16:38.880 --> 00:16:40.930
After they started with
some random initialization.

00:16:40.930 --> 00:16:42.540
And then we'll update them and
they'll change.

00:16:42.540 --> 00:16:44.650
And what's kind of interesting
in computer vision,

00:16:44.650 --> 00:16:48.510
which I won't go into too many details in
this class, but in computer vision they

00:16:48.510 --> 00:16:52.340
learn eventually to detect
certain edges in the first layer.

00:16:52.340 --> 00:16:56.330
In the second layer they'll learn to
detect certain combinations of edges like

00:16:56.330 --> 00:17:01.240
corners, and the third layer they
will learn to basically detect and

00:17:01.240 --> 00:17:04.780
have a very high activation,
these guys here.

00:17:04.780 --> 00:17:09.220
A very high activation when you see

00:17:09.220 --> 00:17:11.670
more complex patterns like stripes and
things like that.

00:17:11.670 --> 00:17:14.680
And as you go higher up through
convolution networks and computer vision,

00:17:14.680 --> 00:17:17.850
you can actually very nicely
visualize what's going on, and

00:17:17.850 --> 00:17:21.810
you identify, like the fifth layer
some neurons actually fire when,

00:17:21.810 --> 00:17:25.240
they see a combination of eyes and
a nose and a mouth.

00:17:25.240 --> 00:17:27.200
Sadly, for NLP we don't have any of that.

00:17:27.200 --> 00:17:29.450
It's one of the reason's they're
not quite as popular in NLP.

00:17:30.590 --> 00:17:31.090
Yep.

00:17:33.660 --> 00:17:35.642
Sure, so you have here m, you filter.

00:17:35.642 --> 00:17:39.810
So in the 1d case, that'll just be, f and

00:17:39.810 --> 00:17:44.510
g are just a single number, and
now you're going to move over f.

00:17:44.510 --> 00:17:47.240
So imagine this was just one dimension.

00:17:47.240 --> 00:17:52.062
And so you move from minus M to M,
as in for the nth time step,

00:17:52.062 --> 00:17:56.406
you're going to multiply the filter,
g[m] here,

00:17:56.406 --> 00:18:01.618
over this function input, and
basically go one times step, and

00:18:01.618 --> 00:18:06.955
you sum up this product between
the two numbers at each time step.

00:18:06.955 --> 00:18:08.470
Does that make sense?

00:18:08.470 --> 00:18:12.480
So you go from minus m,
which if this is minus and

00:18:12.480 --> 00:18:17.592
this is minus, so you start head of n,
n time steps away, and

00:18:17.592 --> 00:18:23.560
then you keep multiplying the numbers
until you have the whole sum.

00:18:23.560 --> 00:18:26.255
And then you have your convolution
at that discrete time step n.

00:18:26.255 --> 00:18:30.706
&gt;&gt; [INAUDIBLE]
&gt;&gt; That's right, m is your window size.

00:18:33.356 --> 00:18:36.840
And we'll go over the exact examples for
NLP in much more detail.

00:18:36.840 --> 00:18:37.340
Yeah.

00:18:39.276 --> 00:18:41.930
How do we figure out the window size?

00:18:41.930 --> 00:18:44.980
We'll actually have a bunch of window
sizes, so maybe this is a good side way

00:18:44.980 --> 00:18:47.140
to talk about the actual
model that we'll use for NLP.

00:18:48.170 --> 00:18:50.303
So this is going to be the first and

00:18:50.303 --> 00:18:53.975
most simple variant of
a convolutional network for NLP.

00:18:53.975 --> 00:18:56.792
You can [INAUDIBLE] go to town and
towards the end they'll,

00:18:56.792 --> 00:19:01.000
show you some examples of how we can
embellish this architecture a lot more.

00:19:01.000 --> 00:19:05.660
This one is based on a really seminal
paper by Collobert and Weston from 2011.

00:19:05.660 --> 00:19:10.183
And then the very particular model
in its various queuing details,

00:19:10.183 --> 00:19:12.890
came from Kim from just three years ago.

00:19:13.950 --> 00:19:16.890
Basically the paper title is,
a Convolutional Neural Network for

00:19:16.890 --> 00:19:18.440
Sentence Classification.

00:19:18.440 --> 00:19:21.690
All right, so as with every model out
there, whenever you wanna write down your

00:19:21.690 --> 00:19:25.770
equations, no worries, we're not gonna
go into a lot more derivatives today.

00:19:25.770 --> 00:19:27.075
Actually no derivatives,

00:19:27.075 --> 00:19:30.830
cuz all the math is really similar
to math we've done before.

00:19:30.830 --> 00:19:34.870
But it's really still important to
identify very clearly your notation.

00:19:34.870 --> 00:19:36.210
So let's start.

00:19:36.210 --> 00:19:39.690
As the question was correctly asked,
we'll actually start with word vectors.

00:19:39.690 --> 00:19:44.720
So we'll have at every time step,
I will have a word vector Xi.

00:19:44.720 --> 00:19:48.210
And that will be here for
us now a k dimensional vector.

00:19:49.700 --> 00:19:54.050
And then we'll represent the entire
sentence through a concatenation.

00:19:54.050 --> 00:19:58.200
So we'll use this plus and

00:19:58.200 --> 00:20:03.380
circle symbol, for concatenating
the vectors of all the words.

00:20:03.380 --> 00:20:08.366
And so we'll describe the entire sentence,
which we'll have for

00:20:08.366 --> 00:20:12.651
our definition here,
n-many words to be X from one to n.

00:20:12.651 --> 00:20:17.410
And that will be the concatenation
of the first to the nth word vector.

00:20:17.410 --> 00:20:17.913
Yeah.

00:20:21.136 --> 00:20:23.440
Great question,
are word vectors concatenated length-wise?

00:20:23.440 --> 00:20:24.020
Yes.

00:20:24.020 --> 00:20:27.570
For now we'll assume they're
all concatenated as a long row.

00:20:34.058 --> 00:20:38.247
All right now we'll introduce
this additional notation here, so

00:20:38.247 --> 00:20:43.105
we don't just go from one to n, but
we might actually want to extract specific

00:20:43.105 --> 00:20:46.689
words in the range,
from time step i to time step i plus j,

00:20:46.689 --> 00:20:50.050
or in general some other
number of time steps.

00:20:50.050 --> 00:20:54.980
So if I have, for instance x two to four,
then I'll take the second, the third,

00:20:54.980 --> 00:20:56.540
and the fourth word vector, and

00:20:56.540 --> 00:21:01.260
I just have a long vector with just those
three word vectors concatenated together.

00:21:04.476 --> 00:21:07.049
I'll let that sink in,
cuz it's all very simple but

00:21:07.049 --> 00:21:09.750
we just need to make sure we
keep track of the notation.

00:21:11.180 --> 00:21:16.081
So in general, our convolutional
filter here will be a vector w

00:21:16.081 --> 00:21:20.797
of parameters, that we're going
to learn with our standard

00:21:20.797 --> 00:21:25.621
stochastic gradient descent-type
optimization methods.

00:21:25.621 --> 00:21:29.362
And we'll define this
convolutional filter here,

00:21:29.362 --> 00:21:33.970
in terms of its window size and
of course the word vector size.

00:21:33.970 --> 00:21:37.270
So h times k, so this is just a vector,
it's not a matrix.

00:21:37.270 --> 00:21:39.090
There's no times in between the two.

00:21:39.090 --> 00:21:42.120
But let's say we want to have
a convolutional filter that at

00:21:42.120 --> 00:21:46.610
each time step, looks at three different
word vectors and tries to combine

00:21:46.610 --> 00:21:50.440
them into a single number, or
some kind of feature representation.

00:21:50.440 --> 00:21:55.370
What we'll then do is,
basically have a three

00:21:55.370 --> 00:21:59.140
times number of dimensions
of each word vector filter.

00:22:00.940 --> 00:22:04.460
So I have a very simple example here.

00:22:04.460 --> 00:22:07.590
Let's say we have two dimensional
word vectors, of course just for

00:22:07.590 --> 00:22:10.020
illustration they'll usually
be 50 dimensional or so.

00:22:10.020 --> 00:22:12.300
Let's say we have two
dimensional word vectors, and

00:22:12.300 --> 00:22:17.910
we look at three different words in
concatenation at each time step,

00:22:17.910 --> 00:22:20.400
we'll basically have
a six dimensional w here.

00:22:27.024 --> 00:22:30.386
All right.

00:22:30.386 --> 00:22:36.640
So now how do we actually compute
anything and why is it a neural network?

00:22:36.640 --> 00:22:39.315
We'll have some non-linearity
here eventually.

00:22:39.315 --> 00:22:43.210
Okay but before we get there,
let's look at again,

00:22:43.210 --> 00:22:48.765
we have our convolutional filter,
goes looks at h words at each time step.

00:22:48.765 --> 00:22:53.035
And again note that w here is
just a single vector; just as

00:22:53.035 --> 00:22:56.645
our word vectors are also
concatenated into a single vector.

00:22:56.645 --> 00:23:01.705
And now in order to compute
a feature at one time step for this,

00:23:01.705 --> 00:23:06.065
what we're going to do is basically just
have an inner product of this w vector of

00:23:06.065 --> 00:23:12.320
parameters, times the i-th time
step plus our window size.

00:23:12.320 --> 00:23:16.902
So in this case here,
we're going to in the c one for

00:23:16.902 --> 00:23:22.027
instance, we'll have W times x one two,
one two three.

00:23:22.027 --> 00:23:23.946
So we have here three, so
one plus three minus one goes to three.

00:23:23.946 --> 00:23:28.877
So we basically just have
the concatenation of those word

00:23:28.877 --> 00:23:31.228
vectors in our product.

00:23:31.228 --> 00:23:36.653
Simple sort of multiplication and
sum of all the element wise.

00:23:39.220 --> 00:23:40.994
Elements of these vectors.

00:23:40.994 --> 00:23:44.192
Then usually,
we'll have our standard bias term and

00:23:44.192 --> 00:23:46.617
we'll add a non-linearity at the end.

00:23:52.108 --> 00:23:54.030
Any questions about.

00:24:11.340 --> 00:24:12.010
That's a great question.

00:24:12.010 --> 00:24:17.060
So, as you do this, the question is don't
the words in the middle appear more often.

00:24:17.060 --> 00:24:21.620
So here, actually show this example,
and I have actually an animation, so

00:24:21.620 --> 00:24:22.640
you are jumping a little bit ahead.

00:24:22.640 --> 00:24:26.250
So what happens for instance,
at the very end here, and

00:24:26.250 --> 00:24:31.990
the answer will just come
have zeros there for the end.

00:24:31.990 --> 00:24:37.180
We'll actually call this
a narrow convolution and

00:24:37.180 --> 00:24:39.890
where you can actually have wide
convolutions which we'll get to later,

00:24:39.890 --> 00:24:44.130
but yes, you're right the center
words will appear more often,

00:24:44.130 --> 00:24:48.860
but really the filters can
adapt to that because you learn

00:24:48.860 --> 00:24:53.310
sort of how much you want to care about
any particular input in the filter.

00:24:59.182 --> 00:25:01.698
Okay, so
let's define this more carefully so

00:25:01.698 --> 00:25:04.290
we can think through the whole process,
yeah?

00:25:22.796 --> 00:25:23.895
So the question is,

00:25:23.895 --> 00:25:28.915
rephrase it a little bit, what happens
when we have different length sentences?

00:25:28.915 --> 00:25:32.935
And there will actually be in two
slides a very clever answer to that.

00:25:32.935 --> 00:25:36.195
Which is at some point we'll
add a pooling operator,

00:25:36.195 --> 00:25:39.415
which will just look at the maximum
value across everything.

00:25:39.415 --> 00:25:40.545
We'll get to that in a second.

00:25:40.545 --> 00:25:43.065
And it turns out the length

00:25:43.065 --> 00:25:46.790
of the sentence doesn't matter that
much once we do some clever pooling.

00:25:51.310 --> 00:25:54.060
How's the size of the filtering
affecting the learning?

00:25:54.060 --> 00:25:55.980
Actually quite significantly.

00:25:55.980 --> 00:26:00.620
One, the longer your filter is the more
computation you have to do and

00:26:00.620 --> 00:26:02.350
the longer context you can capture.

00:26:02.350 --> 00:26:05.710
So for instance if you just had a one
d filter it would just multiply and

00:26:05.710 --> 00:26:07.970
matrix with every word vector and
it actually would,

00:26:07.970 --> 00:26:12.120
you wouldn't gain much, because it would
just transform all the word vectors, and

00:26:12.120 --> 00:26:17.510
you may as well store
transformed word vectors.

00:26:20.160 --> 00:26:24.780
As you go to longer filters, you'll
actually be able to capture more phrases,

00:26:24.780 --> 00:26:28.480
but now you'll also more
likely to over-fit your model.

00:26:28.480 --> 00:26:33.030
So that will actually be, the size of
your filter will be hyperparameter, and

00:26:33.030 --> 00:26:33.870
there are some tricks.

00:26:33.870 --> 00:26:35.790
Namely, you have multiple filters for
multiple lengths,

00:26:35.790 --> 00:26:39.390
which we'll get to in a second, too,
that will allow you to get rid of that.

00:26:42.737 --> 00:26:47.431
Alright, so, let's say again here,
we have our sentence,

00:26:47.431 --> 00:26:51.314
now we have all these possible windows or
length h,

00:26:51.314 --> 00:26:56.900
starting at the first word vector,
going to this, and so on.

00:26:56.900 --> 00:27:02.040
And now what the means is, since we do
this computation here at every time step,

00:27:02.040 --> 00:27:06.990
we'll have basically what
we call a feature map.

00:27:06.990 --> 00:27:10.130
And we will capitalize this
here as having a vector

00:27:10.130 --> 00:27:12.290
of lots of these different c values.

00:27:12.290 --> 00:27:16.590
And again, each c value was
just taking that same w and

00:27:16.590 --> 00:27:20.057
having inter-products with a bunch of
the different windows at each time stamp.

00:27:21.780 --> 00:27:26.220
Now, this c vector is going to be
a pretty long, n-h+1 dimensional vector.

00:27:26.220 --> 00:27:32.230
And it's actually going to
be of different length,

00:27:32.230 --> 00:27:35.670
depending on how many words we have.

00:27:35.670 --> 00:27:38.485
Which is a little odd, right?

00:27:38.485 --> 00:27:42.390
Because in the end, if we want to
plug it into a softmise classifier,

00:27:42.390 --> 00:27:45.380
we would want to have
a fixed dimensional vector.

00:27:47.250 --> 00:27:52.990
But, intuitively here, we'll just, again,
multiply each of these numbers and

00:27:52.990 --> 00:27:56.480
our w here with the concatenation,
and remove along.

00:27:56.480 --> 00:27:57.410
Turns out we'll zero pad.

00:27:58.430 --> 00:28:02.035
And if you now think carefully, you'll
actually realize, well, I kind of cheated

00:28:02.035 --> 00:28:04.560
because really that's what we really
should've done also on the left side.

00:28:04.560 --> 00:28:08.500
So on the left side we will actually
also zero pad the sentence.

00:28:08.500 --> 00:28:13.340
So we do exactly the same in
the beginning at the end of the sentence.

00:28:16.040 --> 00:28:21.117
All right, now, because we have a variable
length vector at this point, and we want

00:28:21.117 --> 00:28:26.054
to have eventually a fixed dimensional
feature vector that represents that whole

00:28:26.054 --> 00:28:31.276
sentence, what we'll now do is introduce a
new type of building block that we haven't

00:28:31.276 --> 00:28:37.040
really looked at that much before, namely,
a pooling operator or pooling layer.

00:28:37.040 --> 00:28:42.040
And in particular, what we'll use
here is a so-called max-over-time or

00:28:42.040 --> 00:28:44.110
max pooling layer.

00:28:44.110 --> 00:28:45.700
And it's a very simple idea,

00:28:45.700 --> 00:28:49.070
namely that we're going to capture
the most important activation.

00:28:49.070 --> 00:28:54.700
So as you have different elements
figured computed for every window,

00:28:54.700 --> 00:28:58.710
you have the hope that the inner
product would be particularly large for

00:28:58.710 --> 00:29:03.350
that filter, if it sees a certain
kind of phrase, all right?

00:29:03.350 --> 00:29:08.330
So, namely, if you have, let's say your
word vectors are relatively normalized,

00:29:08.330 --> 00:29:13.170
if you do an inner product,
you would want to have a very large cosine

00:29:13.170 --> 00:29:16.870
similarity between the filter and the
certain pattern that you're looking for.

00:29:16.870 --> 00:29:20.080
And that one filter would only be
good at picking up that pattern.

00:29:20.080 --> 00:29:23.240
So for instance,
you might hope all your positive words

00:29:23.240 --> 00:29:27.170
are in one part of the vector space and
now you have a two dimensional,

00:29:28.560 --> 00:29:33.300
sorry a two word vector, sorry.

00:29:33.300 --> 00:29:37.250
A filter size of length two
that looks at bigrams, and

00:29:37.250 --> 00:29:40.360
you want to ideally have
that filter be very good and

00:29:40.360 --> 00:29:43.740
have a very large inner product with
all the words that are positive.

00:29:44.790 --> 00:29:49.990
And that would then be captured by having
one of these numbers be very large.

00:29:49.990 --> 00:29:55.060
And so what this intuitively allows
you to do is, as you move over it and

00:29:55.060 --> 00:29:59.400
you then in the end max pool,
if you just have one word pair,

00:29:59.400 --> 00:30:04.260
one biagram that it has a very large
activation for that particular filter w,

00:30:04.260 --> 00:30:08.510
you will basically get
that to your c hat here.

00:30:09.830 --> 00:30:12.950
And it can ignore all
the rest of the sentence.

00:30:12.950 --> 00:30:17.120
It's just going to be able to pick
out one particular bigram very,

00:30:17.120 --> 00:30:19.300
very accurately, or a type of bigram.

00:30:19.300 --> 00:30:22.250
And because word vectors cluster and

00:30:22.250 --> 00:30:25.410
where similar kinds of words
have similar kinds of meaning,

00:30:25.410 --> 00:30:30.080
you might hope that all the positive words
will activate a similar kind of filter.

00:30:31.230 --> 00:30:34.700
Now the problem with this is, of course,
that that is just a single number, right?

00:30:34.700 --> 00:30:39.940
C hat is just a maximum number here
of all the elements in this vector.

00:30:39.940 --> 00:30:42.390
So I would just be five.

00:30:42.390 --> 00:30:44.533
So that could be one activation.

00:30:44.533 --> 00:30:49.510
If we use a relu nonlinearity here,
this will just be a single number.

00:30:49.510 --> 00:30:54.316
So c hat is just that.

00:30:54.316 --> 00:30:59.178
Now of course, we want to be able to do
more than just find one particular type of

00:30:59.178 --> 00:31:04.460
bigram or trigram, we want to have many
more features that we can extract.

00:31:04.460 --> 00:31:07.020
And that's why we're going
to have multiple filters w.

00:31:07.020 --> 00:31:09.730
So instead of just convolving
a single feature w,

00:31:09.730 --> 00:31:12.660
we'll convolve multiple of them.

00:31:12.660 --> 00:31:14.240
And as we train this model,

00:31:14.240 --> 00:31:20.180
eventually we hope that some of the w
filters will fire and be very active and

00:31:20.180 --> 00:31:25.373
have very large inter-products with
particular types of bigrams or

00:31:25.373 --> 00:31:32.225
trigrams, or even four grams.

00:31:32.225 --> 00:31:36.336
So it's also very useful to have some
filters that only pick out bigrams and

00:31:36.336 --> 00:31:38.690
you can actually get quite far with that.

00:31:38.690 --> 00:31:43.120
But then maybe you have someone,
some examples where you say for

00:31:43.120 --> 00:31:48.300
sentiment again very simply example
it's not very good or risk missing

00:31:48.300 --> 00:31:53.530
a much originality and
now you want to have diagrams in filters

00:31:53.530 --> 00:31:58.437
of length K times 3.

00:31:58.437 --> 00:32:02.942
And so, we can have multiple different
window sizes and at the end, each time we

00:32:02.942 --> 00:32:07.127
convolve that filter and we do all
these inner products at each time step.

00:32:07.127 --> 00:32:12.556
We'll basically max pool to get a single
number for that filter for that sentence.

00:32:22.941 --> 00:32:25.088
If we have different filters
of different lengths,

00:32:25.088 --> 00:32:27.550
how do we make sure they
learn different feature?

00:32:27.550 --> 00:32:29.830
Of same length or different lengths, yeah.

00:32:29.830 --> 00:32:33.450
Of same length, how do we make sure
they learn different features?

00:32:33.450 --> 00:32:37.760
Well, they all start at different
random initializations, so

00:32:37.760 --> 00:32:39.310
that helps to break up some symmetry.

00:32:39.310 --> 00:32:42.270
And then actually we don't have
to do anything in particular

00:32:42.270 --> 00:32:45.470
to make sure that happens,
it actually just happens.

00:32:45.470 --> 00:32:48.222
So as we do SGD,
from the random initializations,

00:32:48.222 --> 00:32:52.318
different filters will move and start
to pick up different patterns in order

00:32:52.318 --> 00:32:54.818
to maximize our overall
objective function.

00:32:54.818 --> 00:32:56.901
Which we'll get to,
it'll just be logistic regression.

00:33:01.584 --> 00:33:03.667
They would probably still
learn different values, yeah.

00:33:03.667 --> 00:33:08.711
You update so in the beginning,
well, if they're exactly the same,

00:33:08.711 --> 00:33:13.156
basically, as you pool, right,
you will eventually pick,

00:33:13.156 --> 00:33:17.340
during backpropagation,
the max value here.

00:33:17.340 --> 00:33:20.817
The max value will come,
eventually, from a specific filter.

00:33:20.817 --> 00:33:24.567
And if they have the exact same,
one, you would never do it.

00:33:24.567 --> 00:33:27.610
But two, if you did,
they would have the exact same value.

00:33:27.610 --> 00:33:32.738
And then your computer will have to
choose, randomly, one to be the max.

00:33:32.738 --> 00:33:35.540
And if they're just the same,
whatever, it'll pick one and

00:33:35.540 --> 00:33:38.080
then it'll backpropagate
through that particular filter.

00:33:38.080 --> 00:33:38.670
And then,

00:33:38.670 --> 00:33:43.468
they're also going to be different in the
iteration of your optimization algorithm.

00:33:43.468 --> 00:33:45.447
Yeah?

00:33:45.447 --> 00:33:46.961
&gt;&gt; Is there a reason why
we do the max [INAUDIBLE]?

00:33:49.890 --> 00:33:51.630
&gt;&gt; Is there a reason why we do the max?

00:33:51.630 --> 00:33:55.510
So in theory nothing would
prevent us from using min too.

00:33:55.510 --> 00:34:01.070
Though we in many cases use rectified
linear units which will be max 0x.

00:34:01.070 --> 00:34:06.748
And so max pooling makes a lot more
sense cuz min will often just be 0.

00:34:06.748 --> 00:34:08.641
And so, we've rallies together,

00:34:08.641 --> 00:34:11.676
it makes the most sense to use
the max pooling layer also.

00:34:16.703 --> 00:34:18.080
Could we use average pooling?

00:34:18.080 --> 00:34:22.635
It's actually not totally crazy,
there are different papers that explore

00:34:22.635 --> 00:34:27.547
different pooling schemes and there's no
sort of beautiful mathematical reason

00:34:27.547 --> 00:34:32.029
of why one should work better but
intuitively what you're trying to do here

00:34:32.029 --> 00:34:36.109
is you try to really just fire when
you see a specific type of engram.

00:34:36.109 --> 00:34:38.695
And when you see that
particular type of engram,

00:34:38.695 --> 00:34:42.876
cuz that filter fired very strongly for
it, then you wanna say this happened.

00:34:42.876 --> 00:34:47.910
And you want to give that signal
to the next higher layer.

00:34:47.910 --> 00:34:52.340
And so that is particularly easy if you
choose a specific single value versus

00:34:52.340 --> 00:34:55.380
averaging, where you kind of
conglomerate everything again.

00:34:55.380 --> 00:34:59.088
And the strong signal that you may get
from one particular unigram, or bigram, or

00:34:59.088 --> 00:35:01.253
trigram, might get washed
out in the average.

00:35:15.296 --> 00:35:19.963
Great question, so once we have a bunch of
different c hats from each of the filters,

00:35:19.963 --> 00:35:21.300
how do we combine them?

00:35:21.300 --> 00:35:23.980
And the answer will be,
we'll just concatenate all them.

00:35:23.980 --> 00:35:25.149
We'll get to that in a second.

00:35:32.925 --> 00:35:37.244
Yeah, so the main idea is once you do
max pooling one of the values will

00:35:37.244 --> 00:35:42.234
be the maximum and then all of the other
ones will basically have 0 gradients cuz

00:35:42.234 --> 00:35:46.851
they don't change the layer above,
and then you just flow your gradients

00:35:46.851 --> 00:35:50.375
through the maximum value
that triggered that filter.

00:36:17.990 --> 00:36:21.371
So the question is, doesn't that make
our initialization very important, and

00:36:21.371 --> 00:36:22.974
lead to lots of downstream problems?

00:36:22.974 --> 00:36:25.692
And the answer is yes,
so likewise if you, for

00:36:25.692 --> 00:36:29.811
instance, initialize all your filter
weights such as your rectified

00:36:29.811 --> 00:36:34.440
linear units all return zero then,
you're not gonna learn anything.

00:36:34.440 --> 00:36:38.174
So you have to initialize your
weights such that in the beginning,

00:36:38.174 --> 00:36:41.917
most of your units are active and
something will actually happen.

00:36:41.917 --> 00:36:47.269
And then the main trick to, or the way,
the reason why it doesn’t hurt

00:36:47.269 --> 00:36:52.741
a ton to have these different
randomizations, you have lots filters.

00:36:52.741 --> 00:36:56.218
And each filter can start to pick up
different kinds of signals during

00:36:56.218 --> 00:36:57.980
the optimization.

00:36:57.980 --> 00:37:01.010
But, in general, yes,
these models are highly non-convex and

00:37:01.010 --> 00:37:04.120
if you initialize them incorrectly,
they won’t learn anything.

00:37:04.120 --> 00:37:06.707
But we have relatively
stable initialization

00:37:06.707 --> 00:37:09.567
schemes at this point that
just work in most cases.

00:37:12.823 --> 00:37:14.930
Great questions, all right I like it.

00:37:16.250 --> 00:37:23.576
All right, so we basically now have,
we're almost at the final model.

00:37:23.576 --> 00:37:26.049
But there's another idea here And

00:37:26.049 --> 00:37:32.390
that combines what we've learned about
word vectors, but extends it a little bit.

00:37:32.390 --> 00:37:38.010
And namely, instead of representing the
sentence only as a single concatenation

00:37:38.010 --> 00:37:41.310
of all the word vectors, we'll actually
start with two copies of that.

00:37:41.310 --> 00:37:45.125
And then we're going to backpropagate into

00:37:45.125 --> 00:37:49.361
one of these two channels and
not into the other.

00:37:49.361 --> 00:37:51.420
So why do we do this?

00:37:51.420 --> 00:37:54.954
Remember we had this lecture where
I talked about the television and

00:37:54.954 --> 00:37:58.054
the telly, and
as you back-propagate into word vectors,

00:37:58.054 --> 00:38:01.783
they start to move away from their
Glove or word2vec initialization.

00:38:01.783 --> 00:38:05.450
So again, just quick recap,
word vectors are really great.

00:38:05.450 --> 00:38:07.927
We can train them on a very
large unsupervised scope so

00:38:07.927 --> 00:38:09.782
they capture semantic similarities.

00:38:09.782 --> 00:38:14.590
Now if you start backpropagating your
specific task into the word vectors,

00:38:14.590 --> 00:38:16.270
they will start to move around.

00:38:16.270 --> 00:38:21.116
When you see that word vector in your
supervised classification problem in that

00:38:21.116 --> 00:38:21.778
dataset.

00:38:21.778 --> 00:38:25.179
Now what that means is as you
push certain vectors that you see

00:38:25.179 --> 00:38:27.647
in your training data sets somewhere else,

00:38:27.647 --> 00:38:32.049
the vectors that you don't see in your
training data set stay where they are and

00:38:32.049 --> 00:38:35.662
now might get misclassified if
they only appear in the test set.

00:38:35.662 --> 00:38:40.136
So by having these two channels We'll
basically try to have some of the goodness

00:38:40.136 --> 00:38:41.405
of really trainings,

00:38:41.405 --> 00:38:45.400
the first copy of the word vectors
to be really good on that task.

00:38:45.400 --> 00:38:49.571
But the second set of word vectors to
stay where they are, have the good, nice,

00:38:49.571 --> 00:38:53.870
general semantic similarities in vector
space goodness that we have from unlarge

00:38:53.870 --> 00:38:55.509
and supervised word vectors.

00:38:59.504 --> 00:39:03.675
And in this case here, both of these
channels are actually going to be added to

00:39:03.675 --> 00:39:08.054
each of the CIs before we max-pool, so
we will pool over both of those channels.

00:39:08.054 --> 00:39:14.028
Now, the final model, and this is the
simplest, one I'll get to you in a second.

00:39:14.028 --> 00:39:18.342
Is basic just concatenating
all this c hats,

00:39:18.342 --> 00:39:23.111
so remember each c hats
was one max pool filter.

00:39:24.620 --> 00:39:29.060
And we have this case here
that say m many filters.

00:39:29.060 --> 00:39:31.360
And so our final feature vector for

00:39:31.360 --> 00:39:36.670
that sentence,
has just an r n-dimensional vector,

00:39:36.670 --> 00:39:40.910
where we have m many different filters
that we convolved over the sentence.

00:39:41.910 --> 00:39:45.750
And then we'll just plug that
z directly into softmax, and

00:39:45.750 --> 00:39:49.210
train this with our standard logistic
regression cross entropy error.

00:39:50.780 --> 00:39:51.560
All right, we had a question?

00:39:59.467 --> 00:40:04.090
By having two copies of the work vectors,
are we essentially doubling the size?

00:40:04.090 --> 00:40:08.820
Well, we're certainly doubling
the memory requirements of that model.

00:40:08.820 --> 00:40:12.544
And we just kinda assume, you could
think of it as doubling the size of

00:40:12.544 --> 00:40:16.657
the word vectors, and then the important
part is that only the second half of

00:40:16.657 --> 00:40:20.402
the word vectors you're going to
back propagate into for that task.

00:40:28.610 --> 00:40:31.490
That's right, we can use the same
convolutional weights, or

00:40:31.490 --> 00:40:34.860
you can also use different convolutional
weights, and then filter, and

00:40:34.860 --> 00:40:38.145
you can have multiple, and this model
will have many of them actually.

00:40:38.145 --> 00:40:44.276
It could have 100 bigram filters,
100 trigram filters,

00:40:44.276 --> 00:40:50.210
maybe 24 filters, and
maybe even some unigram filters.

00:40:50.210 --> 00:40:53.480
So you can have a lot of different hyper
parameters on these kinds of models.

00:40:53.480 --> 00:40:54.927
So quickly.

00:41:11.412 --> 00:41:15.565
For a given sentence does
the convolutional matrix stay the same?

00:41:15.565 --> 00:41:19.015
So this matrix is the only
matrix that we have.

00:41:19.015 --> 00:41:21.975
This is just our standard soft matrix and

00:41:21.975 --> 00:41:26.925
then before we had these w filters,
these vectors.

00:41:26.925 --> 00:41:32.704
And yes each w is the same as you convolve
it over all the windows of one sentence.

00:41:38.600 --> 00:41:43.572
So lots of inner products for
a bunch of concatenated word vectors,

00:41:43.572 --> 00:41:48.670
and then you max pool, find the largest
value from all the n-grams.

00:41:48.670 --> 00:41:54.066
And that's a CNN layer and

00:41:54.066 --> 00:41:57.748
a pooling layer.

00:41:57.748 --> 00:42:01.490
Now, here's graphical description of that.

00:42:02.830 --> 00:42:09.930
Here, instead of concatenating them,
this just kind of simplified this, so

00:42:09.930 --> 00:42:15.040
imagine here you have n same notation.

00:42:15.040 --> 00:42:20.080
We have n many words in that sentence,
and each word has

00:42:20.080 --> 00:42:25.880
k as a k dimensional feature vector,
or word vector associated with it.

00:42:25.880 --> 00:42:31.085
So these could be our glove or
other word to vector initializations, and

00:42:31.085 --> 00:42:36.717
now this particular model here shows us
two applications of a bigram filter and

00:42:36.717 --> 00:42:39.230
one of a trigram filter.

00:42:39.230 --> 00:42:46.569
So here this bigram filter looks at
the concatenation of these two vectors and

00:42:46.569 --> 00:42:51.110
then max pool them into a single number.

00:42:53.330 --> 00:42:55.874
And as you go through this,

00:42:55.874 --> 00:43:01.193
you'll basically get lots
of different applications.

00:43:01.193 --> 00:43:05.522
And you basically, for
each of the features,

00:43:05.522 --> 00:43:09.407
you'll get one long set of features, and

00:43:09.407 --> 00:43:14.846
then you'll get a single number
after max pooling over all

00:43:14.846 --> 00:43:19.856
these activations from
[INAUDIBLE] grand positions.

00:43:22.473 --> 00:43:28.081
So you see here for instance so
the bigram filter is this channel and

00:43:28.081 --> 00:43:31.422
then we'll basically max pool.

00:43:31.422 --> 00:43:36.340
Over that, again, notice how here they use
indeed the same filter on the second word

00:43:36.340 --> 00:43:39.611
vector channel,
the one we might back propagate into.

00:43:39.611 --> 00:43:42.750
But they will all
basically end up in here.

00:43:42.750 --> 00:43:47.330
So just, again, inner products
plus bias and non linearity and

00:43:47.330 --> 00:43:51.380
then we'll max pool all those numbers
into a single number up there.

00:43:51.380 --> 00:43:55.160
And now,
a different namely this guy up there.

00:43:55.160 --> 00:44:00.154
The trigram also convolves over that
sentence and basically combines a bunch of

00:44:00.154 --> 00:44:04.937
different numbers here and then gets
max pooled over a single number there.

00:44:25.007 --> 00:44:25.830
Great question.

00:44:25.830 --> 00:44:30.670
So do we always max
pool over particularly,

00:44:30.670 --> 00:44:35.390
just a set of features that are all
coming from the same filter.

00:44:35.390 --> 00:44:37.500
And the answer is in this model we do, and

00:44:37.500 --> 00:44:40.170
it's the simplest model that
actually works surprisingly well.

00:44:40.170 --> 00:44:44.935
But there are going to be,
right after our quick research highlight,

00:44:44.935 --> 00:44:48.350
a lot of modifications and
tweaks that we'll do.

00:44:48.350 --> 00:44:53.229
There are no more questions,
let's do the research highlight and

00:44:53.229 --> 00:44:57.170
then we'll get to how to tune
that model should be on.

00:44:57.170 --> 00:44:57.835
&gt;&gt; Hello?
&gt;&gt; Yeah.

00:44:57.835 --> 00:44:58.542
It's cool.

00:45:01.659 --> 00:45:04.458
&gt;&gt; So hi, everyone, my name's Amani and
today I thought I would share with you

00:45:04.458 --> 00:45:08.235
a very interesting paper called
Character-Aware Neural Language Models.

00:45:08.235 --> 00:45:11.325
So on a high level as the title implies
the main goal of this paper is to come up

00:45:11.325 --> 00:45:12.255
with a powerful and

00:45:12.255 --> 00:45:15.910
robust language model that effectively
utilizes subword information.

00:45:15.910 --> 00:45:17.280
So to frame this in a broader context,

00:45:17.280 --> 00:45:21.630
most prior neural language models do not
really include the notion that words that

00:45:21.630 --> 00:45:25.670
are structurally very similar should have
very similar representations in our model.

00:45:25.670 --> 00:45:28.785
Additionally, many prior neural language
models suffered from a rare-word problem.

00:45:28.785 --> 00:45:31.921
Where the issue is that if we don't really
see a word that often or at all in our

00:45:31.921 --> 00:45:35.205
dataset then it becomes very hard to come
up with an accurate representation for

00:45:35.205 --> 00:45:36.660
that word.

00:45:36.660 --> 00:45:39.310
And this can be very problematic
in languages that have long tail

00:45:39.310 --> 00:45:42.480
frequency distributions or in domains
where vocabulary is constantly changing.

00:45:43.620 --> 00:45:46.420
So to address some of these problems,
the authors propose the following model,

00:45:46.420 --> 00:45:49.490
where essentially we will read in
our inputs at the character level,

00:45:49.490 --> 00:45:52.570
but then we will make our
predictions still at the word level.

00:45:52.570 --> 00:45:53.960
So let's dive a little bit
deeper into the model and

00:45:53.960 --> 00:45:55.140
see exactly what's happening here.

00:45:56.190 --> 00:45:57.780
So the first thing we do
is that we take our input,

00:45:57.780 --> 00:46:00.130
and we break it apart
into a set of characters.

00:46:00.130 --> 00:46:01.274
Where for each character,

00:46:01.274 --> 00:46:04.126
we associate it with an embedding
that we learned during training.

00:46:04.126 --> 00:46:06.850
We then take the convolutional network and
take its filters and

00:46:06.850 --> 00:46:09.730
convolve them over them the embeddings
to produce a feature map.

00:46:09.730 --> 00:46:13.470
And finally, we apply max pooling over
time, which intuitively is selecting out

00:46:13.470 --> 00:46:16.340
the dominant n-grams or substrings
that were detected by the filters.

00:46:17.930 --> 00:46:20.491
We then take the output of
the convolutional network, and

00:46:20.491 --> 00:46:21.910
pipe it into a highway network.

00:46:21.910 --> 00:46:25.284
Which we're going to use to essentially
model the interactions between various

00:46:25.284 --> 00:46:25.810
n-grams.

00:46:25.810 --> 00:46:29.250
And you can think of this layer as being
very similar to an LSTM memory cell,

00:46:29.250 --> 00:46:31.880
where the idea is that we want to
transform part of our input, but

00:46:31.880 --> 00:46:34.470
also keep around and
memorize some of the original information.

00:46:36.520 --> 00:46:40.274
We then take the output of the highway
network and pipe it into a single timeset

00:46:40.274 --> 00:46:44.579
of LSTM, which is being trained to produce
sequence given the current inputs.

00:46:44.579 --> 00:46:48.413
And the only thing different to note here
is that we're using hierarchical softmax

00:46:48.413 --> 00:46:52.040
to make predictions due to
the very large output vocabulary.

00:46:52.040 --> 00:46:53.670
So let's analyze some of the results.

00:46:53.670 --> 00:46:55.430
So as we can see here from
the table on the right,

00:46:55.430 --> 00:46:58.900
the model is able to obtain comparable
performance with state of the art

00:46:58.900 --> 00:47:03.171
methods on the data set while utilizing
fewer parameters in the process.

00:47:03.171 --> 00:47:06.840
What's also really remarkable is I was
able to outperform its word level and

00:47:06.840 --> 00:47:09.900
working level counterparts across
a variety of other rich languages,

00:47:09.900 --> 00:47:13.880
such as Arabic, Russian,
Chinese, and French.

00:47:13.880 --> 00:47:17.100
While using, again, fewer parameters
in the process because now

00:47:17.100 --> 00:47:20.420
we don't have to have an embedding for
every single word in our vocabulary but

00:47:20.420 --> 00:47:22.360
now only for
every single character that we use.

00:47:24.010 --> 00:47:26.520
We can also look at some of
the qualitative results to see what is it

00:47:26.520 --> 00:47:28.060
the results is exactly learning.

00:47:28.060 --> 00:47:30.830
So in this table we have done,
is that we have extracted the intermediate

00:47:30.830 --> 00:47:33.950
representations of words at
various levels of the network and

00:47:33.950 --> 00:47:36.000
then computed their nearest neighbors.

00:47:36.000 --> 00:47:39.950
And what we find is that, after applying
the CNN, we are grouping together words

00:47:39.950 --> 00:47:43.290
with strong sub-word similarity and
that after applying the highway network,

00:47:43.290 --> 00:47:46.670
we are also now grouping together words
that have strong semantic similarities.

00:47:46.670 --> 00:47:50.280
So now the word Richard is
close to no other first names.

00:47:52.380 --> 00:47:55.000
We can also look and
see how it handles noisy words.

00:47:55.000 --> 00:47:59.070
So in this case, the model is able
to effectively handle the word look

00:47:59.070 --> 00:48:01.620
with a lot of O's in between,
which it has never seen before.

00:48:01.620 --> 00:48:05.455
But it is now able to assign it
to reasonable nearest neighbors.

00:48:05.455 --> 00:48:09.001
And on the plot on the right, what we
see is that if we take the in-grammar

00:48:09.001 --> 00:48:11.924
presentations learned by the model and
plot them with PCA.

00:48:11.924 --> 00:48:14.992
We see that it is able to isolate
the ideas of suffixes, prefixes, and

00:48:14.992 --> 00:48:16.080
hyphenated words.

00:48:16.080 --> 00:48:18.720
Which shows that, at its core, the model
really is learning something intuitive.

00:48:20.330 --> 00:48:23.500
So in conclusion, I wanna sort of
highlight a few key takeaway points.

00:48:23.500 --> 00:48:27.230
The first is that this paper shows that
it is possible to use inputs other than

00:48:27.230 --> 00:48:30.200
word embeddings to obtain superlative
performance on language modeling.

00:48:30.200 --> 00:48:32.780
While using fewer
parameters in the process.

00:48:32.780 --> 00:48:33.758
Second it shows that,

00:48:33.758 --> 00:48:37.213
it demonstrates the effectiveness of
CNNs in the domain to language modeling.

00:48:37.213 --> 00:48:39.085
And shows that, in this case, the CNNs and

00:48:39.085 --> 00:48:41.841
Highway Network are able to extract
which types of semantic and

00:48:41.841 --> 00:48:44.550
orthographic information from
the character level inputs.

00:48:45.630 --> 00:48:49.423
And finally, what's most important is
that this paper is combining the ideas of

00:48:49.423 --> 00:48:50.818
language modelings, CNNs,

00:48:50.818 --> 00:48:53.073
LTMs, hierarchical softmax,
embeddings all into one model.

00:48:53.073 --> 00:48:56.303
Which shows that basically we can treat
the concepts that we've learned over

00:48:56.303 --> 00:48:58.250
the course of the quarter
as building blocks.

00:48:58.250 --> 00:49:01.420
And learn to compose them in
very interesting ways to produce

00:49:01.420 --> 00:49:03.479
more powerful or more nuanced models.

00:49:03.479 --> 00:49:07.045
And that is a very useful insight to have
as you approach some of your own projects,

00:49:07.045 --> 00:49:08.810
not only in the class, but also beyond.

00:49:09.850 --> 00:49:12.138
And with that, I would like to conclude
and thank you for your attention.

00:49:12.138 --> 00:49:19.130
&gt;&gt; [APPLAUSE]
&gt;&gt; Character models are awesome.

00:49:19.130 --> 00:49:22.480
Usually when you have a larger downstream
task, like question answering or

00:49:22.480 --> 00:49:27.470
machine translation, they can give
you 2 to 5% boost in accuracy.

00:49:27.470 --> 00:49:31.050
Sadly, when you run any kind
of model over characters,

00:49:31.050 --> 00:49:34.980
you think you have a sentence or
document with 500 words.

00:49:34.980 --> 00:49:40.100
Well, now you have a sequence
of 500 times maybe 5 or

00:49:40.100 --> 00:49:45.920
10 characters, so now you have
a 5,000 dimensional time sequence.

00:49:45.920 --> 00:49:50.300
And so when you train your
model with character levels,

00:49:50.300 --> 00:49:54.500
think extra hard about how long it
will take you to run your experiments.

00:49:54.500 --> 00:49:56.800
So it's kind of a very clear,

00:49:56.800 --> 00:50:01.510
sort of accuracy versus time
tradeoff in many cases.

00:50:03.742 --> 00:50:07.660
All right, so I mentioned the super
simple model where we really just

00:50:07.660 --> 00:50:10.540
do a couple of inner products,
over a bunch of these filters,

00:50:10.540 --> 00:50:14.100
find the max, and
then pipe all of that into the softmax.

00:50:14.100 --> 00:50:19.496
Now that by itself doesn't work to get
you into state-of-the-art performance so

00:50:19.496 --> 00:50:23.682
there are a bunch of tricks that
were employed by Kim In 2014.

00:50:23.682 --> 00:50:28.010
And I'm going to go through a couple
of them since they apply to a lot of

00:50:28.010 --> 00:50:32.300
different kinds of models that
you might wanna try as well.

00:50:32.300 --> 00:50:35.988
The first one is one that I think
we've already covered, dropout, but

00:50:35.988 --> 00:50:37.330
we did right?

00:50:37.330 --> 00:50:42.062
But it's a really neat trick and you can
apply it lots of different contexts.

00:50:42.062 --> 00:50:45.897
And its actually differently applied for
convolutional networks and

00:50:45.897 --> 00:50:47.523
recurrent neural networks.

00:50:47.523 --> 00:50:50.616
So it's good to look at
another application here for

00:50:50.616 --> 00:50:53.440
this particular convolution
neural network.

00:50:53.440 --> 00:50:58.190
So just to recap, the idea was to
essentially randomly mask or dropout or

00:50:58.190 --> 00:51:03.280
set to 0 some of the feature weights that
you have in your final feature vector.

00:51:03.280 --> 00:51:04.860
And in our case that was z,

00:51:04.860 --> 00:51:09.380
remember z was just a concatenation
of the max built filters.

00:51:09.380 --> 00:51:14.690
And another way of saying that is that
we're going to create a mask vector r.

00:51:14.690 --> 00:51:19.371
Of basically, random Bernoulli
distributed variables with

00:51:19.371 --> 00:51:23.160
probability that I would
probability p set to 1.

00:51:24.600 --> 00:51:26.910
And probably 1 minus p set to 0.

00:51:27.990 --> 00:51:31.430
And so what this ends up doing
is to essentially delete

00:51:31.430 --> 00:51:33.860
certain features at training time.

00:51:33.860 --> 00:51:37.565
So as you go through all your filters,
and you actually had a great biagram.

00:51:37.565 --> 00:51:40.821
And another good biagram,
it might accidentally or

00:51:40.821 --> 00:51:43.440
randomly delete one of the two biagrams.

00:51:43.440 --> 00:51:46.180
And what that essentially helps us to do

00:51:46.180 --> 00:51:50.950
is to have the final classifier not
overfit to say, it's only positive for

00:51:50.950 --> 00:51:53.560
instance if I see these
exact two biagrams together.

00:51:53.560 --> 00:51:56.125
Maybe it's also positive if I
see just one of the biagrams.

00:51:58.550 --> 00:52:03.090
So another way of saying that is
that it will prevent co-adaptation

00:52:03.090 --> 00:52:05.220
of these different kinds of features.

00:52:05.220 --> 00:52:06.908
And it's a very, very useful thing.

00:52:06.908 --> 00:52:11.888
Basically every state-of-the-art
model out there that you'll observe,

00:52:11.888 --> 00:52:16.717
hopefully somewhere in its experimental
section it will tell you how much it

00:52:16.717 --> 00:52:20.957
dropped out of weights and
what exactly the scheme of dropout was.

00:52:20.957 --> 00:52:24.281
Cuz you can dropout, for instance,
through recurrent neural network,

00:52:24.281 --> 00:52:26.960
you can dropout the same set of
features at every time step or

00:52:26.960 --> 00:52:29.120
different sets of features
at every time step.

00:52:29.120 --> 00:52:31.770
And it all makes a big
difference actually.

00:52:31.770 --> 00:52:39.202
So this is a great paper by Geoff Hinton
and a bunch of collaborators from 2012.

00:52:39.202 --> 00:52:43.826
Now, if you carefully think through what
happens here, well, at training time,

00:52:43.826 --> 00:52:47.873
we're basically, let's say it's 0.5,
probability p is 0.5.

00:52:47.873 --> 00:52:51.635
So half of all the features are randomly
getting deleted at training time.

00:52:51.635 --> 00:52:56.935
Well then, the model is going to
get used to seeing a much smaller

00:52:56.935 --> 00:53:01.853
in norm feature vector z or
had a more product here or time z.

00:53:01.853 --> 00:53:05.552
And so, basically at test time,
when there's no dropout,

00:53:05.552 --> 00:53:09.410
of course at test time,
we don't want to delete any features.

00:53:09.410 --> 00:53:12.740
We want to use all the information
that we have from the sentence,

00:53:12.740 --> 00:53:15.890
our feature vector z
are going to be too large.

00:53:15.890 --> 00:53:17.870
And so what we'll do is,
in this care here,

00:53:17.870 --> 00:53:22.200
we'll actually scale the final vector
by the Bernoulli probability p.

00:53:22.200 --> 00:53:24.587
So, our Ws here, the softmax weights,

00:53:24.587 --> 00:53:28.112
are just going to be multiplied,
and essentially halved.

00:53:28.112 --> 00:53:29.099
And that way,

00:53:29.099 --> 00:53:33.960
we'll end up in the same order of
magnitude as we did at training time.

00:53:36.093 --> 00:53:40.286
Any questions about dropout?

00:53:49.543 --> 00:53:50.400
What's the intuition?

00:53:50.400 --> 00:53:53.580
So some people liken dropout
to assembling models.

00:53:53.580 --> 00:53:57.556
And intuitively here you could have,
let's say,

00:53:57.556 --> 00:54:03.576
deterministically you were dropping out
the first half of all your filters.

00:54:03.576 --> 00:54:06.671
And you only train one model on
the first half of the filters, and

00:54:06.671 --> 00:54:09.970
you train the second model on
the second half of the filters.

00:54:09.970 --> 00:54:12.720
And then in the end you average the two.

00:54:12.720 --> 00:54:14.150
That's kind of similar, but

00:54:14.150 --> 00:54:17.609
in a very noisy variant of what
you end up doing with dropout.

00:54:19.750 --> 00:54:24.210
And so, in many cases this can give
you like 2%-4% improved accuracy.

00:54:24.210 --> 00:54:28.577
And when we look a the numbers, you'll
notice that it's those 2%-4% that gets you

00:54:28.577 --> 00:54:31.424
that paper published and
people looking at your method.

00:54:31.424 --> 00:54:35.297
Whereas if it's 4% below,
it's getting closer and

00:54:35.297 --> 00:54:40.191
closer to a very simple back or
forwards model with discrete counts.

00:54:46.047 --> 00:54:49.010
Is it possible to dropout
the link instead of the node?

00:54:50.240 --> 00:54:54.670
So you could actually dropout some
of the weight features as well.

00:54:54.670 --> 00:54:58.240
And yes, there is actually
another variant of dropout.

00:54:58.240 --> 00:55:02.480
There's the filter weight dropout and
there is the activation dropout.

00:55:03.530 --> 00:55:06.910
So in this case here we
have activation dropout.

00:55:06.910 --> 00:55:09.670
And they have different advantages and
disadvantages.

00:55:09.670 --> 00:55:13.146
I think it's fair to say that,
especially for NLP,

00:55:13.146 --> 00:55:16.866
the jury is still out on which
one should you always use.

00:55:16.866 --> 00:55:21.848
I think the default, you just filter
out and the original dropout is just

00:55:21.848 --> 00:55:26.190
to set to 0 randomly the activations and
not the filter reads.

00:55:27.307 --> 00:55:34.570
All right, now, one last question.

00:55:39.756 --> 00:55:43.300
So basically,
this will have a certain norm.

00:55:43.300 --> 00:55:48.861
And at training time, the norm of this
is essentially, say halved if you have

00:55:48.861 --> 00:55:55.230
a probability of p To multiply
the features with zero.

00:55:56.330 --> 00:56:02.270
And so what that means is that, overall,
this matrix vector product will have

00:56:02.270 --> 00:56:06.090
a certain size and a certain certainty
also, once you apply the softmax.

00:56:07.320 --> 00:56:10.930
And if you don't wanna basically
be overly confident in anything,

00:56:10.930 --> 00:56:16.040
you wanna scale your W because at test
time you will not drop out anything.

00:56:16.040 --> 00:56:20.600
You will have the full z vector, not
half of all the values of the z vector.

00:56:20.600 --> 00:56:24.346
And so at test time you wanna use as
much information you can get from z.

00:56:24.346 --> 00:56:28.940
And because of that,
you now have a larger norm for z.

00:56:28.940 --> 00:56:31.230
And hence,
you're going to scale back W, so

00:56:31.230 --> 00:56:34.200
that the multiplication of the two
ends up in roughly the same place.

00:56:43.066 --> 00:56:45.190
Very good question, so
what's the softmax here?

00:56:45.190 --> 00:56:49.980
So, basically z was our vector for
some kind of sentence.

00:56:49.980 --> 00:56:53.440
And I use the example sentiment because
that is one of the many tasks that

00:56:53.440 --> 00:56:54.360
you could do with this.

00:56:54.360 --> 00:56:59.425
So, generally sentence classification,
or document classification,

00:56:59.425 --> 00:57:03.170
are the sort of most common task
that you would use this model for.

00:57:03.170 --> 00:57:10.319
We'll go over a bunch of examples in three
slides or so and a bunch of data sets.

00:57:11.566 --> 00:57:12.183
Awesome, so

00:57:12.183 --> 00:57:16.181
now there's one last regularization trick
that this paper by Kim used in 2014.

00:57:16.181 --> 00:57:19.750
It's actually not one that
I've seen anywhere else.

00:57:19.750 --> 00:57:22.730
And so I don't think we'll have to
spend too much time on it but they

00:57:22.730 --> 00:57:27.170
essentially also constrain the l2 norm of
the wave vectors of each of the classes.

00:57:27.170 --> 00:57:31.990
So we have here, remember this is
our softmax weight matrix W and

00:57:31.990 --> 00:57:36.630
c dot was the row for the cth class.

00:57:36.630 --> 00:57:41.190
And they basically have this additional
scheme here where whenever the norm

00:57:41.190 --> 00:57:45.045
of one of the rows for one of these
classes is above a certain threshold, S.

00:57:45.045 --> 00:57:47.420
Which is another
hyperparameter they'll select,

00:57:47.420 --> 00:57:49.068
it will rescale it to be exactly S.

00:57:49.068 --> 00:57:52.416
So basically they'll force the model
to never be too certain and

00:57:52.416 --> 00:57:55.085
have very large weights for
any particular class.

00:57:55.085 --> 00:57:59.748
Now it's a little weird cuz in general
we have l2 regularization on all

00:57:59.748 --> 00:58:01.390
the parameters anyway.

00:58:02.455 --> 00:58:04.295
But they saw a minor improvement.

00:58:04.295 --> 00:58:08.221
It's actually the only paper that I can
remember in recent years that does that,

00:58:08.221 --> 00:58:10.542
so I wouldn't overfit
too much on trying that.

00:58:10.542 --> 00:58:16.073
Now, it's important to set back and
think carefully.

00:58:16.073 --> 00:58:20.988
I described this model and I described it
very carefully but when you think about

00:58:20.988 --> 00:58:25.554
it, you now have a lot of different
kinds of tweaks and hyperparameters.

00:58:25.554 --> 00:58:29.537
And you have to be very conscious in all
your projects and every application in

00:58:29.537 --> 00:58:34.180
industry and research and everywhere of
what your hyperparameters really are.

00:58:34.180 --> 00:58:38.890
And which ones actually matter to your
final performance, how much they matter.

00:58:38.890 --> 00:58:40.404
And in an ideal world,

00:58:40.404 --> 00:58:45.688
you'll actually run an ablation where
maybe you have these two word vectors.

00:58:45.688 --> 00:58:47.850
The ones you back propagate into and then
the ones you don't back propagate into.

00:58:47.850 --> 00:58:49.880
How much does that actually help?

00:58:49.880 --> 00:58:55.180
Sadly, in very few examples,
people actually properly ablate,

00:58:55.180 --> 00:58:57.980
and properly show you all
the experiments they ran.

00:58:57.980 --> 00:59:04.741
And so let's go over the options and the
final hyperparameters that Kim chose for

00:59:04.741 --> 00:59:09.425
this particular convolutional
neural network model.

00:59:09.425 --> 00:59:12.845
The one amazing thing is they actually
had the same set of hyperparameters for

00:59:12.845 --> 00:59:14.535
a lot of the different experiments.

00:59:14.535 --> 00:59:17.164
A lot of these are sentiment analysis,

00:59:17.164 --> 00:59:21.637
subjectivity classification,
as most of the experiments here.

00:59:21.637 --> 00:59:25.035
But they had the same set of
hyperparameters, which is not very common.

00:59:25.035 --> 00:59:28.730
Sometimes you also say, all right,
here are all my options.

00:59:28.730 --> 00:59:30.630
And now, for every one of my datasets,

00:59:30.630 --> 00:59:34.840
I will run cross-validation over
all the potential hyperparameters.

00:59:34.840 --> 00:59:39.360
Which, if you think about it,
is exponential, so it would be too many.

00:59:39.360 --> 00:59:43.540
So then, the right thing to often do
is actually to set boundaries for

00:59:43.540 --> 00:59:44.530
all your hyperparameters.

00:59:44.530 --> 00:59:48.100
And then, randomly just sample
in between those boundaries.

00:59:48.100 --> 00:59:52.880
So for instance, let's say you might
have 100 potential feature maps for

00:59:52.880 --> 00:59:56.440
each filter, for each window size.

00:59:56.440 --> 00:59:59.366
Now you say, all right,
maybe I'll have between 20 and 200.

00:59:59.366 --> 01:00:03.121
And you just say, for
each of my cross-validation experiments,

01:00:03.121 --> 01:00:06.550
I will randomly sub sample
a number between 20 and 200.

01:00:06.550 --> 01:00:09.610
Then, I'll run experiments
with this number of filters

01:00:09.610 --> 01:00:12.350
on my developments split and
I'll see how well I do.

01:00:12.350 --> 01:00:15.960
And you say I have maybe 100
experiments of this kind.

01:00:15.960 --> 01:00:19.435
You'll quickly notice that, again,
why you need to start your project early.

01:00:19.435 --> 01:00:23.410
Cuz your performance will also depend
highly on your hyperparameters.

01:00:23.410 --> 01:00:25.440
And if you don't have
time to cross-validate,

01:00:25.440 --> 01:00:27.780
you may lose out on some of the accuracy.

01:00:27.780 --> 01:00:31.530
And especially as you get closer to
potentially state-of-the-art results,

01:00:31.530 --> 01:00:33.890
which I think some of the groups will.

01:00:33.890 --> 01:00:38.310
That last couple percent that you can
tweak and squeeze out of your model with

01:00:38.310 --> 01:00:42.470
proper hyperparameter search can
make the difference between having

01:00:42.470 --> 01:00:46.610
a paper submission or having a lot of
people be very excited about your model.

01:00:46.610 --> 01:00:48.780
Or ignoring it, sadly.

01:00:48.780 --> 01:00:49.280
Yep.

01:00:53.762 --> 01:00:55.470
Great question.

01:00:55.470 --> 01:00:58.360
Do you do that sampling for
one hyperparameter at a time or not?

01:00:58.360 --> 01:01:03.290
In the end, in the limit,
it doesn't matter which scheme you use.

01:01:03.290 --> 01:01:06.490
But in practice, you set the ranges for
all your hyperparameters and

01:01:06.490 --> 01:01:08.440
then you sample all of them for
each of your runs.

01:01:09.820 --> 01:01:10.660
And it's very,

01:01:10.660 --> 01:01:14.680
very counterintuitive that that would
work better than even a grid search.

01:01:14.680 --> 01:01:18.650
Where you say, all right,
instead of having 100 feature maps and

01:01:18.650 --> 01:01:20.580
randomly sample between 20 and 200.

01:01:20.580 --> 01:01:25.002
I'm gonna say, I'm gonna try it for
20, 50, 75, 100, 150, or

01:01:25.002 --> 01:01:26.917
200, or something like that.

01:01:26.917 --> 01:01:29.512
And then I just multiply all these six or
so

01:01:29.512 --> 01:01:33.040
options with all the other
options that I have.

01:01:33.040 --> 01:01:36.760
It quickly blows up to a very,
very large number.

01:01:36.760 --> 01:01:40.970
Let's say each of these you
try five different options,

01:01:40.970 --> 01:01:45.610
that's five to the number of many
hyperparameters that you have.

01:01:45.610 --> 01:01:48.300
Which if you have ten parameters or
so that's 5 to the 10,

01:01:48.300 --> 01:01:53.620
that's impossible to run a proper grid
search on all these hyperparameters.

01:01:53.620 --> 01:01:57.490
It turns out computationally and through
a variety of different experiments,

01:01:57.490 --> 01:02:01.310
I think the papers by Yoshua Bengio and
some of his students a couple years ago.

01:02:01.310 --> 01:02:04.479
That random hyperparameter search,
works surprisingly well, and

01:02:04.479 --> 01:02:07.221
sometimes even better than
a relatively fine grid search.

01:02:13.474 --> 01:02:17.300
Until you run out of money on your GPU.

01:02:17.300 --> 01:02:19.370
Or until the paper
deadline comes around or

01:02:19.370 --> 01:02:21.420
the class project deadline comes around,
yeah.

01:02:21.420 --> 01:02:22.960
In the perfect setting,

01:02:22.960 --> 01:02:28.720
you have the final model that you think
has all the ingredients that you'd want.

01:02:28.720 --> 01:02:32.330
And then you can let it run
until you run out of resources.

01:02:32.330 --> 01:02:34.370
Either or time, or money, or GPU time,

01:02:34.370 --> 01:02:39.412
or you annoy all your co-PhD students,
such as I did a couple years ago.

01:02:39.412 --> 01:02:43.653
[LAUGH] Fortunately, we learned at
some point to have, what is it,

01:02:43.653 --> 01:02:47.844
preemptable jobs, so that I could run,
use the entire cluster.

01:02:47.844 --> 01:02:51.141
But then when somebody else
wants to use the machine,

01:02:51.141 --> 01:02:55.901
it'll just put my job into the cache,
onto memory, or even save it to disk and

01:02:55.901 --> 01:02:58.400
anybody else can kinda sort of come in.

01:02:58.400 --> 01:03:01.790
But yeah, ideally you’ll run with all
the computational resources you have.

01:03:01.790 --> 01:03:02.640
And, of course,

01:03:02.640 --> 01:03:06.650
this is depending on how much you
care about that last bit of accuracy.

01:03:06.650 --> 01:03:08.560
For some of the papers it
can really matter, for

01:03:08.560 --> 01:03:10.520
some applications if
your work in medicine.

01:03:11.520 --> 01:03:14.900
You try to classify breast cancer or
something really serious, of course,

01:03:14.900 --> 01:03:18.130
you want to squeeze out as much
performance as you possibly can.

01:03:18.130 --> 01:03:21.735
And use as many computational
resources to run more hyperparameters.

01:03:26.748 --> 01:03:30.948
So there are actually some
people who tried various

01:03:30.948 --> 01:03:35.931
interesting Bayesian models of
Gaussian processes to try to

01:03:35.931 --> 01:03:40.940
identify the overall function
into hyperparameter space.

01:03:40.940 --> 01:03:43.600
So you basically run a meta optimization.

01:03:43.600 --> 01:03:48.420
Where instead of optimizing over
the actual parameters w of your model,

01:03:48.420 --> 01:03:53.040
you've run an optimization over
the hyperparameters of those models.

01:03:53.040 --> 01:03:57.679
Can do that,
it turns out The jury is sort of out, but

01:03:57.679 --> 01:04:01.553
a lot of people now say just do
a random hyperparameter search.

01:04:01.553 --> 01:04:04.159
It's very surprising,
but that is, I think,

01:04:04.159 --> 01:04:07.952
what the current type of hypothesis is for
being the best way to do it.

01:04:14.682 --> 01:04:15.510
You can't.

01:04:15.510 --> 01:04:18.310
So the question is how do we make
sure the same set of hyper parameters

01:04:18.310 --> 01:04:19.360
end up with the same results?

01:04:19.360 --> 01:04:24.170
They never do, and some people, this gets,

01:04:24.170 --> 01:04:25.740
we could talk a lot about this,
this is kind of fun.

01:04:25.740 --> 01:04:30.090
This is like the secret sauce in some ways
of the learning, but some people also say,

01:04:30.090 --> 01:04:34.560
I'm going to run the same model with
the same hyper parameters five times, and

01:04:34.560 --> 01:04:38.510
then I'm going to average and ensemble
those five models, because they're all end

01:04:38.510 --> 01:04:42.180
up in a different local optimum and
that assembling can also often help.

01:04:42.180 --> 01:04:46.460
So at the end of every project that you're
in, if you have 100 models that you've

01:04:46.460 --> 01:04:52.580
trained, you could always take the top 5
models that you've had over the course of

01:04:52.580 --> 01:04:55.310
your project, and ensemble that, and you'd
probably squeeze out another 1 or 2%.

01:04:55.310 --> 01:04:59.910
But again, probably don't have to go
that far for your class projects.

01:04:59.910 --> 01:05:03.130
It's only if it really matters and
you're down some application,

01:05:03.130 --> 01:05:06.300
medical applications or
whatever, to need to do that.

01:05:06.300 --> 01:05:11.120
And in many cases what you'll observe
is in papers in competitions,

01:05:11.120 --> 01:05:15.350
people write this is my best single model
and this is my best ensemble model.

01:05:15.350 --> 01:05:18.130
And then in the best ensemble model
you can claim state of the art and

01:05:18.130 --> 01:05:21.090
the best single model might be sometimes
also the best single model, but

01:05:21.090 --> 01:05:23.340
sometimes you also have
a more diverse setup model,

01:05:23.340 --> 01:05:27.415
so all right last question about
assembling and crazy model header.

01:05:27.415 --> 01:05:32.180
[BLANK

01:05:32.180 --> 01:05:38.270
AUDIO] Great question.

01:05:38.270 --> 01:05:40.160
Why does ensembling still work?

01:05:40.160 --> 01:05:42.730
And shouldn't we just have
a better single model?

01:05:42.730 --> 01:05:43.960
You're totally right.

01:05:43.960 --> 01:05:47.350
There are various ML researchers who say,
I don't like ensembling at all.

01:05:47.350 --> 01:05:49.890
We should just work harder
on better single models.

01:05:49.890 --> 01:05:55.740
And dropout is actually one such idea that
you can do there, other optimization,

01:05:55.740 --> 01:06:00.870
ideas that try to incorporate that,
yeah you're right.

01:06:00.870 --> 01:06:04.580
In some ways, what that means is we still
don't have the perfect optimization

01:06:04.580 --> 01:06:10.369
algorithms that properly explore the
energy landscape of our various models.

01:06:11.400 --> 01:06:15.360
All right, so let's go over the extra
hyperparameters that they used here.

01:06:15.360 --> 01:06:21.080
So basically we want to find all
these hyperparameters on the dev set.

01:06:21.080 --> 01:06:21.900
Super important,

01:06:21.900 --> 01:06:26.760
you get minus 10% if I see you run on
the final test set all your hyperparameter

01:06:26.760 --> 01:06:31.770
optimization, because that means you're
now overfitting to your final test set.

01:06:31.770 --> 01:06:33.950
One of the number one 101
machine learning rules.

01:06:33.950 --> 01:06:38.250
Never run all your hyperparameter across
validation on your final test set.

01:06:38.250 --> 01:06:42.370
That's the one thing you want to run maybe
once or twice, it can ruin your entire

01:06:42.370 --> 01:06:46.190
career if you do that and
you publish it, it's never worth it.

01:06:46.190 --> 01:06:47.240
Don't do it.

01:06:47.240 --> 01:06:52.300
All right, so on the development set,
on your development test set, your, or

01:06:52.300 --> 01:06:58.090
sometimes called dev set, we or Kim here
tried various different nonlinearities and

01:06:58.090 --> 01:07:00.140
in the end chose the rectify linear unit.

01:07:00.140 --> 01:07:01.420
So that's actually very common,

01:07:01.420 --> 01:07:04.830
and nowadays you almost don't
have to run and try that.

01:07:04.830 --> 01:07:07.030
You just use rally as the default.

01:07:07.030 --> 01:07:12.890
He had try grams, four grams and
five grams for the various filter sizes.

01:07:12.890 --> 01:07:16.190
Somewhat surprising,
note by grams, that surprised me.

01:07:16.190 --> 01:07:20.570
He had 100 different feature maps for
each of these sizes.

01:07:20.570 --> 01:07:23.180
So 100 tri-gram filters, 100 4-gram
filters, and 100 5-gram filters.

01:07:24.890 --> 01:07:25.910
So the more you have,

01:07:25.910 --> 01:07:29.850
the more likely each of them can
capture different kinds of things.

01:07:29.850 --> 01:07:33.570
So if you have, for instance, just
a simple sentiment classifier, you can

01:07:33.570 --> 01:07:38.090
have 100 shots at trying to capture
various types of negation, for instance.

01:07:39.370 --> 01:07:43.690
Then drop out just simple in
the middle half of the time

01:07:43.690 --> 01:07:45.360
all the features are set to zero.

01:07:46.400 --> 01:07:51.239
He chose for this funky
regularization trick, s equals three.

01:07:52.490 --> 01:07:55.610
Somewhat surprising your mini
batch size will often also change

01:07:55.610 --> 01:07:58.050
your performance significantly.

01:07:58.050 --> 01:08:01.880
So, you don't usually want to have
gigantic mini batch sizes for

01:08:01.880 --> 01:08:03.890
most NLP models.

01:08:03.890 --> 01:08:06.840
So here you had a mini batch size of 50.

01:08:06.840 --> 01:08:11.585
During mini batch SGD training and
to use the word vectors

01:08:11.585 --> 01:08:16.100
pre-trained on a large corpus and
you had 300 dimensional word vectors.

01:08:16.100 --> 01:08:20.671
So that was a lot of hyper parameter
search you can think that was going on in

01:08:20.671 --> 01:08:22.533
the background here, yeah.

01:08:30.179 --> 01:08:32.918
Wouldn't a higher mini batch size
guarantee that we have less noise

01:08:32.918 --> 01:08:33.649
while training?

01:08:33.649 --> 01:08:37.958
The answer is yes, but
you actually want the noise.

01:08:37.958 --> 01:08:41.070
You have a very nonconvex
objective function here.

01:08:41.070 --> 01:08:46.310
And, what dropout does in SGD is
to actually introduce noise so

01:08:46.310 --> 01:08:49.820
that you're more likely to explore the
energy landscape, instead of just being

01:08:49.820 --> 01:08:53.380
very certain about being stuck in one
of the many local optimi that you have.

01:08:53.380 --> 01:08:56.660
So, a lot of optimization tricks and

01:08:56.660 --> 01:09:00.390
training tricks of neural networks
in the last couple years can

01:09:00.390 --> 01:09:03.310
be described as adding noise
into the optimization process.

01:09:04.830 --> 01:09:06.970
And now, this one here's also
super important during training,

01:09:06.970 --> 01:09:08.340
how do you select your best model?

01:09:08.340 --> 01:09:14.230
So one option is you just let it run, and
at the very end you take that last output.

01:09:14.230 --> 01:09:18.516
But what you'll often observe
is a pattern like this.

01:09:18.516 --> 01:09:21.570
Let's say you start training.

01:09:21.570 --> 01:09:26.970
So these are your iterations, and this
might be your accuracy or your f1 score,

01:09:26.970 --> 01:09:30.500
your wu score, your blue score,
whatever you're using for your model.

01:09:30.500 --> 01:09:39.170
And now, you'll often observe something
like this, as you train over time.

01:09:39.170 --> 01:09:43.810
And now, if you take just
the very last one, maybe here.

01:09:43.810 --> 01:09:48.377
Maybe that wasn't as good as this
random spot that as it did stochastic

01:09:48.377 --> 01:09:52.200
gradient descent, it found just a randomly
really good spot on your dev set, and

01:09:52.200 --> 01:09:55.230
so what Kim does and
what actually a lot of people do

01:09:55.230 --> 01:09:58.140
is during training you keep
checking the performance here.

01:09:58.140 --> 01:10:02.910
Again this is your
development accuracy and

01:10:02.910 --> 01:10:05.310
then you pick the one with
the highest accuracy and

01:10:05.310 --> 01:10:08.690
you set those weights to be the weights
for that particular experiment.

01:10:11.484 --> 01:10:14.781
And another side trick, just because
they're fun, you can also sample multiple

01:10:14.781 --> 01:10:17.530
times, and then ensemble those weights or
average those weights.

01:10:17.530 --> 01:10:19.790
And that sometimes also works better,
all right.

01:10:19.790 --> 01:10:20.630
Still some black magic.

01:10:22.110 --> 01:10:27.897
All right, so that was a lot of good
details for how you tune Junior models.

01:10:27.897 --> 01:10:32.366
So here now some of the results, so here
we basically have only sadly in this whole

01:10:32.366 --> 01:10:36.377
paper four implementations, and four of
the options that are really carefully

01:10:36.377 --> 01:10:41.045
outlined and all the other hyperparameters
we don't know how important they were or

01:10:41.045 --> 01:10:42.400
the variance of them.

01:10:42.400 --> 01:10:42.900
Yeah?

01:10:48.749 --> 01:10:50.150
You can do both.

01:10:50.150 --> 01:10:53.350
So you can average the weights,
which is very counter-intuitive,

01:10:53.350 --> 01:10:55.470
but also often works.

01:10:55.470 --> 01:10:58.620
If you average the predictions,
now you have to keep around and

01:10:58.620 --> 01:11:00.370
say you have an ensemble
of the four top models,

01:11:00.370 --> 01:11:04.500
you have to keep around your model size
times four, which is very slow and

01:11:04.500 --> 01:11:07.580
not that great, so it's less commonly
done, especially not in practice.

01:11:10.743 --> 01:11:13.210
Sorry, can I define appellation?

01:11:13.210 --> 01:11:17.040
So an appellation study is essentially
a study where you start with

01:11:17.040 --> 01:11:20.550
your fancy new model that you
described in all its details.

01:11:20.550 --> 01:11:25.850
And then you say how much did each of
the components actually matter to my final

01:11:25.850 --> 01:11:28.530
accuracy that I describe in my abstract.

01:11:28.530 --> 01:11:33.360
So let's say we have a cool deep learning
model with five-layer LSTMs, and

01:11:33.360 --> 01:11:37.260
some attention mechanisms,
and some other clever ways of

01:11:37.260 --> 01:11:41.570
reversing the input in machine
translation, for instance, and so on.

01:11:41.570 --> 01:11:47.530
And now you say, all right, overall
this model got me a roster of 30 or 25.

01:11:47.530 --> 01:11:53.260
Now [BLANK AUDIO] as a practitioner,
I, when I read that paper and

01:11:53.260 --> 01:11:57.080
even as a researcher, I want to know well,
you mentioned five tricks,

01:11:57.080 --> 01:12:01.220
which of the five were actually
the ones that got you to 25?

01:12:01.220 --> 01:12:02.450
Was it?
Yeah?

01:12:02.450 --> 01:12:05.417
You want to know because you might
not want to use that entire model but

01:12:05.417 --> 01:12:07.200
you want to use that one trick.

01:12:07.200 --> 01:12:10.300
So in this case here for instance,
he has this regularization trick.

01:12:10.300 --> 01:12:11.410
And he has the dropout.

01:12:12.430 --> 01:12:16.140
How much did the dropout actually help,
versus this trick?

01:12:16.140 --> 01:12:18.310
Which one should I now use
in my downstream task.

01:12:19.420 --> 01:12:21.910
In this paper here, and
I don't want to single him out,

01:12:21.910 --> 01:12:24.860
this is really, sadly,
very common in the field.

01:12:26.260 --> 01:12:30.985
Nobody give you a nice plot for
every single hyper parameter that says for

01:12:30.985 --> 01:12:35.169
this hyper parameter let's say
dropout p between zero and one,

01:12:35.169 --> 01:12:36.658
this was my accuracy.

01:12:39.750 --> 01:12:46.140
Yeah, so the appellation would say I will
take out one particular modeling decision.

01:12:46.140 --> 01:12:48.400
So let's say in his case, for instance.

01:12:48.400 --> 01:12:52.570
The application here is do we need
to multichannel two word vectors.

01:12:52.570 --> 01:12:57.560
One you back probe into one you don't, or
do we only have a static channel as in

01:12:57.560 --> 01:13:01.410
we keep he called static here just keeping
the word vectors fixed in the beginning

01:13:01.410 --> 01:13:05.550
versus having only a single channel where
we back propagate into the word vectors

01:13:05.550 --> 01:13:09.600
versus having both of
the word vector sets.

01:13:09.600 --> 01:13:12.180
And this is the ablation he does here.

01:13:12.180 --> 01:13:16.450
The ablation over should we have
two sets of work vectors or not?

01:13:16.450 --> 01:13:20.525
And as you can see here, well,
it actually, sometimes it buys you 0.7 and

01:13:20.525 --> 01:13:25.320
here 0.9 or so, but
sometimes it also hurts.

01:13:25.320 --> 01:13:29.340
So here having the two
channels actually hurt by 0.4.

01:13:29.340 --> 01:13:31.820
These are relatively small data sets,
all of them, so

01:13:31.820 --> 01:13:33.400
the variance is actually relatively high.

01:13:34.800 --> 01:13:36.920
And the first, the simplest one,

01:13:36.920 --> 01:13:41.500
is you actually just have random word
vectors, and you back-propagate into them,

01:13:41.500 --> 01:13:44.510
and you just learn the word
vectors as part of your task.

01:13:44.510 --> 01:13:47.110
So no pre-training of
word vectors whatsoever.

01:13:47.110 --> 01:13:50.090
That's actually fine,
if you have a gigantic training dataset.

01:13:50.090 --> 01:13:54.050
So if you do machine translation
on five gigabytes of,

01:13:54.050 --> 01:13:55.130
don't do it for your project.

01:13:55.130 --> 01:13:57.690
I hope we discouraged
everybody from trying that.

01:13:57.690 --> 01:13:59.990
But if you do machine
translation on a very,

01:13:59.990 --> 01:14:02.970
very large corpus, it turns out you
can just have random word vectors.

01:14:02.970 --> 01:14:07.047
And you have so much data on that task,
that as you back propagate into them and

01:14:07.047 --> 01:14:10.142
update them with SGD,
they will become very good as well.

01:14:24.067 --> 01:14:24.923
That's totally right.

01:14:24.923 --> 01:14:27.271
So, Arun correctly points out and

01:14:27.271 --> 01:14:32.067
mention this like this is actually very
small datasets and so, there's very

01:14:32.067 --> 01:14:36.528
little statistical significance
between most of these results here.

01:14:36.528 --> 01:14:38.350
Maybe SST, 0.9.

01:14:38.350 --> 01:14:41.960
I forgot all the various thresholds
of statistics, assuming against, for

01:14:41.960 --> 01:14:42.730
the various papers.

01:14:42.730 --> 01:14:45.410
I think this one might be significant.

01:14:45.410 --> 01:14:48.430
But certainly, MPQA for instance,
is a very small data set.

01:14:48.430 --> 01:14:52.055
So this 0.1 difference is not
statistically significant

01:15:09.749 --> 01:15:14.575
Great question, so
the question is, as you do this,

01:15:14.575 --> 01:15:18.025
let's say you had your full data set.

01:15:18.025 --> 01:15:20.435
You say, this is my training data.

01:15:21.600 --> 01:15:25.040
This is my development split and
this is my final test split, and

01:15:25.040 --> 01:15:29.540
we had properly randomized
them in some way.

01:15:29.540 --> 01:15:36.200
Now, if I choose based on my development
split and this development accuracy here,

01:15:36.200 --> 01:15:41.360
then this model is only trained on this.

01:15:41.360 --> 01:15:45.910
And only in sort of some meta kind of way,
used that dev split.

01:15:45.910 --> 01:15:48.820
Now, what you could also do and
what you should always do if you have and

01:15:48.820 --> 01:15:53.890
actual convex problem, which, sadly,
we don't have in this class very much,

01:15:53.890 --> 01:15:57.200
what you would do is you find your
best hyper parameter setting and

01:15:57.200 --> 01:16:01.330
then you'll actually retrain with that
whole thing all the way until convergence.

01:16:01.330 --> 01:16:03.100
Which if you have a convex problem,
it's great.

01:16:03.100 --> 01:16:05.810
You know you have a global optimum, and
you probably have a better global optimal

01:16:05.810 --> 01:16:08.210
because you used you
entire trained data set.

01:16:08.210 --> 01:16:13.220
Now, it turns out in many cases,
because there can be a lot of variance,

01:16:13.220 --> 01:16:17.430
In your training for these very
non-convex neural network models.

01:16:17.430 --> 01:16:22.010
It helps a little bit to
just ignore that final,

01:16:22.010 --> 01:16:28.560
that deaf part of your data set, and just
use it to only choose the highest point.

01:16:28.560 --> 01:16:31.380
But yeah, it's largely because it's
a non-convex problem, great question.

01:16:36.140 --> 01:16:39.740
All right, we have four more minutes.

01:16:39.740 --> 01:16:44.110
So, one of the problems with this
comparison here, was actually that

01:16:44.110 --> 01:16:48.880
the dropout for instance gave it two
to four percent accuracy improvement.

01:16:48.880 --> 01:16:52.560
And overall, and you'll see this
in a lot of deep learning papers,

01:16:52.560 --> 01:16:54.529
they make claims about,
this is the better model.

01:16:55.740 --> 01:17:00.380
Sadly, when you look at it there are some
models here that they're comparing to

01:17:00.380 --> 01:17:04.560
that came out after or
before dropout was invented.

01:17:04.560 --> 01:17:12.110
So we can be very certain that some
models from pre-2014 didn't use any of

01:17:12.110 --> 01:17:18.170
the kinds of tricks like dropout and hence
the comparisons actually kind of flop.

01:17:18.170 --> 01:17:20.910
And sadly,
you'll observe this in most papers.

01:17:20.910 --> 01:17:27.390
Almost very, very few people in the
community will go re-run with the newest

01:17:27.390 --> 01:17:32.460
and fanciest optimization tricks like
Dropout or add in better optimizers and

01:17:32.460 --> 01:17:37.160
so on and reimplement all the baseline
models of previous authors, and

01:17:37.160 --> 01:17:41.270
then have a proper comparison run
the same amount of cross validation on

01:17:41.270 --> 01:17:44.690
the second best model and
other people's models, and

01:17:44.690 --> 01:17:49.320
then have a really proper scientific study
to say this is the actual better model

01:17:49.320 --> 01:17:53.960
versus this model came out later, had the
benefit of a lot of optimization tricks.

01:17:53.960 --> 01:17:55.560
And hence came out on top.

01:17:55.560 --> 01:17:57.100
So you'll see that a lot,

01:17:57.100 --> 01:18:02.150
and it's in some ways understandable
because it takes a long time to reproduce

01:18:02.150 --> 01:18:05.290
ten other people's results and
then start tuning them.

01:18:05.290 --> 01:18:08.325
But you have to take a lot of
these with a grain of salt,

01:18:08.325 --> 01:18:12.775
because the optimization,
as we see here, makes a big difference.

01:18:12.775 --> 01:18:17.589
So two to four percent, when you look
at even some of my old papers, four

01:18:17.589 --> 01:18:22.914
precent is the difference between whether
this model is the better one or not.

01:18:25.187 --> 01:18:30.150
Still, it is kind of a very cool
architecture, this convolutional network.

01:18:30.150 --> 01:18:32.690
The fact that it can do so well overall.

01:18:32.690 --> 01:18:34.980
Something that is quite remarkable.

01:18:34.980 --> 01:18:38.740
It's relatively simple, and
the nice thing is, with these filters,

01:18:38.740 --> 01:18:40.860
each of the filters is
essentially independent, right?

01:18:40.860 --> 01:18:44.750
We run max pooling over each
of the filters independently,

01:18:44.750 --> 01:18:48.650
so each filter can be run
on one core of your GPU.

01:18:48.650 --> 01:18:53.660
And so
despite having 300 different filters,

01:18:53.660 --> 01:18:56.590
you can run all of those 300 in parallel,
maximum peril.

01:18:56.590 --> 01:19:00.390
And then you have very quickly it can
compute that one feature back there and

01:19:00.390 --> 01:19:01.910
pipe it into the softmax.

01:19:01.910 --> 01:19:05.515
So that is actually a huge
advantage of these kinds of models.

01:19:06.645 --> 01:19:10.435
Now, we don't have that much time left, so
I'm not going to go into too many details,

01:19:10.435 --> 01:19:14.215
but you can really go to town and
put together

01:19:14.215 --> 01:19:17.709
lots of convolutions on top of pooling
layers in a variety of different ways.

01:19:19.030 --> 01:19:22.940
We spend a lot of time trying to
gain intuitions of why this LSTM

01:19:22.940 --> 01:19:24.890
node gate has this effect.

01:19:24.890 --> 01:19:28.890
I don't think we have a good chance of
going here in the third CNN layer and

01:19:28.890 --> 01:19:32.820
having some intuition of why it's
this kind of layer versus another.

01:19:32.820 --> 01:19:35.320
They're really,
they really get quite unwieldy.

01:19:35.320 --> 01:19:37.930
You can have various kinds of convolution,
so

01:19:37.930 --> 01:19:40.218
those are in some sense hyper parameters.

01:19:40.218 --> 01:19:46.200
You can ignore basically
zero pad the outside or

01:19:46.200 --> 01:19:50.840
you can just not run anything that would
require an outside multiplication and

01:19:50.840 --> 01:19:54.260
only run convolutions when you have for
the insides.

01:19:54.260 --> 01:19:56.850
Basically this is the narrow
versus the white convolution.

01:19:57.860 --> 01:20:02.320
You can eventually run the convolution
also, not over the times steps, but

01:20:02.320 --> 01:20:04.340
in later layers over the feature maps.

01:20:04.340 --> 01:20:06.590
So they're a lot of different options.

01:20:06.590 --> 01:20:09.602
And at some point there's
no more intuition of,

01:20:09.602 --> 01:20:13.283
why should you do this in the third
layer of these texts CNN?

01:20:16.081 --> 01:20:20.315
One of the most exciting applications
was actually to take such a CNN,

01:20:20.315 --> 01:20:23.401
have various pooling operations,
and in the end,

01:20:23.401 --> 01:20:28.440
take that as input to a recurrent
neural network for machine translation.

01:20:28.440 --> 01:20:33.260
So this was one of the first deep learning
machine translation models from 2013 that

01:20:33.260 --> 01:20:37.625
actually combined these
fast parallelizable CNNs,

01:20:37.625 --> 01:20:41.480
with a recurrent neural network to do
the machine translation that we've seen.

01:20:41.480 --> 01:20:45.734
So, we've essentially described just
model entirely in a lecture before,

01:20:45.734 --> 01:20:48.270
but now we're replacing the encoder part,

01:20:48.270 --> 01:20:51.140
instead of having an LSTM
here we have a CNN here, and

01:20:51.140 --> 01:20:58.040
we give that as an input to all the time
steps at the decoder part of the model.

01:20:58.040 --> 01:20:59.980
Very cool model.

01:20:59.980 --> 01:21:04.500
Now, probably I'll end on this slide and
we'll maybe talk about

01:21:04.500 --> 01:21:08.190
this quasi recurring neural networks that
combines the best of both recurrent and

01:21:08.190 --> 01:21:09.680
convolutional models.

01:21:09.680 --> 01:21:14.280
For another lecture, but basically you
now know some of the most important and

01:21:14.280 --> 01:21:17.210
most widely used models for
deep learning for NLP.

01:21:17.210 --> 01:21:19.090
We have the bag of vectors,

01:21:19.090 --> 01:21:24.030
surprisingly works quite well when you
combine it with a couple of relu layers.

01:21:24.030 --> 01:21:27.520
And can actually even in some benchmarks
beat this convolutional network

01:21:27.520 --> 01:21:28.810
that we just described.

01:21:28.810 --> 01:21:30.810
So very good base line to run for

01:21:30.810 --> 01:21:33.880
a variety of different projects
that we're discussing.

01:21:33.880 --> 01:21:37.510
We've discussed the window model
already where we have basically

01:21:37.510 --> 01:21:42.130
a very clean model to classify
words in their context.

01:21:42.130 --> 01:21:44.840
Now we know
the Convolutional Neural Networks and

01:21:44.840 --> 01:21:48.200
we had a lot of variants of
Recurrent Neural Networks.

01:21:48.200 --> 01:21:50.786
So, hopefully,
you have most of the tools on Thursday.

01:21:50.786 --> 01:21:53.194
Chris will talk about
Recursive Neural Networks or

01:21:53.194 --> 01:21:57.172
tree structured Recursive Neural Networks
that will be much more grammatically and

01:21:57.172 --> 01:22:00.044
linguistically plausible, but
also have some downsides.

01:22:00.044 --> 01:22:04.050
All right, thank you.

