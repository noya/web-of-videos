WEBVTT
Kind: captions
Language: en

00:00:01.950 --> 00:00:02.700
All right now.

00:00:02.700 --> 00:00:03.880
In the previous segment,

00:00:03.880 --> 00:00:09.460
we saw how to compute PPMI,
positive pointwise mutual information.

00:00:09.460 --> 00:00:11.810
In this segment,
we'll see how to take those values and

00:00:11.810 --> 00:00:13.290
compute similarity between words.

00:00:14.940 --> 00:00:18.820
First, let's talk about a different
kind of context than just word context.

00:00:18.820 --> 00:00:21.260
A common way to define
context is to use syntax.

00:00:21.260 --> 00:00:25.630
And this again relates back to early
linguistic arguments that the meaning

00:00:25.630 --> 00:00:30.580
has to do with the restriction of
combination of entities grammatically.

00:00:32.160 --> 00:00:36.240
So in other words, two words are similar
if they have similar parse contexts.

00:00:36.240 --> 00:00:41.220
And we haven't talked a lot about parsing,
but to give you the intuition, the words

00:00:41.220 --> 00:00:47.170
duty and the words responsibility can
both be modified by similar adjectives,

00:00:47.170 --> 00:00:50.640
so we can have additional duty,
administrative duty, assumed duty, or

00:00:50.640 --> 00:00:54.820
additional responsibility, administrative
responsibility, assumed responsibility.

00:00:54.820 --> 00:00:59.610
And they can also be objects of similar
verbs; assert duty, assign a duty,

00:00:59.610 --> 00:01:03.290
assume a duty, assign a responsibility,
assume a responsibility.

00:01:03.290 --> 00:01:05.610
So it's not just that they have
similar words around them, but

00:01:05.610 --> 00:01:07.970
that their grammatical
context can be similar.

00:01:07.970 --> 00:01:09.610
They have similar parse context.

00:01:10.940 --> 00:01:14.120
And we can capture this by using
co-occurrence vectors that are based on

00:01:14.120 --> 00:01:15.970
syntactic dependencies.

00:01:15.970 --> 00:01:17.920
So we say that the context,

00:01:17.920 --> 00:01:22.570
instead of being counts of words with
previous ten words or following ten words.

00:01:22.570 --> 00:01:27.680
The context instead are how many times
I have a particular word as my subject,

00:01:27.680 --> 00:01:30.860
or how many times I've a particular
word as my adjectival modifier.

00:01:30.860 --> 00:01:33.014
So here's an example from.

00:01:33.014 --> 00:01:35.840
We have the word sell, and

00:01:35.840 --> 00:01:40.090
we say, how often was this
the subject of the verb absorb?

00:01:40.090 --> 00:01:41.190
Well once.

00:01:41.190 --> 00:01:42.950
How often was the subject
of the verb adapt?

00:01:42.950 --> 00:01:44.760
How about the subject verb of behave?

00:01:44.760 --> 00:01:47.280
How about the prepositional
object of inside?

00:01:47.280 --> 00:01:50.230
So, we can get our counts for
each of this context.

00:01:50.230 --> 00:01:52.990
And now, our vector is determine to not by

00:01:52.990 --> 00:01:55.700
accounts of words that
occurred in ten words of me.

00:01:55.700 --> 00:01:59.400
But counts of times I occurred
are particular grammatical relation with

00:01:59.400 --> 00:02:00.020
the context.

00:02:02.210 --> 00:02:06.360
And just as we saw with word counts,
we can use PMI or

00:02:06.360 --> 00:02:09.660
PPMI to our dependency relations.

00:02:09.660 --> 00:02:15.210
So the intuition comes from early work by
Don Hindle and if I count in a corpus, and

00:02:15.210 --> 00:02:20.160
I parse the corpus and I see that the verb
drink has the object it three times and

00:02:20.160 --> 00:02:22.090
it has the I drink anything three times,

00:02:22.090 --> 00:02:25.370
I drink wine twice,
I drink liquid twice and so on.

00:02:26.540 --> 00:02:32.520
Well, drink it, or drink anything,
are in fact more common than drink wine,

00:02:32.520 --> 00:02:35.730
but we'd like to say that wine is
a more drinkable thing than it.

00:02:35.730 --> 00:02:40.510
If I found a wine occurring a lot
with a verb, two different verbs,

00:02:40.510 --> 00:02:43.730
I would think that those verbs are
probably similar, more than if I found,

00:02:43.730 --> 00:02:46.110
it occurring as the object
of the two verbs.

00:02:46.110 --> 00:02:49.190
And if I compute the PMIs,

00:02:49.190 --> 00:02:53.500
the PMI between the object of the verb and
the verb, drink.

00:02:53.500 --> 00:02:56.430
And now I see that wine and

00:02:56.430 --> 00:03:00.920
tea and liquid have a much
higher PMI than it or anything.

00:03:01.970 --> 00:03:07.040
So if I sort by PMI,
now I see that tea and liquid and

00:03:07.040 --> 00:03:12.040
wine are the most associated words
to be the object of the verb drink.

00:03:13.330 --> 00:03:20.300
So PMI used for noun associations
of a word in the context,

00:03:20.300 --> 00:03:23.510
and also for word associations for
dependency relations.

00:03:26.110 --> 00:03:29.520
All right, now we've seen how
to compute the term context or

00:03:29.520 --> 00:03:32.250
word context matrix,
how to weight it with PMI.

00:03:32.250 --> 00:03:34.940
And we've talked about computing
in two ways based on just

00:03:34.940 --> 00:03:37.290
words that are in my
neighborhood of ten words.

00:03:37.290 --> 00:03:41.290
Or whether I'm in a particular syntactic
or grammatical relationship with words.

00:03:41.290 --> 00:03:43.660
Now we're ready to use those to
compute the actual similarity.

00:03:44.790 --> 00:03:46.058
In the cosine metric,

00:03:46.058 --> 00:03:50.478
we're going to use just the same cosine
that we saw from information retrieval.

00:03:50.478 --> 00:03:55.013
So remember we had a dot product, we said
that the cosine similarity between two

00:03:55.013 --> 00:03:58.208
vectors, two vectors indicating
the counts of words.

00:03:58.208 --> 00:04:02.750
It's just the dot product between
the similarity is the dot product

00:04:02.750 --> 00:04:07.500
between the two words normalized
by the length of the two vectors.

00:04:07.500 --> 00:04:13.190
So the dot product v dot w
over length of v times length

00:04:14.310 --> 00:04:19.310
of w or we could compute, think of it
as computing separate unit vectors that

00:04:19.310 --> 00:04:23.190
normalizing v by its length, normalizing
w by its length to get unit vectors, and

00:04:23.190 --> 00:04:24.480
just multiplying them together.

00:04:24.480 --> 00:04:28.350
Or we can multiply the whole thing out,
so here's our dot product.

00:04:28.350 --> 00:04:32.603
For each dimension of v and each dimension
of w, we multiply the values together and

00:04:32.603 --> 00:04:36.552
then we normalize by the square root of
the sum squared values to get the length

00:04:36.552 --> 00:04:37.428
of the vectors.

00:04:37.428 --> 00:04:41.000
And now, we let's say we're doing PPMI, so

00:04:41.000 --> 00:04:44.948
this sub i is the PPMI value for
word v in context i.

00:04:44.948 --> 00:04:50.158
And w sub i is the PPMI value for
word w in context i.

00:04:50.158 --> 00:04:55.494
And remember that cosine asymmetric
if two vectors are orthogonal,

00:04:55.494 --> 00:04:58.070
we're going to have a cosine of 0.

00:04:58.070 --> 00:05:01.770
If they point in opposite directions,
we have a cosine -1.

00:05:01.770 --> 00:05:04.790
If they point in the same direction,
they'll have a cosine of plus 1.

00:05:04.790 --> 00:05:08.810
And it turns out that raw frequency or
PPMI are non-negative,

00:05:08.810 --> 00:05:10.320
they're always 0 or greater.

00:05:10.320 --> 00:05:13.180
So that means that the cosine
range is always 0 to 1,

00:05:13.180 --> 00:05:15.600
we're always on this part of the slope.

00:05:15.600 --> 00:05:20.620
So cosine as a similarity metric,
if we use PPMI weighted counts.

00:05:20.620 --> 00:05:22.538
We're going to get all roughly
conceived for that matter.

00:05:22.538 --> 00:05:26.498
We're going to get a number between 0 and
1.

00:05:26.498 --> 00:05:30.258
So let's compute,
use cosine to compute similarity.

00:05:30.258 --> 00:05:33.690
You and I have taken a little subset
of the example we saw earlier.

00:05:33.690 --> 00:05:36.330
So we have apricot,
digital and information.

00:05:36.330 --> 00:05:38.180
And we have the context vector.

00:05:38.180 --> 00:05:40.820
We have large data and computer.

00:05:40.820 --> 00:05:43.810
I'm just going to use counts here
instead of PPMI values just for

00:05:43.810 --> 00:05:47.445
making the example more simple to see,
but in real life of course we'd use PPMI.

00:05:48.530 --> 00:05:54.110
So, the cosine between apricot and
information is the dot product.

00:05:54.110 --> 00:05:58.180
So, from apricot to information
we have one times one

00:05:58.180 --> 00:06:01.570
plus zero times six plus zero times one or
one plus zero plus zero.

00:06:02.680 --> 00:06:05.640
Over the square root of the length

00:06:05.640 --> 00:06:11.110
of apricot one squared plus zero
squared plus zero squared over

00:06:11.110 --> 00:06:15.490
the length of information one squared
plus six squared plus one squared.

00:06:16.920 --> 00:06:21.620
And that's going to be one over
the square root of 38 or 0.16.

00:06:21.620 --> 00:06:27.570
And similarly, the cosine between
digital information we have from digital

00:06:27.570 --> 00:06:33.790
to information we have 0 times 1,
plus 1 times 6, plus 2 times 1, so

00:06:33.790 --> 00:06:40.450
that's going to be 0 plus six plus 2 over
the square root of the length digital.

00:06:40.450 --> 00:06:42.708
So that's, the length of digital,
I'm sorry.

00:06:42.708 --> 00:06:47.050
So that's the square root of 0
squared plus 1 squared plus 2 squared,

00:06:47.050 --> 00:06:48.880
so root of 0 plus 1 plus 4.

00:06:48.880 --> 00:06:54.800
And then the length of information missing
as we saw before, and now we get 0.58.

00:06:54.800 --> 00:06:58.420
And similarly for apricot and digital,

00:06:58.420 --> 00:07:03.960
now the dot product between apricot and
digital 1 times 0,

00:07:03.960 --> 00:07:08.260
0 times 1, 0 time 2 is 0, so
we're going to get a similarity of 0.

00:07:08.260 --> 00:07:13.250
There are a number of
possible similarity metrics.

00:07:13.250 --> 00:07:18.060
Besides cosine, we can use the Jaccard
method that we saw earlier in information

00:07:18.060 --> 00:07:22.850
retrieval, the Dice symmetric or there's
a family of information theoretic methods

00:07:22.850 --> 00:07:27.000
like Jensen Shannon divergence that
can be also used for similarity.

00:07:27.000 --> 00:07:32.058
But cosine is probably the most popular.

00:07:32.058 --> 00:07:35.971
Similarity for distributional methods
is evaluated just the same as it is for

00:07:35.971 --> 00:07:40.550
thesaurus-based methods, either intrinsic
evaluation where compute its correlation

00:07:40.550 --> 00:07:43.458
with some human number,
human word similarity number.

00:07:43.458 --> 00:07:47.211
Or extrinsically, we pick some
task like taking the TOEFL exam or

00:07:47.211 --> 00:07:51.210
detecting spelling errors where
we can compute some error rate.

00:07:51.210 --> 00:07:54.190
And now we see if our similarity
metric results in a better error rate.

00:07:56.620 --> 00:07:57.470
In summary,

00:07:57.470 --> 00:08:02.250
distributional similarity metrics use
a method of association like PPMI and

00:08:02.250 --> 00:08:07.880
a metric for similarity like cosine to
give you a similarity between two words.

00:08:07.880 --> 00:08:10.750
And distributional and
thesaurus based methods both

00:08:10.750 --> 00:08:14.650
very useful in deciding if two words
are similar for any NLP application.

