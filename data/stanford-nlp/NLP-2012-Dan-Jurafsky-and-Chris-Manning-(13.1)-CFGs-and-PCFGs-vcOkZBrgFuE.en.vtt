WEBVTT
Kind: captions
Language: en

00:00:00.840 --> 00:00:04.110
In this segment, I'm going to
introduce context-free grammars and

00:00:04.110 --> 00:00:07.740
their extension into probabilities,
probabilistic context-free grammars.

00:00:09.020 --> 00:00:13.330
So this is what a linguist calls a phrase
structure grammar which is also known In

00:00:13.330 --> 00:00:17.050
computer science there's a context
free grammar, same thing.

00:00:17.050 --> 00:00:22.360
And, so what we have are these
various rules where we have a category

00:00:22.360 --> 00:00:27.690
which rewrites as a sequence
of other categories and then

00:00:27.690 --> 00:00:33.760
eventually this writes down to, what are
called terminal symbols, which are words.

00:00:33.760 --> 00:00:37.200
And so using this grammar
we can produce sentences.

00:00:37.200 --> 00:00:40.170
So we start with the start symbol S and

00:00:40.170 --> 00:00:44.635
then we can expand down using any of the
rules of the grammar, so S goes to NP, VP,

00:00:44.635 --> 00:00:49.935
and then there is a rule
that says a noun phrase

00:00:49.935 --> 00:00:55.065
goes to a noun and
then a noun can go to say people.

00:00:58.777 --> 00:01:02.527
And then, a verb phrase, and
go to a verb and a noun phrase.

00:01:02.527 --> 00:01:07.884
And the verb can go to say fish and

00:01:07.884 --> 00:01:14.234
the noun phrase can go to a noun again,

00:01:14.234 --> 00:01:20.798
and the noun, can go to, say, tanks.

00:01:24.160 --> 00:01:27.990
And so using this grammar,
we can make sentences of the language.

00:01:27.990 --> 00:01:29.810
So here are two sentences in the language.

00:01:29.810 --> 00:01:33.200
We just saw that this one was
a sentence of the language, and

00:01:33.200 --> 00:01:36.000
this is another sentence of the language.

00:01:36.000 --> 00:01:38.330
If you look carefully and
play around with it a bit,

00:01:38.330 --> 00:01:42.680
you'll see that this is actually a very,
very ambiguous grammar.

00:01:42.680 --> 00:01:46.120
So sentences like, people,
people, people, people,

00:01:46.120 --> 00:01:50.600
people or fish, fish, fish,
are also sentences of this language.

00:01:53.440 --> 00:01:56.230
Okay, so
what is context free grammar formally?

00:01:56.230 --> 00:02:00.140
So context free grammar
formally is a two ball,

00:02:00.140 --> 00:02:03.090
consisting of a set of terminal symbols.

00:02:03.090 --> 00:02:06.400
So they were our words like fish and
people.

00:02:06.400 --> 00:02:08.350
A set of nonterminal symbols.

00:02:08.350 --> 00:02:12.300
Those were our ones like S and NP for
noun phrase, and VP for verb phrase.

00:02:13.370 --> 00:02:18.170
A start symbol,
which is one of the nonterminal symbols.

00:02:18.170 --> 00:02:23.430
And then we have a set of rules or
productions of the form and

00:02:23.430 --> 00:02:29.800
nonterminal rewrites as some
sequence of nonterminals or

00:02:29.800 --> 00:02:35.680
terminals, like VP goes to V NP PP and
that's our sequence.

00:02:37.420 --> 00:02:39.250
So this gives us a grammar, and

00:02:39.250 --> 00:02:42.230
what we'll say is that
a grammar generates a language.

00:02:42.230 --> 00:02:47.420
A language L is all the sentences
that can be produced by doing that

00:02:47.420 --> 00:02:52.270
process of rewriting from the start symbol
down to down to a sequence of terminals.

00:02:52.270 --> 00:02:55.890
And in many cases when there's any
complexity in there the size of that

00:02:55.890 --> 00:03:00.350
language will be infinite and it won't
actually include all possible strings.

00:03:01.660 --> 00:03:04.860
So that's the formal definition
of context-free grammar.

00:03:04.860 --> 00:03:08.580
In practice, when we're working on
linguistics and computational linguistics,

00:03:08.580 --> 00:03:13.180
we always make a couple of refinements
from that to this, which aren't super

00:03:13.180 --> 00:03:17.480
theoretically interesting, but give us a
more natural form for linguistic purposes.

00:03:17.480 --> 00:03:19.730
So let's just quickly look at this.

00:03:19.730 --> 00:03:22.690
So here's how NLP phase structure grammar.

00:03:22.690 --> 00:03:24.250
So how's it different?

00:03:24.250 --> 00:03:28.620
It's different because we've
introduced set of pre terminal symbols.

00:03:28.620 --> 00:03:34.300
So in practice when we're doing linguistic
grammars always most always we have

00:03:34.300 --> 00:03:40.340
rules with non terminals like noun
phrase goes to determine a noun.

00:03:41.370 --> 00:03:47.200
And then we have, and this part up here,
we often refer to as our grammar.

00:03:48.750 --> 00:03:52.400
And then we have a lexicon
where we have words,

00:03:52.400 --> 00:03:55.658
terminal symbols that
belong to categories.

00:03:55.658 --> 00:04:03.040
So and we write as the or
nn, can rewrite as man.

00:04:04.530 --> 00:04:09.090
So in the first definition preterminals,

00:04:09.090 --> 00:04:12.330
the lexical categories
had no special status.

00:04:12.330 --> 00:04:15.480
But we'd kind of like them to,
and so that's what we do here.

00:04:15.480 --> 00:04:21.180
So we say that our categories, our nouns,
verbs, and so on, are the preterminals.

00:04:21.180 --> 00:04:23.260
And then we have terminals.

00:04:23.260 --> 00:04:25.080
Now the words as before and

00:04:25.080 --> 00:04:27.975
then the non-terminals really
become our phrasal category.

00:04:27.975 --> 00:04:29.720
These things like noun phrase.

00:04:31.250 --> 00:04:33.930
Okay, but nothing else really changes.

00:04:33.930 --> 00:04:35.740
We have a start symbol.

00:04:37.812 --> 00:04:44.000
Our lexicon is now just rules of the form
that a preterminal rewrites as a terminal,

00:04:44.000 --> 00:04:49.370
and then our grammar is a set of rewrite
rules where we rewrite our sequences

00:04:49.370 --> 00:04:56.100
as before, but the left hand side is
always and the phrasal non terminal.

00:04:56.100 --> 00:05:00.140
And the right hand side can
include phrasal non terminals and

00:05:00.140 --> 00:05:01.980
the lexical categories.

00:05:03.950 --> 00:05:08.060
So by convention, for example,
often the start symbol

00:05:08.060 --> 00:05:12.910
is taken to actually be s because
S can also stand for sentence.

00:05:12.910 --> 00:05:17.100
But let me note that for a lot of NOP
purposes including the kind of treatments

00:05:17.100 --> 00:05:21.330
we look at, you always have some symbol
above S, because sometimes you can find

00:05:21.330 --> 00:05:25.570
things in text that aren't full sentences,
they might be a fragment like

00:05:25.570 --> 00:05:29.960
a prepositional phrase, or they might
just be a whole noun phrase or something.

00:05:29.960 --> 00:05:34.731
So we always have some symbol above
that that will rewrite as/s S or

00:05:34.731 --> 00:05:36.980
P-P or fragment of whatever.

00:05:36.980 --> 00:05:41.170
Commonly, that's taking
the name either root or top.

00:05:41.170 --> 00:05:45.850
And then one fine point in here is
when you can have the right hand

00:05:45.850 --> 00:05:50.680
side gamma be any sequence of phrasal, and

00:05:50.680 --> 00:05:55.070
lexical categories that actually
includes the empty sequence.

00:05:55.070 --> 00:05:58.970
But often it seems a bit funny just have
nothing on the right hand side that might

00:05:58.970 --> 00:06:03.450
be confusing the people so
if people commonly right an italic e

00:06:03.450 --> 00:06:06.210
to mean the right hand
side is actually empty.

00:06:06.210 --> 00:06:10.500
Let's see all of those conventions at work
in our phrase structure grammar here.

00:06:11.960 --> 00:06:14.386
Okay, so here we have the.

00:06:14.386 --> 00:06:19.474
Now the phrase rewrites as NP, so
linguists often use NP categories in

00:06:19.474 --> 00:06:24.920
grammar because it seems useful to
describe things as missing something.

00:06:24.920 --> 00:06:29.370
So, for example, as well as having
the sentence people fish tanks,

00:06:29.370 --> 00:06:32.912
you can make the imperative fish tanks.

00:06:36.210 --> 00:06:42.940
Or you can have an unspecified object,
and you can say people fish.

00:06:46.170 --> 00:06:49.660
And I don't want to get into
the linguistic analysis a lot and

00:06:49.660 --> 00:06:52.740
this suddenly isn't
linguistically refined grammar.

00:06:52.740 --> 00:06:57.580
But the idea is that a way you might want
to think about explaining things like this

00:06:57.580 --> 00:07:02.260
is to say, well actually there is
a subject NP to this sentence,

00:07:02.260 --> 00:07:04.045
but it's empty.

00:07:04.045 --> 00:07:09.385
And there is an object, NP, here,
that it's also empty and unexpressed.

00:07:09.385 --> 00:07:13.975
And so that's why grammars will often have
rules like this with empties in them.

00:07:13.975 --> 00:07:15.945
And we'll come back to them and

00:07:15.945 --> 00:07:20.860
often talk about ways of getting
rid of them and NLP a bit later.

00:07:20.860 --> 00:07:27.210
Okay, so this is our empty, and
then the other creatures we have,

00:07:27.210 --> 00:07:32.230
these things are called unary rules,
when one category rewrites

00:07:32.230 --> 00:07:38.320
is another category and we have a bunch
of things here that are binary rules.

00:07:38.320 --> 00:07:40.790
But then you're also able to have rules

00:07:40.790 --> 00:07:43.710
where something rewrites
to three of more things.

00:07:43.710 --> 00:07:46.690
And so here is a case that
here where a verb phrase

00:07:46.690 --> 00:07:51.260
rewrites as a sequence of a verb noun
phrase and prepositional phrase.

00:07:51.260 --> 00:07:55.480
And these are also things that
we often want to get rid of for

00:07:55.480 --> 00:07:58.590
doing our NLP grammars,
as I'll explain in a bit.

00:07:58.590 --> 00:08:02.420
Okay, so over here is our grammar rules.

00:08:02.420 --> 00:08:04.550
And here now over here is our lexicon.

00:08:08.574 --> 00:08:13.670
Okay, so this has just given us a phrase
structure grammar, context free grammar.

00:08:13.670 --> 00:08:16.530
But we wanted to have
probabilities in our grammar.

00:08:16.530 --> 00:08:19.110
So how do we go about doing that?

00:08:19.110 --> 00:08:23.760
Well, this is really actually a very
simple extension to what we had so far.

00:08:23.760 --> 00:08:27.930
So all of this stuff is just like
our context free grammar before, but

00:08:27.930 --> 00:08:29.840
what we do is we add on and

00:08:29.840 --> 00:08:33.830
say that there is one extra thing,
we have a probability function.

00:08:33.830 --> 00:08:39.060
And so the idea of the probability
function is takes each rule and

00:08:39.060 --> 00:08:43.980
gives it a probability; it maps it to
some real number between zero and one.

00:08:43.980 --> 00:08:48.230
Well, you don't just let it map to any
number completely under constrain.

00:08:48.230 --> 00:08:50.520
We add on this constraint here.

00:08:50.520 --> 00:08:53.080
That for any non-terminal

00:08:53.080 --> 00:08:57.510
the sums of the probabilities
of its rewrites add up to one.

00:08:57.510 --> 00:09:01.190
So that you have a probability
distribution over how,

00:09:01.190 --> 00:09:03.351
say, noun-phrase rewrites.

00:09:04.650 --> 00:09:09.760
And if you make just this condition and
actually there are a couple of technical

00:09:09.760 --> 00:09:15.000
assumptions that in practice when estimate
values of tree thinks are always true so

00:09:15.000 --> 00:09:19.630
I want to explain here,
the end result that you get You get that

00:09:19.630 --> 00:09:23.150
a grammar produces a language model.

00:09:23.150 --> 00:09:25.730
In the technical sense we
introduced this before hand when

00:09:25.730 --> 00:09:28.690
we were talking about language models.

00:09:28.690 --> 00:09:32.240
If you look at a language model
L generated by a grammar,

00:09:32.240 --> 00:09:35.570
here I have just expressed that, you know,

00:09:35.570 --> 00:09:40.840
consider all sequences of terminals and
then work out their probability here.

00:09:40.840 --> 00:09:45.780
If you sum those probabilities,
those probabilities will sum to one.

00:09:45.780 --> 00:09:48.770
So it's a language model in
the same sense as language models.

00:09:50.820 --> 00:09:53.430
Here's an example of a PCFG.

00:09:53.430 --> 00:09:56.800
So it's just like what we had before,

00:09:56.800 --> 00:10:00.450
except that now next to each role
we're giving it a probability.

00:10:00.450 --> 00:10:03.020
So there's only rewrite for

00:10:03.020 --> 00:10:07.085
s so it has a probability of 1 to
meet the condition we gave before.

00:10:07.085 --> 00:10:09.770
But to take a more interesting example.

00:10:09.770 --> 00:10:12.490
There are three re-writes for NP and

00:10:12.490 --> 00:10:17.750
if you sum these three re-writes
up they sum to one as required.

00:10:17.750 --> 00:10:23.350
Now this is just about the grammar that
we showed you before but I actually made

00:10:23.350 --> 00:10:29.410
one change to it here which is I deleted
the noun phrase goes to empty rule.

00:10:29.410 --> 00:10:33.820
And the reason I did that is this is
already a really an ambiguous grammar.

00:10:33.820 --> 00:10:37.020
And if I left it in the example
that I'm about to show you,

00:10:37.020 --> 00:10:41.690
would get way too ambiguous and complex as
you can try and work out for yourselves.

00:10:43.230 --> 00:10:45.410
So it's slightly simplified.

00:10:45.410 --> 00:10:48.040
Okay so using this PCFG.

00:10:48.040 --> 00:10:51.500
Let's go through an example of
working with probabilities.

00:10:51.500 --> 00:10:53.970
And so there are two things
that we want to look at.

00:10:53.970 --> 00:10:59.810
So first of all if I draw a tree,
we can work out the probability of a tree.

00:10:59.810 --> 00:11:01.470
And that's actually pretty easy.

00:11:01.470 --> 00:11:05.428
All we do is actually take the
probabilities in the grammar and lexicon

00:11:05.428 --> 00:11:09.803
and what that are used to generate the
tree and we multiply them all together.

00:11:09.803 --> 00:11:14.280
Then the slightly trickier thing that we
want to do is the language model question.

00:11:14.280 --> 00:11:19.190
We want to know the probability of a
string of words and well for that we have

00:11:19.190 --> 00:11:23.410
to consider all possible tree structures
that could have generated the string.

00:11:23.410 --> 00:11:29.460
And so the probability of
a sequence of words is the sum of

00:11:31.280 --> 00:11:37.610
all possible trees where the tree
is the part of a sentence.

00:11:37.610 --> 00:11:42.020
But since the tree includes
the sentence down the bottom.

00:11:42.020 --> 00:11:45.980
That's just the sum of all
trees with that condition.

00:11:45.980 --> 00:11:48.180
Let's look at a concrete example.

00:11:50.190 --> 00:11:53.810
So my sentence here is
people fish tanks with rods.

00:11:54.850 --> 00:11:58.682
And here is one pass the the grammar
generates for it and

00:11:58.682 --> 00:12:04.362
what I've done is write next to each
parent category, what is the probability?

00:12:04.362 --> 00:12:09.689
So this 0.4 says that the VP goes to V,
NP,

00:12:09.689 --> 00:12:15.460
PP role and probability 0.4 But
we get this.

00:12:15.460 --> 00:12:21.490
This is the pause where we get the
prepositional phrase modifying the verb.

00:12:21.490 --> 00:12:24.720
That as we've seen before,
we also get the other pause

00:12:24.720 --> 00:12:29.100
where we have the prepositional
phrase modifying the noun.

00:12:29.100 --> 00:12:31.740
And we can write out its pause as well.

00:12:31.740 --> 00:12:35.930
And the less ambiguous grammar without the
empty, these are the only two parsers for

00:12:35.930 --> 00:12:36.550
this sentence.

00:12:36.550 --> 00:12:40.538
And so doing that,
we can work out the probability for

00:12:40.538 --> 00:12:43.108
each tree, so the probability for

00:12:43.108 --> 00:12:48.531
each tree is just we multiply
the probabilities of each rule expansion.

00:12:48.531 --> 00:12:51.610
And we get,
as always a very little number here.

00:12:51.610 --> 00:12:56.980
And then, this is the probability for
the other tree during the noun attachment.

00:12:56.980 --> 00:13:01.088
So to work out the probability of
the sequence of words, the sentence,

00:13:01.088 --> 00:13:05.620
people fish tanks with rods, we simply
sum the probabilities of all pausers.

00:13:05.620 --> 00:13:10.620
And that gives us this probability
here as the language model score.

00:13:12.300 --> 00:13:17.290
And if we look at this, as well as getting
the language model score, we can also see

00:13:17.290 --> 00:13:22.725
which paths the PCFG would chose as
the most likely parse for the same tense.

00:13:22.725 --> 00:13:24.325
And the answer is, it's this one.

00:13:24.325 --> 00:13:27.115
It chooses the verb attachment,

00:13:27.115 --> 00:13:32.080
which seems like it is the most natural
reading of this sentence in English.

00:13:32.080 --> 00:13:35.750
It might be interesting that you
guys to look and just think for

00:13:35.750 --> 00:13:39.930
a moment more as to quiet
chooses that parse.

00:13:39.930 --> 00:13:43.350
And if you start looking at it,
what you'll see is that a lot of

00:13:43.350 --> 00:13:48.200
the probabilities in the tree
are exactly the same for both parses.

00:13:48.200 --> 00:13:52.630
There's a really only one area of
difference wherein the VP attach,

00:13:52.630 --> 00:13:54.830
we use this rewrite rule here.

00:13:54.830 --> 00:14:00.170
Well, maybe I should draw it sort of
bigger to include that little sub-tree.

00:14:00.170 --> 00:14:05.105
Whereas in the NP attach, we have

00:14:06.725 --> 00:14:10.205
this VP rewrite rule, and

00:14:10.205 --> 00:14:15.112
then we have an extra NP rewrite rule,
so this bit is different.

00:14:15.112 --> 00:14:20.709
And so then if you look at those
that what we do is we found

00:14:20.709 --> 00:14:26.064
that here we had a 0.4 that
was unique to this run,

00:14:26.064 --> 00:14:30.838
and here we had,
there was sort of this piece.

00:14:30.838 --> 00:14:36.684
A 0.6 times 0.2 which
was unique to this one.

00:14:36.684 --> 00:14:40.590
So a 0.4 versus 0.12.

00:14:40.590 --> 00:14:46.470
And so if you compare those,
end result is that the verb attaches 3.33,

00:14:46.470 --> 00:14:51.530
3 and a third times more
likely than the NP attach,

00:14:51.530 --> 00:14:54.670
purely accounted for by that difference.

00:14:54.670 --> 00:14:56.637
That could even make you
a little bit suspicious of PCFG.

00:14:56.637 --> 00:14:59.252
It's something we'll come back to later,

00:14:59.252 --> 00:15:03.350
because it sort of looks like this
one's getting lower probability.

00:15:03.350 --> 00:15:07.130
Because there's more depth to the tree,
more rewrites being done.

00:15:07.130 --> 00:15:11.280
And PCFGs tend to have some
slightly odd effects like that.

00:15:11.280 --> 00:15:14.690
But we'll stop that exploration there for
now and

00:15:14.690 --> 00:15:18.460
we'll just content ourselves
by saying the PCFG.

00:15:18.460 --> 00:15:21.900
Is choosing the right paths for
the sentence in this case and

00:15:21.900 --> 00:15:25.110
feeling that hopefully
now you guys understand

00:15:25.110 --> 00:15:28.420
both what context free grammar is and
its probabilistic state extension.

