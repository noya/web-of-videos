WEBVTT
Kind: captions
Language: en

00:00:00.630 --> 00:00:04.830
We've now looked in detail how to
make maximum entropy classifiers.

00:00:04.830 --> 00:00:07.980
And what I want to do in this
segment is extend from there

00:00:07.980 --> 00:00:11.640
to show how we can build sequence
models out of classifiers.

00:00:11.640 --> 00:00:16.090
In particular, we are going to make
maximum entropy sequence models.

00:00:16.090 --> 00:00:20.870
Many problems in NLP have data
which is a sequence of something.

00:00:20.870 --> 00:00:25.780
It might be a sequence of characters,
a sequence of words, sequence of phrases,

00:00:25.780 --> 00:00:29.280
sequence of lines, paragraphs, sentences.

00:00:29.280 --> 00:00:31.510
And we can think of the task,

00:00:31.510 --> 00:00:36.100
perhaps with a little coding,
as one of labeling each item.

00:00:36.100 --> 00:00:39.433
So, a very straight forward example is
part of speech tagging that we just

00:00:39.433 --> 00:00:40.300
looked at.

00:00:40.300 --> 00:00:42.372
So here, we have the words and for

00:00:42.372 --> 00:00:46.600
each word we're going to label it
with a particular part of speech.

00:00:47.650 --> 00:00:50.450
Other examples are fairly
straightforward as well.

00:00:50.450 --> 00:00:54.129
So, for named entity recognition,
here we have the word,

00:00:54.129 --> 00:00:58.986
and we're going to label it with an entity
if it is one, like organization, or

00:00:58.986 --> 00:01:03.330
if it's not an entity,
we're going to label it with an O.

00:01:03.330 --> 00:01:08.020
Some other cases of things that
can be done as sequence problems,

00:01:08.020 --> 00:01:10.975
it's a little bit subtle to
think about the encoding.

00:01:10.975 --> 00:01:15.430
But, is actually quite straight forward
once you've worked through the ideas.

00:01:15.430 --> 00:01:18.200
So, here we have a piece of Chinese text,
and

00:01:18.200 --> 00:01:20.840
what we want to do is word segmentation.

00:01:20.840 --> 00:01:21.920
So this is a word.

00:01:21.920 --> 00:01:23.580
This is a second word.

00:01:23.580 --> 00:01:24.670
This is a third word.

00:01:24.670 --> 00:01:26.810
This is a fourth word.

00:01:26.810 --> 00:01:30.680
This is a single character word,
and so on.

00:01:30.680 --> 00:01:35.900
And what we can do is,
we can use a labeling to capture that.

00:01:35.900 --> 00:01:41.290
So here we have what's called BI labeling,

00:01:41.290 --> 00:01:45.140
where we distinguishing just two states,
begin, and inside.

00:01:45.140 --> 00:01:49.330
And that's sufficient to represent
tasks like word segmentation.

00:01:49.330 --> 00:01:53.040
So, this says,
this is the beginning of a new word.

00:01:53.040 --> 00:01:57.420
But then, the next token says it's
also the beginning of a new word.

00:01:57.420 --> 00:02:01.980
So we start a word here, but
then we've got these I class so

00:02:01.980 --> 00:02:06.430
we continue, another I class, we continue,
so that's a three character word.

00:02:06.430 --> 00:02:12.260
And then we go back to a B and we start
a new word, which is a two character word.

00:02:12.260 --> 00:02:17.420
So, although, really we're segmenting
the characters into sub-sequences,

00:02:17.420 --> 00:02:20.770
we can encode the decisions
we have to make by regarding

00:02:20.770 --> 00:02:23.350
the problem as a sequence labeling task.

00:02:23.350 --> 00:02:26.190
Here's one more slightly
different example.

00:02:26.190 --> 00:02:31.300
So what we have here is a stretch of text,
and we can think of the text as

00:02:31.300 --> 00:02:36.290
lines of a old fashioned file with
hard line breaks or sentences.

00:02:36.290 --> 00:02:40.360
And so what this is, is an FAQ,
as you find commonly on websites.

00:02:40.360 --> 00:02:43.700
And what we want to do is to
automatically process and

00:02:43.700 --> 00:02:47.480
work out where are the questions and
where are the answers.

00:02:47.480 --> 00:02:50.650
And so we're going to regard that
as a sequence labeling task,

00:02:50.650 --> 00:02:54.190
where each of our items is
representing a line or a sentence.

00:02:54.190 --> 00:02:56.650
And so then, we're encoding our decisions

00:02:56.650 --> 00:03:01.480
using exactly the same kind of
two-class classification over here.

00:03:01.480 --> 00:03:06.520
So, each line is being classified as
either a question line or an answer line.

00:03:06.520 --> 00:03:11.285
And then, we have the answer lines
grouped together for a particular answer.

00:03:11.285 --> 00:03:14.985
So what we're going to do with
our maximum entropy models is,

00:03:14.985 --> 00:03:17.375
we're going to put them
into a sequence model.

00:03:17.375 --> 00:03:22.095
And these are usually referred to as
Maximum Entropy Markov Models, MEMMs, or

00:03:22.095 --> 00:03:24.355
Conditional Markov Models.

00:03:24.355 --> 00:03:29.920
So, what the classifier does is it makes
a single decision at a time, but it's able

00:03:29.920 --> 00:03:35.579
to be conditioned on evidence both from
observations and from previous decisions.

00:03:35.579 --> 00:03:40.436
So here, we're showing a picture where in
the middle of doing part of speech taking,

00:03:40.436 --> 00:03:45.010
and we've already given part of
speech tags the first three words.

00:03:45.010 --> 00:03:47.500
And so we're proceeding left to right, and

00:03:47.500 --> 00:03:52.010
then what we're up to is giving a part
of speech tag to this word here.

00:03:52.010 --> 00:03:55.230
And so the idea is, for
features for classification,

00:03:55.230 --> 00:03:58.730
we're going to be able to use
features of the current word,

00:03:58.730 --> 00:04:02.656
we're going to be able to use
features of other words, if we wish.

00:04:02.656 --> 00:04:07.076
But, we're also are going to be allowed
to use features of the previously

00:04:07.076 --> 00:04:08.925
assigned part of speech tag.

00:04:08.925 --> 00:04:13.300
Perhaps the part of
speech tag to backwards.

00:04:13.300 --> 00:04:17.250
And all of these can
influence the classification.

00:04:17.250 --> 00:04:20.830
And so that's what's being shown
over here for our features.

00:04:20.830 --> 00:04:25.210
So we have current word,
next word, previous word,

00:04:25.210 --> 00:04:30.320
previous tag, previous two tags
taken jointly is a feature.

00:04:30.320 --> 00:04:34.540
And then we can also define other features
of the kind that we've discussed before.

00:04:34.540 --> 00:04:37.911
So here we have,
a hasDigit feature of the current word,

00:04:37.911 --> 00:04:42.547
which is being used to generalize over
different numbers, because maybe we don't

00:04:42.547 --> 00:04:46.788
know very much or anything about
the number, 22.6 in particular.

00:04:46.788 --> 00:04:51.120
So generalizing a little, this is
the picture of how we move from our

00:04:51.120 --> 00:04:54.720
basic maximum classifier
to a sequence model.

00:04:54.720 --> 00:04:59.280
So overall,
we have Sequence Data here, and

00:04:59.280 --> 00:05:02.720
what we want to do is classification
at the sequence level.

00:05:02.720 --> 00:05:06.040
So, we have individual words or
characters and

00:05:06.040 --> 00:05:09.470
we want to assign to them their classes.

00:05:09.470 --> 00:05:13.190
But the way we're going to do that
is we're going to look at each

00:05:13.190 --> 00:05:14.960
decision individually.

00:05:14.960 --> 00:05:19.230
So, we're going to say, okay, there's
a particular classification of interest,

00:05:19.230 --> 00:05:20.095
this one here.

00:05:20.095 --> 00:05:23.016
And how are we going to
make that classification?

00:05:23.016 --> 00:05:24.980
Well, what we're going to do is say,

00:05:24.980 --> 00:05:30.190
for that particular classification,
there's data locally of interest to it.

00:05:30.190 --> 00:05:35.035
So there's the current word,
the previous word, the previous class.

00:05:35.035 --> 00:05:36.765
And so, what we're going to do,

00:05:36.765 --> 00:05:40.365
is we're going to feature
extraction over that data.

00:05:40.365 --> 00:05:43.415
And so, we have the label that
we're trying to predict, or

00:05:43.415 --> 00:05:46.665
which we can see in
supervised training data.

00:05:46.665 --> 00:05:51.325
We've got features of the observed
data in previous classifications.

00:05:51.325 --> 00:05:54.770
And then, we're building
a Maximum Entropy Model over that.

00:05:54.770 --> 00:05:58.570
And so, at that point, we're doing all
the stuff that we've talked about.

00:05:58.570 --> 00:06:02.160
We're doing optimization of a model,
we're doing smoothing.

00:06:02.160 --> 00:06:03.120
And, at the end of the day,

00:06:03.120 --> 00:06:07.880
we build a little local classifier
that makes the individual decisions.

00:06:07.880 --> 00:06:11.710
So, what we'll do, is we'll do it this
position, but then, we'll repeat the same

00:06:11.710 --> 00:06:16.060
thing over at the next position, and
we'll go along taking our sequence.

00:06:17.200 --> 00:06:22.320
It's extremely easy to see how to do
that in one way, where we first fold

00:06:22.320 --> 00:06:28.180
aside this label, then we go on to decide
this label, and this label, this label,

00:06:28.180 --> 00:06:32.970
and at each point we can use preceding
labels to help determine the next one.

00:06:32.970 --> 00:06:36.220
And if we do that, we have a greedy

00:06:36.220 --> 00:06:41.110
sequence modeler that's just deciding
one sequence at a time and moving on.

00:06:41.110 --> 00:06:44.240
That, in many applications,
actually works quite well.

00:06:44.240 --> 00:06:48.750
But commonly, people want to explore
the search space a little bit better.

00:06:48.750 --> 00:06:54.580
Because sometimes, although here,
you might decide one part of speech tag

00:06:54.580 --> 00:06:59.650
is best, then later on once you've looked
over here, that would give some reason for

00:06:59.650 --> 00:07:04.270
you to think that maybe you should have
chosen differently back over here.

00:07:04.270 --> 00:07:07.810
And so, there are a couple of methods
that are commonly used to do that.

00:07:07.810 --> 00:07:10.080
One method is Beam Inference.

00:07:10.080 --> 00:07:15.000
So a Beam Inference at each position,
rather than just deciding the most

00:07:15.000 --> 00:07:19.680
likely label,
we can keep several possibilities.

00:07:19.680 --> 00:07:24.690
So, we might keep the top
k complete sub-sequences

00:07:24.690 --> 00:07:27.200
up to the point where we're at to so far.

00:07:27.200 --> 00:07:31.320
And so then, at each stage we'll
consider each partial sub-sequence and

00:07:31.320 --> 00:07:33.320
extend that to one further position.

00:07:34.680 --> 00:07:39.190
Now, Beam Inference is also very fast and
easy to implement.

00:07:39.190 --> 00:07:42.420
And it turns out that a lot of
the time that beam sizes of three or

00:07:42.420 --> 00:07:47.330
five give you enough maintenance with
few possibilities, and works almost as

00:07:47.330 --> 00:07:52.800
well as doing exact inference to find
the best possible state sequence.

00:07:52.800 --> 00:07:54.570
And it's easy to implement.

00:07:54.570 --> 00:07:57.010
Of course, that's not necessarily true,

00:07:57.010 --> 00:08:01.530
you get no guarantees that you found the
globally best part of speech tag sequence,

00:08:01.530 --> 00:08:03.510
or whatever your sequence problem is.

00:08:03.510 --> 00:08:08.430
And it can certainly be the case that
possibilities that would later have been

00:08:08.430 --> 00:08:13.380
shown to be good, might fall of the beam
before they get to be shown to be good.

00:08:13.380 --> 00:08:14.940
We can do better than that.

00:08:14.940 --> 00:08:18.570
We can actually find
the best sequence of states

00:08:18.570 --> 00:08:21.350
that has the globally
highest score in the model.

00:08:21.350 --> 00:08:25.990
And doing that is referred to in
NLP as doing Viterbi Inference,

00:08:25.990 --> 00:08:30.890
since Andrew Viterbi invented
a lot of these algorithms finding

00:08:30.890 --> 00:08:32.290
the best way of doing things.

00:08:32.290 --> 00:08:35.430
Viterbi inference is a form
of dynamic programming, or

00:08:35.430 --> 00:08:38.260
you can also think of it as memorization.

00:08:38.260 --> 00:08:40.890
And you can do this kind
of dynamic programming,

00:08:40.890 --> 00:08:44.990
providing you have a small
window of state influence.

00:08:44.990 --> 00:08:49.760
So for example,
when you're trying to decide this label,

00:08:49.760 --> 00:08:54.340
if it depends on words
however it wants to,

00:08:56.150 --> 00:09:03.010
but only depends on, say, the previous
label and the one before that.

00:09:03.010 --> 00:09:07.940
Then, you end up with this sort of fixed
small window of stuff you need to know

00:09:07.940 --> 00:09:13.420
about to make decisions, and nothing back
here is affecting what decision you make.

00:09:13.420 --> 00:09:16.690
Providing that's true,
you can write dynamic programming

00:09:16.690 --> 00:09:20.480
algorithms to find
the optimal states sequence.

00:09:20.480 --> 00:09:23.490
And so, that has the obvious advantage
that you're guaranteed to find

00:09:23.490 --> 00:09:27.338
the best states sequence, but
it has some disadvantages.

00:09:27.338 --> 00:09:34.088
It requires a bit more work to implement,
and it forces you to restrict your use of

00:09:34.088 --> 00:09:39.764
previous labels in your inference
process to a fixed small window.

00:09:42.343 --> 00:09:48.158
That is a restriction, but a lot of
the time it's not such a bad Restriction,

00:09:48.158 --> 00:09:53.260
because in practice, it's hard to
get long distance interactions to be

00:09:53.260 --> 00:09:58.363
working effectively anyway,
even in something like a bean model,

00:09:58.363 --> 00:10:01.995
which does allow long
distance interactions.

00:10:01.995 --> 00:10:05.435
Okay, I've introduced
Maximum Entropy Markov Models.

00:10:05.435 --> 00:10:08.346
And I hope that you now feel
that you understand them and

00:10:08.346 --> 00:10:11.045
will be able to build
them as an assignment.

00:10:11.045 --> 00:10:15.735
Before I end, I should just very briefly
mention Conditional Random Fields.

00:10:15.735 --> 00:10:20.215
So, Conditional Random Fields are another
probabilistic sequence model.

00:10:20.215 --> 00:10:25.945
And if you just, sort of, look big picture
in the math of a Conditional Random Field,

00:10:25.945 --> 00:10:30.562
boy, that equation looks exactly
like the equation that we've been

00:10:30.562 --> 00:10:33.590
starring at in all of the recent slides.

00:10:33.590 --> 00:10:38.393
The difference with Conditional Random
Fields is that these probabilities

00:10:38.393 --> 00:10:41.100
are in terms of the entire sequences.

00:10:41.100 --> 00:10:47.690
So, this is the sequence of classes and
the sequence of observed data values,

00:10:47.690 --> 00:10:52.310
that it's not in terms of
particular points in that space.

00:10:52.310 --> 00:10:55.300
And so, we get this whole
sequence conditional model

00:10:55.300 --> 00:10:57.920
rather than the chaining of local models.

00:10:57.920 --> 00:11:01.940
That looks as if it would be
very difficult to deal with,

00:11:01.940 --> 00:11:07.310
because the space of a sequence of
c's is exponential in its length.

00:11:07.310 --> 00:11:12.500
And the space of a sequence of data
items represented as features,

00:11:12.500 --> 00:11:16.310
is at a minimum huge, and
perhaps, even infinite.

00:11:16.310 --> 00:11:22.300
But it turns out that, providing the fi
features remain local, permitting dynamic

00:11:22.300 --> 00:11:28.270
programming, that the conditional sequence
likelihood can be calculated exactly.

00:11:28.270 --> 00:11:35.110
Training is somewhat slower, but CRFs
have theoretical advantages of avoiding

00:11:35.110 --> 00:11:40.820
certain causal-competition biases that can
occur with Maximum Entropy Markov Models.

00:11:40.820 --> 00:11:45.400
However, to explain the details
of these models, we'd have to go

00:11:45.400 --> 00:11:50.530
quite a way afield, looking at in general,
how to do Markov random field inference.

00:11:50.530 --> 00:11:54.060
Which is, I feel,
is a better topic for other courses.

00:11:54.060 --> 00:11:58.806
So, let me just mention these, and
say that these days, using CRFs or

00:11:58.806 --> 00:12:03.231
variants of them using a max margin
criterion coming out of SVMs,

00:12:03.231 --> 00:12:08.157
are seen as the state-of-the-art
method for doing sequence models.

00:12:08.157 --> 00:12:12.298
And there are various bits of software,
including the Stanford software for

00:12:12.298 --> 00:12:16.330
named-entity recognition,
you can download that implements CRFs.

00:12:16.330 --> 00:12:21.830
But a thing to know is, that while
CRFs are theoretically cleaner, and

00:12:21.830 --> 00:12:27.040
can avoid some problems of MEMMs,
that in practice,

00:12:27.040 --> 00:12:33.010
when you're building models with rich
features which condition on observed data,

00:12:33.010 --> 00:12:38.030
both before and after you, that in
practice they tend to have performance

00:12:38.030 --> 00:12:41.700
that can't really be disguised from
Maximum Entropy Markov Models.

00:12:41.700 --> 00:12:45.919
And so, there's really no problem with
using Maximum Entropy Markov Models to do

00:12:45.919 --> 00:12:51.310
the job of sequence classification, and
that's what we'll use in the assignment.

00:12:51.310 --> 00:12:55.620
Okay, and so, I hope that you
guys now have a concrete idea

00:12:55.620 --> 00:12:58.450
of how you can build
a MaxEnt Classifier and

00:12:58.450 --> 00:13:01.910
then incorporate it into a system for
doing sequence inference.

