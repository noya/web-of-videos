WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.734
[MUSIC]

00:00:04.734 --> 00:00:07.260
Stanford University.

00:00:07.260 --> 00:00:10.730
&gt;&gt; I am very excited to
introduce to you Navdip.

00:00:10.730 --> 00:00:17.920
Navdip did his PhD At Toronto with Geoff
Hinton the godfather of deep learning.

00:00:17.920 --> 00:00:22.404
He's done some really exciting work
on end-to-end speech recognition.

00:00:22.404 --> 00:00:26.655
Really his name is on most of the exciting
breakthrough papers of the last couple of

00:00:26.655 --> 00:00:28.310
years when it comes to speech.

00:00:28.310 --> 00:00:29.982
So very excited to have him here.

00:00:29.982 --> 00:00:34.600
He's now at Nvidia and I'm guessing,
continuing to work on speech recognition.

00:00:46.634 --> 00:00:51.189
Okay, Is that working?

00:00:51.189 --> 00:00:54.190
Okay, hi everyone.

00:00:54.190 --> 00:00:58.957
So, today I thought I'd give
a high-level overview of methods

00:00:58.957 --> 00:01:03.561
that we're looking at for
end-to-end speech processing.

00:01:03.561 --> 00:01:06.000
So here's the plan for the lecture today.

00:01:06.000 --> 00:01:10.760
I'll start by taking a brief look at
traditional speech recognition systems.

00:01:10.760 --> 00:01:13.301
Then I'll give a little motivation and

00:01:13.301 --> 00:01:16.846
a description of what I mean
by an end-to-end model.

00:01:16.846 --> 00:01:21.466
Then I'll talk about two different models
for an end-to-end speech recognition

00:01:21.466 --> 00:01:25.860
systems, one by the name of
Connectionist Temporal Classification.

00:01:25.860 --> 00:01:28.130
And another recent one based on Listen,
Attend and

00:01:28.130 --> 00:01:31.210
Spell which is
a sequence-to-sequence model,

00:01:31.210 --> 00:01:34.275
something I believe you guys
are familiar with it at this point.

00:01:34.275 --> 00:01:38.660
Then I'll talk about some of the work
we've been doing on making improved

00:01:38.660 --> 00:01:43.620
versions of these end-to-end models,
and end with talking a little

00:01:43.620 --> 00:01:48.240
bit about how language models
influence speech recognition, and

00:01:48.240 --> 00:01:52.310
some efforts at improving decoding which
is an important part of these models.

00:01:54.176 --> 00:01:57.409
Okay, so starting with
the basic definition of what is

00:01:57.409 --> 00:02:00.782
Automatic Speech Recognition,
in the era of OK Google,

00:02:00.782 --> 00:02:04.459
I guess I don't really need to
describe it, but here it goes.

00:02:04.459 --> 00:02:09.780
You have a person or
an audio source saying something textual.

00:02:09.780 --> 00:02:14.480
And you have a bunch of microphones
which are receiving the audio signals.

00:02:14.480 --> 00:02:19.720
And what is received the microphone of
course depends on how it is oriented

00:02:19.720 --> 00:02:21.680
with respect to the person.

00:02:21.680 --> 00:02:25.950
And you get these signals from
the devices one or many devices.

00:02:25.950 --> 00:02:29.075
And you pass it to
an Automatic Speech Recognition system,

00:02:29.075 --> 00:02:33.990
whose job is now to infer
the original source

00:02:33.990 --> 00:02:38.190
transcript that the person spoke or
that the device played.

00:02:40.209 --> 00:02:41.874
So why is ASR so important?

00:02:42.975 --> 00:02:47.126
Well firstly it's a very
natural interface for

00:02:47.126 --> 00:02:52.521
human communication,
you don't need a mouse or a keyboard,

00:02:52.521 --> 00:02:57.519
so it's obviously a good way
to interact with machines.

00:02:57.519 --> 00:03:02.261
You don't even really need to learn
a new techniques because we most people

00:03:02.261 --> 00:03:07.076
have learned how to speak by certain time,
and of course it's a very natural

00:03:07.076 --> 00:03:12.320
interface for talking with simple
devices such as cars or handout phones.

00:03:12.320 --> 00:03:17.920
Or even more complicated and
intelligent devices such as your

00:03:17.920 --> 00:03:23.680
call center people or chatbots, and
eventually our robotic overlords.

00:03:23.680 --> 00:03:27.386
So I think we need a good
speech recognition system.

00:03:27.386 --> 00:03:31.934
Okay so how is this done classically?

00:03:31.934 --> 00:03:35.571
I'll be focusing mostly on
stuff we've been doing lately,

00:03:35.571 --> 00:03:40.206
which are all neural inspired models, but
I thought it would be a nice start by

00:03:40.206 --> 00:03:43.988
talking about how these things
have been built classically and

00:03:43.988 --> 00:03:48.080
see how they've been replaced with
single neural net over the time.

00:03:48.080 --> 00:03:49.757
Okay, so the classic model for

00:03:49.757 --> 00:03:53.566
speech recognition is to build
something called a generative model,

00:03:53.566 --> 00:03:57.520
I don't know how many people are familiar
with generative models here?

00:03:58.540 --> 00:04:03.490
Perfect, so the classic way of building
a Speech Recognition System is just to

00:04:03.490 --> 00:04:05.340
build a generative model.

00:04:05.340 --> 00:04:08.150
You build the generative
model of language.

00:04:08.150 --> 00:04:15.474
On the left you say, you produce a certain
sequence of words from language models.

00:04:15.474 --> 00:04:19.974
And then for each word you have
a pronunciation model which says, hey,

00:04:19.974 --> 00:04:23.420
this is how this
particular word is spoken.

00:04:23.420 --> 00:04:28.686
Typically it's written out as the sequence
of phonemes which are basic units

00:04:28.686 --> 00:04:33.711
of sound, but for our vocabulary we'll
just say it's a sequence of tokens

00:04:33.711 --> 00:04:39.400
where tokens have been a cluster of things
that have defined by a linguistic expert.

00:04:39.400 --> 00:04:43.103
So you have the pronunciation models which
now convert the sequence of text into

00:04:43.103 --> 00:04:45.450
a sequence of pronunciation tokens.

00:04:45.450 --> 00:04:48.850
And then the models feed
into an acoustic model

00:04:48.850 --> 00:04:51.940
which basically says how does
a given token sound like?

00:04:53.050 --> 00:04:56.990
And typically these are built
using a Gaussian mixture model or

00:04:56.990 --> 00:05:01.220
they were in the past, and these Gaussian
mixture models with a very specific

00:05:01.220 --> 00:05:03.140
sort of architectures
associated with them.

00:05:03.140 --> 00:05:07.927
You'd have three state left to right
Gaussian mixture models that would

00:05:07.927 --> 00:05:09.505
output frames of data.

00:05:09.505 --> 00:05:13.110
And these models were now used
to describe the data themselves.

00:05:13.110 --> 00:05:18.387
So here the data would be x,
which is the sequence

00:05:18.387 --> 00:05:22.520
of frames of audio features x1 to xT.

00:05:22.520 --> 00:05:27.265
Typically these features are something
that signal processing experts

00:05:27.265 --> 00:05:31.931
have defined things like features
that look at frequency components of

00:05:31.931 --> 00:05:35.100
the audio wave forms that are captured.

00:05:35.100 --> 00:05:39.986
Things called spectrograms and
bell filter banks spectrograms

00:05:39.986 --> 00:05:43.976
that are sort of very
similar to human beings.

00:05:43.976 --> 00:05:47.750
So the classical pipeline
proceeds as described.

00:05:49.440 --> 00:05:52.440
Now each of these different components
in this pipeline uses a different

00:05:52.440 --> 00:05:53.840
statistical model.

00:05:53.840 --> 00:05:58.600
In the past, language models
were typically N-gram models,

00:05:58.600 --> 00:06:00.390
they served very well.

00:06:00.390 --> 00:06:02.860
So here obviously in this class

00:06:02.860 --> 00:06:06.200
I don't really need to define
language models for you,

00:06:06.200 --> 00:06:10.680
you have essentially tables describing
probabilities of sequences of tokens.

00:06:10.680 --> 00:06:14.970
The pronunciation models were just
simple lookup tables with probabilities

00:06:14.970 --> 00:06:19.290
associated with pronunciations
most words will have at least

00:06:19.290 --> 00:06:23.260
a couple of pronunciations and
if you have an accent in exchange further.

00:06:23.260 --> 00:06:25.790
So these pronunciation tables would

00:06:25.790 --> 00:06:29.540
just be very large tables of
different pronunciations.

00:06:29.540 --> 00:06:34.800
Acoustic models as I said would
be Gaussian Mixture Models and

00:06:34.800 --> 00:06:37.130
the speech processing was predefined.

00:06:38.860 --> 00:06:41.010
So the way,
once you have this model built,

00:06:41.010 --> 00:06:46.250
you can do recognition by just doing
inference on the data you receive.

00:06:46.250 --> 00:06:49.830
So you get some waveform,
you compute the features for it and

00:06:49.830 --> 00:06:53.270
you get x, you look into your model.

00:06:53.270 --> 00:06:58.209
And then using some fancy search
procedure, you figure out, okay,

00:06:58.209 --> 00:07:02.895
what's the sequence of y's that
would give rise to this sequence

00:07:02.895 --> 00:07:05.464
of x with the highest probability?

00:07:05.464 --> 00:07:08.682
So in a nutshell that's basically all,

00:07:08.682 --> 00:07:13.227
the way classical speech
recognition systems happen and

00:07:13.227 --> 00:07:17.690
all the magic is in how these
little pieces are refined.

00:07:20.844 --> 00:07:23.100
Okay, so now welcome to
the neural network invasion.

00:07:24.270 --> 00:07:27.976
Over time people started noticing
that each of these components could

00:07:27.976 --> 00:07:30.247
be done better if we
used a neural network.

00:07:30.247 --> 00:07:35.629
So, if you take an N-gram language model
and you built a neural language model and

00:07:35.629 --> 00:07:40.855
feed that into a speech recognition system
to restore things that were produced

00:07:40.855 --> 00:07:45.930
by a first path speech recognition system,
then results are improved a lot.

00:07:47.130 --> 00:07:51.222
Also they looked into pronunciation
models and figured out, hey, how do

00:07:51.222 --> 00:07:55.842
we do pronunciation for new sequence of
characters that we've never seen before?

00:07:55.842 --> 00:08:00.072
So most pronunciation tables will not
cover everything that you could hear

00:08:00.072 --> 00:08:02.500
They found that they could
use a neural network.

00:08:02.500 --> 00:08:06.290
And then learn to predict token
sequences from character sequences, and

00:08:06.290 --> 00:08:07.726
that improve pronunciation models.

00:08:07.726 --> 00:08:12.808
So very, More

00:08:12.808 --> 00:08:17.959
production related speech recognition
systems such as the Google one,

00:08:17.959 --> 00:08:21.615
will build pronunciation
models just from RNNs.

00:08:22.680 --> 00:08:24.740
Acoustic models, also the same story.

00:08:25.980 --> 00:08:29.737
People used to use Gaussian
mixture models, and

00:08:29.737 --> 00:08:34.865
they found that if they could use DNN,
or Deep Neural Networks, or

00:08:34.865 --> 00:08:40.455
LSTM-based models, then you could
actually get much better scores for

00:08:40.455 --> 00:08:42.859
weather frames for real or wrong.

00:08:42.859 --> 00:08:47.911
Interestingly enough,
even the speech processing that was

00:08:47.911 --> 00:08:55.050
built with analytical thought process
in mind about the production of speech.

00:08:55.050 --> 00:08:58.560
Even those who are found to be
replaceable with convolutional

00:08:58.560 --> 00:09:00.690
neural networks on raw speech signals.

00:09:00.690 --> 00:09:02.780
So each of the pieces over time,

00:09:02.780 --> 00:09:04.830
people have found that neural
networks just do better.

00:09:07.770 --> 00:09:09.280
However, there's still a problem.

00:09:09.280 --> 00:09:12.611
There's neural networks
in every component, but

00:09:12.611 --> 00:09:17.422
the errors in each one are different,
so they may not play well together.

00:09:17.422 --> 00:09:21.316
So that's the basic motivation for

00:09:21.316 --> 00:09:25.597
trying to go to a process where you train

00:09:25.597 --> 00:09:30.157
the entire model as one big model itself.

00:09:30.157 --> 00:09:33.322
And so the stuff I'll be talking about
from here is basically an attempt or

00:09:33.322 --> 00:09:35.120
different attempts to do the same thing.

00:09:35.120 --> 00:09:39.400
And we call these end-to-end models
because they try and encompass more and

00:09:39.400 --> 00:09:43.070
more of the pipeline
that I described before.

00:09:43.070 --> 00:09:46.870
And the first of these models is called
Connectionist Temporal Classification, and

00:09:46.870 --> 00:09:53.350
is in wide use these days in Baidu and
even at Google, the production systems.

00:09:53.350 --> 00:09:58.545
However, it requires a lot of training.

00:09:58.545 --> 00:10:01.785
And recently,
the trend in the area has been to try and

00:10:01.785 --> 00:10:06.184
build an end-to-end model that does
not require hand customization.

00:10:06.184 --> 00:10:09.306
And sequence-to-sequence models
are very useful for that and

00:10:09.306 --> 00:10:12.672
help talk about Listen Attend and
Spell, which is one of these models

00:10:15.029 --> 00:10:19.380
Okay, so the basic motivation is we want
to do end-to-end speech recognition.

00:10:19.380 --> 00:10:21.240
We're given some audio x.

00:10:21.240 --> 00:10:23.340
It's a sequence of frames x1 to xt.

00:10:24.630 --> 00:10:30.421
And we're also given during training
the corresponding output text y, y1 to yL.

00:10:30.421 --> 00:10:34.426
And each of these y's is
one of whatever 27, 28,

00:10:34.426 --> 00:10:39.200
some number of tokens,
letters, a, b, c, d, e, f.

00:10:39.200 --> 00:10:43.250
Not sounds, we're trying to going straight
towards a model that goes audio straight

00:10:43.250 --> 00:10:47.470
to text, so we didn't wanna use
any pre-defined notions of what it

00:10:47.470 --> 00:10:49.150
means to be a different phoneme.

00:10:49.150 --> 00:10:53.770
Instead these models will start with x and
they have a goal to try and model y.

00:10:53.770 --> 00:10:58.690
So y is just the transcript, and
x is the audio possibly processed

00:10:58.690 --> 00:11:03.560
with some very minimal amount
of frequency based processing.

00:11:03.560 --> 00:11:06.600
So now, what we want to do is
perform speech recognition

00:11:06.600 --> 00:11:11.090
by just learning a very,
very powerful model, P of Y given X.

00:11:11.090 --> 00:11:15.170
So the first model that
describe the classical way of

00:11:15.170 --> 00:11:18.030
doing these things is the one at the top.

00:11:18.030 --> 00:11:22.385
You start with y, it's a language model,
we look into pronunciation models.

00:11:22.385 --> 00:11:25.230
And you look into acoustic models and
you get some scores.

00:11:25.230 --> 00:11:29.871
These models, for end-to-end, actually
just collapse them into one big model and

00:11:29.871 --> 00:11:33.460
reverse the flow of the arrows,
so they're discriminative.

00:11:33.460 --> 00:11:37.000
You start with data,
which is X and the features, and

00:11:37.000 --> 00:11:40.790
your goal is to directly predict
the target sequences, Y, themselves.

00:11:42.580 --> 00:11:47.250
Obviously, this requires a very
powerful probabilistic model

00:11:47.250 --> 00:11:50.580
because you're doing a very
difficult inversion task.

00:11:50.580 --> 00:11:54.267
And I'd say, the only reason this is
possible now is because we have these

00:11:54.267 --> 00:11:56.896
very strong probabilistic
models that can do that.

00:12:00.079 --> 00:12:04.950
Okay, so the first of these models is
Connectionist Temporal Classification.

00:12:04.950 --> 00:12:08.440
This is a probabilistic model p(Y|X),
where again,

00:12:08.440 --> 00:12:13.890
X is a sequence of frames of data,
X1, X2, Xt.

00:12:13.890 --> 00:12:18.757
Y itself is the output tokens of length l,
Y1 to YL.

00:12:18.757 --> 00:12:24.890
We require, because of the way the model's
constructed, that T be greater than L.

00:12:24.890 --> 00:12:29.150
And this model has a very specific
structure that makes it suited for speech,

00:12:29.150 --> 00:12:30.390
and I'll describe that in a second.

00:12:30.390 --> 00:12:36.148
So again, X is the spectrogram,
Y is the corresponding output transcript,

00:12:36.148 --> 00:12:39.170
in this case, this is the spectrogram.

00:12:39.170 --> 00:12:41.950
Okay, so
the way this model works is as follows.

00:12:41.950 --> 00:12:44.390
You get the spectrogram at the bottom, X.

00:12:44.390 --> 00:12:46.881
You feed it into
a recurrent neural network.

00:12:46.881 --> 00:12:50.641
You'll notice that their arrows
are pointed both directions.

00:12:50.641 --> 00:12:55.308
This is just my way of drawing
out a bidirectional RNN.

00:12:55.308 --> 00:12:58.080
I'm assuming everybody knows
what a bidirectional RNN is.

00:13:00.800 --> 00:13:03.973
Okay, so it's a bidirectional RNN.

00:13:03.973 --> 00:13:05.084
As a result,

00:13:05.084 --> 00:13:11.540
this arrow pointing at anytime step
depends on the entire input data.

00:13:11.540 --> 00:13:16.332
So it can compute a fairly complicated
function of the entire data X.

00:13:16.332 --> 00:13:20.428
Now, this model at the top has softmaxes
at every time frame corresponding to

00:13:20.428 --> 00:13:21.098
the input.

00:13:21.098 --> 00:13:24.240
And the softmax is on
a vocabulary which is the size of

00:13:24.240 --> 00:13:25.990
the vocabulary you're interested in.

00:13:25.990 --> 00:13:28.730
Say, in this case,
you had lowercase letters a to z and

00:13:28.730 --> 00:13:30.090
some punctuation symbols.

00:13:31.600 --> 00:13:36.390
So the vocabulary for connectionist,
for CTC, would be all that and

00:13:36.390 --> 00:13:38.090
an extra token called a blank token.

00:13:39.270 --> 00:13:42.789
And I'll get into the reason for
why the blank token exists in a second.

00:13:46.543 --> 00:13:49.830
I think I forgot to point out
one important thing here.

00:13:49.830 --> 00:13:54.820
Each frame of the prediction
here is basically

00:13:54.820 --> 00:14:00.540
producing a log probability for
a different token class at that time step.

00:14:00.540 --> 00:14:01.880
And we'll call that a score.

00:14:01.880 --> 00:14:07.874
In this case, a score s(k,t) is
the log the probability of category k,

00:14:07.874 --> 00:14:13.560
not the letter k, but a category k
at time step t given the data x.

00:14:13.560 --> 00:14:17.143
So you'd have, let’s say,
you took x4 here.

00:14:17.143 --> 00:14:21.283
The probability of, if you look at
the softmax the first, let’s say,

00:14:21.283 --> 00:14:25.012
the first index corresponds to
the probability of character a.

00:14:25.012 --> 00:14:29.876
The second symbol corresponds to
the probability of character b, c,

00:14:29.876 --> 00:14:30.870
and so forth.

00:14:30.870 --> 00:14:35.600
And the last symbol in this softmax will
correspond to the blank symbol itself.

00:14:35.600 --> 00:14:39.694
So when you look at
the softmax at any frame,

00:14:39.694 --> 00:14:44.020
you can get a probability
of the class itself.

00:14:44.020 --> 00:14:48.570
Okay, so what CTC does is if you look at
just the softmaxes that are produced by

00:14:48.570 --> 00:14:51.930
the recurring neural network
over the entire time step,

00:14:51.930 --> 00:14:54.870
you're interested in
finding the probability of

00:14:54.870 --> 00:14:58.660
the transcript through these
individual softmaxes over time.

00:14:58.660 --> 00:15:03.139
What it does is,
it says I can represent all these paths.

00:15:03.139 --> 00:15:06.530
I can take a path through
the entire space of softmaxes,

00:15:06.530 --> 00:15:10.800
and look at just the symbols that
correspond to each of the time steps.

00:15:10.800 --> 00:15:15.020
So if you take the third symbol,
that's a C in the first time step.

00:15:15.020 --> 00:15:20.440
If you take the first, the third symbol
again at the second time step C, and

00:15:20.440 --> 00:15:22.730
then you go through a blank symbol.

00:15:22.730 --> 00:15:26.570
It's essential that every symbol
go through a blank symbol,

00:15:26.570 --> 00:15:28.680
that's a constraint that the model has.

00:15:28.680 --> 00:15:32.729
And now, you go through a blank symbol,
and then you produce the next character,

00:15:32.729 --> 00:15:35.266
A, and then you produce A again for
another frame.

00:15:35.266 --> 00:15:38.686
And then you have produce a blank,
and you can transition to a T, and

00:15:38.686 --> 00:15:41.150
then you have to produce a blank again.

00:15:41.150 --> 00:15:46.000
So when you go through these paths
with the constraint that you can

00:15:46.000 --> 00:15:51.150
only transition between either the same
phoneme from one step to the next one.

00:15:51.150 --> 00:15:53.840
Or take it from not a phoneme,
but a label,

00:15:55.590 --> 00:15:59.720
either from the same label to itself,
or from that label to a blank symbol.

00:15:59.720 --> 00:16:02.810
You end up with different ways of
representing an output sequence.

00:16:02.810 --> 00:16:07.340
So here we had the output
sequence path being representing

00:16:07.340 --> 00:16:12.570
in these frames as cc
blank aa blank t blank.

00:16:12.570 --> 00:16:17.810
There's many other paths to also
correspond to the character sequence cat.

00:16:17.810 --> 00:16:22.970
So for example, if you wanted to produce
cat from the sequence of tokens here,

00:16:22.970 --> 00:16:27.980
you could also have produced it from
the way the second line here maps it out

00:16:27.980 --> 00:16:32.580
where you would say cc blank blank,
and then produce an a,

00:16:32.580 --> 00:16:36.240
then you produce a blank, and then you
produce a t, and then you produce a blank.

00:16:36.240 --> 00:16:39.842
So all this sounds complicated,
but really all it is, is saying,

00:16:39.842 --> 00:16:44.770
there's some paths you can take
through this sequence of softmax and

00:16:44.770 --> 00:16:48.110
it's got a certain constraint that you
have to follow, namely, you can only

00:16:48.110 --> 00:16:51.920
transition between yourself and the same
symbol again or yourself and a blank.

00:16:52.920 --> 00:16:57.680
Given these constraints, it turns out even
though there's an exponential number of

00:16:57.680 --> 00:17:02.770
paths by which you can produce the same
output symbol, you can actually do it

00:17:02.770 --> 00:17:05.950
correctly, because there exist
the dynamic programming algorithm.

00:17:07.500 --> 00:17:10.340
I'll spare you the details of that
dynamic programming algorithm today,

00:17:10.340 --> 00:17:15.400
but it's not as complicated as it sounds,
I'd refer you to the paper if you're

00:17:15.400 --> 00:17:18.720
interested in finding
out what that is about.

00:17:18.720 --> 00:17:23.820
Anyhow, the nice thing about this model is
you are able to sort of take in inputs and

00:17:23.820 --> 00:17:28.050
produce output tokens and learn this
mapping because the probabilities can be

00:17:28.050 --> 00:17:31.120
computed accurately and
not only can the probabilities for

00:17:31.120 --> 00:17:34.420
an output sequence be computed accurately,
you can get a gradient

00:17:34.420 --> 00:17:37.630
which is the learning signal that
you require to learn a model.

00:17:37.630 --> 00:17:40.885
And once you get the gradient from that
learning signal, you can back-propagate

00:17:40.885 --> 00:17:44.014
that into the recurrent neural network and
learn the parameters of the model.

00:17:46.846 --> 00:17:50.240
Feel free to ask any
questions at any time.

00:17:50.240 --> 00:17:51.405
I'm happy to answer questions.

00:17:59.419 --> 00:18:00.708
Sure, mm-hm?

00:18:13.514 --> 00:18:17.586
So the question is,
are we using processed raw audio, or

00:18:17.586 --> 00:18:23.000
can we can we actually do raw audio or
maybe even what do we do in practice?

00:18:24.800 --> 00:18:29.600
So we found some years ago that
we could actually use raw audio,

00:18:29.600 --> 00:18:34.760
however it didn't actually beat the best
way of processing the raw audio minimally,

00:18:34.760 --> 00:18:37.170
which would be just
computing a spectrogram and

00:18:37.170 --> 00:18:41.460
then adding the sort of bias that
human hearing apparatus has.

00:18:41.460 --> 00:18:44.270
It turns out the human hearing
apparatus doesn't have a linear

00:18:44.270 --> 00:18:45.980
resolution on frequencies.

00:18:45.980 --> 00:18:50.434
We're able to separate frequencies that
are fairly close by in lower ranges and

00:18:50.434 --> 00:18:52.229
then it becomes logarithmic and

00:18:52.229 --> 00:18:56.096
you have to be really far apart in
frequency space to tell them apart.

00:18:56.096 --> 00:18:57.548
So if you impose that bias,

00:18:57.548 --> 00:19:01.840
you get a log mel spectrogram instead
of just a linear spectrogram.

00:19:01.840 --> 00:19:06.250
People have tried to improve
upon the log mel spectrogram but

00:19:06.250 --> 00:19:11.450
the attempts have been not very good
when it comes to single channel.

00:19:11.450 --> 00:19:14.310
There's the case where you
might have multiple devices

00:19:14.310 --> 00:19:17.525
where you have multiple microphones
that are recording things, and

00:19:17.525 --> 00:19:22.410
there you can actually subtleties such as
one microphone being closer to a person

00:19:22.410 --> 00:19:26.875
than the other and then it turns out
that you can actually use raw wave

00:19:26.875 --> 00:19:30.370
forms to produce better
results then in spectrograms.

00:19:30.370 --> 00:19:34.440
So I haven't talked about that much here,
but what you feed in,

00:19:34.440 --> 00:19:39.810
it's really not that important for
the sake of calling it end to end.

00:19:39.810 --> 00:19:43.320
Let's just say it's a little
convolutional model that works on

00:19:43.320 --> 00:19:44.620
frames of raw wave forms.

00:19:44.620 --> 00:19:47.680
So you just take the raw wave form and

00:19:47.680 --> 00:19:53.003
you split it up into little frames and
that works just as well.

00:19:53.003 --> 00:19:57.833
Unfortunately, it doesn't work better yet
as we had originally hoped unless you

00:19:57.833 --> 00:20:00.776
have multiple microphones
in which case it does.

00:20:04.999 --> 00:20:09.720
So here's some results for CTC,
how it functions on a given audio.

00:20:11.260 --> 00:20:16.110
This audio stream corresponds
to his friends, and

00:20:16.110 --> 00:20:21.050
if you look at the model I've aligned or
lifted from the paper, so we have aligned

00:20:21.050 --> 00:20:24.620
a raw wave form at the bottom, and
the corresponding predictions at the top.

00:20:26.530 --> 00:20:30.740
So you'll see, it's producing
the symbol H, at a certain point,

00:20:30.740 --> 00:20:35.030
it gets a very high probability,
it goes straight from 0 to 1 so

00:20:35.030 --> 00:20:39.460
it's confident it's heard
the sound corresponding to H.

00:20:39.460 --> 00:20:43.630
There's a faint line here which
corresponds to the blank symbol, and

00:20:43.630 --> 00:20:45.730
you'll see that when you
want to emit symbols,

00:20:45.730 --> 00:20:48.186
the blank symbol probability
starts to dip down to zero.

00:20:48.186 --> 00:20:52.950
So they swap in terms of probability
space because you're not

00:20:52.950 --> 00:20:54.860
confident you want to produce an H symbol.

00:20:55.962 --> 00:20:59.990
Over time, you can see this
H now dies down to zero and

00:20:59.990 --> 00:21:05.770
of course, as the audio proceeds,
you start getting high probabilities for

00:21:05.770 --> 00:21:09.050
the other characters
corresponding to the sounds.

00:21:11.760 --> 00:21:14.480
To give you some examples on what this is,

00:21:14.480 --> 00:21:20.010
how this looks like when you just
take about 81 hours of data and

00:21:20.010 --> 00:21:23.330
just train to the text
corresponding to 81 hour of audio.

00:21:23.330 --> 00:21:25.130
So imagine you're a child.

00:21:25.130 --> 00:21:31.280
You are born, you listen for 10 days and
you start producing text like this.

00:21:31.280 --> 00:21:34.761
So the target is to illustrate the point,

00:21:34.761 --> 00:21:40.136
that is if you listen eight hours a day,
which most kids don't.

00:21:40.136 --> 00:21:43.975
To illustrate the point
a prominent Middle East analyst in

00:21:43.975 --> 00:21:48.110
Washington recounts
a call from one campaign.

00:21:48.110 --> 00:21:53.350
Two alstrait the point
a prominent Midille East analyst

00:21:53.350 --> 00:21:58.153
im Washington Recouncacacall
from one campaign.

00:21:58.153 --> 00:22:03.406
Here's another one,
I'll let you read that yourself,

00:22:03.406 --> 00:22:07.784
I'll just point out boutique,
we can bootik,

00:22:07.784 --> 00:22:13.130
it's kind of cute and
sometimes it gets it quite good.

00:22:13.130 --> 00:22:18.060
So it's pretty interesting

00:22:18.060 --> 00:22:23.180
that it produces text that very much is
like the output text that you desire.

00:22:23.180 --> 00:22:28.300
Of course,
it turns out these sound very good.

00:22:28.300 --> 00:22:31.290
If you read out the transcript,
it sounds like what you've heard.

00:22:31.290 --> 00:22:32.410
So, clearly something is missing.

00:22:34.200 --> 00:22:39.180
What's missing is correct spelling and
also a notion of grammar.

00:22:39.180 --> 00:22:42.430
So if you had some way of figuring out,

00:22:42.430 --> 00:22:45.845
of ranking these different paths
that you produce from your model and

00:22:45.845 --> 00:22:48.770
re-rank them by just the language model,
you should get much better.

00:22:48.770 --> 00:22:54.210
It turns out in this case, the original
base model had a word error rate of 30%.

00:22:54.210 --> 00:23:00.440
That means of the words that it was
producing, 30% were wrong which seems

00:23:00.440 --> 00:23:05.860
like a very big number, but even a small
spelling mistake will cause that error.

00:23:05.860 --> 00:23:10.350
Now if you use a language model to
sort of re-rank different hypothesis,

00:23:10.350 --> 00:23:15.550
you can get that word error rate done to
8.7% which is just using 81 hours of data.

00:23:17.540 --> 00:23:22.610
Subsequent to this work, Google looked
into using CTC where they actually

00:23:22.610 --> 00:23:26.600
have a language model as part of
the model itself during training, and

00:23:26.600 --> 00:23:31.050
that's kinda the production models you use
now when you call in with Ok Google and

00:23:31.050 --> 00:23:33.390
that fixed a lot of these issues.

00:23:33.390 --> 00:23:37.030
And of course they used thousands of hours
of data instead of 81 hours of data.

00:23:38.750 --> 00:23:42.120
There's no such thing as Big Data,
or big enough.

00:23:43.950 --> 00:23:47.490
So if you look at their paper and look at
some interesting results when you change

00:23:47.490 --> 00:23:51.200
the targets, instead of using characters,
you can actually use words.

00:23:51.200 --> 00:23:54.160
You can have a different
vocabulary size for words and

00:23:54.160 --> 00:23:56.280
see how the recognition system performs.

00:23:56.280 --> 00:24:01.202
So here the top What the panel is where
there are 7,000 words in the vocabulary

00:24:01.202 --> 00:24:05.367
and the bottom panel is where there's
90,000 words in the vocabulary.

00:24:05.367 --> 00:24:09.755
The actual text to be produced is
to become a dietary nutritionist.

00:24:09.755 --> 00:24:14.180
What classes should I take for
a two year program in a community college?

00:24:16.930 --> 00:24:21.940
If you'll look carefully in the panel not
sure if it's entirely visible all the way

00:24:21.940 --> 00:24:25.510
back, but these things are color-coded
in terms of the different words

00:24:25.510 --> 00:24:30.440
that were produced in this window and
corresponds to the probability.

00:24:30.440 --> 00:24:35.000
So here, there's the blank
symbol which again, goes up and

00:24:35.000 --> 00:24:38.410
down, depending on weather symbols
are being produced or not.

00:24:38.410 --> 00:24:43.235
It has this word to which is
the first word here in green,

00:24:43.235 --> 00:24:45.465
so it's got a high probability.

00:24:45.465 --> 00:24:49.394
Also, note that it also is confused
a little bit about the word do at

00:24:49.394 --> 00:24:50.314
the same time.

00:24:50.314 --> 00:24:51.818
It's either to or do.

00:24:51.818 --> 00:24:53.687
Do has a much lower probability.

00:24:53.687 --> 00:24:58.878
So it produces to and then it
produces nothing, but a blank symbol.

00:24:58.878 --> 00:25:04.117
And then it gets to the next word and
it produces the word become,

00:25:04.117 --> 00:25:08.310
and then the word a shows up,
and then dietary.

00:25:08.310 --> 00:25:12.709
Turns out dietary is not in
the vocabulary size of 7000, so

00:25:12.709 --> 00:25:16.278
it just produces diet which
is in the vocabulary and

00:25:16.278 --> 00:25:20.683
you'll see this row here
corresponding to diet being yellow.

00:25:20.683 --> 00:25:24.177
It also produces Terry, the name.

00:25:24.177 --> 00:25:27.915
It doesn't have dietary, but
just produces Terry as a response.

00:25:27.915 --> 00:25:30.642
It also produces military and
some other targets.

00:25:30.642 --> 00:25:34.987
But overall, it gets most of the words
that it's expecting correct.

00:25:34.987 --> 00:25:38.319
If you increase the vocabulary
size to be large enough, so

00:25:38.319 --> 00:25:42.603
that dietary is in the vocabulary,
you find that it actually also produces

00:25:42.603 --> 00:25:46.280
dietary as an output in there,
although it also produces diet.

00:25:47.460 --> 00:25:49.036
So, a language model would
fix that kind of an issue.

00:25:52.653 --> 00:25:55.084
So, that's what I have to say about CTC.

00:25:55.084 --> 00:25:57.928
I'm afraid I have to switch here and.

00:26:01.873 --> 00:26:05.930
Let's see, there we go.

00:26:05.930 --> 00:26:10.116
I promise to manage with four switches,
yeah.

00:26:18.729 --> 00:26:22.101
So now,
switching gears in terms of the models.

00:26:22.101 --> 00:26:23.929
The CTC model is interesting.

00:26:23.929 --> 00:26:27.941
But if you were paying attention very
carefully from a modeling perspective,

00:26:27.941 --> 00:26:31.481
you'd find that the model makes
predictions just based on the data.

00:26:31.481 --> 00:26:34.506
And once it's done with making
those predictions for each frame,

00:26:34.506 --> 00:26:36.679
there's no way of
adjusting that prediction.

00:26:36.679 --> 00:26:40.590
It has to the best it can
with those predictions.

00:26:40.590 --> 00:26:43.070
An alternative model is
the sequence-to-sequence models,

00:26:43.070 --> 00:26:48.800
which you guys have been reading
about from looking at your lectures.

00:26:48.800 --> 00:26:53.199
So, I won't talk too much about
the basics and jump straight in.

00:26:53.199 --> 00:26:56.741
We have a model here,
which basically does next step prediction.

00:26:56.741 --> 00:27:01.846
You're given some data x and
you've produced some symbols y1 to yi,

00:27:01.846 --> 00:27:07.816
and your model just going to predict the
probability of the next symbol of yi+1,

00:27:07.816 --> 00:27:12.165
and your goal was basically to
learn a very good model for p.

00:27:12.165 --> 00:27:16.470
And if you can do that, then you have
a model for p of any arbitrary y given x.

00:27:18.210 --> 00:27:22.586
So this model that does speech recognition
with the sequence to sequence framework,

00:27:22.586 --> 00:27:23.202
changes x.

00:27:23.202 --> 00:27:26.106
In translation,
this would be a source language.

00:27:26.106 --> 00:27:30.893
In speech, the x itself is
this huge sequence of audio.

00:27:30.893 --> 00:27:33.617
That is now encoded with
a recurrent neural network.

00:27:36.009 --> 00:27:39.584
What it needs to function is
the ability to look at different parts

00:27:39.584 --> 00:27:43.098
on temporal space,
because the input is really, really long.

00:27:43.098 --> 00:27:45.431
I think if you look at
translation results,

00:27:45.431 --> 00:27:49.795
you'll find that translation gets worse
as the source sentence becomes larger.

00:27:49.795 --> 00:27:51.943
It's because it's really hard for

00:27:51.943 --> 00:27:54.814
the model to look precisely
at the right place.

00:27:54.814 --> 00:27:58.628
Turns out that problem is aggravated
a lot more when you have audio streams.

00:27:58.628 --> 00:28:00.127
Audio streams are much longer.

00:28:00.127 --> 00:28:04.270
Typically a frame is like,
you have a hundred frames for a second.

00:28:04.270 --> 00:28:07.337
And when you wanna transcribe
something that's ten seconds long,

00:28:07.337 --> 00:28:08.582
that's about a thousand.

00:28:08.582 --> 00:28:11.782
Token thousand input times
are long as opposed to.

00:28:11.782 --> 00:28:16.682
In translation, you might have like 40
token words that you're gonna translate.

00:28:16.682 --> 00:28:19.044
So it's a very aggravating problem and

00:28:19.044 --> 00:28:22.918
you need to do attention if you
wanna make this model work at all.

00:28:22.918 --> 00:28:25.727
Whereas with translation,
you can get by without attention.

00:28:25.727 --> 00:28:28.813
So, how exactly does attention work here?

00:28:28.813 --> 00:28:32.343
You're trying to produce
the first character C and

00:28:32.343 --> 00:28:35.964
you have this way of producing
an attention vector.

00:28:35.964 --> 00:28:40.151
I'll go into that shortly how that's done,
but it's fairly standard.

00:28:40.151 --> 00:28:42.000
This attention vector,

00:28:42.000 --> 00:28:47.115
essentially looks at different parts
of the time steps of the input.

00:28:47.115 --> 00:28:50.798
Here it's saying,
I want to produce the next token.

00:28:50.798 --> 00:28:55.108
And to produce that token, I should really
look at the features that were over here

00:28:55.108 --> 00:28:57.113
and the features that were over here.

00:28:57.113 --> 00:29:01.377
So once it looks at the features
corresponding to those time steps, it's

00:29:01.377 --> 00:29:05.913
able to produce a character of C and then
it feeds in the character C to itself and

00:29:05.913 --> 00:29:10.283
then produces the next character which
is A after changing the attention.

00:29:10.283 --> 00:29:14.877
The attention now looks further down
from where it was looking at the first

00:29:14.877 --> 00:29:18.742
time step and then you feed in
the character A into your model.

00:29:18.742 --> 00:29:21.090
And again, you recompute attention and

00:29:21.090 --> 00:29:24.340
it automatically just moves
forward once its learned.

00:29:24.340 --> 00:29:30.703
So if you keep doing this
over the entire input stream,

00:29:30.703 --> 00:29:37.050
then you the moving

00:29:37.050 --> 00:29:43.285
forward attention just
learned by the model itself.

00:29:43.285 --> 00:29:47.016
So here, it's producing the output
sequence cancel, cancel, cancel.

00:29:53.177 --> 00:29:58.048
The question was, are we no longer doing
predict the previous token or the break?

00:29:58.048 --> 00:30:01.974
So this is a different model,
which is a sequence to sequence model,

00:30:01.974 --> 00:30:06.241
you feed in the entire data as an input
conditioning and there is no notion of

00:30:06.241 --> 00:30:09.858
consuming a little bit of the input and
then producing output.

00:30:09.858 --> 00:30:13.792
Instead, the entire input is
looked at every time step.

00:30:13.792 --> 00:30:16.634
And so you don't really need
to add breaks anywhere.

00:30:16.634 --> 00:30:18.459
You just produce one token,

00:30:18.459 --> 00:30:23.034
then you produce the next one condition
on the last token you produced.

00:30:23.034 --> 00:30:27.719
So going back, it's essentially
doing next step prediction.

00:30:32.618 --> 00:30:36.247
So it's just doing next step
prediction in this model,

00:30:36.247 --> 00:30:41.192
you have a neural net which is the decoder
in a sequence-to-sequence model

00:30:41.192 --> 00:30:44.608
that looks at the entire
input which is the encoder.

00:30:44.608 --> 00:30:46.331
It feeds in the path
symbols that you produce,

00:30:46.331 --> 00:30:47.976
because it's a recurrent neural network.

00:30:47.976 --> 00:30:51.805
You can just keep feeding symbols and
the length issue does not arise.

00:30:51.805 --> 00:30:55.640
So you fed in the path symbols as
the recurrent neural network and

00:30:55.640 --> 00:30:59.628
then you're just predicting the next
token itself as the output.

00:31:11.731 --> 00:31:13.155
I need to switch a third time.

00:31:20.185 --> 00:31:21.401
This is the second last one, I promise.

00:31:27.470 --> 00:31:31.238
So, how does this attention model work?

00:31:31.238 --> 00:31:36.586
So firstly,
you have an encoder on the left-hand side.

00:31:36.586 --> 00:31:39.621
It seems to have a special structure.

00:31:39.621 --> 00:31:42.451
I'll go into that in a few slides.

00:31:42.451 --> 00:31:45.659
For now, just forget the fact
that it has a special structure.

00:31:45.659 --> 00:31:50.813
And just remember that for every time step
of the input, it's producing some vector

00:31:50.813 --> 00:31:56.700
representation which encodes the input and
that's represented as ht at time step t.

00:31:56.700 --> 00:31:58.550
So you have hidden vector, ht.

00:31:58.550 --> 00:31:59.930
At time step t and

00:31:59.930 --> 00:32:04.790
you were generating the next character
at every time step with the decoder.

00:32:04.790 --> 00:32:08.310
So what you do is you take
the state vector of the decoder.

00:32:08.310 --> 00:32:11.570
So bottom layer of the recurring
neural network that is the decoder.

00:32:11.570 --> 00:32:14.510
And you now compare
the state vector against

00:32:14.510 --> 00:32:17.395
each of the hidden time
steps of the encoder.

00:32:17.395 --> 00:32:22.284
And so semantically what that means is
you kind of have this query in mind

00:32:22.284 --> 00:32:23.841
which is this state S.

00:32:23.841 --> 00:32:27.921
And you have places ht that you're
looking at as possible places where

00:32:27.921 --> 00:32:30.080
the information is present.

00:32:30.080 --> 00:32:32.955
So you take this query and
you compare it to every ht.

00:32:32.955 --> 00:32:37.661
You could have done something very simple
like take a dot product in which case,

00:32:37.661 --> 00:32:41.200
the vectors don't have to
be the same sort of size.

00:32:41.200 --> 00:32:43.840
Or you could have done something
much more sophisticated,

00:32:43.840 --> 00:32:46.370
which is you take the hidden state

00:32:46.370 --> 00:32:50.130
that you want to compare the query
against concatenate them into a vector.

00:32:50.130 --> 00:32:53.749
And then put them in a neural network
which produces a scalar value, and

00:32:53.749 --> 00:32:55.770
turns out that's what we do.

00:32:55.770 --> 00:32:58.900
So you basically, have this function here,

00:32:58.900 --> 00:33:04.460
function f which takes in a concatenation
of the hidden state at a time step t.

00:33:04.460 --> 00:33:09.388
With the state of the recurrent neural
network which is the decoder state and

00:33:09.388 --> 00:33:11.704
then produces a single number e of t.

00:33:11.704 --> 00:33:14.571
Now, you do that for
every time step of the encoder and so

00:33:14.571 --> 00:33:17.140
you have a trend in time
in the encoder space.

00:33:17.140 --> 00:33:21.020
And that's kind of like
a similarity between your query and

00:33:21.020 --> 00:33:22.930
your source from the encoder.

00:33:24.790 --> 00:33:28.379
So you get this trend eft, and
of course these are just scalers.

00:33:28.379 --> 00:33:32.210
Yeah, you want to keep these
magnitudes under control.

00:33:32.210 --> 00:33:37.200
So you can pass them through a softmax
which normalizes across the timesteps.

00:33:37.200 --> 00:33:38.633
And so you get something that sums to one.

00:33:38.633 --> 00:33:42.941
And that what's called the attention
vector that turns out showing you what's

00:33:42.941 --> 00:33:47.688
basically the trends of these attention
vector as the query changed overtime.

00:33:47.688 --> 00:33:48.520
So every timestep,

00:33:48.520 --> 00:33:51.965
you got an attention vector which shows
you where you look at for that timestep.

00:33:51.965 --> 00:33:55.660
Then you move to the next timestep, you
recompute your new attention vector, and

00:33:55.660 --> 00:33:58.970
you do that over and over again.

00:33:58.970 --> 00:34:02.283
So now that you an attention vector,
what you can do is now

00:34:02.283 --> 00:34:06.783
use these probabilities over timestep
to blend the hidden states together.

00:34:06.783 --> 00:34:11.081
And get one context value which is this
representation that is of interest to you

00:34:11.081 --> 00:34:14.390
in actually doing the prediction for
that time step.

00:34:14.390 --> 00:34:17.390
So here,
you would take all the hidden states and

00:34:17.390 --> 00:34:19.390
the corresponding attention value.

00:34:19.390 --> 00:34:22.237
And just multiply them and
add them together and

00:34:22.237 --> 00:34:24.237
that gives you a content vector.

00:34:24.237 --> 00:34:28.333
And this content vector is really
the content that will guide the prediction

00:34:28.333 --> 00:34:29.490
that you make.

00:34:29.490 --> 00:34:30.888
So you take this content vector,

00:34:30.888 --> 00:34:34.830
you can catenate that with
the state of your r and n.

00:34:34.830 --> 00:34:38.630
And you pass it through a neural net and
you get a prediction at that time step.

00:34:41.230 --> 00:34:45.390
And this prediction of course is the
probability of the next token, given all

00:34:45.390 --> 00:34:51.580
the past tokens you produced and all the
input that int was fed into the encoder.

00:34:57.170 --> 00:34:59.673
This is exciting for me,
I don't have to switch after this.

00:35:24.370 --> 00:35:29.421
Okay, so now what's this funny
business with the encoder?

00:35:29.421 --> 00:35:32.624
You're used to seeing
a recurring neural network,

00:35:32.624 --> 00:35:36.400
which basically proceeds at
the same time step as the input.

00:35:36.400 --> 00:35:39.290
So you got an input you process
with through some hidden stage.

00:35:39.290 --> 00:35:43.030
You passes through one occurrent step and
you move on.

00:35:43.030 --> 00:35:45.380
We found that for when we did that for

00:35:45.380 --> 00:35:48.752
audio sequences that we're really long,
such as realistic speech in

00:35:48.752 --> 00:35:51.830
Wall Street Journal which is
one of the speech corpora.

00:35:51.830 --> 00:35:55.385
It was just not able to learn
a very good attention model.

00:35:55.385 --> 00:35:57.552
Things just wouldn't go off the ground,
and

00:35:57.552 --> 00:36:01.210
that makes sense because you have
a lot of timesteps to look over.

00:36:01.210 --> 00:36:05.274
So typically, you'll get something like
7 seconds which would be 700 frames and

00:36:05.274 --> 00:36:07.600
you're doing a softmax over 700 timesteps.

00:36:07.600 --> 00:36:11.881
It's hard for you to sort of
initially learn where to propagate

00:36:11.881 --> 00:36:15.290
the signal down to,
to predict what the token is.

00:36:15.290 --> 00:36:17.115
So it never really catches on very fast.

00:36:17.115 --> 00:36:21.446
So this hierarchical encoder
is a replacement for

00:36:21.446 --> 00:36:24.029
a recurring neural network.

00:36:24.029 --> 00:36:28.357
So that just instead of one frame
of processing at every time step,

00:36:28.357 --> 00:36:33.040
you collapse neighboring frames
as you feed into the next layer.

00:36:33.040 --> 00:36:37.370
What this does is that every timestep,
it reduces the number of timesteps that

00:36:37.370 --> 00:36:40.650
you have to process and
also makes the processing faster.

00:36:40.650 --> 00:36:44.270
So if you do this a few times, by the time
you get to the top layer of the encoder.

00:36:44.270 --> 00:36:47.250
Your number of timesteps has
been reduced significantly and

00:36:47.250 --> 00:36:49.430
your attention model is
able to work a lot better.

00:36:53.252 --> 00:36:54.743
So here's some example.

00:36:54.743 --> 00:36:56.445
Output that this model produces.

00:36:56.445 --> 00:37:02.210
And I specifically want to out
the outputs are very multimodal.

00:37:02.210 --> 00:37:03.785
And what do I mean by that?

00:37:03.785 --> 00:37:09.970
So you have an input the truth is
call a a a roadside assistance.

00:37:09.970 --> 00:37:13.648
The model produces call a a a roadside
assistance as the first output.

00:37:13.648 --> 00:37:17.600
But it also produces call
triple a roadside assistance.

00:37:18.880 --> 00:37:23.970
So the same audio can be used to produce
very different kinds of transcripts.

00:37:23.970 --> 00:37:26.001
Which is really the power of the model and

00:37:26.001 --> 00:37:29.240
says how this model can actually
learn very complex functions.

00:37:29.240 --> 00:37:33.120
And actually solve this task with just
one model instead of requiring many.

00:37:35.010 --> 00:37:37.798
So interestingly if you look down,
you'll see

00:37:37.798 --> 00:37:42.057
the reason why this model is able to
produce call a a a, and call triple a.

00:37:42.057 --> 00:37:44.971
Because the training set
has a lot of call xxx.

00:37:44.971 --> 00:37:49.729
And so the model learns a very
specific pattern in a table to sort of

00:37:49.729 --> 00:37:52.510
transfer that to a different domain.

00:37:55.848 --> 00:38:01.040
Another aspect of the model is causality,
and what do I mean by that?

00:38:01.040 --> 00:38:06.265
Here, you have an output which is St.

00:38:06.265 --> 00:38:09.745
Mary's Animal Clinic
which is the transcript.

00:38:09.745 --> 00:38:13.888
So if you look at the attention over
time when you produce the first token s.

00:38:13.888 --> 00:38:18.036
It just look said this little blob at
the top, and then when you look at t,

00:38:18.036 --> 00:38:22.212
it looks moves forward at
the pretty center of the character.

00:38:22.212 --> 00:38:26.832
And the models per multimodels, so
instead of just producing st marries.

00:38:26.832 --> 00:38:30.432
It can also produce Saint Mary

00:38:30.432 --> 00:38:33.332
which is a truly different
transcript from the same audio.

00:38:33.332 --> 00:38:38.496
And now if you look at where the attention
goes before would produce st.

00:38:38.496 --> 00:38:43.840
And then start moving
forward when Mary came along.

00:38:43.840 --> 00:38:45.760
Here, it's the same word saint.

00:38:45.760 --> 00:38:50.122
So it actually dwells at the same
timestep in attention space.

00:38:50.122 --> 00:38:53.531
So that's a notion where,
whatever symbol you've produced,

00:38:53.531 --> 00:38:57.420
really affects how the neural network
behaves at the next few timesteps.

00:38:57.420 --> 00:39:01.133
And that's really a very strong
characteristic of this model.

00:39:29.135 --> 00:39:33.894
Okay, the question is the fact
that the model produces

00:39:33.894 --> 00:39:38.240
two different transcript
a result of the fact that

00:39:38.240 --> 00:39:43.220
there's ambiguity in
the pronunciation model itself?

00:39:43.220 --> 00:39:49.360
So I think that is essentially
what the model tries to capture.

00:39:49.360 --> 00:39:52.780
The fact that the same word can
be pronounced multiple ways or

00:39:52.780 --> 00:39:57.300
that the same pronunciation can be
written out multiple ways are sort of,

00:39:57.300 --> 00:39:59.530
Two different, but related, problems.

00:39:59.530 --> 00:40:02.720
One is different pronunciation
producing the same token,

00:40:02.720 --> 00:40:06.960
which does not require
multimodality in a model

00:40:06.960 --> 00:40:10.310
as long as one source of sound
only produces the same token.

00:40:10.310 --> 00:40:12.810
What's happening here is
more interesting in that

00:40:12.810 --> 00:40:15.790
the same sound can be
written out in multiple ways.

00:40:15.790 --> 00:40:21.167
During training, clearly it must have
heard both sides, it's heard saint written

00:40:21.167 --> 00:40:26.638
out as S-T and another time it must have
heard saint written out as an S-A-I-N-T.

00:40:26.638 --> 00:40:30.900
So you need it in the training data but
what's nice is the model's really powerful

00:40:30.900 --> 00:40:34.050
enough to realize that the same
sound can actually do this and

00:40:34.050 --> 00:40:36.918
it can actually produce
very different transcripts.

00:40:43.812 --> 00:40:47.180
So when we did this about a year and
a half ago,

00:40:47.180 --> 00:40:51.270
these are old results,
things are more exciting now.

00:40:52.350 --> 00:40:57.250
We found that our model was when
you didn't use a language model it

00:40:57.250 --> 00:41:02.240
produced a word error rate of
around 14% whereas the best system

00:41:02.240 --> 00:41:05.197
that we had in Google at the time was 8.

00:41:05.197 --> 00:41:10.736
At this point I would say I was
an intern at Google many years ago,

00:41:10.736 --> 00:41:17.723
and then the word error rate was 16% and
that was a result of 45 years of work.

00:41:17.723 --> 00:41:22.798
Where people had customized all the speech
recognition components very carefully for

00:41:22.798 --> 00:41:23.850
all these years.

00:41:23.850 --> 00:41:29.013
And this model by one, just one
single model that just goes straight

00:41:29.013 --> 00:41:35.464
from audio to text gets a lower word error
rate than what we were getting in 2011.

00:41:35.464 --> 00:41:39.652
So that, I think, something to write
home about which is pretty exciting.

00:41:39.652 --> 00:41:43.837
Turns out it still benefits from
having a language model, so

00:41:43.837 --> 00:41:49.430
if you feed in a language model the 14%
word error rate comes down to 10.3.

00:41:49.430 --> 00:41:54.748
So obviously, there's no substitute for
billions and billions

00:41:54.748 --> 00:42:01.430
of written text sentences in trying to
disambiguate speech recognition better.

00:42:03.360 --> 00:42:06.056
But just the basic model
by itself does very well.

00:42:09.600 --> 00:42:12.090
So, now what are the limitations
of this model?

00:42:12.090 --> 00:42:16.680
One of the big limitations preventing
its use in an online system is that

00:42:16.680 --> 00:42:21.134
if you notice, the output is produced
conditioned on the entire input.

00:42:21.134 --> 00:42:26.722
So you get this next step prediction
of xt plus 1 given all the input x and

00:42:26.722 --> 00:42:30.539
all the sorry,
yt plus 1 given all the input x and

00:42:30.539 --> 00:42:35.029
all the tokens you've produced so
far which is y1 to yt.

00:42:35.029 --> 00:42:37.509
So it's really just doing
next step prediction but

00:42:37.509 --> 00:42:40.519
the next step prediction is
conditioned on the entire input.

00:42:40.519 --> 00:42:44.263
So if you were gonna try and put it
in a real speech recognition system,

00:42:44.263 --> 00:42:45.786
you'd have to first wait for

00:42:45.786 --> 00:42:49.862
the entire audio to be received before
you can start outputting the symbol.

00:42:49.862 --> 00:42:53.827
Because, by definition,
the mathematical model is the next

00:42:53.827 --> 00:42:57.960
token in this condition on the entire
input and the past tokens.

00:42:59.760 --> 00:43:04.050
Another problem is that the attention
model itself is a computational bottleneck

00:43:04.050 --> 00:43:07.240
for every time stamp, you have to
look at the entire input sequence.

00:43:07.240 --> 00:43:12.100
So there's this comparison as
long as the length of the input

00:43:12.100 --> 00:43:16.340
which makes it a lot slower and
harder to learn as well.

00:43:18.590 --> 00:43:24.200
Further, as the input receives and becomes
longer, the word error rate goes down.

00:43:24.200 --> 00:43:27.710
This is really an old slide,
this doesn't happen much anymore.

00:43:27.710 --> 00:43:33.440
But I'll talk about the methods we came
up with to prevent this a little later.

00:43:37.120 --> 00:43:41.640
So I'm gonna now switch gears to
another model which it's called

00:43:41.640 --> 00:43:43.720
the online sequence to sequence model.

00:43:43.720 --> 00:43:46.100
This model was designed to try and

00:43:46.100 --> 00:43:49.965
overcome the limitations of sequence to
sequence models where we don't want to

00:43:49.965 --> 00:43:55.525
wait till the entire input has arrived
before we start producing the output.

00:43:55.525 --> 00:44:00.305
And also wanna try and avoid attention
model itself over the entire sequence,

00:44:00.305 --> 00:44:02.975
because that seems to
be an overkill as well.

00:44:02.975 --> 00:44:08.855
So you want to produce outputs as inputs
arrive and it has to solve this problem

00:44:08.855 --> 00:44:13.600
which is, am I ready to produce an output
now that I've received this much input?

00:44:13.600 --> 00:44:17.550
So, the model has changed a little bit,
not only does it have to produce a symbol,

00:44:17.550 --> 00:44:21.062
it has to know when it has enough
information to produce the next symbol.

00:44:23.333 --> 00:44:27.071
So, this model is called the a neural
transducer, and in essence,

00:44:27.071 --> 00:44:28.521
it's a very simple idea.

00:44:28.521 --> 00:44:33.291
You take the input as it comes in, and
every so often at regular intervals,

00:44:33.291 --> 00:44:39.000
you run a sequence to sequence model
on what you received in the last block.

00:44:39.000 --> 00:44:44.930
And so you have this situation here
where you basically have the encoder.

00:44:44.930 --> 00:44:49.455
And now instead of the encoder
attention looking at the entire input,

00:44:49.455 --> 00:44:51.986
it just looks at this little block, and

00:44:51.986 --> 00:44:57.452
this decoder which we call the transducer
here will now produce the output symbols.

00:44:57.452 --> 00:45:03.225
Now notice that since we've locked up
the inputs, we have this situation where

00:45:03.225 --> 00:45:08.235
you may have received some input,
but you can't produce an output.

00:45:08.235 --> 00:45:15.105
And so now we're sort of, we need this
blank symbol back again in this model.

00:45:15.105 --> 00:45:18.762
Because it really can be the situation
where you got a long pause,

00:45:18.762 --> 00:45:23.810
you haven't heard any words, so you really
shouldn't be producing any symbols.

00:45:23.810 --> 00:45:29.296
So we reintroduce this symbol called
the end-of-block symbol here in this model

00:45:29.296 --> 00:45:34.950
to sort of encapsulate the situation that
you shouldn't be producing any outputs.

00:45:37.560 --> 00:45:41.203
One nice thing about this model now,
is that it maintains causality.

00:45:41.203 --> 00:45:42.968
So if you remember the CTC model,

00:45:42.968 --> 00:45:45.816
it also had this notion of
not producing any outputs.

00:45:45.816 --> 00:45:47.917
But when you produce these symbols,

00:45:47.917 --> 00:45:51.046
you did not feed back what
you had produced in the past.

00:45:51.046 --> 00:45:54.138
And so it didn't have these notions
where the same input can produce

00:45:54.138 --> 00:45:55.069
multiple outputs.

00:45:55.069 --> 00:45:59.791
And it didn't have the notion of causality
where depending on what you produce so

00:45:59.791 --> 00:46:03.622
far, you would really just change
the computation they're on.

00:46:03.622 --> 00:46:05.828
So here in the neural transducer,

00:46:05.828 --> 00:46:10.022
it preserves disadvantage of
a sequence-to-sequence model.

00:46:10.022 --> 00:46:14.085
And it also, of course,
now introduces an alignment problem,

00:46:14.085 --> 00:46:17.560
just like these slides have
an alignment problem too.

00:46:18.830 --> 00:46:20.218
So, in essence,

00:46:20.218 --> 00:46:25.500
what you want to know is you have
to produce some symbols as outputs.

00:46:25.500 --> 00:46:29.089
But you don't know which chunk
should these symbols be aligned to.

00:46:29.089 --> 00:46:33.817
And you have to solve that
problem during learning.

00:46:33.817 --> 00:46:40.717
I want describe this very carefully but
I'll make a go off it.

00:46:40.717 --> 00:46:45.494
You have some output symbols,
y1 to S that have to be produced, and now,

00:46:45.494 --> 00:46:47.898
if you have an input that is B blocks.

00:46:47.898 --> 00:46:53.526
You can now output these S symbols along
with B end-of-block markers anywhere

00:46:53.526 --> 00:46:59.780
to describe the actual alignment in
the way the symbols are produced.

00:46:59.780 --> 00:47:03.420
And of course there's a combinatorial
number of ways in which you can align

00:47:03.420 --> 00:47:07.330
the original input to
the actual block symbols.

00:47:07.330 --> 00:47:12.963
So the probability distribution
turns out to be the probability

00:47:12.963 --> 00:47:18.280
of y1 to yS given x is modeled as
the sum over all the different

00:47:18.280 --> 00:47:23.512
ways in which you can align y1
to S to the original B blocks.

00:47:23.512 --> 00:47:28.353
And the B, the extra B in length comes
from the fact that there's B blocks and

00:47:28.353 --> 00:47:31.012
an H block ends with
an end-of-block symbol.

00:47:31.012 --> 00:47:35.386
So now it's similar to CTC,
you have some output sequence.

00:47:35.386 --> 00:47:38.682
You can produce them from
a bunch of different ways.

00:47:38.682 --> 00:47:41.111
And all of those ways
have some probability and

00:47:41.111 --> 00:47:44.250
if you have to learn in
spite of that model.

00:47:44.250 --> 00:47:48.440
Unlike CTC, of course, this model is
not independent at every time step.

00:47:49.780 --> 00:47:54.124
Once you make the predictions you feedback
your previous tokens that changes

00:47:54.124 --> 00:47:57.587
the entire probability distribution
at the next timestamp.

00:47:57.587 --> 00:48:02.127
What this means is there no decomposition
between different parts of the input,

00:48:02.127 --> 00:48:03.071
given the data.

00:48:03.071 --> 00:48:07.635
So you can’t really do a dynamic
programming algorithm that just simplifies

00:48:07.635 --> 00:48:08.838
this computation.

00:48:08.838 --> 00:48:14.628
So, we came up with a simple way of doing
this approximation of the sum, which was,

00:48:14.628 --> 00:48:19.708
let's just find the best possible
alignment given your current model.

00:48:19.708 --> 00:48:23.070
So, you basically try and
do some kind of a beam search.

00:48:23.070 --> 00:48:27.664
And you find the best paths as the output
and then you use that during training.

00:48:31.384 --> 00:48:35.760
Okay, so
sorry one more point that I should make.

00:48:37.400 --> 00:48:40.130
That's the same process
we used during inference.

00:48:40.130 --> 00:48:43.920
The model itself is you want to
produce these symbols, y1 to ys,

00:48:43.920 --> 00:48:45.860
you can do it in a variety of ways.

00:48:45.860 --> 00:48:48.290
During inference you find the best one and

00:48:48.290 --> 00:48:51.260
you go with that one as
being the actual transcript.

00:48:51.260 --> 00:48:56.429
During learning if you take
a gradient of that combinatorial sum,

00:48:56.429 --> 00:49:02.260
it comes down to this particular
form which boils down to saying.

00:49:02.260 --> 00:49:08.060
Give me a sample from all the probability
of aligning the outputs given the input,

00:49:08.060 --> 00:49:11.300
and then I will train the log
probability of that sample.

00:49:12.620 --> 00:49:15.800
If that sounds like Greek I wouldn't
worry too much about it but

00:49:15.800 --> 00:49:17.180
I'll say it one more time.

00:49:17.180 --> 00:49:22.069
It's basically this happens in
cases where models are a sum

00:49:22.069 --> 00:49:24.440
of combinatorial of terms.

00:49:24.440 --> 00:49:27.860
If you want to optimize such a model you

00:49:27.860 --> 00:49:31.840
basically have to take the gradient of the
log prob and the gradient of the log prob

00:49:31.840 --> 00:49:37.140
turns out to be a sum of the log probs
over the posteriors of the samples.

00:49:37.140 --> 00:49:39.199
And that's the case in this model as well.

00:49:39.199 --> 00:49:42.290
Of course this is really
hard to optimize and so

00:49:42.290 --> 00:49:46.177
we replaced this entire really
complicated procedure for

00:49:46.177 --> 00:49:50.697
optimization by giving an output
symbol find the best alignment and

00:49:50.697 --> 00:49:55.712
just make that alignment better,
it's kind of like a terby sort of trick.

00:50:00.052 --> 00:50:02.065
So I'm just gonna skip this part.

00:50:08.974 --> 00:50:13.670
Okay, the finding of the best
path is kind of interesting.

00:50:13.670 --> 00:50:16.240
So I'll cover this part.

00:50:17.720 --> 00:50:21.981
Turns out if you want to find the best
path there's also a combinatorial

00:50:21.981 --> 00:50:25.815
number of ways, and so what you can
do is kind of do a beam search,

00:50:25.815 --> 00:50:30.004
where you keep a bunch of candidates
around and then you extend them and

00:50:30.004 --> 00:50:34.071
as you extend them you now re-rank
them and throw away the best one.

00:50:34.071 --> 00:50:37.632
However, turns out if you do beam search,
it doesn't really work.

00:50:37.632 --> 00:50:40.902
And so what we discovered
was a Dynamic programming,

00:50:40.902 --> 00:50:43.599
which is approximate that works very well.

00:50:43.599 --> 00:50:47.867
And the way this works
is you consider the best

00:50:47.867 --> 00:50:52.921
candidates that are produced
at the end of a block from

00:50:52.921 --> 00:50:57.527
producing either j-1 tokens or j-2 tokens,

00:50:57.527 --> 00:51:02.520
j-1 tokens, or
j tokens at the end of block b -1.

00:51:02.520 --> 00:51:07.620
So you know that if I wanted
to produce j-2 tokens at

00:51:07.620 --> 00:51:10.470
the end of the previous block,
what's the best probability?

00:51:10.470 --> 00:51:12.775
And now,
that corresponds to this dot here.

00:51:12.775 --> 00:51:18.403
So from that dot you can now extend
either by one symbol or by two symbols or

00:51:18.403 --> 00:51:23.855
by three symbols, and you get
different paths that reach the source.

00:51:23.855 --> 00:51:28.307
And so now when you're considering
the different ways of entering a source,

00:51:28.307 --> 00:51:31.265
you just find the best one and
you keep that around.

00:51:31.265 --> 00:51:35.370
And you then now extend those
ones to the next time step.

00:51:35.370 --> 00:51:38.557
It's kind of an approximate procedure,

00:51:38.557 --> 00:51:43.026
because this ability to extend
a symbol is not Markovian.

00:51:43.026 --> 00:51:47.528
And so if we take this max as the max
of the previous step extended by one,

00:51:47.528 --> 00:51:51.659
that maybe wrong because the correct
path maybe two steps away and

00:51:51.659 --> 00:51:53.557
that would've been better.

00:51:53.557 --> 00:51:57.486
However, it seems to work very
well to find an alignment that

00:51:57.486 --> 00:52:01.506
trains the online
sequence-to-sequence model properly.

00:52:01.506 --> 00:52:04.820
So some results on this model,
if you change the window size,

00:52:04.820 --> 00:52:07.420
that's how the block is constructed.

00:52:07.420 --> 00:52:13.270
You find that using different block
sizes when there's an attention model,

00:52:13.270 --> 00:52:14.600
mixed model work very well.

00:52:14.600 --> 00:52:17.760
So in these blocks we can have
attention instead of just running

00:52:17.760 --> 00:52:19.080
a sequence-to-sequence.

00:52:19.080 --> 00:52:22.510
So it's not affected by
the window size of the blocks.

00:52:22.510 --> 00:52:25.490
And those are these lines at the bottom.

00:52:25.490 --> 00:52:28.130
It also turns out that you
don't really need attention.

00:52:28.130 --> 00:52:33.070
If the window size is small
which side steps this problem of

00:52:33.070 --> 00:52:35.870
doing an attention over
the entire input sequence.

00:52:35.870 --> 00:52:41.145
And that's where we're really trying
to get that is to try and get the model

00:52:41.145 --> 00:52:46.254
that could do sequence-to-sequence
output symbols when it needs to but

00:52:46.254 --> 00:52:51.464
really, not have to do all these
computation with respect to the length.

00:52:54.790 --> 00:52:58.142
So that was basically the online
sequence-to-sequence model.

00:52:58.142 --> 00:53:02.285
I wanna touch a little bit about how you
can make the sequence-to-sequence models

00:53:02.285 --> 00:53:03.362
better themselves.

00:53:03.362 --> 00:53:08.099
One of the things that people are doing
these days borrowing from vision is to use

00:53:08.099 --> 00:53:10.520
convolutional neural networks.

00:53:10.520 --> 00:53:15.060
So in vision related tasks, convolutional
neural networks have been very powerful.

00:53:15.060 --> 00:53:17.550
Some of the best models for
object detection and

00:53:17.550 --> 00:53:20.090
object recognition use
convolutional models.

00:53:20.090 --> 00:53:24.935
They are also very effective in speech,
so, we tried to do this architecture and

00:53:24.935 --> 00:53:27.302
speech for the encoder side of things.

00:53:27.302 --> 00:53:30.742
So, you take the traditional model for
the pyramid, and

00:53:30.742 --> 00:53:34.828
instead of doing the pyramid by
just stacking two things together,

00:53:34.828 --> 00:53:38.960
you can actually put a fancy
architecture when you do the stacking.

00:53:38.960 --> 00:53:43.680
So don't just stack two times, step,
and feed it to the next layer, but

00:53:43.680 --> 00:53:49.750
instead stack them as feature maps and
put a convolutional neuronetwork on top.

00:53:49.750 --> 00:53:54.709
I think you guys have not been exposed
o convolutional neuronetworks yet,

00:53:54.709 --> 00:53:58.960
but let's just say it's a very
specific kind of model that looks

00:53:58.960 --> 00:54:02.675
at some subset of the input
instead of the entire input.

00:54:02.675 --> 00:54:08.020
And so the subset that it looks at
has to be matched to the structure.

00:54:08.020 --> 00:54:12.285
So if you are in a task such as vision,
an image patch is natural structure or

00:54:12.285 --> 00:54:15.349
substructure to look at
instead of the entire image.

00:54:15.349 --> 00:54:20.352
For speech, also if you look at
the frequency bands and the time stamps of

00:54:20.352 --> 00:54:25.289
the features, that corresponds to
a natural substructure to look at.

00:54:25.289 --> 00:54:28.282
So convulational model will
just look at the substructure.

00:54:28.282 --> 00:54:31.296
So what we did in this work was to say,
okay,

00:54:31.296 --> 00:54:35.612
now we're gonna change this
computation which is a pyramid and

00:54:35.612 --> 00:54:41.243
add a lot of depth to it by adding this
convolutional architectures in every step.

00:54:41.243 --> 00:54:45.376
So, in the past it was just a simple
linear projection of two time steps

00:54:45.376 --> 00:54:49.860
together, but now it's a very deep
convolutional model which of course for

00:54:49.860 --> 00:54:54.416
deep learning experts is great, because
the believe the deeper the number of

00:54:54.416 --> 00:54:59.138
nonlinearities the better it is, and
this model actually has a lot of that way.

00:55:00.879 --> 00:55:04.075
When we did that,
we found very good results.

00:55:04.075 --> 00:55:07.879
If you take a baseline on a task
called Wall Street Journal,

00:55:07.879 --> 00:55:12.444
it goes from something like 14.76
word error rate down to 10.5

00:55:12.444 --> 00:55:17.030
just by using this very specific trick
on how to do these convolutions.

00:55:19.050 --> 00:55:22.966
So deeper continues to be a good model.

00:55:22.966 --> 00:55:27.702
Okay, now switching to what is
the output space that's a very

00:55:27.702 --> 00:55:30.370
appropriate one for speech.

00:55:30.370 --> 00:55:33.774
So in translation,
what happens is there's multiple

00:55:33.774 --> 00:55:37.925
ways people have discovered on how
to encode the output sequence.

00:55:37.925 --> 00:55:42.206
You might produce character sequences,
you might produce words and

00:55:42.206 --> 00:55:46.560
character sequences, or you might
produce a subset of characters and

00:55:46.560 --> 00:55:49.620
use that as the output vocabulary.

00:55:49.620 --> 00:55:54.480
In speech, that seems not natural because
what you want to do is you want to

00:55:54.480 --> 00:55:58.890
produce output tokens that corresponds
to some notion of the sound that was

00:55:58.890 --> 00:56:00.640
being produced in the input.

00:56:00.640 --> 00:56:02.759
So what you would like to do
is change the vocabulary.

00:56:02.759 --> 00:56:06.386
So it's not just characters but
maybe bigrams or

00:56:06.386 --> 00:56:11.086
trigrams of characters that
corresponds to some audio token.

00:56:11.086 --> 00:56:12.332
So basically these are,

00:56:12.332 --> 00:56:15.429
I guess the slide is talking about
the different ways to do it.

00:56:15.429 --> 00:56:19.657
As I said you can either represent
the word or the characters or

00:56:19.657 --> 00:56:24.250
the word and characters, but for
speech you want to use N-grams.

00:56:24.250 --> 00:56:27.980
However there's a problem here, should we

00:56:27.980 --> 00:56:33.440
decide the end graphs before hand and
then just use those during training?

00:56:33.440 --> 00:56:35.480
That kind of defies the end to end model

00:56:36.600 --> 00:56:40.750
where we want to actually learn
the entire process, as one big model.

00:56:40.750 --> 00:56:44.898
So we decided okay,
what we could do is build this

00:56:44.898 --> 00:56:49.858
vocabulary which is unigrams,
bigrams, trigrams, and

00:56:49.858 --> 00:56:55.950
whatever number of N-grams of characters,
and put them in a softmax.

00:56:55.950 --> 00:56:58.841
And now the problem arises,
if you have a word like hello,

00:56:58.841 --> 00:57:00.794
it can be decomposed in multiple ways.

00:57:00.794 --> 00:57:06.259
Might spell as H-E-L-L-O, or
it might spell as HE-L-L-O

00:57:06.259 --> 00:57:10.928
if HE happens to be in a target
set that you've chosen.

00:57:10.928 --> 00:57:16.090
So it's really indefine,
undefine problem or it's a problem where

00:57:16.090 --> 00:57:21.520
now you have to deal with multiple
ways of out putting the same sequence.

00:57:21.520 --> 00:57:23.660
So how should we make this choice?

00:57:23.660 --> 00:57:27.460
One way of making this choice is
you get a word such as CAT SITS.

00:57:27.460 --> 00:57:30.880
You just look in your token space.

00:57:30.880 --> 00:57:36.920
If you have CA in your tokens, you just
say I'm gonna choose CA as the input.

00:57:36.920 --> 00:57:42.269
The you do T and then SI and then T and S.

00:57:42.269 --> 00:57:47.376
So this is just very greedy in terms
of how you produce the tokens.

00:57:47.376 --> 00:57:50.291
Another way is to look at the compression,

00:57:50.291 --> 00:57:53.857
the sequence of tokens that
have the highest probability.

00:57:53.857 --> 00:57:57.765
Here, it's basically about reuse,
it's like encoding.

00:57:57.765 --> 00:58:00.661
And you would just sort of use
a minimum description line.

00:58:00.661 --> 00:58:04.230
So AT happens to be a lot
more frequent than CA, so

00:58:04.230 --> 00:58:07.024
you would rather choose AT as a token.

00:58:07.024 --> 00:58:14.002
In this case, the decomposition for
CAT SITS would be C, AT, SI, and T, S.

00:58:14.002 --> 00:58:18.905
That would be another way of course,
it's not clear for

00:58:18.905 --> 00:58:21.730
the audio which is the best way.

00:58:23.460 --> 00:58:26.620
So our approach was try to
learn this automatically.

00:58:26.620 --> 00:58:32.310
So you have some output y*,
which is the correct output sequence,

00:58:32.310 --> 00:58:36.400
and you try out all the possible
decompositions of the same output.

00:58:36.400 --> 00:58:40.800
So you basically look at all possible
ways of producing the token.

00:58:40.800 --> 00:58:45.591
And when you do the learning, you take
the gradient of all possibilities of

00:58:45.591 --> 00:58:50.090
producing the output sequence and
propagate that error signal down.

00:58:51.230 --> 00:58:53.001
Just to know when this class end?

00:59:00.282 --> 00:59:04.001
If you look at how this model performs,

00:59:04.001 --> 00:59:08.870
turns out it helps to use
larger N-gram pieces.

00:59:08.870 --> 00:59:11.914
So if you take a character
based model which was just CTC,

00:59:11.914 --> 00:59:15.289
with no language model,
it would have 27% word error rate.

00:59:15.289 --> 00:59:20.062
If you take the sequence to sequence rate
of the last model with character output,

00:59:20.062 --> 00:59:22.359
it produces 14.7 word error rate.

00:59:22.359 --> 00:59:27.968
If you used a 2 gram it does better,
with a 3 gram it does it even better.

00:59:27.968 --> 00:59:30.236
The 4 gram does better on training, but

00:59:30.236 --> 00:59:34.521
worst generalization presumably because
our dataset was really limited.

00:59:34.521 --> 00:59:38.180
It's just 81 hours of data and once you
start using larger and larger tokens.

00:59:38.180 --> 00:59:41.652
You can imagine it doesn't
enough evidence for

00:59:41.652 --> 00:59:45.655
out lot of the longer tokens,
similarly for 5 grams.

00:59:47.904 --> 00:59:54.785
To show you an example, the actual
test is shamrock's pretax profit

00:59:54.785 --> 01:00:00.371
from the sale was $125
million a spokeswoman said.

01:00:00.371 --> 01:00:06.437
The character model produces
shamrock as C-H-A-M-R-O-C-K.

01:00:06.437 --> 01:00:12.145
The 4 gram model will take the sh sound
straight up as SH, which is nice.

01:00:12.145 --> 01:00:16.350
And then does a lot of these
things with single characters.

01:00:16.350 --> 01:00:19.988
But you can see common and bigrams and

01:00:19.988 --> 01:00:23.985
trigrams being used as a result of this.

01:00:27.764 --> 01:00:31.191
If you look at whether or
not the model is actually using it.

01:00:31.191 --> 01:00:35.576
Numerically, you find out if you had
trained the model by just one kind

01:00:35.576 --> 01:00:39.920
of algorithm,
where you just took the maximum extension.

01:00:39.920 --> 01:00:46.380
It used the N-grams more, because it
trained to use these longer N-grams.

01:00:46.380 --> 01:00:48.991
However, if you look at
the results that come out,

01:00:48.991 --> 01:00:51.501
where you actually learned
to use the N-grams.

01:00:51.501 --> 01:00:55.733
It still does a better job, does
a good job of learning the N-grams and

01:00:55.733 --> 01:00:57.690
it gets a lower word error rate.

01:00:59.550 --> 01:01:03.782
So that's quite promising in terms
of achieving what we wanted to do.

01:01:06.156 --> 01:01:10.646
So now I'm switching gears here and going
into some of the final shortcomings of

01:01:10.646 --> 01:01:15.080
sequence to sequence models when they're
applied into speech recognition.

01:01:16.670 --> 01:01:19.170
If you look at the transcripts
that are produced

01:01:19.170 --> 01:01:23.390
in terms of the probabilities of every
token, you find an interesting pattern.

01:01:24.830 --> 01:01:28.181
Here, at the top is the actual sequence.

01:01:28.181 --> 01:01:33.728
So South Africa the Solution by
Francis Candold and Leoguen Low.

01:01:33.728 --> 01:01:37.097
It's actually this is
not the right solution,

01:01:37.097 --> 01:01:39.810
this is the highest probability one.

01:01:39.810 --> 01:01:45.891
Below each token you have the alternate
tokens that had a probability which

01:01:45.891 --> 01:01:51.310
was some thresholds lower than
the probability of the best token.

01:01:51.310 --> 01:01:54.730
So if there's no tokens below a token,
that means there's no ambiguity.

01:01:54.730 --> 01:01:58.609
It's really sure that that's the token
that's got the right answer.

01:01:58.609 --> 01:02:03.402
If there's many, that means it's a little
confused at this part of the token when

01:02:03.402 --> 01:02:05.353
it's producing the next token.

01:02:05.353 --> 01:02:09.923
So you find a very interesting trend that
there's a lot of ambiguity at the start at

01:02:09.923 --> 01:02:11.258
the first characters.

01:02:11.258 --> 01:02:15.754
But as soon as you produce the first few
characters there's very little ambiguity

01:02:15.754 --> 01:02:18.200
as to what the next characters are.

01:02:18.200 --> 01:02:23.094
Unless it's things like names, so
Francis here, there's some confusion

01:02:23.094 --> 01:02:27.616
on how to sound it out and Candold
has some probability issues as well.

01:02:27.616 --> 01:02:32.577
So this might seem surprising but
it's natural.

01:02:32.577 --> 01:02:35.160
If you're doing language
modelling on a character level.

01:02:35.160 --> 01:02:37.339
Once you have the first
few characters of a word,

01:02:37.339 --> 01:02:39.630
you pretty much know what the word is.

01:02:39.630 --> 01:02:44.670
And so what you really wanna do is produce
these, be much more discriminative

01:02:44.670 --> 01:02:49.800
at the starts of words instead of,
because that's where you'll make an error.

01:02:49.800 --> 01:02:53.010
If you make an error at the start of a
word, you're never gonna recover from it.

01:02:54.600 --> 01:03:03.090
And so if we want to fix this problem,
we need to sort of address this issue.

01:03:03.090 --> 01:03:08.030
The repercussion of this problem is that
if you're over confident about the wrong

01:03:08.030 --> 01:03:10.831
word, not even a language
model can help you.

01:03:10.831 --> 01:03:14.329
Because you've basically decided early
on what the word is going to be.

01:03:14.329 --> 01:03:16.904
And you need very precise language

01:03:16.904 --> 01:03:20.907
model probabilities to kind
of get you out of the rut.

01:03:20.907 --> 01:03:24.238
So we've found that there's this little
technique called entropy regularization.

01:03:24.238 --> 01:03:28.228
Which prevents your softmax from ever
being too confident that really just

01:03:28.228 --> 01:03:29.850
solves this problem.

01:03:29.850 --> 01:03:32.210
So every time you predict
the next character,

01:03:32.210 --> 01:03:35.792
you make sure you're not becoming
probability of one for one symbol.

01:03:35.792 --> 01:03:39.356
Instead you say if you're getting to
confident and penalize it, that you're

01:03:39.356 --> 01:03:43.460
force to spread the probability of
distribution to the other characters.

01:03:43.460 --> 01:03:46.280
Once you do that,
this problem really just goes away.

01:03:46.280 --> 01:03:50.630
And the baseline model that we
had just improved massively.

01:03:50.630 --> 01:03:54.110
So if you remember,
we had CTC on an intent task on

01:03:54.110 --> 01:03:56.350
Wall Street Journal which
had some error like 27.3.

01:03:56.350 --> 01:04:02.930
Then we had, there's a baseline sequence
to sequence model that wasn't ours.

01:04:02.930 --> 01:04:07.370
But Yoshua Bengio's group that
had an 18.6 word error rate.

01:04:07.370 --> 01:04:13.935
Our baseline for some reason was 12.9
where error rate then once we applied

01:04:13.935 --> 01:04:20.814
this technique of entropy regularization
that error rate went down to 10.5.

01:04:22.760 --> 01:04:25.850
There's different ways by which
you can do this regularization.

01:04:25.850 --> 01:04:27.849
One is, you just do entropy.

01:04:27.849 --> 01:04:31.025
And other is, you say the probability
distribution must look like

01:04:31.025 --> 01:04:33.985
the unigram probability
distributions of the alpha tokens.

01:04:33.985 --> 01:04:37.797
And that seems to work better than just
doing fully entropy regularization.

01:04:41.180 --> 01:04:43.080
So that's one problem.

01:04:43.080 --> 01:04:46.080
Another big problem that arises
during decoding is this lack of

01:04:46.080 --> 01:04:47.810
generative penalty.

01:04:47.810 --> 01:04:49.750
I think there was a slide
in your translation

01:04:51.650 --> 01:04:54.890
lecture which talked about
this in a different setting.

01:04:54.890 --> 01:04:58.420
But I'll talk about it
in the context of audio.

01:04:58.420 --> 01:05:00.770
When you have a very
long input sequence and

01:05:00.770 --> 01:05:05.280
you're decoding it one character
at a time, what you're

01:05:05.280 --> 01:05:09.920
doing is you're comparing your hypothesis
against all alternative hypothesis.

01:05:09.920 --> 01:05:16.140
So, every time you produce a new token
you pay a cost for that extra token.

01:05:16.140 --> 01:05:19.580
If your input's very long
then you're gonna obviously

01:05:20.670 --> 01:05:23.670
associate it with a long input,
have to produce a lot of tokens because

01:05:23.670 --> 01:05:26.470
probably the transcript that
you're producing is very long.

01:05:26.470 --> 01:05:28.670
So let's say you have to
produce 100 transcripts and

01:05:28.670 --> 01:05:30.780
you're paying an average cost of one.

01:05:30.780 --> 01:05:35.630
That means you're gonna pay a cost of
100 for even a correct transcript.

01:05:35.630 --> 01:05:40.630
Now In your beam search,
you'll probably get some examples

01:05:40.630 --> 01:05:45.900
which say the end-of-token symbol has
a probability less than minus 100.

01:05:45.900 --> 01:05:51.110
I think this is a very subtle point but
the upshot of it is,

01:05:51.110 --> 01:05:53.960
your model thinks it's okay
to terminate an output

01:05:55.020 --> 01:05:58.180
without even looking at the rest
of the input when it's not.

01:05:58.180 --> 01:05:59.829
And the reason this happens is,

01:05:59.829 --> 01:06:02.709
the model has no notion of
explaining the entire input.

01:06:02.709 --> 01:06:07.301
And because it doesn't have to explain the
entire input, it just terminates early.

01:06:07.301 --> 01:06:11.235
And very often you'll produce very short
output sequences when you should be

01:06:11.235 --> 01:06:13.363
producing very large output sequences.

01:06:16.569 --> 01:06:19.870
So to give an example,
if the output transcript is,

01:06:19.870 --> 01:06:24.824
chase is nigeria's registrar and
the society is an independent organization

01:06:24.824 --> 01:06:30.090
hired to count votes, If you look at
the language model cost, it's minus 108.

01:06:30.090 --> 01:06:33.820
If you look at the model cost it's,

01:06:33.820 --> 01:06:36.850
this is just the model from
the last model which is minus 34.

01:06:36.850 --> 01:06:40.130
And you look at the other alternatives,

01:06:40.130 --> 01:06:44.580
you get chase is Nigeria's registrar
which has a low cost of minus 31.

01:06:44.580 --> 01:06:49.900
So it's just happy It just produce a short
token, instead of this really long one.

01:06:49.900 --> 01:06:53.210
If you look at chase is Nigeria's or

01:06:53.210 --> 01:06:57.460
chase's nature is register,
that also has a small probability.

01:06:57.460 --> 01:07:01.540
In fact the best probability is just
to produce nothing, which is -12.5.

01:07:01.540 --> 01:07:06.446
So, this is tying up in
the issue where discriminative

01:07:06.446 --> 01:07:11.180
models don't explain the entire data and
so they can make such errors.

01:07:12.520 --> 01:07:17.230
What we found worked quite
simply was train as usual but

01:07:17.230 --> 01:07:22.618
during test time add a little
penalty which says I'm gonna try and

01:07:22.618 --> 01:07:27.650
make sure the sum of probabilities.

01:07:27.650 --> 01:07:31.390
Over a given output time step
is greater than a threshold.

01:07:31.390 --> 01:07:36.480
So look at all the frames of the input,
has someone looked at them or not?

01:07:36.480 --> 01:07:40.090
If there's enough frames that
nobody's looked at them or

01:07:40.090 --> 01:07:43.790
at least with some threshold tell,
then you pay a cost for that.

01:07:43.790 --> 01:07:47.780
And when you do that all these other
hypothesis that terminate early are now

01:07:47.780 --> 01:07:52.663
paying a lot of cost for every frame that
did not explain and so they fall down and

01:07:52.663 --> 01:07:57.180
our re-rank the out.

01:07:57.180 --> 01:08:00.626
So we need do this little trick,

01:08:00.626 --> 01:08:05.229
our model now really performs quite well.

01:08:05.229 --> 01:08:09.627
This model for sequence to sequence is now
able to get six point seven word error

01:08:09.627 --> 01:08:11.468
rate on the Wall Street Journal.

01:08:11.468 --> 01:08:16.660
Which is the lowest for end to end models
including CTC with a language model.

01:08:16.660 --> 01:08:21.290
It's very very promising in that sequence
to sequence can achieve such low numbers,

01:08:21.290 --> 01:08:23.040
with such low amounts of data.

01:08:25.750 --> 01:08:29.410
I should point out that this
really is changing the model.

01:08:29.410 --> 01:08:32.210
You train a model with one objective,
but during test time,

01:08:32.210 --> 01:08:36.790
you really fiddled with the loss
that you claimed was the best one.

01:08:36.790 --> 01:08:41.117
So, technically, there's something
wrong with the model that needs fixing.

01:08:45.701 --> 01:08:46.545
So finally,

01:08:46.545 --> 01:08:52.850
something very relevant to an NLP class is
what do you do about language models here?

01:08:52.850 --> 01:08:56.960
No matter how much audio you have,
you'll always have more text.

01:08:56.960 --> 01:09:00.620
And that text can be really
useful in correcting up errors

01:09:00.620 --> 01:09:01.610
that this model will have.

01:09:01.610 --> 01:09:04.580
So I could train on 2,000 hours of data,
but

01:09:04.580 --> 01:09:07.860
that would just be about 3
million small utterances, and

01:09:07.860 --> 01:09:12.170
the language model you learn from
that is not gonna be very powerful.

01:09:12.170 --> 01:09:15.283
So the speech organizer is going
to make a mistake no matter what.

01:09:25.467 --> 01:09:29.285
Since I'm an Nvidia now,
I cannot take claim, for that not working.

01:09:29.285 --> 01:09:33.683
&gt;&gt; [LAUGH]
&gt;&gt; So the question arises,

01:09:33.683 --> 01:09:37.098
how can we do better
language model blending?

01:09:37.098 --> 01:09:39.808
In these models cuz they're
really end to end models and

01:09:39.808 --> 01:09:43.860
you just are basically training the entire
tasks you're doing an acoustic model.

01:09:43.860 --> 01:09:48.692
You're doing a language model all in one
model, and now suddenly you're saying

01:09:48.692 --> 01:09:53.840
hey can I please revert back and find
ways to add my language model in here.

01:09:53.840 --> 01:09:56.390
Well one obvious way is to add

01:09:56.390 --> 01:10:00.050
the law of probabilities every time
you do a next stop prediction.

01:10:00.050 --> 01:10:03.190
You make a prediction for the next time
stamp, and then you blend in a language

01:10:03.190 --> 01:10:10.050
model prediction with it, and then
hopefully that fixes a lot of the errors.

01:10:10.050 --> 01:10:13.670
There's some cool work from
Yoshua Benjio's group which tries

01:10:13.670 --> 01:10:18.580
a bunch of other tricks to do this, which
they call shallow and deep fusion models.

01:10:20.310 --> 01:10:23.930
It's basically an extension of the simple
idea that I've just described but

01:10:23.930 --> 01:10:29.735
it kind of does a little bit more
in how the blending is done.

01:10:29.735 --> 01:10:34.625
Instead of just blending the actual
feature, the actual log probs,

01:10:34.625 --> 01:10:39.035
it uses a model that does a linear
projection, and it learns that together.

01:10:40.145 --> 01:10:44.850
I think fundamentally this is
an interesting approach where

01:10:44.850 --> 01:10:50.230
you basically have two streams of
predictions and you combine them.

01:10:50.230 --> 01:10:55.120
However, one of the things that's
lacking about this model is that

01:10:55.120 --> 01:10:57.980
the model in this case,
it's for translation.

01:10:57.980 --> 01:11:02.490
The translation model predictions
don't actually affect the internals

01:11:02.490 --> 01:11:04.810
of the language model and visa versa.

01:11:04.810 --> 01:11:07.736
What you would like to
do is have a model that,

01:11:07.736 --> 01:11:12.972
where diffusion actually means changing
of all the features that are computed,

01:11:12.972 --> 01:11:17.364
rather than just summing the logic,
but it's a pretty good start.

01:11:22.739 --> 01:11:25.510
I apologise, I forgot what the slide was.

01:11:27.540 --> 01:11:31.640
So when you're producing very long
sequences with next step prediction,

01:11:31.640 --> 01:11:35.160
what it's doing is just
looking at the next token, and

01:11:35.160 --> 01:11:36.790
I highlighted why that can be a problem.

01:11:36.790 --> 01:11:40.110
For example, when you producing words
If you produced two characters,

01:11:40.110 --> 01:11:44.990
the next one is almost necessarily
just decided right away.

01:11:44.990 --> 01:11:49.440
So when you have a lost function
that's just looking at the next step,

01:11:49.440 --> 01:11:51.350
it doesn't have a very long horizon.

01:11:51.350 --> 01:11:52.510
When it makes a mistake here,

01:11:53.610 --> 01:11:57.640
it might just go into a wrong path
that it can never correct out of.

01:11:57.640 --> 01:12:00.790
So, people have been looking
at how to fix this problem.

01:12:00.790 --> 01:12:02.780
One of the ways is scheduled sampling.

01:12:02.780 --> 01:12:04.870
And I think you guys
have looked at this or

01:12:04.870 --> 01:12:07.030
at least talked about
this in the last lecture.

01:12:07.030 --> 01:12:12.480
But what it does is, instead of taking
the model and feeding in the ground

01:12:12.480 --> 01:12:16.250
truth at every time step, you feed in
the predictions of the model itself.

01:12:16.250 --> 01:12:17.980
Or you sample from the model.

01:12:17.980 --> 01:12:21.850
So, what you're learning is,
during test time I'm going to screw up and

01:12:21.850 --> 01:12:23.580
feed in something wrong.

01:12:23.580 --> 01:12:25.560
At training time, let me do the same so

01:12:25.560 --> 01:12:28.110
that I'm resilient to
that kind of mistake.

01:12:28.110 --> 01:12:31.182
What that does is it's actually
changing your model so

01:12:31.182 --> 01:12:33.867
it respects long range
structure much better.

01:12:33.867 --> 01:12:38.039
There are other methods also which would
be based on reinforcement learning that

01:12:38.039 --> 01:12:40.133
optimize the word error rate directly.

01:12:40.133 --> 01:12:44.840
I wouldn't talk about this at all, but
other than to say that it's another way

01:12:44.840 --> 01:12:49.621
of letting your model roll forward and
generate a bunch of different candidates

01:12:49.621 --> 01:12:54.070
and than compute an error based on
the candidates that you've computed.

01:12:55.300 --> 01:12:56.786
A very cool paper is this one,

01:12:56.786 --> 01:13:00.007
called Sequence-to-Sequence
as Beam Search Optimization.

01:13:00.007 --> 01:13:04.521
And this kind of also runs your model
forward, but with some interesting

01:13:04.521 --> 01:13:09.331
tricks where it runs the model forward
until you've made some mistakes, and

01:13:09.331 --> 01:13:11.334
then it stops and does it again.

01:13:13.669 --> 01:13:19.790
So finally, what can we do with this
model that we couldn't have done before?

01:13:20.940 --> 01:13:24.901
There's some things that you can do,
one of which is multi-speaker,

01:13:24.901 --> 01:13:28.280
multi-channel setup that
people don't really do yet.

01:13:28.280 --> 01:13:32.400
So, flashback to some years ago,
the motivating problem for

01:13:32.400 --> 01:13:36.410
speech recognition was this thing
called the cocktail party problem.

01:13:36.410 --> 01:13:40.080
You wanted to be in a room walking around
and listening to a bunch of things

01:13:40.080 --> 01:13:43.530
happening and get your recognizer
to produce all the entire output.

01:13:45.010 --> 01:13:48.552
Somewhere along the way, people
forgot about that particular problem.

01:13:48.552 --> 01:13:53.180
And they've been focused on just single
microphone set-ups where you're stationary

01:13:53.180 --> 01:13:55.056
with respect to another speaker and

01:13:55.056 --> 01:13:57.845
you kind of produce only
one output as a transcript.

01:13:57.845 --> 01:14:01.935
I tell the reason this happened is
because you have a generative model,

01:14:01.935 --> 01:14:05.140
traditionally, which has
some transcription mind and

01:14:05.140 --> 01:14:07.469
then it generative describes the data.

01:14:07.469 --> 01:14:12.341
And that's not very natural way to sort
of do the inverse problem where you

01:14:12.341 --> 01:14:16.160
can mix in a bunch of people
in many different ways.

01:14:16.160 --> 01:14:19.980
The inverse problem is just follow
track one particular individual and

01:14:19.980 --> 01:14:22.194
that's much easier to do than try and

01:14:22.194 --> 01:14:26.100
sort of invert a generative model
where you have a bunch of sources.

01:14:26.100 --> 01:14:31.095
So a model, such as this, such as
sequence model should work very well

01:14:31.095 --> 01:14:35.273
in trying to do multi-speaker
multi-channel set up.

01:14:36.910 --> 01:14:39.510
And then there's this really cool
paper that came out recently,

01:14:39.510 --> 01:14:44.550
which talks about direct translation and
transcription at the same time.

01:14:44.550 --> 01:14:49.560
So, it takes in audio in French and
it produces

01:14:49.560 --> 01:14:53.450
English text as an output with just one
sequence-to-sequence model, which just

01:14:53.450 --> 01:14:58.140
blends the last model and a translation
model together, which is quite exciting.

01:14:59.380 --> 01:15:03.220
If you look at their paper, it's got
this really exciting attention plot.

01:15:03.220 --> 01:15:06.630
So if you take the neural machine
translation attention model,

01:15:06.630 --> 01:15:11.650
they're translating how much is
the breakfast to com bien coute

01:15:11.650 --> 01:15:13.460
le petit dejeuner, whatever.

01:15:13.460 --> 01:15:21.510
In French, it's how much does
the little breakfast cost, or whatever.

01:15:21.510 --> 01:15:26.040
If you look at the attention model,
it's looking at the right words.

01:15:26.040 --> 01:15:30.399
How it pairs with combien,
which means how in French and

01:15:30.399 --> 01:15:33.810
much pairs with the two of them as well.

01:15:33.810 --> 01:15:38.450
If you look at the corresponding
attention model from the wave form and

01:15:38.450 --> 01:15:41.900
the text,
it also focuses on the same sort of parts.

01:15:41.900 --> 01:15:46.987
Even through it was trained in a different
way, with a different modality as input.

01:15:46.987 --> 01:15:50.713
So, it's really cool that is able
to learn, to focus on the audio,

01:15:50.713 --> 01:15:53.272
even though it’s translating
at the same time.

01:15:57.110 --> 01:15:58.273
And so, I'd like to end here.

01:15:58.273 --> 01:16:01.370
There's some acknowledgements.

01:16:01.370 --> 01:16:07.210
Most of this work was done at Brain,
at least the ones I was involved in.

01:16:08.460 --> 01:16:13.715
And I've had the luck to work
with some phenomenal interns.

01:16:13.715 --> 01:16:19.635
Yeah, Google Brain gets a very fantastic
bunch of students going by and

01:16:19.635 --> 01:16:22.495
I'd be lucky to work with them.

01:16:22.495 --> 01:16:25.715
And the Google Brain team is
a phenomenal place to work at.

01:16:25.715 --> 01:16:27.776
So, I want to thank them.

01:16:27.776 --> 01:16:34.540
&gt;&gt; [APPLAUSE]

