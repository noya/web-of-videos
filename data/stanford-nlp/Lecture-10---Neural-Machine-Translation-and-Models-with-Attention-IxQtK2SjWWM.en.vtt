WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.499
[SOUND] Stanford University.

00:00:09.584 --> 00:00:14.923
&gt;&gt; Okay, so we're back again with CS224N.

00:00:14.923 --> 00:00:17.200
So Let's see.

00:00:17.200 --> 00:00:21.100
So in terms of what we're gonna do today,
I mean

00:00:21.100 --> 00:00:25.775
I think it's gonna be a little bit muddled
up, and going forwards and backwards.

00:00:25.775 --> 00:00:30.155
Officially in the syllabus
today's lecture is,

00:00:30.155 --> 00:00:32.975
sequence to sequence models and attention.

00:00:32.975 --> 00:00:36.795
And next Tuesday's lecture
is machine translation.

00:00:36.795 --> 00:00:38.505
But really,

00:00:38.505 --> 00:00:43.500
Richard already started saying some things
about machine translation last week.

00:00:43.500 --> 00:00:46.320
And so I thought for
various reasons, it probably makes

00:00:46.320 --> 00:00:51.360
sense to also be saying more stuff
about machine translation today.

00:00:51.360 --> 00:00:53.220
So expect that.

00:00:53.220 --> 00:00:58.090
But I am gonna cover the main content of
what was meant to be in today's lecture,

00:00:58.090 --> 00:00:59.880
and talk about attention today.

00:00:59.880 --> 00:01:02.790
And that's a really useful
thing to know about.

00:01:02.790 --> 00:01:07.417
I mean almost certainly if you're gonna
be doing anything in the space of sort of

00:01:07.417 --> 00:01:11.840
reading comprehension, question
answering models, such as, for instance,

00:01:11.840 --> 00:01:12.959
assignment four.

00:01:12.959 --> 00:01:16.921
But also kinds of things that a whole
bunch of people have proposed for

00:01:16.921 --> 00:01:21.308
final projects, you definitely
wanna know about and use attention.

00:01:21.308 --> 00:01:24.620
But then I actually thought
I'd do a little bit of

00:01:24.620 --> 00:01:27.056
going backwards next Tuesday.

00:01:27.056 --> 00:01:32.310
And I want to go back and actually say a
bit more about these kind of gated models,

00:01:32.310 --> 00:01:36.550
like the GRUs and
LSTMs that become popular lately.

00:01:36.550 --> 00:01:41.450
and try and have a bit more of a go at
saying just a little bit more about well,

00:01:41.450 --> 00:01:43.553
why do people do this and
why does it work?

00:01:43.553 --> 00:01:48.300
And see if I can help make it
a little bit more intelligible.

00:01:48.300 --> 00:01:50.790
So we'll mix around between those topics.

00:01:50.790 --> 00:01:55.850
But somehow over these two weeks of
classes, we're doing recurrent models,

00:01:55.850 --> 00:01:59.390
attention, MT and
all those kinds of things.

00:02:00.700 --> 00:02:01.710
Okay.

00:02:01.710 --> 00:02:04.050
Other reminders and comments.

00:02:04.050 --> 00:02:06.655
So the midterm is over yay!

00:02:06.655 --> 00:02:12.671
And your dear TAs and me spent all
last night grading that midterm.

00:02:12.671 --> 00:02:18.151
So we're sort of 99%
over with the midterm.

00:02:18.151 --> 00:02:23.139
There's a slight catch that a couple
of people haven't done it yet,

00:02:23.139 --> 00:02:26.710
because of various complications.

00:02:26.710 --> 00:02:31.375
So essentially next Tuesday is,
when we're gonna be able to be sort of

00:02:31.375 --> 00:02:35.390
releasing solutions to the midterm and
handing them back.

00:02:35.390 --> 00:02:37.440
Some people did exceedingly well.

00:02:37.440 --> 00:02:40.610
The highest score was extremely high 90s.

00:02:40.610 --> 00:02:42.230
Most people did pretty well.

00:02:42.230 --> 00:02:44.120
It has a decent median.

00:02:44.120 --> 00:02:45.645
A few few not so well.

00:02:45.645 --> 00:02:48.140
[LAUGH] You know how what
these things are like.

00:02:48.140 --> 00:02:53.370
But yeah, overall we're pretty
pleased with how people did in it.

00:02:53.370 --> 00:02:56.746
I just thought I should
mention one other issue,

00:02:56.746 --> 00:03:00.460
which I will say sort of
send a Piazza note about.

00:03:00.460 --> 00:03:05.610
I mean I know that a few people
were quite unhappy with the fact

00:03:05.610 --> 00:03:10.870
that some students kept on writing
after the official end of the exam.

00:03:10.870 --> 00:03:13.250
And I mean I totally understand that.

00:03:13.250 --> 00:03:14.650
Because the fact of the matter is,

00:03:14.650 --> 00:03:19.700
these kind of short midterm exams
do end up quite time-limited,

00:03:19.700 --> 00:03:25.000
and many people feel like they could
do more if they had more time.

00:03:25.000 --> 00:03:27.310
I mean on the other hand,

00:03:27.310 --> 00:03:33.310
I honestly feel like I don't know
quite what to do about this problem.

00:03:33.310 --> 00:03:40.638
Both Richard and me came from educational
traditions, where we had exam proctors.

00:03:40.638 --> 00:03:45.406
And when it was time to put your
pens down, you put your pens down or

00:03:45.406 --> 00:03:48.290
else dire consequences happen to you.

00:03:48.290 --> 00:03:51.980
Whereas my experience at Stanford is that,

00:03:51.980 --> 00:03:57.380
every exam I've ever been in at Stanford,
there are people who keep writing

00:03:57.380 --> 00:04:02.290
until you forcibly remove
the exam out of their hands.

00:04:02.290 --> 00:04:05.740
And so there seems to be
a different tradition here.

00:04:05.740 --> 00:04:10.840
And in theory this is meant to be
student regulated by the honor code,

00:04:10.840 --> 00:04:15.930
but we all know that there are some
complications there as well.

00:04:15.930 --> 00:04:19.006
So it's not that I'm not
sensitive to the issue.

00:04:19.006 --> 00:04:24.000
And you know really exactly what
I said to the TAs before the end

00:04:24.000 --> 00:04:28.600
of the exam is, so it's a real problem
at Stanford, people going on writing, so

00:04:28.600 --> 00:04:30.670
could everyone get in the room
as quickly as possible,

00:04:30.670 --> 00:04:33.780
and collect everyone's exams
to minimize that problem.

00:04:33.780 --> 00:04:38.080
But obviously, it's a little bit
difficult when there are 680 students.

00:04:38.080 --> 00:04:40.422
But we did the best we could.

00:04:40.422 --> 00:04:43.710
And I think basically we
have to proceed with that.

00:04:44.810 --> 00:04:45.310
Okay.

00:04:46.570 --> 00:04:47.760
Other topics.

00:04:47.760 --> 00:04:50.272
Assignment three is looming.

00:04:50.272 --> 00:04:53.470
Apologies that we were a bit
late getting that out.

00:04:53.470 --> 00:04:57.090
Though with the midterm,
it wouldn't have made much difference.

00:04:57.090 --> 00:05:00.470
We have put a little bit of
extension to assignment three.

00:05:00.470 --> 00:05:07.116
I guess we're really nervous about,
giving more extension to assignment three.

00:05:07.116 --> 00:05:10.774
Not because we don't want you to have
time to do assignment three, but

00:05:10.774 --> 00:05:15.238
just because we realized that anything we
do is effectively stealing days away from

00:05:15.238 --> 00:05:18.652
the time you have to do the final
project or assignment four.

00:05:18.652 --> 00:05:20.880
So we don't wanna do that too much.

00:05:20.880 --> 00:05:23.703
We hope that assignment
three isn't too bad.

00:05:23.703 --> 00:05:26.465
And the fact that you can do
it in teams can help, and

00:05:26.465 --> 00:05:28.780
that that won't be such a problem.

00:05:28.780 --> 00:05:32.800
Another thing that we want people
to do but are a bit behind on, but

00:05:32.800 --> 00:05:38.440
I hopefully can get in place tomorrow, is
giving people access to Microsoft Azure,

00:05:38.440 --> 00:05:41.580
to be able to use GPUs
to do the assignments.

00:05:41.580 --> 00:05:44.900
We really do want people to do that for
assignment three.

00:05:44.900 --> 00:05:49.245
Since it's just great experience to have
and will be useful to know about, for

00:05:49.245 --> 00:05:52.410
then going on for assignment four and
the final project.

00:05:52.410 --> 00:05:55.570
So we hope we can have
that in place imminently.

00:05:55.570 --> 00:06:00.340
And it really will allow you to do things
much quicker for assignment three.

00:06:00.340 --> 00:06:02.410
So the kind of models
that you're building for

00:06:02.410 --> 00:06:07.230
assignment three, should run at least
an order of magnitude, sort of ten,

00:06:07.230 --> 00:06:11.960
12 times or something faster, if you're
running them on a GPU rather than a CPU.

00:06:11.960 --> 00:06:14.280
So look forward to
hearing more about that.

00:06:14.280 --> 00:06:20.630
The final reminder I want to mention is,
I'm really really encouraging people

00:06:20.630 --> 00:06:26.090
to come to final project office hours for
discussion.

00:06:26.090 --> 00:06:31.081
Richard was really disappointed how few
people came to talk to him about final

00:06:31.081 --> 00:06:33.590
projects on Tuesday after the exam.

00:06:33.590 --> 00:06:36.430
Now maybe that's quite
understandable why no one turned up.

00:06:36.430 --> 00:06:43.070
But at any rate moving forward from here,
I really really encourage you to do that.

00:06:43.070 --> 00:06:48.010
So I have final project office
hours tomorrow from one to three.

00:06:48.010 --> 00:06:51.380
Richard is gonna be doing
it again next Tuesday.

00:06:51.380 --> 00:06:56.340
The various other PhD students
having their office hours.

00:06:56.340 --> 00:07:00.590
So really do for the rest of quarter,
try and get along to those.

00:07:00.590 --> 00:07:04.755
and check in on projects
as often as possible.

00:07:04.755 --> 00:07:09.513
And in particular, make really really sure
that either next week or the week after,

00:07:09.513 --> 00:07:14.840
that you do talk to your project mentor,
to find out their advice on the project.

00:07:14.840 --> 00:07:16.060
Okay, all good?

00:07:16.060 --> 00:07:17.475
Any questions?

00:07:20.788 --> 00:07:25.090
Okay, so
let's get back into machine translation.

00:07:25.090 --> 00:07:29.937
And I just thought I'd sort of say
a couple of slides of how important

00:07:29.937 --> 00:07:31.818
is machine translation.

00:07:31.818 --> 00:07:36.743
Now really a large percentage of
the audience of these Stanford classes

00:07:36.743 --> 00:07:38.722
are not American citizens.

00:07:38.722 --> 00:07:43.385
So probably a lot of you realize that,
machine translation is important.

00:07:43.385 --> 00:07:48.080
But for the few of you that
are native-born American citizens.

00:07:48.080 --> 00:07:52.800
I think a lot of native-born Americans
are sort of, very unaware of

00:07:52.800 --> 00:07:58.590
the importance of translation, because
they live in an English-only world.

00:07:58.590 --> 00:08:00.110
Where most of the resources for

00:08:00.110 --> 00:08:05.170
information are available in English, and
America is this, sort of, a big enough

00:08:05.170 --> 00:08:09.590
place that you're not often dealing with
stuff outside the rest of the world.

00:08:09.590 --> 00:08:14.900
But really in general, for
humanity and commerce, translation,

00:08:14.900 --> 00:08:19.530
in general, and machine translation in
particular, are just huge things, right?

00:08:19.530 --> 00:08:24.920
That for places like the European Union
to run, is completely dependent on having

00:08:24.920 --> 00:08:30.480
translation happen, so it can work across
the many languages of the European Union.

00:08:30.480 --> 00:08:34.560
So, the translation industry is
a $40 billion a year industry.

00:08:34.560 --> 00:08:38.690
And that's basically the amount
that's spent on human translation,

00:08:38.690 --> 00:08:42.920
because most of what's done as
machine translation at the moment

00:08:42.920 --> 00:08:47.090
is in the form of free services,
and so it's a huge issue in Europe,

00:08:47.090 --> 00:08:52.020
it's growing in Asia, lots of needs in
every domain, as well as commercial,

00:08:52.020 --> 00:08:55.010
there's social, government,
and military needs.

00:08:55.010 --> 00:08:59.890
And so the use of machine translation
has itself become a huge thing.

00:08:59.890 --> 00:09:04.800
So Google now translate over 100
billion words per day, right?

00:09:04.800 --> 00:09:09.458
There are a lot of people that
are giving Google stuff to translate.

00:09:09.458 --> 00:09:14.029
It's then important for
things like having social connections.

00:09:14.029 --> 00:09:15.263
So I mean in 2016,

00:09:15.263 --> 00:09:20.740
last year Facebook rolled out their
own homegrown machine translation.

00:09:20.740 --> 00:09:24.260
Prior to that they've made use of
other people's translation, but

00:09:24.260 --> 00:09:28.390
essentially what they had found was that
the kind of commercial machine translation

00:09:28.390 --> 00:09:33.620
offerings didn't do a very good job
at translating social chit chat.

00:09:33.620 --> 00:09:37.530
And the fact of the matter is that doing
a better job at that is sufficiently

00:09:37.530 --> 00:09:42.130
important to a company like Facebook that
they're developing their own in house

00:09:42.130 --> 00:09:44.260
machine translation to do it.

00:09:44.260 --> 00:09:48.080
One of the quotes that came along with
that was when they were testing it and

00:09:48.080 --> 00:09:52.890
turned off the machine translation for
some users, that they really went nuts,

00:09:52.890 --> 00:09:56.400
that lots of people really
do actually depend on this.

00:09:56.400 --> 00:09:57.880
Other areas as well.

00:09:57.880 --> 00:10:02.850
So eBay makes extensive use of machine
translation to enable cross-border trade.

00:10:02.850 --> 00:10:06.260
So that if you are going
to be able to successfully

00:10:06.260 --> 00:10:08.910
sell products in different markets,
well, you have

00:10:08.910 --> 00:10:13.940
to be able to translate the descriptions
into things that people can read.

00:10:13.940 --> 00:10:18.010
Okay, and so that then leads us into
what we're gonna be focusing on here,

00:10:18.010 --> 00:10:20.670
which is neural machine translation.

00:10:20.670 --> 00:10:27.320
And so, neural machine translation or NMT
is sort of a commonly used slogan name.

00:10:27.320 --> 00:10:32.450
And it's come to have a sort of
a particular meaning that's slightly more

00:10:32.450 --> 00:10:34.970
than neural plus machine translation.

00:10:34.970 --> 00:10:40.160
That neural machine translation is
used to mean what we want to do

00:10:40.160 --> 00:10:45.320
is build one big neural
network which we can train

00:10:45.320 --> 00:10:51.430
the entire end-to-end machine translation
process in and optimize end to end.

00:10:51.430 --> 00:10:55.790
And so systems that do that are then
what are referred to as an MT system.

00:10:55.790 --> 00:10:58.220
So that the kind of picture here

00:10:58.220 --> 00:11:01.200
is that we're going to have
this big neural network.

00:11:01.200 --> 00:11:04.430
It's gonna take input text that's
somehow going to encode into

00:11:04.430 --> 00:11:06.070
neural network vectors.

00:11:06.070 --> 00:11:11.010
It's then gonna have a decoder and
out would come text at the end.

00:11:11.010 --> 00:11:13.630
And so we get these
encoder-decoder architectures.

00:11:15.040 --> 00:11:19.800
Before getting into the modern stuff,
I thought I'd take two slides

00:11:19.800 --> 00:11:24.700
to tell you about the archaeology
of neural networks.

00:11:28.520 --> 00:11:33.410
Neural networks had sorta been very
marginal or dead as a field for

00:11:33.410 --> 00:11:35.120
a couple of decades.

00:11:35.120 --> 00:11:39.140
And so I think a lot of the time people
these days think of deep learning

00:11:39.140 --> 00:11:43.570
turned up around 2012,
with the ImageNet breakthroughs.

00:11:43.570 --> 00:11:46.060
And boy has it been amazing since then.

00:11:46.060 --> 00:11:48.540
But really there have been
earlier ages of neural networks.

00:11:48.540 --> 00:11:53.441
And in particular there's a boom in the
use of neural networks in the second half

00:11:53.441 --> 00:11:57.905
of the 80s into the early 90s which
corresponds to when Rumelhart and

00:11:57.905 --> 00:12:03.101
McClelland, so that's the James McClelland
that's still in the Psych Department

00:12:03.101 --> 00:12:07.930
at Stanford, pioneered or re-pioneered
the use of neural networks partly as

00:12:07.930 --> 00:12:11.790
a cognitive science tool, but
also as a computing tool.

00:12:11.790 --> 00:12:15.210
And many of the technologies
that we've been talking about

00:12:15.210 --> 00:12:18.420
really the math of them are worked
out during that period.

00:12:18.420 --> 00:12:23.174
So it was in the 80s there was really
worked out of how to do general back

00:12:23.174 --> 00:12:27.310
propagation algorithms for
multi-layer neural networks.

00:12:27.310 --> 00:12:30.350
And it was also during
that period when people

00:12:30.350 --> 00:12:33.510
worked out how to do the math
of recurrent neural networks.

00:12:33.510 --> 00:12:39.070
So algorithms like backpropagation through
time were worked out in this period,

00:12:39.070 --> 00:12:43.790
in the late 80s, often by people who were
psychologists, cognitive scientists,

00:12:43.790 --> 00:12:47.440
rather than hard core CS
people in those days.

00:12:47.440 --> 00:12:52.932
And so, also in that period, was
actually when neural MT, in having these

00:12:52.932 --> 00:12:58.710
encoder decoder architectures for
doing translation, was first tried out.

00:12:58.710 --> 00:13:02.980
The systems that were built were
incredibly primitive and limited,

00:13:02.980 --> 00:13:07.420
which partly reflects the computational
resources of those days.

00:13:07.420 --> 00:13:10.240
But they still were in
coder/decoder architectures.

00:13:10.240 --> 00:13:15.340
So as far as I've been able to work out,
the first neural MT system was this system

00:13:15.340 --> 00:13:18.300
that was done by Bob Allen in 1987,

00:13:18.300 --> 00:13:23.360
the very first international
conference on neural networks, and so

00:13:23.360 --> 00:13:29.580
he constructed 3,000 English/Spanish
pairs over a tiny vocabulary.

00:13:29.580 --> 00:13:31.850
Sort of a 30 to 40 word vocabulary and

00:13:31.850 --> 00:13:35.640
the sentences were actually kind of
constructed based on the grammar,

00:13:35.640 --> 00:13:40.253
it wasn't actually kind of we'll just
collect together human language use, but

00:13:40.253 --> 00:13:45.160
you know you sort of had sentences like
this with some variation of word order and

00:13:45.160 --> 00:13:46.390
things like that.

00:13:46.390 --> 00:13:50.610
And he built this simple encoded decoded

00:13:50.610 --> 00:13:54.920
network that you can see on the right
that was not a recurrent model.

00:13:54.920 --> 00:13:59.010
You just had sort of a binary
representation of the sequence of words in

00:13:59.010 --> 00:14:05.490
a sentence and the sentences were only
short and then were pumped through that.

00:14:05.490 --> 00:14:09.780
A few years after that, Lonnie Chrisman.

00:14:09.780 --> 00:14:12.420
Lonnie Chrisman is actually
a guy who lives in the Bay Area.

00:14:12.420 --> 00:14:15.535
He works at a tech firm still to this day.

00:14:15.535 --> 00:14:18.940
[LAUGH] Not doing neural networks anymore.

00:14:18.940 --> 00:14:24.300
So Lonnie Chrisman in the early 90s
then developed a more sophisticated

00:14:25.310 --> 00:14:31.650
neural network architecture for
doing encoded decoder MT architecture.

00:14:31.650 --> 00:14:38.510
So he was using this model called
RAAMS Recursive Auto Associative Memories

00:14:38.510 --> 00:14:42.530
which were developed in the early 90s.

00:14:42.530 --> 00:14:45.280
Not worth explaining the details of them.

00:14:45.280 --> 00:14:48.472
But a RAAM is in some sense kind of
like recurrent network of the kind

00:14:48.472 --> 00:14:50.740
that we've already started to look at.

00:14:50.740 --> 00:14:52.990
And he was building those ones.

00:14:52.990 --> 00:14:55.680
And so that then leads into our modern

00:14:55.680 --> 00:15:00.050
encoder decoder architectures
that Richard already mentioned.

00:15:00.050 --> 00:15:04.470
Where we're having, perhaps a recurrent
network that's doing the encoding and

00:15:04.470 --> 00:15:09.790
then another recurrent network there's
then decoding out in another language.

00:15:09.790 --> 00:15:13.000
And where in reality they're not
normally as simple as this, and

00:15:13.000 --> 00:15:17.210
we have more layers and more stuff,
and it all gets more complicated.

00:15:18.310 --> 00:15:23.085
I just wanted to mention
quickly a couple more

00:15:23.085 --> 00:15:25.565
things about the space of these things.

00:15:25.565 --> 00:15:28.355
So you can think of what
these encoder decoder

00:15:28.355 --> 00:15:32.587
architectures are as a conditional
recurrent language model.

00:15:32.587 --> 00:15:36.267
So if we want to generate a translation,

00:15:36.267 --> 00:15:41.767
we're encoding the source so
we're producing a Y from the source.

00:15:41.767 --> 00:15:46.050
And then from that Y
we're going to decode,

00:15:46.050 --> 00:15:50.500
we're going to run a recurrent neural
network to produce the translation.

00:15:50.500 --> 00:15:56.770
And so you can think of that decoder there
as a conditional recurrent language model.

00:15:56.770 --> 00:16:00.094
So it's essentially being a language
model that's generating forward

00:16:00.094 --> 00:16:01.700
as a recurrent language model.

00:16:01.700 --> 00:16:05.609
And the only difference from
any other kind of recurrent or

00:16:05.609 --> 00:16:10.207
neural language model is that you're
conditioning on one other thing,

00:16:10.207 --> 00:16:14.840
that you've calculated this Y
based on the source sentence.

00:16:14.840 --> 00:16:17.290
And that's the only
architecture difference.

00:16:18.410 --> 00:16:21.673
So if we then look down into
the details a little bit,

00:16:21.673 --> 00:16:25.023
there are different ways
that you can do the encoder.

00:16:25.023 --> 00:16:30.534
The most common way to do the encoder has
been with these gated recurrent units,

00:16:30.534 --> 00:16:32.681
whether the GRUs or the LSTMs,

00:16:32.681 --> 00:16:38.100
which are another kind of gated recurrent
unit that Richard talked about last time.

00:16:38.100 --> 00:16:40.191
I mean, people have tried other things.

00:16:40.191 --> 00:16:44.431
I mean the modern resurgence
of neural machine translation,

00:16:44.431 --> 00:16:49.126
actually the very first paper that
tried to do it was this paper by

00:16:49.126 --> 00:16:54.080
Nal Kalchbrenner and Phil Blunsom
who now both work at DeepMind.

00:16:54.080 --> 00:16:55.525
And they actually for

00:16:55.525 --> 00:17:00.992
their encoder they were using a recurrent
sequence of convolutional networks.

00:17:00.992 --> 00:17:04.810
Not the kind of gated recurrent
networks that we talked about.

00:17:04.810 --> 00:17:08.160
And sometime later in the course
we'll talk a bit more

00:17:08.160 --> 00:17:11.310
about convolutional networks and
how they're used in language.

00:17:11.310 --> 00:17:14.425
They're not nearly as much used
in language, they're much,

00:17:14.425 --> 00:17:15.732
much more used in Vision.

00:17:15.732 --> 00:17:20.322
And so if next quarter you do CS231N and
get even more neural

00:17:20.322 --> 00:17:25.300
networks then you'll spend way more of
the time on convolutional networks.

00:17:25.300 --> 00:17:30.177
But the one other idea I sort of
wanted to just sort of put out

00:17:30.177 --> 00:17:34.460
there is sort of another
concept to be aware of.

00:17:34.460 --> 00:17:40.940
So we have this Y that we've
encoded the source with.

00:17:40.940 --> 00:17:44.320
And then there's this
question of how you use that.

00:17:44.320 --> 00:17:49.366
So for the models that we've shown
up until now and that Richard had,

00:17:49.366 --> 00:17:53.547
essentially what happened was
we calculated up to here.

00:17:53.547 --> 00:17:55.238
This was our y.

00:17:55.238 --> 00:18:01.181
And we just used the y as the starting
point of the hidden layer,

00:18:01.181 --> 00:18:04.112
and then we started to decode.

00:18:04.112 --> 00:18:08.918
So this was effectively the Google
tradition of the way of doing it,

00:18:08.918 --> 00:18:12.727
the model that Sutskever
et al proposed in 2014.

00:18:12.727 --> 00:18:17.712
And so effectively, if you're doing
it this way, you're putting most of

00:18:17.712 --> 00:18:22.190
the pressure on the forget gates
not doing too much forgetting.

00:18:22.190 --> 00:18:27.001
Because you have the entire knowledge
of the source sentence here.

00:18:27.001 --> 00:18:31.926
And you have to make sure you're carrying
enough of it along through the network.

00:18:31.926 --> 00:18:36.816
That you'll be able to continue to access
the source sentence's semantics all

00:18:36.816 --> 00:18:40.463
the way through your generation
of the target sentence.

00:18:40.463 --> 00:18:45.430
So it's especially true in
that case that you will really

00:18:45.430 --> 00:18:50.032
lose badly if you got something
like a plain recurrent neural

00:18:50.032 --> 00:18:53.808
network which isn't very good
at having a medium term memory.

00:18:53.808 --> 00:18:57.045
And you can do much better
with something like an LSTM.

00:18:57.045 --> 00:19:00.995
Which is much more able to
maintain a medium term memory with

00:19:00.995 --> 00:19:04.550
the sort of ideas that Richard
started to talk about.

00:19:05.660 --> 00:19:09.270
But that isn't actually
the only way of doing it.

00:19:09.270 --> 00:19:14.200
And so the other pioneering work
in neural machine translation

00:19:14.200 --> 00:19:18.110
was work that was done at the University
of Montreal by Kyunghyun Cho and

00:19:18.110 --> 00:19:21.213
colleagues and
that wasn't actually the way they did it.

00:19:21.213 --> 00:19:25.071
The way they did it was once
they'd calculated the Y as

00:19:25.071 --> 00:19:27.656
the representation of the source.

00:19:27.656 --> 00:19:34.100
They fed that Y into every time step
during the period of generation.

00:19:34.100 --> 00:19:38.190
So when you were generating at each state,
you were getting a hidden

00:19:38.190 --> 00:19:41.874
representation which was kind
of just your language model.

00:19:41.874 --> 00:19:43.571
And then you were getting two inputs.

00:19:43.571 --> 00:19:48.587
You were getting one input which
was the previous word, the x_t.

00:19:48.587 --> 00:19:50.733
And then you were getting a second input,

00:19:50.733 --> 00:19:53.136
which was the y that you
were conditioning on.

00:19:53.136 --> 00:19:57.326
So you were directly feeding that
conditioning in at every time step.

00:19:57.326 --> 00:20:01.082
And so then you're less dependent on
having to sort of preserve it along

00:20:01.082 --> 00:20:03.030
the whole sequence.

00:20:03.030 --> 00:20:07.954
And in a way having the input
available at every time step,

00:20:07.954 --> 00:20:10.577
that seems to be a useful idea.

00:20:10.577 --> 00:20:15.046
And so that's actually the idea that will
come back when I talk about attention.

00:20:15.046 --> 00:20:19.820
That attention is again going to give
us a different mechanism of getting at

00:20:19.820 --> 00:20:21.500
the input when we need it.

00:20:21.500 --> 00:20:23.070
And to being able to condition on it.

00:20:24.350 --> 00:20:28.958
Let me just sort of give you
a couple more pictures and

00:20:28.958 --> 00:20:34.421
a sense of how exciting neural
machine translation has been.

00:20:34.421 --> 00:20:39.215
So for machine translation,
there are a couple of prominent

00:20:39.215 --> 00:20:43.356
evaluations of machine
translation that are done.

00:20:43.356 --> 00:20:47.014
But I mean I think the most prominent
one has been done by what's called

00:20:47.014 --> 00:20:49.290
the workshop on machine translation.

00:20:49.290 --> 00:20:51.235
Which has a yearly evaluation.

00:20:51.235 --> 00:20:54.033
And so this is showing results from that.

00:20:54.033 --> 00:20:58.326
And most of the results shown
are the results from Edinburgh Systems.

00:20:58.326 --> 00:21:02.200
And the University of Edinburgh's
traditionally been one of the strongest

00:21:02.200 --> 00:21:04.570
universities at doing machine translation.

00:21:04.570 --> 00:21:06.600
And they have several systems.

00:21:06.600 --> 00:21:13.045
And so what we can see from these results,
up is good of machine translation quality.

00:21:13.045 --> 00:21:18.204
So we have the phrase-based syntactic
machine translation systems

00:21:18.204 --> 00:21:24.513
which is the kind of thing that you saw
on Google Translate until November 2016.

00:21:24.513 --> 00:21:27.825
That although they work reasonably,

00:21:27.825 --> 00:21:32.904
there is sort of a feeling that
although they are a pioneering

00:21:32.904 --> 00:21:37.351
a good use of large data
machine learning systems.

00:21:37.351 --> 00:21:41.000
That they had kind of stalled.

00:21:41.000 --> 00:21:45.206
So there really was very little
progress in phrase-based machine

00:21:45.206 --> 00:21:47.692
translation systems in recent years.

00:21:47.692 --> 00:21:52.506
Until neural machine translation came
along, the idea that people were most

00:21:52.506 --> 00:21:56.955
actively exploring was building
syntax-based statistical machine

00:21:56.955 --> 00:22:02.310
translation systems, which made more
use of the structure of language.

00:22:02.310 --> 00:22:06.990
They were improving a little bit
more quickly but not very quickly.

00:22:06.990 --> 00:22:11.100
How quickly kind of partly depends
on how you draw that line.

00:22:11.100 --> 00:22:15.540
It sort of depends on whether
you believe 2015 was a fluke or

00:22:15.540 --> 00:22:20.270
whether I should draw the line as I have,
in the middle between them.

00:22:20.270 --> 00:22:23.010
But you got slightly more slope,
then not a lot.

00:22:23.010 --> 00:22:25.079
But so compared to those two things,

00:22:25.079 --> 00:22:29.839
I mean actually just this amazing thing
happened with neural machine translation.

00:22:29.839 --> 00:22:34.380
So it was only in 2014,
after the WMT evaluation,

00:22:34.380 --> 00:22:37.354
that people started playing with.

00:22:37.354 --> 00:22:41.441
Could we build an end-to-end
neural machine translation system?

00:22:41.441 --> 00:22:46.163
But then extremely quickly,
people were able to build these systems.

00:22:46.163 --> 00:22:53.210
And so by 2016 they were clearly winning
in the workshop and machine translation.

00:22:53.210 --> 00:22:56.340
In terms of how much slope you have for
improvement,

00:22:56.340 --> 00:22:58.412
that the slope is extremely high.

00:22:58.412 --> 00:23:03.895
And indeed the numbers are kind of
continuing to go up too in the last year.

00:23:03.895 --> 00:23:06.632
So that's actually been super exciting.

00:23:06.632 --> 00:23:08.899
As I say in the next slide.

00:23:08.899 --> 00:23:12.804
That so
neural MT really went from this sort of,

00:23:12.804 --> 00:23:20.069
fringe research activity of let's try this
and see if it could possibly work in 2014.

00:23:20.069 --> 00:23:21.361
To two years later,

00:23:21.361 --> 00:23:25.783
it had become this is the way that
you have to do machine translation.

00:23:25.783 --> 00:23:30.102
Because it just works better
than everything else.

00:23:30.102 --> 00:23:32.257
So I'll say more about
machine translation.

00:23:32.257 --> 00:23:35.716
But I thought I'd just highlight
at the beginning, well,

00:23:35.716 --> 00:23:39.327
why do we get these big wins
from neural machine translation?

00:23:39.327 --> 00:23:42.200
And I think there are maybe
sort of four big wins.

00:23:42.200 --> 00:23:45.800
At any rate,
this is my attempt at dividing it up.

00:23:45.800 --> 00:23:48.390
So the first big win

00:23:48.390 --> 00:23:51.950
is the fact that you're just
training these models end-to-end.

00:23:51.950 --> 00:23:57.094
So if you can train all parameters
of the model simultaneously for

00:23:57.094 --> 00:23:59.614
one target driven loss function.

00:23:59.614 --> 00:24:03.820
That's just proved
a really powerful notion.

00:24:03.820 --> 00:24:08.831
And indeed I think quite a lot of
the success of deep learning systems is

00:24:08.831 --> 00:24:13.950
that because we have these sort
of big computational flow graphs

00:24:13.950 --> 00:24:19.427
that we can optimize everything over
in one big back propagation process.

00:24:19.427 --> 00:24:21.872
So it's easy to do end to end training.

00:24:21.872 --> 00:24:26.132
But that's been a very productive
way to do end to end training.

00:24:26.132 --> 00:24:31.549
And it's the end to end training
more than neural nets are magical.

00:24:31.549 --> 00:24:34.984
I think sometimes they're just
given enormous amounts of power to

00:24:34.984 --> 00:24:35.850
these systems.

00:24:35.850 --> 00:24:37.940
But there are other factors as well.

00:24:37.940 --> 00:24:39.522
So as we stressed a lot,

00:24:39.522 --> 00:24:44.048
these distributed representations
are actually just worth a ton.

00:24:44.048 --> 00:24:48.763
So that they allow you to kind of
share statistical strength between

00:24:48.763 --> 00:24:51.253
similar words, similar phrases.

00:24:51.253 --> 00:24:54.377
And you can exploit that to
just get better predictions,

00:24:54.377 --> 00:24:57.240
and that's given a lot of improvement.

00:24:57.240 --> 00:25:01.660
A third big cause of improvement
has been these neural MT

00:25:01.660 --> 00:25:06.220
systems are just much better
at exploiting context.

00:25:06.220 --> 00:25:09.388
So Richard briefly mentioned
traditional language models.

00:25:09.388 --> 00:25:14.094
So those were things like four gram and
five gram models which were just

00:25:14.094 --> 00:25:17.940
done on counts of how often
sequences of words occurred.

00:25:17.940 --> 00:25:23.521
And those were very useful parts
of machine translation systems.

00:25:23.521 --> 00:25:27.445
But the reality was that
the language models on

00:25:27.445 --> 00:25:32.180
the generation side only
used a very short context.

00:25:32.180 --> 00:25:34.360
And when you are translating words and

00:25:34.360 --> 00:25:38.400
phrases that the standard systems
did that completely context free.

00:25:38.400 --> 00:25:43.581
So the neural machine translation systems
are just able to use much more context and

00:25:43.581 --> 00:25:46.147
that means that they can do a lot better.

00:25:46.147 --> 00:25:49.349
And there's an interesting way
in which these things kind of

00:25:49.349 --> 00:25:51.117
go together in a productive way.

00:25:51.117 --> 00:25:55.831
So precisely the reason why your
machine translation systems can

00:25:55.831 --> 00:25:58.497
practically use much more context.

00:25:58.497 --> 00:26:02.507
Is because there are these distributed
representations that allow you

00:26:02.507 --> 00:26:04.390
to share statistical strength.

00:26:04.390 --> 00:26:08.581
Effectively you could never use more
context in traditional systems.

00:26:08.581 --> 00:26:12.342
Because you were using these
one-hot representations of words.

00:26:12.342 --> 00:26:17.143
And therefore you couldn't build more than
five gram models usefully because you were

00:26:17.143 --> 00:26:20.325
just being killed by
the sparseness of the data.

00:26:20.325 --> 00:26:24.943
And then the fourth thing that
I want to call out is sort of

00:26:24.943 --> 00:26:28.400
really related to all of one two or three.

00:26:28.400 --> 00:26:30.750
But I think it's just sort
of worth calling out.

00:26:30.750 --> 00:26:35.305
Is something really powerful
that's happened in the last couple

00:26:35.305 --> 00:26:37.719
of years with neural NLP methods.

00:26:37.719 --> 00:26:44.089
Is that they've proven to just be
extremely good for generating fluent text.

00:26:44.089 --> 00:26:49.149
So, I think it's fair to say
that the field of sort of

00:26:49.149 --> 00:26:57.220
natural language generation was sort
of fairly moribund in the 2000s decade.

00:26:57.220 --> 00:27:00.638
Because although there were sort
of simple things that you can do,

00:27:00.638 --> 00:27:03.948
writing a printf,
that's a text generation method.

00:27:03.948 --> 00:27:07.463
[LAUGH] But people could do a bit
better than that with grammar driven

00:27:07.463 --> 00:27:09.130
text generation and so on.

00:27:09.130 --> 00:27:13.267
But there really were not a lot of good
ideas as how to produce really good,

00:27:13.267 --> 00:27:15.819
high quality natural language generation.

00:27:15.819 --> 00:27:20.262
Whereas, it's just proven
extremely easy and productive.

00:27:20.262 --> 00:27:21.812
To do high-quality,

00:27:21.812 --> 00:27:26.470
natural language generation using
these neural language models.

00:27:26.470 --> 00:27:29.651
Because it's very easy for
them to use big contexts,

00:27:29.651 --> 00:27:33.829
condition on other goals at the same time,
and they work really well.

00:27:33.829 --> 00:27:38.400
And so one of the big reasons why
neural machine translation has been so

00:27:38.400 --> 00:27:41.280
successful and the results look very good.

00:27:41.280 --> 00:27:45.099
Is that the text that they're
generating is very fluent.

00:27:45.099 --> 00:27:48.957
In fact it's sometimes the case
that the actual quality

00:27:48.957 --> 00:27:51.018
of the translation is worse.

00:27:51.018 --> 00:27:55.840
That the quality of the generation
in terms of fluency is much better.

00:27:57.520 --> 00:28:00.502
It's also worth knowing
what's not on that list.

00:28:00.502 --> 00:28:04.207
So one thing that's not on that list,
that's a good thing,

00:28:04.207 --> 00:28:07.660
is we don't have any separate
black box component models for

00:28:07.660 --> 00:28:11.840
things like reordering and
transliteration and things like that.

00:28:11.840 --> 00:28:17.020
And traditional statistical MT systems
have lots of these separate components.

00:28:17.020 --> 00:28:21.870
You had lexicalized reordering components
and distortion models and this models and

00:28:21.870 --> 00:28:22.870
that models.

00:28:22.870 --> 00:28:27.455
And getting rid of all of that with
this end to end system is great.

00:28:27.455 --> 00:28:30.019
There are some other things
that are not so great.

00:28:30.019 --> 00:28:35.366
That our current NMT models really make
no use of any kind of explicit syntax or

00:28:35.366 --> 00:28:36.850
semantics.

00:28:36.850 --> 00:28:39.990
You could sort of say, well maybe some
interesting stuff is happening inside

00:28:39.990 --> 00:28:42.030
the word vectors and maybe it is.

00:28:42.030 --> 00:28:44.980
Sorry, there are current
hidden state vectors and

00:28:44.980 --> 00:28:47.710
maybe it is, but it's sort of unclear.

00:28:47.710 --> 00:28:50.029
But actually this is something
that has started to be worked on.

00:28:50.029 --> 00:28:52.959
There have been a couple of papers
that have come out just this year.

00:28:52.959 --> 00:28:57.325
Where people are starting to put more
syntax into neural machine translation

00:28:57.325 --> 00:28:59.820
models, and
are getting gains from doing so.

00:28:59.820 --> 00:29:04.112
So I think that's something
that will revive itself.

00:29:04.112 --> 00:29:09.485
Also another huge failing of machine
translation, has been a lot of the errors.

00:29:09.485 --> 00:29:13.373
That higher level textual
notions are really badly done

00:29:13.373 --> 00:29:16.020
by machine translation systems.

00:29:16.020 --> 00:29:20.440
So those are things of sort of discourse
structure, clause linking, anaphora and

00:29:20.440 --> 00:29:21.710
things like that.

00:29:21.710 --> 00:29:24.070
And we haven't solved those ones.

00:29:25.170 --> 00:29:28.622
Yeah, so that's been the general picture.

00:29:28.622 --> 00:29:34.444
Before going on, one of the things we
haven't done very much of in this class.

00:29:34.444 --> 00:29:39.660
Is actually looking at linguistic
examples and having language on slide.

00:29:39.660 --> 00:29:44.650
So I thought I'd do at least one
sentence of machine translation.

00:29:44.650 --> 00:29:48.190
And I kind of guessed that
the highest density of

00:29:48.190 --> 00:29:51.430
knowledge of another language
in my audience is Chinese.

00:29:51.430 --> 00:29:53.732
So we're doing Chinese.

00:29:53.732 --> 00:29:58.760
And this is my one sentence test set for

00:29:58.760 --> 00:30:04.428
Chinese to English machine translation.

00:30:04.428 --> 00:30:07.667
So I guess back in the mid 2000s,

00:30:07.667 --> 00:30:13.050
we were doing Chinese to
English machine translation.

00:30:13.050 --> 00:30:16.590
And there was this evaluation
that we did kind of badly on.

00:30:16.590 --> 00:30:21.699
And one of the sentences that we
translated terribly was this sentence.

00:30:21.699 --> 00:30:25.891
And ever since then, I've been using
this as my one sentence evaluation set.

00:30:25.891 --> 00:30:31.614
So I guess this sentence, it actually
comes from Jared Diamond's book,

00:30:31.614 --> 00:30:34.010
Guns, Germs, and Steel.

00:30:34.010 --> 00:30:37.660
So in a sense it's sort of a funny one
since it's starting with the Chinese

00:30:37.660 --> 00:30:40.600
translation of Jared Diamond's text.

00:30:40.600 --> 00:30:44.386
And then we're trying to translate it
back into English, but never mind!

00:30:44.386 --> 00:30:45.858
That's our sentence for now.

00:30:45.858 --> 00:30:47.329
So what have we got here?

00:30:47.329 --> 00:30:50.813
So this is the 1519 year,

00:30:50.813 --> 00:30:55.509
there were 600 Spanish people and

00:30:55.509 --> 00:30:58.849
their landing in Mexico.

00:30:58.849 --> 00:31:01.250
And then we've got "to conquer".

00:31:01.250 --> 00:31:05.650
And the first bit I want to focus
on is then this next bit here.

00:31:05.650 --> 00:31:12.110
The several million population
of the Aztec Empire.

00:31:13.340 --> 00:31:19.310
And so, what you get in Chinese is so
here's our "Aztec Empire".

00:31:19.310 --> 00:31:25.850
So in general in Chinese all modifiers
of a noun are appearing before the noun.

00:31:25.850 --> 00:31:29.692
And Chinese has this really handy little
morpheme right here, the [FOREIGN].

00:31:29.692 --> 00:31:35.330
This is saying the thing
that comes before it,

00:31:35.330 --> 00:31:40.820
shown in that brownish color, is
a modifier of this noun that follows it.

00:31:40.820 --> 00:31:44.695
And this one's saying the sort
of several million population.

00:31:44.695 --> 00:31:48.826
So it's the Aztec Empire with
the population of a few million.

00:31:48.826 --> 00:31:53.489
And there's this very specific linguistic
marker that tells you how you're meant to

00:31:53.489 --> 00:31:54.820
translate it.

00:31:54.820 --> 00:32:00.340
And then after that We
then got the part here,

00:32:00.340 --> 00:32:04.790
where then we've got so
first time confronted them,

00:32:06.370 --> 00:32:09.210
losses, two-thirds.

00:32:09.210 --> 00:32:12.680
And so that's just sort of tacked
on to the end of the sentence, so

00:32:12.680 --> 00:32:16.530
they lost two-thirds of their
soldiers in the first clash.

00:32:16.530 --> 00:32:20.590
This is just an interesting
thing in how translation works.

00:32:20.590 --> 00:32:26.330
So you could in an English translation try
and tack that onto the end of the sentence

00:32:26.330 --> 00:32:31.800
and sort of say "losing two thirds of
their soldiers in the first clash" or

00:32:31.800 --> 00:32:35.990
"and they lost two thirds of their
soldiers in the first clash".

00:32:35.990 --> 00:32:38.780
But neither of those sound
very good in English.

00:32:38.780 --> 00:32:42.060
So, below here what we
have is the reference

00:32:42.060 --> 00:32:46.920
translation which is where we got some
competent human to translate this.

00:32:46.920 --> 00:32:51.410
And so, interestingly what they did and
I think correctly actually here is that

00:32:51.410 --> 00:32:55.810
they decide it would actually be much
better to make this into two sentences.

00:32:55.810 --> 00:32:59.450
And so, they put in a period and
then they made a second sentence.

00:32:59.450 --> 00:33:02.822
They lost two thirds of their
soldiers in the first clash.

00:33:02.822 --> 00:33:06.354
Okay, so
I won't tell you a bad translation, but

00:33:06.354 --> 00:33:11.417
every year since I've been running
since this sentence through Google and

00:33:11.417 --> 00:33:14.880
so I'll show you the Google translations.

00:33:14.880 --> 00:33:19.110
So on 2009, this is what Google produced.

00:33:19.110 --> 00:33:23.020
1519, 600 Spaniards landed in Mexico.

00:33:23.020 --> 00:33:25.420
So that start's not very good.

00:33:25.420 --> 00:33:28.220
But if we go in particular
to this focus part,

00:33:28.220 --> 00:33:31.490
millions of people to
conquer the Aztec empire.

00:33:31.490 --> 00:33:33.670
No, that's not correct.

00:33:33.670 --> 00:33:38.660
And well it's getting some of the words
right but it's completely not making any

00:33:38.660 --> 00:33:43.630
use of the structure of
the sentence in Chinese.

00:33:43.630 --> 00:33:45.270
And it doesn't get much better.

00:33:45.270 --> 00:33:50.311
The first two-thirds of
soldiers against their loss.

00:33:50.311 --> 00:33:53.814
Okay, so we can go on to 2011.

00:33:53.814 --> 00:33:58.600
I left some of them out so
he font size stayed vaguely readable.

00:33:58.600 --> 00:34:00.440
So it changes a bit but not really.

00:34:00.440 --> 00:34:03.540
1519, 600 Spaniards landed in Mexico.

00:34:03.540 --> 00:34:05.820
Millions of people to
conquer the Aztec empire.

00:34:05.820 --> 00:34:09.440
The initial loss of soldiers
two-thirds of their encounters.

00:34:09.440 --> 00:34:13.397
So that last bit may be a fraction
better but the rest of it is no better.

00:34:13.397 --> 00:34:17.732
In 2013, it seemed like they might
have made a bit of progress.

00:34:17.732 --> 00:34:23.176
1519 600 Spaniards landed in Mexico
to conquer the Aztec empire,

00:34:23.176 --> 00:34:25.760
hundreds of million of people.

00:34:25.760 --> 00:34:27.183
It's unclear if it's made progress.

00:34:27.183 --> 00:34:31.407
The fact that you can read the to conquer
the Aztec empire has mean the Spaniards

00:34:31.407 --> 00:34:35.119
sort of means it might have made some
progress but then after that they

00:34:35.119 --> 00:34:38.577
just dump the hundreds of millions
of people between two commas.

00:34:38.577 --> 00:34:42.351
And so it's really not quite
clear what that's doing but

00:34:42.351 --> 00:34:47.380
it sort of seemed like whatever that
change that was just kind of luck because

00:34:47.380 --> 00:34:52.647
in 2014 it sort of switch back 1519
600 Spaniards landed in Mexico,

00:34:52.647 --> 00:34:55.871
millions of people to
conquer the Aztec empire,

00:34:55.871 --> 00:34:59.760
the first two-thirds of the loss
of soldiers they clash.

00:35:01.680 --> 00:35:07.968
And not only that interestingly
when I ran it again in 2015 and

00:35:07.968 --> 00:35:13.425
2016, the translation
didn't change at all.

00:35:13.425 --> 00:35:18.480
So I don't know what all the people were
doing on the Google MT translation team in

00:35:18.480 --> 00:35:24.180
2015 and 2016, but they definitely weren't
making progress in Chinese translation.

00:35:24.180 --> 00:35:27.120
And I think this sort of
reflects as if the feeling

00:35:27.120 --> 00:35:29.110
that the system wasn't really progressing.

00:35:29.110 --> 00:35:34.388
That they sort of built the models and
mined all the data they could for

00:35:34.388 --> 00:35:39.486
their Chinese English MT system
that wasn't getting any better.

00:35:39.486 --> 00:35:44.407
So then in late 2016, Google rolled out
their neural machine translation system,

00:35:44.407 --> 00:35:47.143
which you're gonna hear
more about in a moment.

00:35:47.143 --> 00:35:51.110
And there's actual and
distinct signs of progress.

00:35:51.110 --> 00:35:54.779
So in 1519 600,
Spaniards landed in Mexico.

00:35:54.779 --> 00:35:59.245
So the beginning of it is a lot better
'cause the whole time it'd just been

00:35:59.245 --> 00:36:04.790
plunking down 1519 and 600,
which wasn't a very promising beginning.

00:36:04.790 --> 00:36:07.879
In the Chinese there's no word for
"in", right?

00:36:07.879 --> 00:36:09.671
So this character here is "year", right?

00:36:09.671 --> 00:36:16.450
So it's sort of 1519 year,
600 people, Spanish people, right?

00:36:16.450 --> 00:36:21.145
But clearly in English you wanna be
putting in it in there and say in 1519.

00:36:21.145 --> 00:36:25.090
But somehow Google it never manage
to get that right where you might

00:36:25.090 --> 00:36:26.170
have thought it could.

00:36:26.170 --> 00:36:27.670
But now it is, right?

00:36:27.670 --> 00:36:30.210
In 1519 comma, great beginning, and

00:36:30.210 --> 00:36:34.820
it continues much better 600
Spaniards landed in Mexico

00:36:34.820 --> 00:36:39.670
to conquer the millions of people of the
Aztec empire, this is getting really good.

00:36:39.670 --> 00:36:42.250
Neural machine translation is much,
much better.

00:36:42.250 --> 00:36:44.438
But there is still some work to do.

00:36:44.438 --> 00:36:49.199
I guess this last part is kind of
difficult in a sense the way it's so

00:36:49.199 --> 00:36:51.848
tacked on to the end of the sentence.

00:36:51.848 --> 00:36:54.824
But you're right,
it still isn't working very well for

00:36:54.824 --> 00:36:59.226
that cuz they've just tacked on the first
confrontation they killed two-thirds,

00:36:59.226 --> 00:37:02.078
which sort of it seems to be
the wrong way around because

00:37:02.078 --> 00:37:05.490
that's suggesting they killed
two-thirds of the Aztecs.

00:37:05.490 --> 00:37:09.910
Whereas meant to be that they
lost two thirds of the Spaniards.

00:37:09.910 --> 00:37:13.860
So there's still work to be done from
proving neural machine translation.

00:37:13.860 --> 00:37:18.740
But, I do actually think that that's
showing very genuine progress and that's,

00:37:18.740 --> 00:37:20.710
in general, what's been shown.

00:37:20.710 --> 00:37:24.440
So neural machine translation
has just given big gains.

00:37:24.440 --> 00:37:27.260
It's been aggressively
rolled out by industry.

00:37:27.260 --> 00:37:29.269
So actually the first
people who rolled out

00:37:30.520 --> 00:37:32.750
neural machine translation was Microsoft.

00:37:32.750 --> 00:37:37.560
So in February 2016, Microsoft

00:37:37.560 --> 00:37:42.480
launched neural machine translation
on Android phones no less.

00:37:42.480 --> 00:37:47.180
And another of the huge selling points
of neural machine translation systems,

00:37:47.180 --> 00:37:50.010
is that they're actually
massively more compact.

00:37:50.010 --> 00:37:54.600
So that they were able to build a neural
machine translation system that actually

00:37:54.600 --> 00:37:56.600
ran on the cellphone.

00:37:56.600 --> 00:38:01.380
And actually that's a very useful
use case, 'cause the commonest

00:38:01.380 --> 00:38:06.350
time when people want machine translation
is when they're not in their home country.

00:38:06.350 --> 00:38:07.790
And at that point it depends.

00:38:07.790 --> 00:38:11.760
But a lot of people don't actually have
cell plans that work in foreign countries,

00:38:11.760 --> 00:38:13.190
at decent prices.

00:38:13.190 --> 00:38:16.960
And so it's really useful to be able to
run your MT system just on the phone.

00:38:16.960 --> 00:38:20.160
And that was sort of essentially
never possible with the huge

00:38:20.160 --> 00:38:24.040
kind of look up tables of phrase
based systems it is now possible.

00:38:24.040 --> 00:38:29.290
Systran is a veteran old MT company
that also launched this system.

00:38:29.290 --> 00:38:34.220
And then Google launched their
neural machine translation system

00:38:34.220 --> 00:38:38.690
with massively more hype than
either of the two predecessors,

00:38:38.690 --> 00:38:44.490
including some huge overclaims of
equaling human translation quality.

00:38:44.490 --> 00:38:46.560
Which we've just seen still isn't true,

00:38:46.560 --> 00:38:50.800
based on my one sentence test set,
that they still have some work to do.

00:38:50.800 --> 00:38:56.210
But on the other hand,
they did publish a really interesting

00:38:56.210 --> 00:39:00.920
paper on the novel research that they've
done on neural machine translation.

00:39:00.920 --> 00:39:04.728
And so for the research highlight
today Emma is gonna talk about that.

00:39:12.994 --> 00:39:16.277
&gt;&gt; Hi, today I'm gonna talk about Google's

00:39:16.277 --> 00:39:21.980
multi lingual NMT system which
enables zero shot translation.

00:39:21.980 --> 00:39:25.561
So as we have seen in the lecture,
this is the standard architecture for

00:39:25.561 --> 00:39:28.260
an NMT system which you have
an encoder and a decoder.

00:39:28.260 --> 00:39:32.220
However, this thin architecture supports
only bilingual translation, meaning

00:39:32.220 --> 00:39:36.306
that we can have only one specific source
language and one specific target language.

00:39:36.306 --> 00:39:39.412
So what if you want to have a system
that's able to do multilingual

00:39:39.412 --> 00:39:40.152
translation?

00:39:40.152 --> 00:39:44.470
Meaning that we can have multiple source
languages and multiple target languages.

00:39:44.470 --> 00:39:47.920
So previously people have proposed
several different approaches.

00:39:47.920 --> 00:39:51.130
The first one, they proposed to have
multiple different encoders and

00:39:51.130 --> 00:39:52.740
multiple different decoders.

00:39:52.740 --> 00:39:57.370
Where each pair correspond to one specific
pair of source and target languages.

00:39:57.370 --> 00:40:01.380
And the second one that proposed to
have a shared encoder that works for

00:40:01.380 --> 00:40:03.340
one specific source language, but

00:40:03.340 --> 00:40:07.950
have different decoders to decode
into different target languages.

00:40:07.950 --> 00:40:11.940
And they also have proposed
the third one is they have

00:40:11.940 --> 00:40:15.350
multiple different encoders to work for
different source languages and

00:40:15.350 --> 00:40:19.690
wants a single shared decoder to work for
more specific target language.

00:40:20.800 --> 00:40:25.340
So what's so special about
Google's multilingual NMT system?

00:40:25.340 --> 00:40:26.080
So first of all,

00:40:26.080 --> 00:40:30.590
it's really simple because here we only
need one single model that is able to

00:40:30.590 --> 00:40:35.740
translate from different source languages
to different target languages, and

00:40:35.740 --> 00:40:42.360
because of the simplicity the system can
trivially scale up to more language pairs.

00:40:42.360 --> 00:40:45.770
And second, the system improves
the translation quality for

00:40:45.770 --> 00:40:46.730
low resource language.

00:40:47.800 --> 00:40:52.110
So because the progress of the model
are shared implicitly and so

00:40:52.110 --> 00:40:55.730
the model is forced to generalize
across language boundaries.

00:40:55.730 --> 00:40:59.560
So it's observed that if we train
the language that has very little training

00:40:59.560 --> 00:41:04.430
data with a language pair that
has a lot of training data in one

00:41:04.430 --> 00:41:06.970
single model, the translation quality for

00:41:06.970 --> 00:41:09.479
the low-resourced language
is significantly improved.

00:41:10.510 --> 00:41:14.320
And also the system is able to
perform zero-shot translation.

00:41:14.320 --> 00:41:17.406
Meaning that the model can
inclusively translate for

00:41:17.406 --> 00:41:20.300
the language pairs it has never
seen during training time.

00:41:20.300 --> 00:41:24.830
For example, if we train a model
on Portuguese to English and

00:41:24.830 --> 00:41:29.630
English to Spanish data,
the model is able to

00:41:29.630 --> 00:41:33.780
generate reasonable translation for
Portuguese to Spanish directly.

00:41:33.780 --> 00:41:37.290
Without seeing any data for
the language pair during training time.

00:41:38.910 --> 00:41:42.080
And this is the architecture for
the models.

00:41:42.080 --> 00:41:45.270
As we can see, this is kind of
the standard architecture for

00:41:45.270 --> 00:41:47.984
the state-of-the-art NMT system.

00:41:47.984 --> 00:41:53.220
Where we have multiple stacked layers of
LSTMs for both decoders and encoders and

00:41:53.220 --> 00:41:57.400
those applied attention mechanism which
we will talk about later in a lecture.

00:41:57.400 --> 00:42:01.870
So what is the magic here that enables the
system to do a multilingual translation?

00:42:02.920 --> 00:42:07.350
So it turns out instead of trying to
modify the architecture, they instead

00:42:07.350 --> 00:42:13.010
modified the input data, by adding the
special artificial token at the beginning

00:42:13.010 --> 00:42:17.860
of every input sentence, to indicate what
target language you want to translate to.

00:42:17.860 --> 00:42:20.800
So for example, if you wanna
translate from English to Spanish,

00:42:20.800 --> 00:42:25.810
we simple add this &lt;2es&gt; token to indicate
that Spanish is the target language.

00:42:25.810 --> 00:42:29.450
And after adding this artificial token,
we simply just put together

00:42:29.450 --> 00:42:32.430
all of the multi-lingual data and
just start training.

00:42:35.312 --> 00:42:37.570
With this simple trick,

00:42:37.570 --> 00:42:41.590
the system is able to surpass
the state-of-the-art performance for

00:42:41.590 --> 00:42:46.630
English to German, French to English,
and German to English translation.

00:42:46.630 --> 00:42:49.822
And they have comparable performance for
English to French translation.

00:42:51.050 --> 00:42:53.240
Both on the WMT benchmark.

00:42:54.570 --> 00:42:58.940
So and here's a little more detail
about a zero-shot translation.

00:42:58.940 --> 00:43:00.240
The setting is like this.

00:43:00.240 --> 00:43:03.920
So during training time we train
a model on Portuguese to English and

00:43:03.920 --> 00:43:05.590
English to Spanish data.

00:43:05.590 --> 00:43:09.070
But during test time we ask the model
to perform Portuguese to Spanish

00:43:09.070 --> 00:43:10.780
translation directly.

00:43:10.780 --> 00:43:14.963
And it's shown here that the model
is able to have comparable

00:43:14.963 --> 00:43:19.620
performance as the phrase based
machine translation system.

00:43:19.620 --> 00:43:22.418
And also the NMT system with bridging.

00:43:22.418 --> 00:43:24.720
And also with a little bit
of incremental training.

00:43:24.720 --> 00:43:32.250
Meaning that we add a little bit of data
for the Portuguese to Spanish translation.

00:43:32.250 --> 00:43:36.620
The model is able to surpass all
of the other models listed above.

00:43:37.630 --> 00:43:44.645
And that's all, thank you.

00:43:44.645 --> 00:43:45.570
[APPLAUSE]
&gt;&gt; So

00:43:45.570 --> 00:43:48.772
I think that actually is
a really amazing result.

00:43:48.772 --> 00:43:53.430
I mean, in some sense,

00:43:53.430 --> 00:43:58.090
it's actually realizing a long-held
dream of machine translation.

00:43:58.090 --> 00:44:02.870
So a traditional problem
with machine translation has

00:44:02.870 --> 00:44:07.780
always been that if you'd like to be able
to translate between a lot of languages,

00:44:07.780 --> 00:44:11.050
or you're then in a product space
of number of systems, right.

00:44:11.050 --> 00:44:15.610
So if you'd like to support around
80 languages as Google does.

00:44:15.610 --> 00:44:19.470
That if you wanna allow translation
between any pairs straightforwardly you

00:44:19.470 --> 00:44:23.240
have to build 6,400 machine
translation systems.

00:44:23.240 --> 00:44:26.020
And that's a lot of machine
translation systems.

00:44:26.020 --> 00:44:27.810
And they never quite did that.

00:44:27.810 --> 00:44:29.690
That was a reference to bridging.

00:44:29.690 --> 00:44:33.880
So if something was being bridged,
what that effectively meant for

00:44:33.880 --> 00:44:36.860
Google was you were translating
twice via an intermediate

00:44:36.860 --> 00:44:39.890
language where the intermediate
language was normally English.

00:44:39.890 --> 00:44:42.948
So the goal has for
a long time has been in MT,

00:44:42.948 --> 00:44:46.680
is to achieve this dream
of an interlingua.

00:44:46.680 --> 00:44:51.330
So that if you had an interlingua in the
middle you have to translate each language

00:44:51.330 --> 00:44:55.970
to and from the interlingua, so you only
need 80 encoders and 80 decoders, so

00:44:55.970 --> 00:44:58.290
it's then the number of languages.

00:44:58.290 --> 00:45:03.310
And that has sort of never been very
successful, which is why effectively

00:45:03.310 --> 00:45:06.840
people just sort of build all
of these bilingual systems but

00:45:06.840 --> 00:45:11.700
this system is now sort of
illustrating how you can actually have

00:45:11.700 --> 00:45:16.200
the encodings of neural MT system
be an effective interlingua.

00:45:17.680 --> 00:45:22.720
Okay, so now on to the main technical
content to get through today,

00:45:22.720 --> 00:45:25.690
is introducing this idea of attention.

00:45:25.690 --> 00:45:28.080
So what's the problem
we want to deal with?

00:45:28.080 --> 00:45:31.844
So if we're into the sort of
vanilla sequence to sequence,

00:45:31.844 --> 00:45:33.900
encoder-decoder model.

00:45:33.900 --> 00:45:39.650
We have this problem because
our only representation of

00:45:39.650 --> 00:45:44.980
the input is this sort of one
fixed-dimensional representation Y

00:45:44.980 --> 00:45:50.840
which was sort of the state
that our encoder was last in.

00:45:50.840 --> 00:45:54.500
And so,
we need to kind of carry that through

00:45:54.500 --> 00:45:58.180
our entire generation of
our translation sentence.

00:45:58.180 --> 00:46:04.160
And that seems like it might be
a difficult thing to do, and

00:46:04.160 --> 00:46:09.435
indeed, what was shown was that was
indeed a difficult thing to do and

00:46:09.435 --> 00:46:12.755
so what people found is
that this initial neural

00:46:12.755 --> 00:46:17.315
machines translations systems
worked well on short sentences.

00:46:17.315 --> 00:46:21.548
But if you tried to use them to translate
very long sentences, that their

00:46:21.548 --> 00:46:27.000
performance started to tank and I'll show
you some numbers on that later; And so

00:46:27.000 --> 00:46:32.540
the idea that people came up with and
this idea was actually first proposed for

00:46:32.540 --> 00:46:37.220
vision but was then moved over and
tried for neural

00:46:37.220 --> 00:46:41.625
machine translation by Kyunghyun Cho and
colleagues at Montreal, was to say,

00:46:41.625 --> 00:46:46.260
well instead of saying that
our Y that we generate

00:46:46.260 --> 00:46:51.990
from is just the last hidden states,
why don't we say all of the hidden states

00:46:51.990 --> 00:46:56.490
of the entire encoding
process are available to us.

00:46:56.490 --> 00:46:59.300
And so
we sort of have this pool of source states

00:46:59.300 --> 00:47:02.250
that we can draw from
to do the translation.

00:47:02.250 --> 00:47:06.560
And so then when we're
translating any particular word,

00:47:06.560 --> 00:47:11.390
we then want to work out which
of those ones to draw from.

00:47:11.390 --> 00:47:16.030
So effectively,
the pool of source states becomes kind

00:47:16.030 --> 00:47:21.220
of like a random access memory which the
neural network is then going to be able

00:47:21.220 --> 00:47:25.490
to retrieve from as needed when
it wants to do its translation.

00:47:25.490 --> 00:47:30.160
And it'll find some stuff from it and
use it for translating each word.

00:47:31.160 --> 00:47:35.530
And so attention for
neural machine translation is one specific

00:47:35.530 --> 00:47:40.870
instantiation of this, but in general this
sort of builds into a bigger concept that

00:47:40.870 --> 00:47:46.390
has actually been a very exciting concept
in recent neural networks research and

00:47:46.390 --> 00:47:49.200
I know at least a couple of Groups
are interested in doing for

00:47:49.200 --> 00:47:52.830
their final projects is
this idea of can we augment

00:47:53.830 --> 00:47:57.580
neural networks with a memory on the side.

00:47:57.580 --> 00:48:01.935
So that we cannot only lengthen our
short term memory with an LSTM, but

00:48:01.935 --> 00:48:04.555
we can actually have a much
longer term memory that

00:48:04.555 --> 00:48:06.995
we can access stuff from as we need it.

00:48:06.995 --> 00:48:09.825
And attention is a simple
form of doing that.

00:48:09.825 --> 00:48:14.296
And then some of the more recent work like
neural turing machines is trying to do

00:48:14.296 --> 00:48:19.860
more sophisticated forms of read-write
memories augmenting neural networks.

00:48:19.860 --> 00:48:24.330
Okay, so if we want to retrieve as needed,
you could think of that as saying,

00:48:24.330 --> 00:48:27.920
okay, well,
out of all of this pool of source states,

00:48:27.920 --> 00:48:33.282
we want to be looking at where in
the input we want to retrieve stuff from.

00:48:33.282 --> 00:48:38.280
So effectively, after we've said, Je, and

00:48:38.280 --> 00:48:42.240
we wanting to translate the next word.

00:48:42.240 --> 00:48:45.320
We should be working out,
well, where in here do

00:48:45.320 --> 00:48:49.590
we want to be paying attention to
decide what to translate next?

00:48:49.590 --> 00:48:52.780
And if it's French,
we wanna be translating the am next.

00:48:52.780 --> 00:48:58.780
And so our attention model effectively
sort of becomes like an alignment model.

00:48:58.780 --> 00:49:00.470
'cause it's saying, well,

00:49:00.470 --> 00:49:03.730
which part of the source are you
next gonna be translating?

00:49:03.730 --> 00:49:08.150
So you've got this implicit alignment
between the source and the translation.

00:49:08.150 --> 00:49:13.514
And that just seems a good idea, 'cause
that's even what human translators do.

00:49:13.514 --> 00:49:16.617
It's not that a human translator
reads the whole of a big,

00:49:16.617 --> 00:49:18.778
long sentence and says, okay, got it.

00:49:18.778 --> 00:49:21.890
And then starts furiously scribbling
down the translation, right?

00:49:21.890 --> 00:49:25.440
They're looking back at
the source as they translate, and

00:49:25.440 --> 00:49:27.900
are translating different phrases of it.

00:49:27.900 --> 00:49:33.013
And so Richard mentioned last
week the idea that in training

00:49:33.013 --> 00:49:38.230
statistical models that one of
the first steps was you worked

00:49:38.230 --> 00:49:43.563
out these word alignments between
the source and the target.

00:49:43.563 --> 00:49:48.141
And that was used to extract phrases
that gave you kind of phrases to use in

00:49:48.141 --> 00:49:50.483
a statistical phrase based system.

00:49:50.483 --> 00:49:54.701
Here, we're not doing that,
it's rather just at

00:49:54.701 --> 00:49:59.718
translation time by process of
using this attention model.

00:49:59.718 --> 00:50:04.059
We're implicitly making connections
between source and target,

00:50:04.059 --> 00:50:06.640
which gives us a kind of alignment.

00:50:06.640 --> 00:50:11.330
But nevertheless, it effectively means
that we're building this end-to-end neural

00:50:11.330 --> 00:50:16.460
machine translation system that's doing
alignments and translation as it works.

00:50:16.460 --> 00:50:21.230
So it achieves this NMT vision, and
you do get these good alignments.

00:50:22.550 --> 00:50:26.980
So we're using this kind of
on the right structure where

00:50:26.980 --> 00:50:31.670
we're sort of filling in where
the alignments have occurred.

00:50:31.670 --> 00:50:37.018
And so you can look at where attention
was laid when you're producing

00:50:37.018 --> 00:50:43.000
a translation,
translating here from French to English.

00:50:43.000 --> 00:50:46.560
And you can see that this model,
which is a model from people at Montreal,

00:50:46.560 --> 00:50:50.930
is doing a good job at deciding
where to place attention.

00:50:50.930 --> 00:50:56.010
So it's starting off with the agreement
on the, and then the interesting part is

00:50:56.010 --> 00:51:01.340
that sort of French typically has
adjectival modifiers after the head down.

00:51:01.340 --> 00:51:04.250
So this is the zone, economic, European,

00:51:04.250 --> 00:51:08.710
which you have to flip in English
to get the European economic area.

00:51:08.710 --> 00:51:11.410
And so
it's kind of correctly modelling that flip

00:51:11.410 --> 00:51:14.290
in deciding where to pay
attention in the source.

00:51:14.290 --> 00:51:17.830
And then kind of goes back to
a more monotonic linear order.

00:51:19.490 --> 00:51:24.430
Okay, so that looks good,
how do we go about doing that?

00:51:24.430 --> 00:51:28.650
So what we're gonna be doing is
we've started to generate, and

00:51:28.650 --> 00:51:30.940
we wanna generate the next word.

00:51:30.940 --> 00:51:36.395
And we want to use our hidden
state to decide where to access

00:51:36.395 --> 00:51:41.819
our random access memory,
which is all the blue stuff.

00:51:41.819 --> 00:51:46.545
And so, well we haven't yet generated
the hidden state for the next word, so

00:51:46.545 --> 00:51:52.580
it seems like our only good choice
is to use, I think I skipped one.

00:51:52.580 --> 00:51:55.310
Okay, the only good choice is to use

00:51:55.310 --> 00:51:58.720
the previous hidden state
as the basis of attention.

00:51:58.720 --> 00:52:04.840
And that's what we do, and then what
we're gonna do is come up with some

00:52:04.840 --> 00:52:10.950
score that combines it and
elements of the hidden state.

00:52:10.950 --> 00:52:15.080
And commonly, people are only using
the highest level of the hidden state for

00:52:15.080 --> 00:52:18.610
attention, and
decides where to pay attention.

00:52:18.610 --> 00:52:23.233
And so this scoring function,
will score each position and

00:52:23.233 --> 00:52:25.918
saying, where to pay attention.

00:52:25.918 --> 00:52:29.000
And I'll get back to the scoring
functions in a minute.

00:52:29.000 --> 00:52:33.930
And so the model that they proposed was,
we get a score for

00:52:33.930 --> 00:52:36.450
each component of the memory.

00:52:36.450 --> 00:52:41.678
And then what we're gonna do is
sort of build a representation

00:52:41.678 --> 00:52:46.617
which combines all of the memories
weighted by the score.

00:52:46.617 --> 00:52:50.307
So what we're gonna do is
we're going to say, okay,

00:52:50.307 --> 00:52:54.760
we'll take those scores and
we'll do our standard trick.

00:52:54.760 --> 00:52:58.679
We'll stick them through
a softmax function and

00:52:58.679 --> 00:53:03.553
that will then give us a probability
distribution of how much

00:53:03.553 --> 00:53:08.062
attention to pay to the different
places in the source.

00:53:08.062 --> 00:53:13.315
And so then, we're going to combine,
okay, then we're going

00:53:13.315 --> 00:53:18.278
to combine together all of
the hidden states of the encoder,

00:53:18.278 --> 00:53:22.581
weighted by how much
attention we're paying to it.

00:53:22.581 --> 00:53:27.241
So that we're taking, these are each
hidden state of the encoder,

00:53:27.241 --> 00:53:31.349
the amount of attention you're
paying to that position.

00:53:31.349 --> 00:53:34.318
And then you're just
calculating a weighted sum and

00:53:34.318 --> 00:53:36.830
that then gives us a context vector.

00:53:36.830 --> 00:53:40.170
So now rather than simply
using the last hidden state

00:53:40.170 --> 00:53:45.210
as our representation of all of meaning,
we're using the entire of our

00:53:45.210 --> 00:53:48.840
hidden states of the encoder as
our representation of meaning.

00:53:48.840 --> 00:53:52.300
And at different points in
time we weight it differently

00:53:52.300 --> 00:53:55.160
to pay attention in different places.

00:53:55.160 --> 00:53:59.650
And so now what we're gonna do,
is based on what we were.

00:54:01.150 --> 00:54:02.650
This is going automatic on me.

00:54:02.650 --> 00:54:05.630
Now what we're gonna do is based
on what we were doing before.

00:54:07.780 --> 00:54:14.927
And so the previous hidden state and the
next and the previous word of the decoder.

00:54:14.927 --> 00:54:18.956
But also conditioned on
this context vector,

00:54:18.956 --> 00:54:22.685
we're then gonna generate the next word.

00:54:22.685 --> 00:54:29.544
Okay, so then the question is, well,
how do we actually score that?

00:54:29.544 --> 00:54:33.388
And at this point we need
some kind of attention

00:54:33.388 --> 00:54:37.910
function that decides how
to work out the score.

00:54:37.910 --> 00:54:42.470
And a very simple idea you could use for
that is just to say, well,

00:54:42.470 --> 00:54:48.522
let's take the dot product between
the decoded hidden state and

00:54:48.522 --> 00:54:50.800
an encoded the hidden state.

00:54:50.800 --> 00:54:55.138
And we wanna find the ones that are
similar, cuz that means we're in the right

00:54:55.138 --> 00:54:59.171
ballpark of words that have the same
meaning, and generate from that.

00:54:59.171 --> 00:55:02.256
And that's a possible
thing that you could do.

00:55:02.256 --> 00:55:08.258
The one that was proposed by the people
in Montreal was this bottom one.

00:55:08.258 --> 00:55:13.073
Where we're effectively using a single
layer of neural net, just like

00:55:13.073 --> 00:55:18.490
the kind of functions that we've been
using everywhere else inside our LSTM.

00:55:18.490 --> 00:55:23.700
So we're taking the concatenation
of the two hidden states.

00:55:23.700 --> 00:55:28.570
We're multiplying the biometrics,
putting it through a tanh function.

00:55:28.570 --> 00:55:31.910
And then multiplying that by another
vector, where both the V and

00:55:31.910 --> 00:55:33.220
W are learned.

00:55:33.220 --> 00:55:35.960
And using that as an attention function.

00:55:35.960 --> 00:55:39.625
And so that's what they did in their work,
and that worked pretty well.

00:55:39.625 --> 00:55:44.962
In the work we did at Stanford, so
principally Thang Luong's work, that we

00:55:44.962 --> 00:55:50.540
proposed using a different attention
function, which is the one in the middle.

00:55:50.540 --> 00:55:53.079
Which is this bilinear attention function,

00:55:53.079 --> 00:55:56.632
which has actually been quite
successful and widely adopted.

00:55:56.632 --> 00:56:00.890
So here, it's kind of like the top
one where you're doing a dot product.

00:56:00.890 --> 00:56:06.125
But you're sticking in between
the dot product a mediating matrix W.

00:56:06.125 --> 00:56:11.095
And so that matrix can effectively
then learn how much weight to

00:56:11.095 --> 00:56:14.489
put on different parts of the dot product.

00:56:14.489 --> 00:56:17.962
To sort of have an idea of
where to pay attention.

00:56:17.962 --> 00:56:22.355
And that's actually turned out to
be a model that works kind of well.

00:56:22.355 --> 00:56:25.144
And I think there's a reason
why it works kind of well.

00:56:25.144 --> 00:56:29.936
Cuz what you would like to do
is kind of have interaction

00:56:29.936 --> 00:56:33.278
terms that look at h_t and h_s together.

00:56:33.278 --> 00:56:37.820
And even the dot product kind of has
this interaction between h_t and h_s.

00:56:37.820 --> 00:56:42.593
And this is a more sophisticated way
of getting an interaction between

00:56:42.593 --> 00:56:43.580
h_t and h_s.

00:56:43.580 --> 00:56:49.330
Whereas if you're using this model with
only a single layer of neural network,

00:56:49.330 --> 00:56:53.630
you don't actually get
interactions between h_t and h_s.

00:56:53.630 --> 00:56:57.300
Because you've got the sort of
two parts of this vector and

00:56:57.300 --> 00:57:01.460
each of them is multiplied by
a separate part of this matrix.

00:57:01.460 --> 00:57:05.330
And then you put it through a tanh, but
that just rescales it element-wise.

00:57:05.330 --> 00:57:09.318
And then you multiply it by a vector,
but that just rescales it element-wise.

00:57:09.318 --> 00:57:14.122
So there's no place that h_t and
h_s actually interact with each other.

00:57:14.122 --> 00:57:18.928
And that's essentially the same
problem of the sort of classic result

00:57:18.928 --> 00:57:22.764
that you can't get an xor function
out of a one layer perceptron is

00:57:22.764 --> 00:57:27.290
because you can't get the two
things to interact with each other.

00:57:27.290 --> 00:57:30.890
So, this is a very
simple low parameter way

00:57:30.890 --> 00:57:33.160
in which you can actually
have interaction terms.

00:57:33.160 --> 00:57:36.010
It seems to work really well for
attention functions.

00:57:36.010 --> 00:57:38.120
It's not the only way
that you could do it.

00:57:38.120 --> 00:57:42.170
Another way that you could do things that
a couple of papers have used is to say,

00:57:42.170 --> 00:57:45.180
well, gee,
a one way neural net's just not enough.

00:57:45.180 --> 00:57:48.370
Let's make it a two layer
feedforward network.

00:57:48.370 --> 00:57:52.330
And then we could have arbitrary
interactions again like the xor model.

00:57:52.330 --> 00:57:55.001
And a couple of people have
also played with that.

00:57:58.537 --> 00:58:02.630
Another thing that has been explored for
attention that I'll just mention.

00:58:02.630 --> 00:58:06.968
So the simple model of attention,
you've got this attention function.

00:58:06.968 --> 00:58:11.288
That spreads attention over
the entire source encoding.

00:58:11.288 --> 00:58:13.445
And you've got a weighting on it.

00:58:13.445 --> 00:58:15.995
That's kind of simple, it's easy to learn.

00:58:15.995 --> 00:58:18.757
It's a continuous,
nice differentiable model.

00:58:18.757 --> 00:58:23.251
It's potentially unpleasant
computationally if you've got very long

00:58:23.251 --> 00:58:24.066
sequences.

00:58:24.066 --> 00:58:28.683
Because that means if you start thinking
about your back prop algorithm that you're

00:58:28.683 --> 00:58:31.361
back propagating into
everywhere all the time.

00:58:31.361 --> 00:58:35.226
So people have also looked some
at having local attention models.

00:58:35.226 --> 00:58:39.826
Where you're only paying attention to
a subset of the states at one time.

00:58:39.826 --> 00:58:44.965
And that's more of an exact notion of
retrieving certain things from memory.

00:58:44.965 --> 00:58:49.036
And that can be good,
especially for long sequences.

00:58:49.036 --> 00:58:54.344
It's not necessarily compellingly better
just for the performance numbers so far.

00:58:54.344 --> 00:58:59.182
Okay, so here's a chart that shows you
how some of the performance works out.

00:58:59.182 --> 00:59:05.684
So what we see is that this
red model has no attention.

00:59:05.684 --> 00:59:08.191
And so this shows the result that,

00:59:08.191 --> 00:59:14.076
a no attention model works reasonably
well up to sentences of about length 30.

00:59:14.076 --> 00:59:19.074
But if you try and run a no
attention machine translation system

00:59:19.074 --> 00:59:21.814
on sentences beyond length 30.

00:59:21.814 --> 00:59:25.602
Performance just starts
to drop off quite badly.

00:59:25.602 --> 00:59:31.550
And so in some sense this is
the glass half full story.

00:59:31.550 --> 00:59:38.060
The glass half full is actually LSTMs
are just miraculous at remembering things.

00:59:38.060 --> 00:59:41.770
I mean,
I think quite to many peoples' surprise,

00:59:41.770 --> 00:59:46.150
you can remember out to about length
30 which is actually pretty stunning.

00:59:46.150 --> 00:59:49.480
But nevertheless,
there's magic and there's magic.

00:59:49.480 --> 00:59:51.400
And you don't get an infinite memory.

00:59:51.400 --> 00:59:55.310
And if you're trying translate
sentences that are 70 words long.

00:59:55.310 --> 01:00:02.140
You start to suffer pretty badly with
the basic LSTM model, oops, okay.

01:00:02.140 --> 01:00:07.944
So then the models that are higher up
is then showing models with attention,

01:00:07.944 --> 01:00:11.037
and I won't go through all the details.

01:00:11.037 --> 01:00:15.160
The interesting thing is that even for
these shorter sentences.

01:00:15.160 --> 01:00:18.478
Actually there are a lot of gains from
putting attention into the models.

01:00:18.478 --> 01:00:23.385
That it actually does just let you do
a much better job of working out where

01:00:23.385 --> 01:00:25.884
to focus on at each generation step.

01:00:25.884 --> 01:00:27.638
And you translate much better.

01:00:27.638 --> 01:00:32.084
But the most dramatic result is
essentially these curves turn into flat

01:00:32.084 --> 01:00:35.680
lines, there's a little
bit of a peak here, maybe.

01:00:35.680 --> 01:00:40.300
But essentially you can be translating
out to 70 word sentences without your

01:00:40.300 --> 01:00:42.390
performance going downhill.

01:00:42.390 --> 01:00:44.118
And that's interesting.

01:00:44.118 --> 01:00:48.859
The one thing that you might think freaky
about all of these charts is that they all

01:00:48.859 --> 01:00:51.169
go downhill for very short sentences.

01:00:51.169 --> 01:00:52.885
That's sort of weird.

01:00:52.885 --> 01:00:59.003
But I think it's sort of just
a weirdo fact about the data.

01:00:59.003 --> 01:01:03.160
That it turns out that
the things that are in this kind

01:01:03.160 --> 01:01:07.427
of data which is
European Parliament data actually.

01:01:07.427 --> 01:01:09.541
That are five word sentences.

01:01:09.541 --> 01:01:12.345
They just aren't sentences,
like, I love my mum,

01:01:12.345 --> 01:01:16.569
which is a four word sentence that has
a really simple grammatical structure.

01:01:16.569 --> 01:01:21.442
That, when you're seeing five
word things that they're normally

01:01:21.442 --> 01:01:23.240
things like titles of x.

01:01:23.240 --> 01:01:26.213
Or that there are half sentences
that were cut off in the middle and

01:01:26.213 --> 01:01:27.089
things like that.

01:01:27.089 --> 01:01:30.202
So that they're sort of weirdish stuff and

01:01:30.202 --> 01:01:33.930
that's why that tends to
prove hard to translate.

01:01:33.930 --> 01:01:36.636
Okay, here are just a couple of examples,

01:01:36.636 --> 01:01:39.952
of giving you again some
examples of translations.

01:01:39.952 --> 01:01:43.320
So we've got a source,
a human reference translation.

01:01:43.320 --> 01:01:47.160
Then down at the bottom,
we have the LSTM model.

01:01:47.160 --> 01:01:49.700
And above it, it's putting in attention.

01:01:50.760 --> 01:01:54.020
So for this sentence,
it does a decent job,

01:01:54.020 --> 01:01:59.030
the base model of translating it,
except for one really funny fact.

01:01:59.030 --> 01:02:03.372
It actually sticks in here a name that
has nothing whatsoever to do with

01:02:03.372 --> 01:02:04.782
the source sentence.

01:02:04.782 --> 01:02:08.336
And that's something that you
actually notice quite a bit in neural

01:02:08.336 --> 01:02:10.091
machine translation systems.

01:02:10.091 --> 01:02:13.446
Especially ones without attention.

01:02:13.446 --> 01:02:16.642
That they are actually
very good language models.

01:02:16.642 --> 01:02:22.328
So that they generate sentences that
are good sentences of the target language.

01:02:22.328 --> 01:02:27.359
But they don't necessarily pay very much
attention to what the source sentence was.

01:02:27.359 --> 01:02:30.572
And so they kind of go, okay,
I'm generating a sentence and

01:02:30.572 --> 01:02:32.659
a name goes there, stick in some name.

01:02:32.659 --> 01:02:36.290
And let's get on with generating, it's got
nothing to do with the source sentence.

01:02:36.290 --> 01:02:38.640
That gets better in the other example,

01:02:38.640 --> 01:02:40.310
where it actually
generates the right name.

01:02:40.310 --> 01:02:42.320
That's an improvement.

01:02:42.320 --> 01:02:47.373
Here's a much more complex example
where there's various stuff going on.

01:02:47.373 --> 01:02:52.483
One thing to focus on though, is that
the source has this "not incompatible"

01:02:52.483 --> 01:02:56.950
whereas the base model translates
that as "not compatible",

01:02:56.950 --> 01:02:59.980
which is the opposite semantics.

01:02:59.980 --> 01:03:05.170
Whereas our one here we're then
getting "the incompatible".

01:03:05.170 --> 01:03:07.110
So not incompatible.

01:03:07.110 --> 01:03:09.270
So that's definitely an improvement.

01:03:09.270 --> 01:03:12.407
None of these translations are perfect.

01:03:12.407 --> 01:03:16.625
I mean in particular one of the things
that they do wrong is "safety and

01:03:16.625 --> 01:03:17.366
security".

01:03:17.366 --> 01:03:19.523
Where in the translation,

01:03:19.523 --> 01:03:23.565
we have exactly the same words,
so it's of the form A and A.

01:03:23.565 --> 01:03:28.162
Now really safety and
security have a fairly similar meaning.

01:03:28.162 --> 01:03:29.497
So it's not actually so

01:03:29.497 --> 01:03:33.180
unreasonable to translate either
of those words with this word.

01:03:33.180 --> 01:03:38.220
But clearly you don't want to translate
safety and security as safety and safety.

01:03:38.220 --> 01:03:41.000
[LAUGH] That's just not
a very good translation.

01:03:41.000 --> 01:03:43.258
So that could be better.

01:03:43.258 --> 01:03:46.610
I'll go on.

01:03:46.610 --> 01:03:47.190
Yeah.

01:03:47.190 --> 01:03:53.510
So this idea of attention
has been a great idea.

01:03:53.510 --> 01:03:57.050
Another idea that's been interesting
is the idea of coverage.

01:03:57.050 --> 01:03:58.520
That when you're attending,

01:03:58.520 --> 01:04:03.670
you want to make sure you've attended
to different parts of the input, and

01:04:03.670 --> 01:04:08.010
that was actually an idea that, sort of,
again, first came up in Vision.

01:04:08.010 --> 01:04:10.670
So, people have done Caption Generation,

01:04:10.670 --> 01:04:15.585
where you're wanting to generate
a caption that summarizes a picture.

01:04:15.585 --> 01:04:18.035
And so
one of the things you might wanna do

01:04:18.035 --> 01:04:22.215
is when you're paying
attention to different places,

01:04:22.215 --> 01:04:26.635
you wanna make sure you're paying
attention to the different main parts.

01:04:26.635 --> 01:04:29.105
So you both wanna pay
attention to the bird.

01:04:29.105 --> 01:04:32.725
And you wanna pay attention to
the background so you're producing

01:04:32.725 --> 01:04:36.640
a caption that's something like "a
bird flying over a body of water".

01:04:38.010 --> 01:04:42.030
And so you don't want to miss
important image patches.

01:04:42.030 --> 01:04:48.510
And so that's an idea that people have
also worked on in the neural MT case.

01:04:48.510 --> 01:04:53.400
So one idea is an idea of doing
sort of attention doubly, and

01:04:53.400 --> 01:04:56.880
so you're sort of working out
an attention in both directions.

01:04:56.880 --> 01:05:01.140
So there's a horizontal attention and
a vertical attention.

01:05:01.140 --> 01:05:06.130
And you're wanting to make sure you've
covered things in both directions.

01:05:08.350 --> 01:05:10.890
Okay, so that's one idea.

01:05:10.890 --> 01:05:16.420
And in general, something interesting
that's been happening is in the last

01:05:16.420 --> 01:05:18.380
roughly a year, I guess.

01:05:18.380 --> 01:05:21.610
That essentially,
people have been taking a number of

01:05:21.610 --> 01:05:26.290
the ideas that have been explored in other
approaches to machine translation and

01:05:26.290 --> 01:05:30.420
building them into more
linguistic attention functions.

01:05:30.420 --> 01:05:34.050
So one idea is this idea of coverage.

01:05:34.050 --> 01:05:38.570
But actually if you look in the older
literature for word alignments, well there

01:05:38.570 --> 01:05:45.750
are some other ideas in those older
machine translation word alignment models.

01:05:45.750 --> 01:05:50.510
Some of the other ideas
were an idea of position.

01:05:50.510 --> 01:05:57.040
So normally attention or alignment isn't
completely sort of random in the sentence.

01:05:57.040 --> 01:06:01.470
Normally although there's some reordering,
stuff near the beginning of the source

01:06:01.470 --> 01:06:05.850
sentence goes somewhere near the beginning
of the translation, and stuff somewhere

01:06:05.850 --> 01:06:09.790
near the end of the source sentence
goes towards the end of the translation.

01:06:09.790 --> 01:06:15.790
And that's an idea you can put in
to your attention model as well.

01:06:15.790 --> 01:06:18.560
And a final idea here is fertility.

01:06:18.560 --> 01:06:21.620
Fertility is sort of
the opposite of coverage.

01:06:21.620 --> 01:06:27.870
It's sort of saying it's bad if you pay
attention to the same place too often.

01:06:27.870 --> 01:06:32.580
Because sometimes one word is gonna
be translated with two words or

01:06:32.580 --> 01:06:36.020
three words in the target
language that happens.

01:06:36.020 --> 01:06:41.930
But if you're translating one word with
six words in your generated translation,

01:06:41.930 --> 01:06:45.430
that probably means that you've
ended up repeating yourself and

01:06:45.430 --> 01:06:48.300
that's another of the mistakes
of sometimes neural

01:06:48.300 --> 01:06:52.430
machine translations systems can make,
that they can repeat themselves.

01:06:52.430 --> 01:06:56.960
And so people have started to build
in those ideas of fertility as well.

01:06:59.070 --> 01:07:00.553
Okay.

01:07:00.553 --> 01:07:03.538
Any questions or
people good with the attention?

01:07:08.109 --> 01:07:08.791
Yeah?

01:07:22.147 --> 01:07:26.350
So the question is that when we're
doing the attention function,

01:07:26.350 --> 01:07:33.050
we were just We were just doing
it based on the hidden state.

01:07:33.050 --> 01:07:39.945
And another thing that we could do is
actually put in the previous word, the xt.

01:07:39.945 --> 01:07:43.540
And also put that into
the attention function.

01:07:44.770 --> 01:07:48.470
I mean one answer is to say yes,
of course you could.

01:07:48.470 --> 01:07:50.810
And you could go off and try that.

01:07:50.810 --> 01:07:54.410
And see if you could get value from it.

01:07:55.720 --> 01:07:59.240
And it's not impossible you could.

01:07:59.240 --> 01:08:03.420
I suspect it's less likely that
that's really going to work

01:08:03.420 --> 01:08:08.410
because I think a lot of the time,
what you get with these LSTMs

01:08:08.410 --> 01:08:12.690
is that the hidden state,
to a fair degree.

01:08:12.690 --> 01:08:16.290
Is still representing the word
that you've just read in, but

01:08:16.290 --> 01:08:20.850
it actually has the advantage that
it's kind of a context-disambiguated

01:08:20.850 --> 01:08:22.790
representation of the words.

01:08:22.790 --> 01:08:28.090
So one of the really useful things that
LSTMs do is that they're sort of very good

01:08:28.090 --> 01:08:33.630
at word-sense disambiguation because
you start with a word representation.

01:08:33.630 --> 01:08:38.330
Which is often the kind of average of
different senses and meanings of a word.

01:08:38.330 --> 01:08:44.020
And the LSTM can use its
preceeding context to decide,

01:08:44.020 --> 01:08:48.380
In this context, I should be
representing this word in this way.

01:08:48.380 --> 01:08:51.140
And you kind of get this
word sense disambiguation.

01:08:51.140 --> 01:08:56.230
So I suspect most of the time
that the hidden state

01:08:56.230 --> 01:09:00.760
records enough about the meaning of the
word and actually improves on it by some

01:09:00.760 --> 01:09:06.860
of this using of context that I'm a little
doubtful whether that would give gains.

01:09:06.860 --> 01:09:10.050
On the other hand, I'm not actually
aware of someone that's tried that.

01:09:10.050 --> 01:09:13.180
So it's totally in the space
of someone could try it and

01:09:13.180 --> 01:09:15.450
see if you could get value from it.

01:09:15.450 --> 01:09:15.968
Yes.

01:09:21.031 --> 01:09:25.403
Yes, there's a very good reason to use
an LSTM as your generator even if you're

01:09:25.403 --> 01:09:26.990
going to do attention.

01:09:26.990 --> 01:09:33.880
Which is, the most powerful part of
these neural machine translation systems

01:09:33.880 --> 01:09:38.950
remains the fact that you've got this
neural language model as your generator

01:09:38.950 --> 01:09:44.930
which is extremely powerful and
good as a fluent text generator.

01:09:44.930 --> 01:09:50.610
And that's still being powered
by the LSTM of the decoder.

01:09:50.610 --> 01:09:51.875
And so.

01:09:51.875 --> 01:09:54.620
[INAUDIBLE]
&gt;&gt; And no, I.

01:09:54.620 --> 01:09:59.135
The power you get from the LSTM at
better remembering the sort of longer

01:09:59.135 --> 01:10:03.810
short-term memory is really useful
as a language model for generation.

01:10:03.810 --> 01:10:06.970
So I'm sure that that's still
giving you huge value, and

01:10:06.970 --> 01:10:09.360
you'd be much worse off without it.

01:10:10.600 --> 01:10:11.100
Yeah.

01:10:11.100 --> 01:10:16.460
I mean the thing that you could wonder,
is in this picture I'm still feeding

01:10:16.460 --> 01:10:23.830
the final state in to initialize
the LSTM for the decoder.

01:10:23.830 --> 01:10:26.620
Do you need to do that, or
could you just cross that off and

01:10:26.620 --> 01:10:31.010
start with a zero hidden state, and
do it all with the attention model?

01:10:31.010 --> 01:10:32.348
That might actually work fine.

01:10:32.348 --> 01:10:33.524
Yeah?

01:10:33.524 --> 01:10:41.150
&gt;&gt; [INAUDIBLE]
&gt;&gt; That's a good question.

01:10:41.150 --> 01:10:42.520
So where do I have that?

01:10:44.550 --> 01:10:45.396
Here, okay, yeah.

01:10:45.396 --> 01:10:52.980
So in this simple case,
if you sort of are making a hard decision

01:10:52.980 --> 01:10:58.760
to pay attention to only
a couple of places,

01:10:58.760 --> 01:11:04.250
that's a hard decision and so
that then kills differentiability.

01:11:04.250 --> 01:11:10.580
And so the easiest way to sort
of keep everything nice and

01:11:10.580 --> 01:11:15.150
simply differentiable is just to say,
use global attention.

01:11:15.150 --> 01:11:19.150
Put some attention weight
on each position that's

01:11:19.150 --> 01:11:21.410
differentiable the whole way through.

01:11:21.410 --> 01:11:26.540
So if you're making a hard decision here,
traditionally,

01:11:26.540 --> 01:11:29.950
the most correct way to
do this properly and

01:11:29.950 --> 01:11:34.795
train the model is to say, okay, we have
to do this as reinforcement learning.

01:11:34.795 --> 01:11:38.440
'Cause, doing a reinforcement
learning system lets you get

01:11:38.440 --> 01:11:40.540
around the non-differentiability.

01:11:40.540 --> 01:11:44.110
And then, you're in this space of deep
reinforcement learning which has been

01:11:44.110 --> 01:11:45.670
very popular lately.

01:11:45.670 --> 01:11:49.370
And, there are a couple of papers
that have used local attention,

01:11:49.370 --> 01:11:53.890
which have done it using
reinforcement learning training.

01:11:53.890 --> 01:11:58.730
So in the paper that Thang did,
that's not what he did.

01:11:58.730 --> 01:12:03.082
He sort of, I think it's true to say that,
to some extent, he sort of fudged

01:12:03.082 --> 01:12:07.160
the non-differentiability but
it seemed to work okay for him.

01:12:07.160 --> 01:12:10.714
But, I mean,
this is actually an area in which,

01:12:10.714 --> 01:12:15.922
there's been some recent work,
in which people have explored methods

01:12:15.922 --> 01:12:21.043
which in some sense continuing this
tradition of fudging by putting it

01:12:21.043 --> 01:12:26.200
on the more of a theoretical footing and
finding this works very well.

01:12:26.200 --> 01:12:31.242
So, an idea that's been explored
quite a bit in recent work is to say,

01:12:31.242 --> 01:12:35.515
in the forward model we're going
to be making some discreet

01:12:35.515 --> 01:12:39.200
choices as to which positions
to pay attention to.

01:12:39.200 --> 01:12:44.006
In the backwards model,
we’re going to be using

01:12:44.006 --> 01:12:48.579
a soft approximation of those decisions,
and

01:12:48.579 --> 01:12:53.400
we will then do the back
propagation using that.

01:12:53.400 --> 01:12:58.228
So that kind of idea is, You are working
out, say, where to pay attention,

01:12:58.228 --> 01:13:02.075
and you are choosing the states
with the sort of a high need for

01:13:02.075 --> 01:13:06.073
attention, is a hard decision,
but in the backwards model you

01:13:06.073 --> 01:13:11.000
are then having a sort of soft attention
still and you are training with that.

01:13:11.000 --> 01:13:15.857
And so, that leads into ideas like
the Straight Through Estimator

01:13:15.857 --> 01:13:19.760
which has been explored by
Yoshua Bengio's group and

01:13:19.760 --> 01:13:24.980
other recent ideas of Gumbel-Softmaxes,
and things like that.

01:13:24.980 --> 01:13:29.331
And that's actually, sort of been
worked out as another way to explain,

01:13:29.331 --> 01:13:33.051
another way to train these not
really differentiable models,

01:13:33.051 --> 01:13:36.936
which is in some ways easier than
using reinforcement learning.

01:13:40.502 --> 01:13:41.860
I'll go on.

01:13:41.860 --> 01:13:47.670
There was one other last thing,
I did want to sort of squeeze in for

01:13:47.670 --> 01:13:53.704
the end of today, is I just wanted
to say a little bit about what's.

01:13:56.464 --> 01:14:03.859
Okay, so assuming that at source time,
we've got our source sentence,

01:14:03.859 --> 01:14:09.242
we encode it in some way that
we're gonna make use of.

01:14:09.242 --> 01:14:14.142
And, decoders,
that really our decoders are just saying,

01:14:14.142 --> 01:14:19.442
okay here's the meaning we want convey,
produce a sentence,

01:14:19.442 --> 01:14:25.760
that expresses that meaning and
how can we do that decoding successfully.

01:14:25.760 --> 01:14:27.648
And I just sort of wanted to mention for

01:14:27.648 --> 01:14:30.844
couple minutes, what are the options and
how do they work.

01:14:30.844 --> 01:14:35.919
So, one thing in theory
we could do is say,

01:14:35.919 --> 01:14:39.420
okay, well, let's just explore

01:14:39.420 --> 01:14:43.890
every possible sequence of words we
can generate up to a certain length.

01:14:43.890 --> 01:14:47.910
Let's score every one of them with
our model and pick the best one.

01:14:47.910 --> 01:14:53.010
So, we'd literally have an exhaustive
search of possible translations.,

01:14:53.010 --> 01:14:56.080
Well, that's obviously
completely impossible to do.

01:14:56.080 --> 01:14:59.720
Because, not only is that exponential
in the length of what we generate,

01:14:59.720 --> 01:15:01.670
we have this enormous vocabulary.

01:15:01.670 --> 01:15:05.640
It's not even like we're doing
exponential on a base of two or three.

01:15:05.640 --> 01:15:09.610
We're doing exponential on the base
of 100,000 or something like that.

01:15:09.610 --> 01:15:12.320
So, that can't possibly work out.

01:15:12.320 --> 01:15:20.340
So, the obvious idea and the first
thing that people do is -- Sorry.

01:15:20.340 --> 01:15:21.892
I'll get to the obvious one next.

01:15:21.892 --> 01:15:26.270
The second thing,
[LAUGH] the not quite so obvious but

01:15:26.270 --> 01:15:31.890
the probabilistically nice and good thing
to do is to do a sampling based approach.

01:15:31.890 --> 01:15:34.720
Which is a sort of a succesive sampling.

01:15:34.720 --> 01:15:38.490
So, it's sometimes referred
to as Ancestral Sampling.

01:15:38.490 --> 01:15:43.850
So, what we're doing then is we've
generated up to word t-1 and

01:15:43.850 --> 01:15:45.570
then saying okay.

01:15:45.570 --> 01:15:51.240
Based on our model, we have a probability
distribution over the t-th word.

01:15:51.240 --> 01:15:56.378
And so, we sample from that probability
distribution one symbol at a time.

01:15:56.378 --> 01:15:59.606
And we keep on generating
one word at a time,

01:15:59.606 --> 01:16:03.460
until we generate our end
of end of sentence symbol.

01:16:03.460 --> 01:16:08.087
So, we generate a word and
then based on what we have now we do

01:16:08.087 --> 01:16:13.650
a probabilistic sample of the next
word and we continue along.

01:16:13.650 --> 01:16:18.530
So, if you are a theoretician that's
the right practical thing to do

01:16:18.530 --> 01:16:20.180
because if you are doing that,

01:16:20.180 --> 01:16:25.010
you've gotten not only an efficient model
of generating, unlike the first model, but

01:16:25.010 --> 01:16:30.620
you've got one that's unbiased,
asymptotically exact, great model.

01:16:30.620 --> 01:16:35.250
If you're a practical person this
is not a very great thing to do

01:16:35.250 --> 01:16:38.810
because what comes out is
very high variants and

01:16:38.810 --> 01:16:42.430
it's different every time you
decode the same sentence.

01:16:42.430 --> 01:16:45.660
Okay, so the practical easy thing to do,

01:16:45.660 --> 01:16:51.290
which is the first thing that everybody
really does, is a greedy search.

01:16:51.290 --> 01:16:54.750
So, we've generated up
to the T minus one word.

01:16:54.750 --> 01:16:57.120
We wanna generate the t-th word.

01:16:57.120 --> 01:17:02.223
We use our model, we work out what's
the most likely word to generate next,

01:17:02.223 --> 01:17:06.850
and we choose it and
then we repeat that over and

01:17:06.850 --> 01:17:13.300
generate successive next words,
so that's then a greedy search.

01:17:13.300 --> 01:17:17.900
We're choosing best thing given
the preceding subsequence.

01:17:17.900 --> 01:17:22.410
But that, doesn't guarantee us
the best whole sentence because we can

01:17:22.410 --> 01:17:26.920
go wrong in any of a number of ways
because of our greedy decisions.

01:17:26.920 --> 01:17:28.590
So it's super-efficient.

01:17:28.590 --> 01:17:30.830
But is heavily suboptimal.

01:17:30.830 --> 01:17:34.030
So, if you want to do a bit better
than that, which people commonly do,

01:17:34.030 --> 01:17:39.320
the next thing that you think about
trying is then doing a beam search.

01:17:39.320 --> 01:17:44.315
So, for a beam search we're
up to word t-1 and we say,

01:17:44.315 --> 01:17:49.520
gee, what are the five most
likely words to generate next.

01:17:49.520 --> 01:17:54.330
And, we generate all of them and
we have a beam of five.

01:17:54.330 --> 01:17:58.340
And then, when we go on to generate
word T plus one, we say for

01:17:58.340 --> 01:18:03.460
each of those sequences up to length T,
what are the five

01:18:03.460 --> 01:18:07.260
most likely words to generate
is the T plus first word and

01:18:07.260 --> 01:18:11.920
we generate all of them and
well then we've got 25 hypotheses and

01:18:11.920 --> 01:18:16.740
if we kept on doing that, we'd again
be exponential but with a smaller base.

01:18:16.740 --> 01:18:17.930
But we don't wanna do that.

01:18:17.930 --> 01:18:23.180
So, what we do is say, well out of
those 25, which are the five best ones?

01:18:23.180 --> 01:18:25.460
And we keep those five best ones.

01:18:25.460 --> 01:18:29.010
And then, we generate five
possibilities from each of those for

01:18:29.010 --> 01:18:31.120
the T plus two time.

01:18:31.120 --> 01:18:35.850
And so we maintain a constant
size k hypotheses and

01:18:35.850 --> 01:18:37.870
we head along and do things.

01:18:39.630 --> 01:18:44.500
So as K goes to infinity,
that becomes unbiased.

01:18:44.500 --> 01:18:48.880
But in practice our K is small,
so it is biased.

01:18:48.880 --> 01:18:53.260
It doesn't necessarily monotonically
improve as you increase K, but

01:18:53.260 --> 01:18:57.320
in practice it usually does
up to some point, at least.

01:18:57.320 --> 01:19:00.523
It turns out that often there's
a limit to how big you can go for

01:19:00.523 --> 01:19:03.000
it improving,
which might even be quite small.

01:19:03.000 --> 01:19:05.537
Because sometimes,
you actually tend to get worse

01:19:05.537 --> 01:19:09.700
if your model is not very good and
you explore things further down.

01:19:09.700 --> 01:19:11.340
It's not as efficient, right?

01:19:11.340 --> 01:19:15.090
That your efficiency is
going down in K squared.

01:19:15.090 --> 01:19:20.370
So, as soon as you're at a beam of 10
you're 2 orders of magnitude slower

01:19:20.370 --> 01:19:24.580
than the greedy search, but
nevertheless it gives good gains.

01:19:24.580 --> 01:19:28.580
So, here are some results.

01:19:28.580 --> 01:19:32.310
So this is from work
again of Kyunghyun Cho's.

01:19:32.310 --> 01:19:36.787
So, in the middle here we
have the greedy decoding.

01:19:36.787 --> 01:19:42.860
And, we're getting these
numbers like 15.5 and 16.66,

01:19:42.860 --> 01:19:47.183
so something I haven't actually done yet
is explain the machine translation

01:19:47.183 --> 01:19:51.800
evaluation, and that's something I'll
actually do in the next lecture.

01:19:51.800 --> 01:19:55.390
But big is good for these scores.

01:19:55.390 --> 01:20:00.750
So, what you see is that if you
sort of sample 50 translations and

01:20:00.750 --> 01:20:02.472
go with the best one,

01:20:02.472 --> 01:20:07.951
although that gives you some
improvement over the greedy one best.

01:20:07.951 --> 01:20:12.364
The amount of improvement it gives
you isn't actually very much because

01:20:12.364 --> 01:20:15.836
there's such a vast space
that you're sampling from and

01:20:15.836 --> 01:20:20.990
it's quite likely that most of your 50
examples are sampling something bad.

01:20:20.990 --> 01:20:25.740
On the other hand, if you're using
a fairly modest beam of size five or

01:20:25.740 --> 01:20:28.190
ten, that's actually
giving you a very good and

01:20:28.190 --> 01:20:34.140
noticeable gain much bigger than you're
getting from the ancestral sampling.

01:20:34.140 --> 01:20:37.115
And so,
that's basically the state of the art for

01:20:37.115 --> 01:20:41.960
neural machine translation, is people
do beam search with a small beam.

01:20:41.960 --> 01:20:45.310
The good news about that
actually is in statistical

01:20:45.310 --> 01:20:50.118
phrase space machine translation,
people always used a very large beam.

01:20:50.118 --> 01:20:54.550
So people would typically use
a beam size of size 100 or 150, and

01:20:54.550 --> 01:20:58.850
really people would have
liked to use larger.

01:20:58.850 --> 01:21:01.870
Apart from where it's just
computationally too difficult.

01:21:01.870 --> 01:21:05.400
But what people found with neural
machine translation systems

01:21:05.400 --> 01:21:09.865
is small beams like 5 or
10 actually work extremely well and

01:21:09.865 --> 01:21:12.975
conversely bigger beams often
don't work much better.

01:21:12.975 --> 01:21:17.385
Okay, and so that gives us sort of Beam
Search with a small beam as the de facto

01:21:17.385 --> 01:21:19.305
standard in NMT.

01:21:19.305 --> 01:21:22.945
Okay, that's it for today, and we'll have
more of these things on next Tuesday.

