WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.682
[MUSIC]

00:00:04.682 --> 00:00:07.650
Stanford University.

00:00:07.650 --> 00:00:09.240
&gt;&gt; What an exciting ride it has been.

00:00:11.950 --> 00:00:15.620
Before we tackle the limits of deep
learning for natural language processing.

00:00:15.620 --> 00:00:22.000
Some organizational things,
I know some nerves are down to the wire.

00:00:22.000 --> 00:00:27.899
So we wanna say first and foremost, sorry
for some of the craziness around PA4.

00:00:27.899 --> 00:00:29.793
It's a very large class.

00:00:29.793 --> 00:00:35.210
It's a very useful class for
a lot of your guys career, and girls.

00:00:35.210 --> 00:00:40.120
So it will be useful,
even if you might get a point here less,

00:00:40.120 --> 00:00:42.200
here and there less.

00:00:42.200 --> 00:00:44.180
It's a very useful class because it is so

00:00:44.180 --> 00:00:49.350
cutting edge with that cutting
edge research vibe and new models.

00:00:49.350 --> 00:00:50.240
And the class size and

00:00:50.240 --> 00:00:54.090
excitement, it is very hard to make
everything perfect the first time.

00:00:54.090 --> 00:00:58.410
So thanks a lot for all your
feedback on the situation with PA4,

00:00:58.410 --> 00:01:04.710
there is a lot of internal discussion
in the TA staff and between Chris and

00:01:04.710 --> 00:01:12.920
me and we are trying to make it as fair as
possible and help you get off the ground.

00:01:12.920 --> 00:01:16.700
The main thing that is I think,
straightforward and

00:01:16.700 --> 00:01:21.670
everybody's happy about it is that we'll
give you a short 33-hour extension for

00:01:21.670 --> 00:01:23.690
assignment 4 as well as the final project.

00:01:23.690 --> 00:01:29.420
So the new deadline that does not use
any late days is 9:00 AM on Sunday.

00:01:29.420 --> 00:01:31.150
That's this upcoming Sunday.

00:01:31.150 --> 00:01:36.050
And then the hard deadline that sadly we
cannot push any further because we'll have

00:01:36.050 --> 00:01:42.430
to actually grade the almost 700 students'
projects, is 9:00 AM on Wednesday.

00:01:42.430 --> 00:01:46.960
We have to submit the grades just a few
days later to let people graduate and

00:01:46.960 --> 00:01:50.993
all that, so that is the hard deadline,
there's no extension.

00:01:50.993 --> 00:01:51.600
Yes?

00:01:51.600 --> 00:01:53.870
&gt;&gt; [INAUDIBLE]
&gt;&gt; How do you submit it?

00:01:53.870 --> 00:01:57.319
The submission instructions
should be on the.

00:02:02.149 --> 00:02:04.118
Ideally, if you do PA4,

00:02:04.118 --> 00:02:08.420
you submit to CodaLab to get
the official number as well.

00:02:08.420 --> 00:02:11.637
You must, not ideally.

00:02:11.637 --> 00:02:14.960
Totally required.

00:02:14.960 --> 00:02:19.867
It looks like there are at
least a dozen or two groups for

00:02:19.867 --> 00:02:24.700
whom it would be ideal, and
hopefully they will get something.

00:02:24.700 --> 00:02:28.290
So we'll go a little bit into
how to help those folks.

00:02:29.590 --> 00:02:31.730
All right,
then don't forget the poster session.

00:02:31.730 --> 00:02:35.690
It is now actually just slightly
before that final deadline.

00:02:35.690 --> 00:02:39.900
But really at the poster session
we wanna be able to get a sense of

00:02:39.900 --> 00:02:40.840
what your project is about.

00:02:41.990 --> 00:02:44.060
Really the last nine hours or

00:02:44.060 --> 00:02:47.940
so that you have of mental time
between the poster session and

00:02:47.940 --> 00:02:52.175
the very final deadline, you should just
be spending on writing a nice report.

00:02:52.175 --> 00:02:55.250
Editing, nicer looking plots and
things like that.

00:02:55.250 --> 00:02:58.130
And maybe finishing that last
cross validation experiment

00:02:58.130 --> 00:03:01.410
to tweak your performance by 1 or 2%.

00:03:01.410 --> 00:03:05.030
So we expect not too many
excuses at the poster sessions,

00:03:05.030 --> 00:03:06.380
saying, this is just a poster.

00:03:06.380 --> 00:03:09.240
But in nine hours it will be much,
much better and different.

00:03:09.240 --> 00:03:14.420
Really will be looking at that
poster as your main project output.

00:03:14.420 --> 00:03:19.120
So the session itself is 5% of your grade,
the final PA4, and

00:03:19.120 --> 00:03:23.170
the final project are 27% of your grade.

00:03:23.170 --> 00:03:25.410
Any questions around the poster session?

00:03:27.340 --> 00:03:29.830
Organizational things?

00:03:29.830 --> 00:03:33.265
All right, so another update.

00:03:33.265 --> 00:03:36.350
I'll get back to poster session in a bit.

00:03:36.350 --> 00:03:38.979
Another updated on PA4.

00:03:38.979 --> 00:03:43.530
First we thought there, okay, there is
a couple groups really struggling hard.

00:03:43.530 --> 00:03:47.490
We'll give them some more helper code.

00:03:47.490 --> 00:03:49.460
It's not really starter
code at this point anymore.

00:03:49.460 --> 00:03:50.820
It's just helping you out.

00:03:50.820 --> 00:03:53.630
Even the modifications of the starter
code were just pretty minor.

00:03:53.630 --> 00:03:58.510
Then, there was a huge backlash of
all the students who did put in

00:03:58.510 --> 00:04:02.870
all the work to get to that
baseline model themselves, and

00:04:02.870 --> 00:04:08.040
that backlash seem to be larger than
the excitement by the students.

00:04:08.040 --> 00:04:10.840
And so again we're trying to
balance things out a lot.

00:04:10.840 --> 00:04:15.880
In general, I hope you appreciate the hard
work that all the TA's are doing.

00:04:15.880 --> 00:04:19.130
Back when I was undergrad in Germany,
people were just like, you're 10 minutes

00:04:19.130 --> 00:04:21.300
late of your assignment submission,
you get zero out of your assignment.

00:04:21.300 --> 00:04:25.695
If you can't make the final project or
the final deadline for the midterm or

00:04:25.695 --> 00:04:28.508
something, you just take
the class next year.

00:04:28.508 --> 00:04:34.470
So, [LAUGH] hopefully we're making
everybody a lot happier than those times,

00:04:34.470 --> 00:04:37.500
and we're trying really to be really fair.

00:04:37.500 --> 00:04:42.560
So with that said,
we'll give you some starter pseudo-code,

00:04:42.560 --> 00:04:47.814
that is our way of trying to make
the two balance the least unhappy.

00:04:47.814 --> 00:04:52.068
Really the startup pseudo-code is super
simple, I've given it to a couple of

00:04:52.068 --> 00:04:56.145
people who were struggling with QA and
who came to my office already before.

00:04:56.145 --> 00:05:00.453
But it's something that you should all
be able to implement fairly quickly at

00:05:00.453 --> 00:05:01.310
this point.

00:05:01.310 --> 00:05:03.280
And so I'll walk you through a little bit.

00:05:03.280 --> 00:05:07.660
This starter code implemented properly and
tuned well,

00:05:07.660 --> 00:05:12.532
the right hyper-parameters and so on,
should get you at least above 50% F1.

00:05:12.532 --> 00:05:16.912
And the code is essentially, you just
pipe your question through an LSTM,

00:05:16.912 --> 00:05:19.636
you get the final hidden
state of a question q.

00:05:19.636 --> 00:05:22.830
You pipe your input through an LSTM.

00:05:22.830 --> 00:05:25.886
You get an output at each hidden state,

00:05:25.886 --> 00:05:29.920
let's call it x_i at
each word in the input.

00:05:29.920 --> 00:05:33.675
And then you just take a neural
network to classify with an input,

00:05:33.675 --> 00:05:36.405
the two inputs are the question vector,
the final hidden state,

00:05:36.405 --> 00:05:39.975
and each hidden state at
a certain time step, x_i.

00:05:39.975 --> 00:05:44.095
And then you predict the start token,
and you can either use the same or

00:05:44.095 --> 00:05:48.975
probably a different one to predict
the end token for each question.

00:05:50.280 --> 00:05:57.230
So something as simple as that should
get you something like 50% F1 score.

00:05:57.230 --> 00:05:59.500
And then on top of that,
you can do all the bells and

00:05:59.500 --> 00:06:02.430
whistles that the TAs
have talked about before.

00:06:02.430 --> 00:06:07.230
You can take all the elements of
the hidden states of the LSTM,

00:06:07.230 --> 00:06:10.090
and you do an inner product
with the inputs and

00:06:10.090 --> 00:06:14.080
your compute this co-attention or context
matrix, and lots of other extensions.

00:06:14.080 --> 00:06:18.684
But really, we hope that this is something
that's possible for everybody, but

00:06:18.684 --> 00:06:23.372
the groups who have already put in all the
work, that should not be a big surprise.

00:06:23.372 --> 00:06:26.654
And they may have some version of these,
and

00:06:26.654 --> 00:06:30.656
probably more advanced
versions than that already.

00:06:30.656 --> 00:06:37.363
All right, any questions about
the starter code, the project?

00:06:56.732 --> 00:07:01.961
So, I guess, the question is any advice
on should we stick to what we have or

00:07:01.961 --> 00:07:03.880
use this simple baseline.

00:07:03.880 --> 00:07:06.710
I guess it depends on where
you are with your F1.

00:07:06.710 --> 00:07:10.080
If you're much above that, then you
probably don't have to get back to this

00:07:10.080 --> 00:07:14.770
and you probably in your current
model capture something of that sort.

00:07:14.770 --> 00:07:18.980
In general, these first two steps are good
steps for pretty much every model, so

00:07:18.980 --> 00:07:22.210
if you haven't done that
just throw that in there.

00:07:22.210 --> 00:07:25.965
These you probably have done
something more advanced by now,

00:07:25.965 --> 00:07:28.345
and if you get that then that's fine.

00:07:28.345 --> 00:07:30.535
Sometimes, there's always a fine balance,

00:07:30.535 --> 00:07:33.365
and you might be really
annoyed with how hard this is.

00:07:33.365 --> 00:07:37.395
But this is really also what we would like
you to teach and learn about the field,

00:07:37.395 --> 00:07:40.445
and sometimes it's frustrating,
and sometimes you're really stuck.

00:07:40.445 --> 00:07:44.405
And then learning exactly how to deal with
this is actually a super valuable skill,

00:07:44.405 --> 00:07:48.150
both for Academic research as
well as industrial research.

00:07:48.150 --> 00:07:50.890
Sometimes it's very hard
to set up your problem and

00:07:50.890 --> 00:07:52.910
know where to get started from.

00:07:52.910 --> 00:07:58.800
And so as you put these together,
sometimes you'll have a trade off.

00:07:58.800 --> 00:08:02.760
You can tune a baseline more and
get higher or

00:08:02.760 --> 00:08:05.600
you have a not well tuned baseline and

00:08:05.600 --> 00:08:10.970
add some more complex model variance
to that baseline and also get better.

00:08:10.970 --> 00:08:13.170
And so it's always a fine balance.

00:08:13.170 --> 00:08:15.675
I think the default Is

00:08:15.675 --> 00:08:19.205
just make sure you have a baseline
that is set up that is correct.

00:08:19.205 --> 00:08:22.817
And that kind of simple baseline
should get you at least 50%.

00:08:22.817 --> 00:08:27.548
Really if you tune that a lot with lots of
crazy interesting dropout over recurrent

00:08:27.548 --> 00:08:32.690
nets and so on you could get up to to
60% F1 with this kind of simple model.

00:08:32.690 --> 00:08:34.580
Now, you don't need to tune it to death.

00:08:34.580 --> 00:08:37.712
Sometimes, you basically get sort
of diminishing returns, right?

00:08:37.712 --> 00:08:40.830
If you tune it a little bit, you get
a couple of percent improvements, and

00:08:40.830 --> 00:08:44.276
then the last couple of improvements of
the baseline might be harder and harder.

00:08:44.276 --> 00:08:45.315
And it might be faster for

00:08:45.315 --> 00:08:47.850
you to just implement a slightly
more sophisticated model.

00:08:49.480 --> 00:08:53.130
And that's true for generally all
sort of people running NLP systems.

00:08:56.551 --> 00:08:57.820
Great question.

00:08:59.691 --> 00:09:00.461
All right,

00:09:00.461 --> 00:09:05.425
now one last note on the poster session
before we get on to the last limits.

00:09:05.425 --> 00:09:07.425
Some of those limits actually
include question and answering.

00:09:07.425 --> 00:09:10.465
So we will talk about the dynamic
co-attention network which some

00:09:10.465 --> 00:09:11.825
of you may know now.

00:09:11.825 --> 00:09:16.250
But, again, everybody is expected
to attend the poster session.

00:09:16.250 --> 00:09:21.274
If you can not attend,
you have to submit a small video and

00:09:21.274 --> 00:09:25.788
ask for an exception,
especially SCPD students.

00:09:25.788 --> 00:09:27.741
Everybody is in two blocks.

00:09:27.741 --> 00:09:30.941
We hope that in the block that you're not
assigned, you can actually walk around and

00:09:30.941 --> 00:09:32.890
see other student's projects.

00:09:32.890 --> 00:09:36.565
I can guarantee you that there's
some really exciting and

00:09:36.565 --> 00:09:38.743
interesting projects out there.

00:09:38.743 --> 00:09:42.871
And it'll be just I think fun to talk to
students even if you're a little sleep

00:09:42.871 --> 00:09:44.940
deprived, maybe just before.

00:09:44.940 --> 00:09:47.801
I'm sure I was in most of mine.

00:09:47.801 --> 00:09:51.064
You will have a very nice lunch,
lots of food.

00:09:51.064 --> 00:09:55.220
And because it's public there's
a lot of excitement around this.

00:09:55.220 --> 00:09:59.210
That's kind of what I meant, too,
of yes, it's much harder, this class and

00:09:59.210 --> 00:10:00.760
especially this PA4.

00:10:00.760 --> 00:10:03.648
But it is also a lot more useful
than a lot of other classes.

00:10:03.648 --> 00:10:07.591
I personally know many dozens of people
who took many versions of this class

00:10:07.591 --> 00:10:10.722
before and they got job offers
just because of that class and

00:10:10.722 --> 00:10:13.560
what they've done and
their projects in this class.

00:10:13.560 --> 00:10:16.330
So there will be lots of companies and

00:10:16.330 --> 00:10:19.880
representatives from those companies,
there will be VCs and who knows you

00:10:19.880 --> 00:10:22.750
might even get some seed funding just
because you have an awesome project.

00:10:22.750 --> 00:10:26.550
So it's hopefully that will make you less

00:10:26.550 --> 00:10:30.530
upset about the struggle of
the last week for this project.

00:10:30.530 --> 00:10:32.960
All right, any last questions
about the poster session?

00:10:37.317 --> 00:10:41.787
All right so let's talk about
the limits of single task learning and,

00:10:41.787 --> 00:10:46.365
in general, deep learning for
natural language processing.

00:10:46.365 --> 00:10:52.190
I think so far the field of deep
learning and NLP has gotten very good at

00:10:52.190 --> 00:10:57.030
taking a single dataset task and model and
metric and then optimizing that setting.

00:10:57.030 --> 00:11:01.650
That's kind of what we've also gone
through a lot of examples in this class.

00:11:01.650 --> 00:11:06.900
And thanks to these end to end trainable,
deep learning models,

00:11:06.900 --> 00:11:10.170
the speed of these improvements has
also gotten better and better over time.

00:11:10.170 --> 00:11:14.720
Which is really exciting to see,
especially if you followed the field for

00:11:14.720 --> 00:11:15.300
a long time.

00:11:16.500 --> 00:11:20.880
However, if we continue to start all
these projects from random parameters,

00:11:20.880 --> 00:11:23.440
which we mostly do,
except maybe the word vectors.

00:11:23.440 --> 00:11:26.870
Word vectors are great sort of to
pre-train a lot of your models.

00:11:26.870 --> 00:11:31.771
We won't ever obtain a single natural
language understanding system,

00:11:31.771 --> 00:11:34.500
that we can just kind of converse with and

00:11:34.500 --> 00:11:39.030
one that understands language
in all of its complexity.

00:11:39.030 --> 00:11:39.980
And so

00:11:39.980 --> 00:11:43.880
I personally don't think that a single
unsupervised task can fix that either.

00:11:43.880 --> 00:11:46.376
In fact you'll hear some
people talk about this and

00:11:46.376 --> 00:11:48.419
this is certainly a point of contention.

00:11:48.419 --> 00:11:52.717
Can we have a single unsupervised task and
just solve that really well and

00:11:52.717 --> 00:11:55.310
then get to some kind
of better AI systems?

00:11:55.310 --> 00:12:00.562
I don't think NLP will fall into that
category because largely language

00:12:00.562 --> 00:12:05.487
has actually a lot of supervision and
different kinds of feedback.

00:12:05.487 --> 00:12:09.650
And requires you in the end to
solve a lot of different tasks.

00:12:09.650 --> 00:12:14.570
In language if you want to have a proper
language system, you may have to do

00:12:14.570 --> 00:12:19.120
some sentiment understanding of what
you're getting, given this input.

00:12:19.120 --> 00:12:23.190
But sometimes you also have to logically
reason over certain kinds of facts.

00:12:23.190 --> 00:12:27.440
And other times you have to retrieve
some different facts from a database or

00:12:27.440 --> 00:12:33.060
maybe logically reason over facts in
the database and do some memory retrieval.

00:12:33.060 --> 00:12:37.378
And yet again other times you have to
ground whatever you're talking about in

00:12:37.378 --> 00:12:39.114
the visual or physical world.

00:12:39.114 --> 00:12:42.251
And so there are a lot of
different kinds of components, and

00:12:42.251 --> 00:12:46.080
if we want to have a system that
understands language better and better,

00:12:46.080 --> 00:12:49.730
ideally that system can incorporate
lots of different things.

00:12:49.730 --> 00:12:53.800
And so in a more scientific way,
and the way we kind of described in

00:12:53.800 --> 00:12:58.610
a lot of tasks, we have different kinds
of frameworks for sequence tagging,

00:12:58.610 --> 00:13:03.806
sentence level kinds of classification or
two sentence kinds of classification.

00:13:03.806 --> 00:13:06.865
Like understanding entailment,
logical entailment and things like that.

00:13:06.865 --> 00:13:09.730
And we have a lot of different kinds
of sequence to sequence models.

00:13:10.810 --> 00:13:15.490
And so
as I mentioned a couple of slides ago,

00:13:15.490 --> 00:13:19.360
we have a bunch of obstacles
to get towards such a system.

00:13:19.360 --> 00:13:24.350
And here's just a couple
of very recent papers.

00:13:24.350 --> 00:13:27.938
Several of which I've been involved with,
so I'm very excited about them.

00:13:27.938 --> 00:13:31.410
And then some also, one from Google.

00:13:31.410 --> 00:13:34.842
Where basically,
we're trying to tackle that limit,

00:13:34.842 --> 00:13:40.460
the limits that we have in natural
language processing, especially deep NLP.

00:13:40.460 --> 00:13:43.270
The first one is one that we actually
already talked about which is we didn't

00:13:43.270 --> 00:13:46.690
have a single architecture
let alone a single model.

00:13:46.690 --> 00:13:50.210
Again, architecture might have different
hyper-parameters, different weights for

00:13:50.210 --> 00:13:52.920
the different tasks that you work on.

00:13:52.920 --> 00:13:56.936
And we all ready basically talked
about this dynamic memory network

00:13:56.936 --> 00:13:59.905
which could also be used for
question answering.

00:13:59.905 --> 00:14:03.830
And some form of that you might even
be able to use for question answering.

00:14:03.830 --> 00:14:04.890
But we all ready talked about that.

00:14:04.890 --> 00:14:09.140
So I want to talk about the next obstacle
which we didn't get to last time.

00:14:09.140 --> 00:14:14.680
And that is to actually jointly
learn many tasks in a single model.

00:14:14.680 --> 00:14:19.510
Now, fully joined multitask
learning is really really hard.

00:14:19.510 --> 00:14:20.690
What do I mean by this?

00:14:20.690 --> 00:14:27.750
So basically so far when people talk about
multi-task learning or many-task learning,

00:14:27.750 --> 00:14:31.670
they assume there's a source task and
then there's a target task.

00:14:31.670 --> 00:14:35.030
And they just kind of hope that
the pre-training your neural network

00:14:35.030 --> 00:14:39.950
on the source task will
improve another target task.

00:14:39.950 --> 00:14:44.416
But in my case, I'd ideally have
both of them be trained jointly, so

00:14:44.416 --> 00:14:47.034
instead of having separate decoders for

00:14:47.034 --> 00:14:51.966
instance for different languages of
different classification problems.

00:14:51.966 --> 00:14:56.899
Ideally we have just a single set of
a very large set of different classes we

00:14:56.899 --> 00:15:00.730
might wanna predict about a certain input,
text input.

00:15:02.690 --> 00:15:04.430
Really have the exact same decoder.

00:15:04.430 --> 00:15:07.078
So if we have a sequence
a sequence model and

00:15:07.078 --> 00:15:09.588
we have a question about each sequence.

00:15:09.588 --> 00:15:13.459
Ideally, the sequence decoder can just
output different kinds of answers

00:15:13.459 --> 00:15:17.120
depending on what the question
was about that input.

00:15:17.120 --> 00:15:22.755
Now when people do multitask learning
in many cases they also just

00:15:22.755 --> 00:15:27.035
share lower layers and train those
jointly, but not these higher layers.

00:15:27.035 --> 00:15:28.565
So what I mean by this,

00:15:28.565 --> 00:15:32.775
in natural language processing mostly
we're sharing just the word vectors.

00:15:32.775 --> 00:15:36.013
We don't share other
higher LSTM layers for

00:15:36.013 --> 00:15:39.699
instance across a whole
host of different tasks.

00:15:39.699 --> 00:15:42.945
And computer vision is actually
a little further ahead in that respect.

00:15:42.945 --> 00:15:45.990
In that pre-trained CNN.

00:15:45.990 --> 00:15:49.233
On a very large dataset like
imageNet can actually be used for

00:15:49.233 --> 00:15:50.896
a lot other tasks pretty well.

00:15:50.896 --> 00:15:55.234
You just change the top layer of a deep
convolution neural network in computer

00:15:55.234 --> 00:15:58.171
vision and
you can still get pretty good accuracy and

00:15:58.171 --> 00:16:01.592
transfer a lot of the learnings
from different visual task.

00:16:01.592 --> 00:16:06.350
We still can't really do that
very convincingly in NLP And

00:16:07.360 --> 00:16:11.410
in many cases you'll only read about
multitask learning in the cases

00:16:11.410 --> 00:16:14.740
where the tasks where somewhat related and
hence, helped each other.

00:16:14.740 --> 00:16:18.710
So we know, for instance,
part of speech tagging helps parsing.

00:16:18.710 --> 00:16:21.530
Cuz the parser makes decisions.

00:16:21.530 --> 00:16:24.814
And if it knows a certain
word is a determiner,

00:16:24.814 --> 00:16:29.966
then it's almost clear which word
should be the dependent of the other.

00:16:29.966 --> 00:16:34.540
However, what you rarely ever read about
is when the tasks aren't perfectly related

00:16:34.540 --> 00:16:37.160
and good matches,
they don't help each other.

00:16:37.160 --> 00:16:38.730
They actually hurt each other.

00:16:38.730 --> 00:16:41.710
And so these kind of negative
results are very hard to publish.

00:16:41.710 --> 00:16:44.280
And hence, not talked about very much.

00:16:44.280 --> 00:16:47.980
And so, yeah, these are all the issues,
or at least some of the issues,

00:16:47.980 --> 00:16:50.180
of why multitask learning is really hard.

00:16:50.180 --> 00:16:56.300
And I think at that perimeter of
the limits of deep learning for NLP.

00:16:56.300 --> 00:16:59.522
And so, this is a paper that's
currently in submission,

00:16:59.522 --> 00:17:01.641
that basically tries to tackle that.

00:17:01.641 --> 00:17:06.299
The title of the paper is A Joint
Many-Task Model: Growing a Neural Network

00:17:06.299 --> 00:17:07.840
for Multiple NLP Tasks.

00:17:09.180 --> 00:17:13.120
And the final model is actually
quite a monster, to be honest.

00:17:13.120 --> 00:17:16.110
It has a lot of different components.

00:17:16.110 --> 00:17:19.770
Fortunately, we now know pretty
much all of these components.

00:17:19.770 --> 00:17:24.550
And hence, we can talk about this very
paper, it's not even published yet,

00:17:24.550 --> 00:17:25.960
I mean it's on arXiv.

00:17:25.960 --> 00:17:30.670
But you should be able to understand
all the components of this model now.

00:17:30.670 --> 00:17:33.530
And be able to implement
something very similar.

00:17:33.530 --> 00:17:36.200
So I'll go over it a little
bit in a high level, and

00:17:36.200 --> 00:17:38.500
then we'll zoom in to
the different aspects.

00:17:38.500 --> 00:17:41.170
And feel free to ask any kind of question.

00:17:41.170 --> 00:17:44.459
So the first that we'll do is, we have
some kind of word vector presentations.

00:17:44.459 --> 00:17:49.570
And there are actually some clever things
in this paper about n-gram vectors too,

00:17:49.570 --> 00:17:52.060
instead of just word vectors.

00:17:52.060 --> 00:17:57.690
Which sometimes you have these unknown
words, you can go subword tokens,

00:17:57.690 --> 00:18:02.140
Chris mentioned character models
are in a similar kind of idea.

00:18:02.140 --> 00:18:07.120
And then, the word vectors are basically
given to a series of LSTMs.

00:18:07.120 --> 00:18:10.500
All of these big blocks here are LSTMs.

00:18:10.500 --> 00:18:16.090
And the output of one LSTM is given
as input to the next one, but

00:18:16.090 --> 00:18:20.110
not the just the output from the softmax
but also the hidden states of the LSTMs,

00:18:20.110 --> 00:18:27.050
as a standard when you stack multiple LSTM
nodes or cells on top of one another.

00:18:27.050 --> 00:18:28.470
So you have these short
circuit connections.

00:18:28.470 --> 00:18:32.540
So the first LSTM here will just classify
a part of speech tags at every word.

00:18:33.700 --> 00:18:37.070
The next one will classify beginnings and
endings of chunks.

00:18:37.070 --> 00:18:40.880
And then this one will
do dependency parsing.

00:18:40.880 --> 00:18:44.436
I'll describe how to do that
with a simple LSTM in a second.

00:18:44.436 --> 00:18:47.908
And then, when we classify
dependency parses, for instance,

00:18:47.908 --> 00:18:52.280
we still take as input these short circuit
connections from part of speech tags,

00:18:52.280 --> 00:18:54.290
to each of these higher level tasks.

00:18:55.990 --> 00:18:58.700
And then, at some point, new tasks and

00:18:58.700 --> 00:19:02.740
higher level tasks will require you
to understand two sentences at once.

00:19:02.740 --> 00:19:05.980
So then we have a simple
sort of pooling scheme,

00:19:05.980 --> 00:19:09.040
similar to what we described with
convolusional neural networks,

00:19:09.040 --> 00:19:14.090
where we pool over time for
classifying relatedness and entailment.

00:19:14.090 --> 00:19:18.804
And in the end we can train this entire
beast jointly in one objective function.

00:19:21.402 --> 00:19:25.020
All right, before I jump into details,
any questions high level?

00:19:27.215 --> 00:19:30.320
Great question,
why do we have two of them?

00:19:30.320 --> 00:19:32.256
So this is just, you can think of it,

00:19:32.256 --> 00:19:36.076
if you only have tasks that require
one sentence, you can just have one.

00:19:36.076 --> 00:19:39.189
It's just if you want to
classify how related is this

00:19:39.189 --> 00:19:42.690
sentence to the other sentence,
we just show two.

00:19:42.690 --> 00:19:49.349
And because that's sort of
the highest level we get to,

00:19:49.349 --> 00:19:56.023
we just showed it in one plot
to have all things in there.

00:20:09.076 --> 00:20:11.739
So the question is,
if the relationship is symmetric,

00:20:11.739 --> 00:20:14.640
wouldn't you wanna use the same system for
both sentences?

00:20:14.640 --> 00:20:16.862
So, we do use the same system for
both sentences.

00:20:16.862 --> 00:20:22.200
These two here are completely
identical pieces, and so is this one.

00:20:22.200 --> 00:20:23.430
It's just once you put,

00:20:23.430 --> 00:20:27.260
pipe them through here, that you basically
take into consideration where they are.

00:20:27.260 --> 00:20:33.170
But you can also pool the,
you can pool cross these

00:20:33.170 --> 00:20:36.980
two different final representations, to
make sure that they're symmetric as well.

00:20:39.461 --> 00:20:43.305
All right, so I think it'll become clear,
sort of what's going on,

00:20:43.305 --> 00:20:45.040
when we zoom into the model.

00:20:45.040 --> 00:20:48.380
So, again, we have this character n-grams,

00:20:48.380 --> 00:20:54.700
as well as standard word vectors,
like word2vec that we've learned about.

00:20:54.700 --> 00:21:00.225
And this first layer here is a very
standard part of speech tagging LSTM.

00:21:00.225 --> 00:21:04.843
At every time step we essentially
just have a single layer LSTM, and

00:21:04.843 --> 00:21:07.265
we pipe that into softmax.

00:21:07.265 --> 00:21:10.905
And then, what we also do is actually
we'll compute a label embedding,

00:21:10.905 --> 00:21:14.690
that essentially will allow us to take

00:21:14.690 --> 00:21:19.060
into consideration some of the uncertainty
that the part of speech tagger had.

00:21:19.060 --> 00:21:19.630
The main idea,

00:21:19.630 --> 00:21:24.040
you can think of this basically as another
layer that takes as output the softmax.

00:21:24.040 --> 00:21:27.840
But you can also write it as this
kind of convex combination here,

00:21:27.840 --> 00:21:33.450
where every label that you have,
has associated with it a vector.

00:21:33.450 --> 00:21:38.190
And you basically sum up all these vectors
in a weighted sum here and the weight

00:21:38.190 --> 00:21:43.540
depends on how certain the model was,
to have that label at that time step.

00:21:43.540 --> 00:21:47.536
So for instance, if you have 3 different,
you have over 40, but let's say you had 3

00:21:47.536 --> 00:21:51.270
different part of speech tags, just
adjectives, nouns and verbs or something.

00:21:52.310 --> 00:21:56.820
And basically each of these 3 will
have a vector associated with it,

00:21:56.820 --> 00:22:00.290
say a 50 dimensional random vector,
it's something you'll learn as well.

00:22:00.290 --> 00:22:03.648
And you have some probabilities, you
think like with 0.9 this is a verb, and

00:22:03.648 --> 00:22:06.670
0.05 it's an adjective or a noun.

00:22:06.670 --> 00:22:10.700
Then you multiply these 3 numbers
with their respective embeddings, and

00:22:10.700 --> 00:22:13.900
that will determine
the label embedding for y.

00:22:15.273 --> 00:22:20.605
And so now, those are the outputs
of the POS tagging LSTM.

00:22:20.605 --> 00:22:25.590
And so to go to the next level,
the chunking model will actually give

00:22:25.590 --> 00:22:29.089
as input, again,
the word vectors directly,

00:22:29.089 --> 00:22:34.460
the hidden states from the POS LSTM,
and that label embedding.

00:22:34.460 --> 00:22:39.490
These are all the inputs, and then we just
plug, those are just concatenated, and

00:22:39.490 --> 00:22:42.250
we plug that into another LSTM.

00:22:42.250 --> 00:22:46.120
And that will, again, do something
very similar, where it has as output

00:22:46.120 --> 00:22:49.850
a hidden state softmax, and then a label
embedding for the chunking labels.

00:22:51.580 --> 00:22:53.860
And you could, in theory, do this a lot.

00:22:53.860 --> 00:22:58.593
And some previous similar kinds of
architectures had actually thought about

00:22:58.593 --> 00:23:01.320
putting all of these into the same layer.

00:23:01.320 --> 00:23:02.883
And we compare that, and

00:23:02.883 --> 00:23:07.275
we find it works better if you have
these three different tasks, POS,

00:23:07.275 --> 00:23:11.906
chunking and dependency parsing,
actually all in their own LSTM layer.

00:23:15.692 --> 00:23:17.310
Any questions about that architecture?

00:23:25.936 --> 00:23:30.232
Cool, now on dependency parsing, it's a
little more complicated because in the end

00:23:30.232 --> 00:23:32.930
we wanna have a tree structure, right?

00:23:32.930 --> 00:23:34.720
And so dependency parsing,

00:23:34.720 --> 00:23:38.790
turns out, in many cases,
used to require some kind of beam search.

00:23:38.790 --> 00:23:42.040
But here this model actually
is incredibly simple.

00:23:42.040 --> 00:23:46.930
We, again, have a standard bi-linear,
bidirectional LSTM.

00:23:46.930 --> 00:23:52.210
With now four inputs, the word vectors,
the hidden state of the chunker,

00:23:52.210 --> 00:23:54.950
and the label embeddings for
POS and chunking.

00:23:54.950 --> 00:23:57.270
So these are just four
inputs at every time step.

00:23:57.270 --> 00:24:00.736
And now a bidirectional LSTM,
as we defined it in class.

00:24:00.736 --> 00:24:07.410
And now basically we'll just run
a quadratic number of classifications of

00:24:07.410 --> 00:24:12.120
just saying is this word the dependent of
that word, or of that word, or that word.

00:24:13.410 --> 00:24:18.210
Run through all of them and
it would just take the maximum for

00:24:18.210 --> 00:24:21.440
each of them and
we just say that's the tree.

00:24:21.440 --> 00:24:24.880
Now if you think about this a little
bit it might not even be a proper tree.

00:24:24.880 --> 00:24:28.026
Maybe none of them said I am
classified as I'm the root, so

00:24:28.026 --> 00:24:31.639
I have all like the potential to
classify I'm the root of the tree.

00:24:31.639 --> 00:24:35.325
Or maybe two things
pointed the same parent or

00:24:35.325 --> 00:24:40.090
the same child or they create loops or
anything like that.

00:24:40.090 --> 00:24:43.640
So in theory, this might not
even create proper trees but

00:24:43.640 --> 00:24:48.260
in practice surprisingly it
does in like 99% of the cases.

00:24:48.260 --> 00:24:52.260
There's a very small number of
cases where this very simple feed

00:24:52.260 --> 00:24:55.630
forward architecture does
not give you proper tree and

00:24:55.630 --> 00:25:00.580
you can use basically some very simple
deterministic rule base systems

00:25:00.580 --> 00:25:05.390
to clean up that last less than
1% of non-proper trees and

00:25:05.390 --> 00:25:10.030
just delete certain edges or
add certain like the route to the tree.

00:25:10.030 --> 00:25:11.460
And then you get a proper tree.

00:25:11.460 --> 00:25:16.510
And this actually resulted in the state
of the art dependency parser,

00:25:16.510 --> 00:25:20.940
submitted it but
since then I think one of Chris's papers.

00:25:20.940 --> 00:25:24.050
Just outperformed it
a little bit already again.

00:25:24.050 --> 00:25:28.930
It's never ending fun race
that we all work on together.

00:25:28.930 --> 00:25:33.800
To work on pushing state
of the art on these tasks.

00:25:33.800 --> 00:25:36.520
But yeah, somewhat surprising,
no beam search required,

00:25:36.520 --> 00:25:38.480
just feed-forward computation.

00:25:38.480 --> 00:25:41.010
And you get pretty good
trees most of the time.

00:25:41.010 --> 00:25:45.512
All right, any questions around
the dependency parsing module?

00:25:45.512 --> 00:25:46.132
Yeah?

00:25:52.212 --> 00:25:57.377
You could do a lot more things to improve
and actually add a proper beam search and

00:25:57.377 --> 00:26:01.725
go through several of the scenarios or
something like that.

00:26:01.725 --> 00:26:05.807
Proper SQL you can do because you have
these continuous vectors usually and

00:26:05.807 --> 00:26:10.165
not SQL is also mostly for consistency
parsing as dependency parsing and so on.

00:26:10.165 --> 00:26:14.730
But you could do a lot more
clever things and slow it down.

00:26:14.730 --> 00:26:16.300
Surprisingly you don't have to.

00:26:16.300 --> 00:26:20.623
We just, all of this computation is
parallelizable, it's super fast,

00:26:20.623 --> 00:26:24.964
there's no extra infrastructure needed for
any kind of tree search.

00:26:27.605 --> 00:26:33.275
All right, now the last level is
basically to train multiple sentences for

00:26:33.275 --> 00:26:37.455
different tasks such as
semantic relatedness.

00:26:37.455 --> 00:26:41.545
And what we do here is basically have
a simple temporal max pooling, so

00:26:41.545 --> 00:26:46.855
that last hidden stage of
this LSTM is basically.

00:26:48.430 --> 00:26:52.440
Just will produce a feature
vector at every time step.

00:26:52.440 --> 00:26:57.490
And you will now just look at across all
the feature, the hidden dimensions of

00:26:57.490 --> 00:27:00.350
all the time steps, where's largest
value and you just pick that one.

00:27:00.350 --> 00:27:03.230
So it's kind of why we call
it temporal max-pooling and

00:27:03.230 --> 00:27:06.700
you can then look at again these
simple things like inner-products

00:27:06.700 --> 00:27:09.950
between those features, and
vector distances and so on.

00:27:09.950 --> 00:27:14.060
Extract some features and
pipe that into another softmax to classify

00:27:14.060 --> 00:27:16.980
both relatedness and
entailment kinds of relationships.

00:27:18.190 --> 00:27:23.360
So it looks kind of complicated, but
really it uses all the components

00:27:23.360 --> 00:27:27.720
that we've carefully went through
in class, just in a clever new way.

00:27:27.720 --> 00:27:30.810
Now sadly when you just say,
all right, this is my whole model.

00:27:30.810 --> 00:27:32.930
Now back propagate every
time you had a softmax,

00:27:32.930 --> 00:27:34.890
we use our standard cross entropy error.

00:27:34.890 --> 00:27:38.968
And you just throw that into it,
it doesn't quite work right away.

00:27:38.968 --> 00:27:41.940
There's one extra idea
that you have to use and

00:27:41.940 --> 00:27:45.560
call this sort of
successive regularization.

00:27:45.560 --> 00:27:50.340
Where basically inside each mini-batch
you allow the model to first focus

00:27:50.340 --> 00:27:53.700
on different tasks and
then as you go higher,

00:27:53.700 --> 00:27:59.100
you will regularized the weights of the
lower levels to not change too much and

00:27:59.100 --> 00:28:03.070
that too much is defined by this
regularization term delta here.

00:28:03.070 --> 00:28:08.330
So this is basically,
then one of the novelties of how to make

00:28:08.330 --> 00:28:12.160
the training more robust, and actually
result in the end, with a final system

00:28:12.160 --> 00:28:16.640
that gets the state of the art on four
out of the five tasks that we looked at.

00:28:16.640 --> 00:28:21.650
And so again, intuitively here you have
at the end of the first mini-batch

00:28:21.650 --> 00:28:25.810
where you focused on just part of speech
tagging, you have a set of weights theta

00:28:25.810 --> 00:28:29.510
that define your label embeddings,
your LSTM weights, and so on.

00:28:30.520 --> 00:28:33.280
And you now say when you train the next

00:28:33.280 --> 00:28:37.960
higher level task in chunking
to not move too far away.

00:28:37.960 --> 00:28:42.073
From those weights that were really
well tuned for part of speech tagging.

00:28:42.073 --> 00:28:44.713
And then, as you go higher and higher,

00:28:44.713 --> 00:28:47.913
you basically try to keep
more things the same.

00:28:47.913 --> 00:28:50.773
But if the higher level task really
wants to change a certain weight,

00:28:50.773 --> 00:28:51.571
it can still do it.

00:29:01.335 --> 00:29:05.991
That's right, so the question is as you
train inside each mini-batch or really

00:29:05.991 --> 00:29:10.644
almost like the whole epoch, you can focus
first on each of the different tasks,

00:29:10.644 --> 00:29:14.406
and you do that in a way that you
start with the lower level tasks and

00:29:14.406 --> 00:29:17.581
then you move up through the network,
that's right.

00:29:22.421 --> 00:29:25.481
So each mini-batch is actually
focused on a single task.

00:29:33.809 --> 00:29:37.087
So each mini-batch focuses on one task,
but as you go and

00:29:37.087 --> 00:29:39.629
you finish on that,
you go to the next task.

00:29:45.530 --> 00:29:46.430
That's exactly right.

00:29:46.430 --> 00:29:51.070
When you go to the next task
you have a soft sort of

00:29:51.070 --> 00:29:56.181
regularization or
clamp on those previous slides.

00:30:00.611 --> 00:30:04.910
So that's something that could actually
work for various projects to some folks

00:30:04.910 --> 00:30:09.855
had the idea of using SNLI or entailment
classification as a pre-training step.

00:30:09.855 --> 00:30:14.285
For question answering and those are all
kinds of ideas that you could try as well.

00:30:15.935 --> 00:30:22.635
So, there are, most of those tasks
that joint training actually helps.

00:30:22.635 --> 00:30:25.685
There's a lot of complexity in
getting all these numbers, and

00:30:25.685 --> 00:30:31.210
this whole paper actually has
like over 12 or 15 tables for

00:30:31.210 --> 00:30:36.740
the various ablation studies of using
the successive regularization, yes or no?

00:30:36.740 --> 00:30:38.860
Using character n-grams, yes or no?

00:30:38.860 --> 00:30:40.740
Versus just word vectors.

00:30:40.740 --> 00:30:43.340
Training various combinations
of tasks together.

00:30:43.340 --> 00:30:47.700
There's a lot of experiments
that went into this task.

00:30:47.700 --> 00:30:53.310
Basically overall, these two are sort
of the summary of the table.

00:30:53.310 --> 00:30:56.150
When you look at all the tasks
separately and they train

00:30:56.150 --> 00:31:00.160
all separately versus they're jointly
trained, they basically all improve.

00:31:00.160 --> 00:31:04.890
And these are all tasks that have
been worked on quite a lot so

00:31:04.890 --> 00:31:06.720
you don't see huge improvements.

00:31:06.720 --> 00:31:08.940
Relatedness here is actually
the lower the better.

00:31:08.940 --> 00:31:10.570
So this is also good.

00:31:10.570 --> 00:31:14.910
For some of the higher level tasks
on smaller data sets you also get

00:31:14.910 --> 00:31:15.660
larger improvement.

00:31:15.660 --> 00:31:20.660
So in general joint training with
different tasks often helps.

00:31:20.660 --> 00:31:22.480
When you have less data.

00:31:22.480 --> 00:31:24.701
It helps more when you have
less data per task, right?

00:31:24.701 --> 00:31:26.321
Because then you can transfer more.

00:31:26.321 --> 00:31:31.051
If you have a simple task, that is
only a binary classification problem,

00:31:31.051 --> 00:31:33.569
for instance, or something like SNLI,

00:31:33.569 --> 00:31:37.389
where you have is this entailment
contradictory or neutral and

00:31:37.389 --> 00:31:42.044
you have hundreds of thousands of
examples for each of the three labels,

00:31:42.044 --> 00:31:48.330
then you can probably just get away with
training everything on just that dataset.

00:31:48.330 --> 00:31:52.268
Of their more complex output spaces, so
machine translation for instance, or

00:31:52.268 --> 00:31:55.675
the smaller your data set this is,
the more you benefit from trying to

00:31:55.675 --> 00:31:58.734
jointly train with different
kinds of objective functions,

00:31:58.734 --> 00:32:01.674
having some unsupervised
pre-training of word vectors,

00:32:01.674 --> 00:32:06.510
then maybe some semi-supervised things,
where you have, you continue to train.

00:32:06.510 --> 00:32:11.220
And supervised word vectors together
with some supervised tasks and so on.

00:32:11.220 --> 00:32:17.157
This result here is in parenthesis
because the part of speech tagging and

00:32:17.157 --> 00:32:22.122
chunking subsets are actually,
sorry, the dependency and

00:32:22.122 --> 00:32:27.403
chunking results actually
overlapping on the DEV and test set.

00:32:27.403 --> 00:32:31.379
And so obviously, chunking will
help a lot, in the dependency part.

00:32:31.379 --> 00:32:36.233
You know that inside this chunk
everything should point to one another

00:32:36.233 --> 00:32:38.298
inside the dependency tree.

00:32:38.298 --> 00:32:41.785
And so,
this result is a little too optimistic, so

00:32:41.785 --> 00:32:45.202
we have one that just trains
on these two jointly.

00:32:45.202 --> 00:32:48.040
You still have an improvement,
but it's less strong.

00:32:48.040 --> 00:32:51.729
So these in parenthesis numbers,
when you carefully look at your training,

00:32:51.729 --> 00:32:55.093
in your dev and in your test sets,
you realize there's some overlap.

00:32:55.093 --> 00:32:57.620
And you need to mention that,
we do in the footnote.

00:32:59.430 --> 00:33:02.750
Any questions around the experiments
over the set of this model?

00:33:04.560 --> 00:33:08.550
Now, these are just some
more results of the various,

00:33:08.550 --> 00:33:12.760
just a subset of the many people who have
worked on all these different tasks, and

00:33:12.760 --> 00:33:13.740
sort of the comparison.

00:33:13.740 --> 00:33:17.730
And this is generally something that
I've encouraged you to do in all your

00:33:17.730 --> 00:33:21.150
proper projects, but also something
that you'll see in most good papers.

00:33:21.150 --> 00:33:23.513
You usually have two sets of tables.

00:33:23.513 --> 00:33:27.078
One set of tables is about
you comparing your best model

00:33:27.078 --> 00:33:29.850
to all other people's best models.

00:33:29.850 --> 00:33:33.900
And then the other subset of tables is
about understanding your model better with

00:33:33.900 --> 00:33:36.580
ablations and
modifications to your model and

00:33:36.580 --> 00:33:39.150
decisions that you made about your model.

00:33:39.150 --> 00:33:42.460
And so this set of tables here
is basically the comparison to

00:33:42.460 --> 00:33:44.790
all the other folks,
who have worked on those tasks.

00:33:45.870 --> 00:33:49.001
And in many cases,
it's basically the state of the art model.

00:33:51.690 --> 00:33:56.961
And this is just one of the many tables
of this paper that tries to understand

00:33:56.961 --> 00:34:02.243
all of the various combinations, and
which tasks help which other tasks.

00:34:02.243 --> 00:34:05.918
All right, any questions about
joint many task learning

00:34:24.529 --> 00:34:29.621
So the question is what's the key insight
that made this model work to be honest,

00:34:29.621 --> 00:34:33.279
there are a bunch of them and
they all matter a little bit.

00:34:33.279 --> 00:34:37.478
So having a better word
representations with character n-grams

00:34:37.478 --> 00:34:39.040
help just a little bit.

00:34:39.040 --> 00:34:42.689
In the paper,
you'll see how much they all help.

00:34:42.689 --> 00:34:46.840
And then, having the short
circuit connections helped and

00:34:46.840 --> 00:34:53.150
we have a table that shows all the deltas
for having the short circuit connections.

00:34:53.150 --> 00:34:56.040
From all of the lower level tasks,
outputs, and

00:34:56.040 --> 00:34:59.980
label embeddings directly
to the higher level tasks.

00:34:59.980 --> 00:35:06.346
And then, the successive regularization
helped a little bit also, so yeah.

00:35:06.346 --> 00:35:08.680
It's actually a sequence of things.

00:35:08.680 --> 00:35:12.980
There's no single insight other than
of course having this main model.

00:35:12.980 --> 00:35:18.331
So we also have a table that shows how
much it helps to have three layers for

00:35:18.331 --> 00:35:21.490
all these tasks versus a three layer LSTM,

00:35:21.490 --> 00:35:27.214
where all the tasks are output at the same
height or same depth of the network.

00:35:27.214 --> 00:35:30.811
And we show that it works better
if they're actually sort of,

00:35:30.811 --> 00:35:32.950
each task has its own LSTM.

00:35:32.950 --> 00:35:37.868
So yeah, it's a combination of those and
because there's so

00:35:37.868 --> 00:35:43.543
many moving pieces, there's so many,
over a dozen tables in the paper

00:35:43.543 --> 00:35:48.857
that show how much each helps for
each of the five different tasks.

00:35:52.880 --> 00:35:56.328
No, many of them we
invented in this paper.

00:35:56.328 --> 00:35:58.898
So they weren't available back then.

00:35:58.898 --> 00:36:06.570
And they're, So the question is, and this
is something I brought up myself, right?

00:36:06.570 --> 00:36:11.030
Can you actually add some of
these things to other models?

00:36:11.030 --> 00:36:12.486
And so the word vectors, for

00:36:12.486 --> 00:36:15.833
instance, are an idea that you
could add to all the other models.

00:36:15.833 --> 00:36:20.574
The success of regularization
doesn't really make

00:36:20.574 --> 00:36:24.454
sense unless you have successive layers,

00:36:24.454 --> 00:36:29.853
which no one had really done for
more than two tasks before.

00:36:29.853 --> 00:36:36.234
Some of the model architectures and
the differences are just very novel and

00:36:36.234 --> 00:36:41.730
then you have to think of what
models would actually do this.

00:36:41.730 --> 00:36:46.120
The majority of papers published
on these different tasks

00:36:46.120 --> 00:36:48.450
aren't extendable in that kind of way,
right?

00:36:48.450 --> 00:36:52.830
They're, for instance, graphical models,
where it wouldn't be obvious to just plug

00:36:52.830 --> 00:36:55.080
this vector into this other thing and
something would happen.

00:36:55.080 --> 00:37:00.140
So it's hard to use these insights
on a lot of these previous models.

00:37:00.140 --> 00:37:05.241
Or they have convolutional operators
instead of LSTMs, and so you don't have

00:37:05.241 --> 00:37:10.591
a nice sort of, at this time step I have
this representation and things like that.

00:37:10.591 --> 00:37:12.972
But yeah, at least the word vectors and
character engrams,

00:37:12.972 --> 00:37:15.504
that's a pretty general insight
that a lot of people could use.

00:37:19.919 --> 00:37:20.750
All right, awesome.

00:37:20.750 --> 00:37:25.630
Now, another obstacle that we also
discussed already briefly before

00:37:25.630 --> 00:37:28.690
is that we don't have zero
shot word predictions.

00:37:28.690 --> 00:37:30.280
And what do I mean by this?

00:37:30.280 --> 00:37:34.586
In almost all the cases,
the various models that we described,

00:37:34.586 --> 00:37:38.905
like the machine translation models,
have softmax at the end.

00:37:38.905 --> 00:37:43.251
And you can only predict the words
that you've seen at training time.

00:37:43.251 --> 00:37:47.310
And we've also already covered
how to fix this with pointers.

00:37:47.310 --> 00:37:50.120
And you'll see now in PA4 already

00:37:50.120 --> 00:37:53.330
that we also have there not just
a pointer to a single word but

00:37:53.330 --> 00:37:58.560
pointers to spans of words, so
beginning and end token pointers.

00:37:58.560 --> 00:38:02.100
And actually a interesting side note here.

00:38:02.100 --> 00:38:03.762
Again, we've covered this already.

00:38:03.762 --> 00:38:09.090
But you can also, in this PA4, and
for general question answering,

00:38:09.090 --> 00:38:14.888
try to predict a sequence of single words,
with a set of pointers like this.

00:38:14.888 --> 00:38:18.504
It actually turns out to not work as
well as pointing to the beginning,

00:38:18.504 --> 00:38:21.959
learning to point to the beginning token,
and then the end token.

00:38:21.959 --> 00:38:26.423
That works better by 2 to 5% or
so, depending on how you do it,

00:38:26.423 --> 00:38:30.390
than pointing to a sequence
of different words.

00:38:30.390 --> 00:38:34.561
Basically you make two decisions versus
having to make five decisions if you point

00:38:34.561 --> 00:38:35.778
to a span of five words.

00:38:38.387 --> 00:38:43.438
All right, now let's have our research
highlight on Neural Turing Machines.

00:38:43.438 --> 00:38:44.653
Take it away Nish.

00:38:44.653 --> 00:38:46.402
&gt;&gt; Thanks Richard.

00:38:46.402 --> 00:38:50.020
Hi everyone, today I'll be presenting
on Neural Turing Machines.

00:38:51.950 --> 00:38:53.540
So we'll be covering two papers.

00:38:53.540 --> 00:38:55.540
One on the Neural Turing Machine itself.

00:38:55.540 --> 00:38:57.850
And then a second paper on
differential neural computers,

00:38:57.850 --> 00:39:00.250
which was both of these
papers were from DeepMind.

00:39:00.250 --> 00:39:04.710
And we'll be seeing the architecture
proposed in the first paper and

00:39:04.710 --> 00:39:06.400
then the results from the second paper.

00:39:06.400 --> 00:39:10.230
The architecture modification of
the second paper is only slight and we can

00:39:10.230 --> 00:39:15.385
really just wanna take the high level idea
that these architectures have introduced.

00:39:15.385 --> 00:39:19.025
So all the neural networks
that we have seen in class so

00:39:19.025 --> 00:39:21.225
far excel at pattern matching.

00:39:21.225 --> 00:39:23.975
So you might have heard
of DeepMind's agent

00:39:23.975 --> 00:39:27.695
that played Atari games such as
Breakout with superhuman performance.

00:39:27.695 --> 00:39:30.155
And these tests are relatively easy for

00:39:30.155 --> 00:39:33.955
the network because they have
to make very reactive decisions.

00:39:33.955 --> 00:39:36.990
However, when it comes to
reasoning from knowledge,

00:39:36.990 --> 00:39:38.890
neural networks still struggle at that.

00:39:38.890 --> 00:39:40.850
Consider the problem of
finding the shortest path.

00:39:40.850 --> 00:39:44.858
Now in our introductory algorithm classes,
any algorithm, such as DFS, or

00:39:44.858 --> 00:39:46.215
breadth-first search,

00:39:46.215 --> 00:39:49.630
usually requires us to store which
nodes we have visited before.

00:39:49.630 --> 00:39:53.800
In the current architectures that we
have seen so far, it's really hard for

00:39:53.800 --> 00:39:55.820
networks to store that information.

00:39:55.820 --> 00:39:58.245
So the solution to this
is having more memory.

00:39:58.245 --> 00:40:03.810
But you might be worried, like LSTMs
didn't they already have memory cells?

00:40:03.810 --> 00:40:04.980
It is a valid question,

00:40:04.980 --> 00:40:08.240
but this is not the right kind
of memory we are looking for.

00:40:08.240 --> 00:40:13.530
So if you understand systems peak, if you
consider LSTM's memory cell as a cache.

00:40:13.530 --> 00:40:17.210
What we really need is random
access memory, or RAM.

00:40:17.210 --> 00:40:19.980
And this is where
Neural Turing Machines come in.

00:40:19.980 --> 00:40:25.429
So you can, okay, yeah,
in this architecture

00:40:25.429 --> 00:40:29.950
diagram, the controller is an RNN.

00:40:29.950 --> 00:40:33.100
And it decides whether to read and
write from the memory cells.

00:40:33.100 --> 00:40:36.580
And we'll see how both of these
operations are implemented.

00:40:36.580 --> 00:40:40.330
How does reading and writing work?

00:40:40.330 --> 00:40:43.740
If you have taken previous
systems classes at Stanford,

00:40:43.740 --> 00:40:48.740
you might have realized that memory is
inherently very fundamentally discrete.

00:40:48.740 --> 00:40:50.940
So how do we make it differentiable,

00:40:50.940 --> 00:40:54.480
because we need to optimize
it using back propagation.

00:40:54.480 --> 00:40:59.160
And the answer to that is our friendly
method of attention mechanism.

00:40:59.160 --> 00:41:04.920
Which is read and write everywhere,
but just for different extents.

00:41:04.920 --> 00:41:07.750
And you'll see about how
we go about doing that.

00:41:07.750 --> 00:41:11.970
So, how does reading from memory work?

00:41:11.970 --> 00:41:13.880
So we have this memory vector and

00:41:13.880 --> 00:41:17.190
we have been provided with an attention
vector corresponding to it.

00:41:17.190 --> 00:41:19.900
So, in this case, the first element,
I'm zero indexing here.

00:41:19.900 --> 00:41:25.410
The first element of the attention vector
is blue, which means it has high value.

00:41:25.410 --> 00:41:31.050
And so we read the first element
from the memory vector itself.

00:41:31.050 --> 00:41:34.003
And it's a weighted sum, so
given the attention vector,

00:41:34.003 --> 00:41:35.700
the reading would be different.

00:41:36.820 --> 00:41:41.780
Similarly, in terms of writing, we have
our old memory and we have a write value.

00:41:41.780 --> 00:41:46.502
We want to write everywhere, but how much
do we write each value in the memory by?

00:41:46.502 --> 00:41:48.930
And, again,we use
the attention mechanism here.

00:41:48.930 --> 00:41:52.400
So you can see that the second
element is blue here.

00:41:52.400 --> 00:41:56.190
And although the write value and
memory at that location is at

00:41:56.190 --> 00:41:59.020
opposition locations you can
see that at the new memory.

00:41:59.020 --> 00:42:01.770
The vector has shrunk just
because of different magnitudes.

00:42:01.770 --> 00:42:05.545
Because of similar magnitudes
in the opposite direction.

00:42:05.545 --> 00:42:09.650
It's a convex combination of the write
value as well as the attention.

00:42:11.270 --> 00:42:14.440
So, in both of these cases of read and
write,

00:42:14.440 --> 00:42:16.910
we assumed that we had
a correct attention vector.

00:42:16.910 --> 00:42:18.900
And how do we go about
actually getting that?

00:42:18.900 --> 00:42:21.490
The controller has a query vector, and

00:42:21.490 --> 00:42:25.660
it looks at each point in the memory and
performs a dot product.

00:42:25.660 --> 00:42:28.210
And only to get which one
it's more similar to.

00:42:28.210 --> 00:42:30.720
So in this diagram,
blue indicated high similarity and

00:42:30.720 --> 00:42:33.510
pink indicates very high dissimilarity.

00:42:33.510 --> 00:42:40.140
We perform a softmax, get the memory
that has the most attention.

00:42:42.030 --> 00:42:44.310
Now we have the attention
from the previous step.

00:42:44.310 --> 00:42:47.240
We interpolate from that
attention to finally get

00:42:47.240 --> 00:42:49.820
what part of the memory vector
we should be focusing on now.

00:42:51.120 --> 00:42:53.930
Finally, we can perform the shift vector.

00:42:53.930 --> 00:42:59.270
Now this is what enables us to read
at different locations around that

00:42:59.270 --> 00:43:01.080
focused attention.

00:43:01.080 --> 00:43:04.780
And we then sharpen it to get our
final attention distribution.

00:43:04.780 --> 00:43:09.160
This final attention distribution is then
fed into the read and write operations.

00:43:12.390 --> 00:43:16.000
We can now see your result, I'm not
sure if the media has been incorporated.

00:43:16.000 --> 00:43:17.557
Okay, let's see.

00:43:21.897 --> 00:43:25.033
Just know that this video is from
the differential neural computers which is

00:43:25.033 --> 00:43:27.877
a slightly new architecture compared
to the Neural Turing Machine.

00:43:27.877 --> 00:43:31.764
But uses the overlying same principle
of having an external memory bank.

00:43:36.399 --> 00:43:40.800
So, in this case, our task is to
infer relations from a family tree.

00:43:40.800 --> 00:43:44.434
In most cases it's a graph traversal
problem, as well as a storage problem.

00:43:44.434 --> 00:43:47.892
And standard LSTM would struggle
really hard at this problem,

00:43:47.892 --> 00:43:50.840
which is where
Neural Turing Machines really shine.

00:43:55.326 --> 00:43:58.510
Keep in mind that the memory vector
is being updated as we see for

00:43:58.510 --> 00:43:59.480
each one, right.

00:44:01.980 --> 00:44:04.560
Just like to acknowledge the papers and
the resources I used, and

00:44:04.560 --> 00:44:09.040
back to Richard, thanks a lot.

00:44:09.040 --> 00:44:09.540
&gt;&gt; Yeah.
&gt;&gt; [APPLAUSE]

00:44:10.650 --> 00:44:13.250
&gt;&gt; All right, now to another obstacle.

00:44:13.250 --> 00:44:18.680
And that is that we actually
have multiple superfluous,

00:44:18.680 --> 00:44:21.330
if you will, word representations.

00:44:21.330 --> 00:44:27.530
So I mentioned that we share Word2Vec and
GloVe kinds of pre-training vectors.

00:44:27.530 --> 00:44:33.578
And now if we train a output, such as for
machine translation or language modeling.

00:44:33.578 --> 00:44:37.920
We'll actually have another
set of weights in the Softmax,

00:44:37.920 --> 00:44:43.510
one vector for every single word that we
have in the output, the Softmax output.

00:44:45.590 --> 00:44:52.960
Now what that means is at the top
here we have this large Softmax.

00:44:52.960 --> 00:44:57.000
It is the size of our vocabulary times
the hidden dimensions of the LSTM.

00:44:58.500 --> 00:45:03.550
And in the input, we also have word
vectors for every word vectors.

00:45:03.550 --> 00:45:08.120
So, again, the same size v times
the size of our word vectors.

00:45:08.120 --> 00:45:11.510
Now, a really cool paper and result and

00:45:11.510 --> 00:45:16.950
idea that actually came from two students
who took this class or 224D last year.

00:45:16.950 --> 00:45:20.780
Was to tie these two vectors together,

00:45:20.780 --> 00:45:24.430
just say they have to be the exact same,
vectors.

00:45:25.480 --> 00:45:32.860
So your Softmax weights for every word are
the exact same as your input word vectors.

00:45:32.860 --> 00:45:36.090
And you train them both jointly,
you just back propagate.

00:45:36.090 --> 00:45:39.920
Take the same derivatives, but
now, they are actually the same.

00:45:39.920 --> 00:45:42.870
It's very easy to implement if you don't
have to take the derivatives yourself,

00:45:42.870 --> 00:45:45.360
and tensor flow and so on.

00:45:45.360 --> 00:45:47.630
Would be a little harder otherwise.

00:45:47.630 --> 00:45:52.620
They also have some really nice
theory about the Softmax and

00:45:52.620 --> 00:45:55.200
various sort of temperatures
when you do this.

00:45:55.200 --> 00:45:58.180
But we're not gonna go
into all those details.

00:45:58.180 --> 00:46:05.722
But basically, it's a very simple idea and
it turns out to quite significantly help.

00:46:05.722 --> 00:46:08.911
So, here we basically have, again,

00:46:08.911 --> 00:46:13.405
this language modeling task
over the Penn Treebank.

00:46:13.405 --> 00:46:18.284
We mentioned that at this
Pointer Sentinel idea got 70.9.

00:46:18.284 --> 00:46:23.440
And then these very,
very large sort of 38 different LSTMs.

00:46:23.440 --> 00:46:29.100
For instance,
with 2.5 billion parameters, I get 68.

00:46:29.100 --> 00:46:33.960
But this simple idea,
where we basically tie

00:46:33.960 --> 00:46:38.620
the word vectors together with
just 51 million parameters.

00:46:38.620 --> 00:46:43.300
I can get the lowest test perplexity
when that paper came out.

00:46:43.300 --> 00:46:45.470
Which is kind of incredible,
like the amount, again,

00:46:45.470 --> 00:46:49.910
the speed in which this perplexity
has now been reduced more and more.

00:46:49.910 --> 00:46:50.530
And we're better and

00:46:50.530 --> 00:46:54.970
better able to predict this next
word is kind of incredible.

00:46:54.970 --> 00:46:59.690
So it's a very simple idea that
you can actually use every time

00:46:59.690 --> 00:47:03.890
you have an output space that includes
all the words in your vocabulary.

00:47:03.890 --> 00:47:06.800
As well as word vectors you
can use this idea and one,

00:47:06.800 --> 00:47:10.680
you're reducing one of the largest
sets of parameters in your model.

00:47:10.680 --> 00:47:13.777
So you use less ram,
you can have larger mini batches, or

00:47:13.777 --> 00:47:16.845
you can train faster,
use less gpu ram and everything.

00:47:16.845 --> 00:47:22.030
And it's more statistically efficient,
whenever you see a word.

00:47:22.030 --> 00:47:25.420
And the output,
it benefits also its input representation.

00:47:25.420 --> 00:47:30.930
So very neat idea, very simple,
gives you a nice improvement.

00:47:30.930 --> 00:47:32.188
Any questions about this idea?

00:47:36.087 --> 00:47:39.267
It's one of those nice examples
where everybody kind of assumes,

00:47:39.267 --> 00:47:41.625
you just have a Softmax,
and you have word vectors.

00:47:41.625 --> 00:47:44.880
So nobody really thinks about it, and
then sometimes people think about it.

00:47:44.880 --> 00:47:48.200
And question some of the basic
assumptions of the field, and

00:47:48.200 --> 00:47:50.120
find a way to do a better job, so.

00:47:50.120 --> 00:47:54.980
It's a really cool result and
one of the best projects from that class.

00:47:56.120 --> 00:47:59.850
Now, obstacle 5 is something
that's very relevant to PA4.

00:47:59.850 --> 00:48:05.401
So we spend a little bit more time on
it But basically tackles the problem

00:48:05.401 --> 00:48:11.793
that in many cases questions that we might
ask a system have representations that

00:48:11.793 --> 00:48:17.442
are independent of the current context or
the input that we might have.

00:48:17.442 --> 00:48:22.941
So, a kind of fun example is the question,
may I cut you, should be interpreted

00:48:22.941 --> 00:48:28.703
very differently if I am holding a knife
or whether you're standing in line right.

00:48:28.703 --> 00:48:33.329
And so you might want to have your
question be reinterpreted given

00:48:33.329 --> 00:48:38.291
the context and the input and the reason I
brought up the dynamic memory network is

00:48:38.291 --> 00:48:43.024
that this is in some ways a further
refinement of this kind of idea.

00:48:43.024 --> 00:48:47.810
You will still have some kind of
document encoder, you'll have some kind

00:48:47.810 --> 00:48:51.513
of question encoders,
you 'll have an answer module but

00:48:51.513 --> 00:48:56.020
this answer module actually
predict indices of the answers.

00:48:56.020 --> 00:49:01.262
And then you have this coattention encoder
instead of this episodic memory module you

00:49:01.262 --> 00:49:06.018
have seen before and now this coattention
encode looks kind of complicated.

00:49:06.018 --> 00:49:11.454
And is a little bit complicated in
real life but not too badly so so

00:49:11.454 --> 00:49:18.015
let's walk a little bit through it and
the paper gives you all the equations.

00:49:18.015 --> 00:49:22.096
And this is a reasonable model to
try to implement again once you

00:49:22.096 --> 00:49:25.339
to have your baselines implemented and
bug free.

00:49:25.339 --> 00:49:30.331
You can really actually in many ways
start from just this first step here,

00:49:30.331 --> 00:49:33.290
similar to the pseudocode I gave you.

00:49:33.290 --> 00:49:37.461
And then several of these modules you
can actually add one by one and see for

00:49:37.461 --> 00:49:39.218
each one how much it improves.

00:49:39.218 --> 00:49:43.094
And in fact, Caiming Xiong is
the first author of this paper,

00:49:43.094 --> 00:49:45.039
that's exactly what he did.

00:49:45.039 --> 00:49:49.110
He looked at it, looked at errors, and
then tried to add more coattention.

00:49:49.110 --> 00:49:52.641
And then tried to add LSTM to
incorporate all the facts again for

00:49:52.641 --> 00:49:55.049
multiple time steps and things like that.

00:49:55.049 --> 00:49:58.880
So there's kind of a hill climbing on
the architecture kind of approach.

00:49:58.880 --> 00:50:03.700
So and on a very high level,
let's say you have a question queue here.

00:50:03.700 --> 00:50:06.075
And you have the hidden states of an LSTM.

00:50:06.075 --> 00:50:10.390
And you have some document input D here.

00:50:10.390 --> 00:50:13.251
And you have m + 1 steps here.

00:50:13.251 --> 00:50:16.870
You actually have this sentinels
too that's why it's +1.

00:50:16.870 --> 00:50:21.364
Now what you can do is essentially take
the inner products between all these

00:50:21.364 --> 00:50:22.371
hidden states.

00:50:22.371 --> 00:50:26.440
And that's how you get these
sort of context matrices and

00:50:26.440 --> 00:50:31.885
then you can multiply these again with
the hidden states in these products.

00:50:31.885 --> 00:50:36.600
And you can concatenate
various combinations of

00:50:36.600 --> 00:50:41.545
these products between
these two sets of vectors.

00:50:41.545 --> 00:50:44.979
So you have these outer products
compute these context vectors and

00:50:44.979 --> 00:50:49.100
then you concatenate them in multiple
ways until you have the final state here.

00:50:50.130 --> 00:50:54.241
And now that one,
you'll pipe each input here,

00:50:54.241 --> 00:50:57.259
you pipe it through
a bidirectional LSTM again.

00:50:57.259 --> 00:51:02.182
And that will now be the question
dependent interpretation

00:51:02.182 --> 00:51:05.340
of every word in your input document.

00:51:06.790 --> 00:51:09.421
So basically just a lot
of inter products and

00:51:09.421 --> 00:51:12.774
outer products between
the hidden states of two LSTMs.

00:51:12.774 --> 00:51:18.573
Such that you understand how related
is this time step of this question at

00:51:18.573 --> 00:51:25.680
this word of the question, to this time
step at that word at the input document.

00:51:25.680 --> 00:51:28.450
Lots of inner and outer products and
then you try to agglomerate

00:51:28.450 --> 00:51:32.310
all these different facts again
in the bidirectional LSTM.

00:51:32.310 --> 00:51:37.379
And now once you have an output
a hidden state of that LSTM that will

00:51:37.379 --> 00:51:42.833
be given as input to a classifier
that essentially tries to identify.

00:51:42.833 --> 00:51:46.836
And classify with these highway networks,

00:51:46.836 --> 00:51:52.794
basically just neural networks
with short circuit connections.

00:51:52.794 --> 00:51:57.660
At each location of this now question
dependent input representation

00:51:57.660 --> 00:52:01.056
you classified which of
these is the start token.

00:52:01.056 --> 00:52:06.406
And that start token is then given its
input to yet another neural network

00:52:06.406 --> 00:52:11.766
that will now take the previous start
token that we classified together

00:52:11.766 --> 00:52:16.607
with a potential end token across
all these different vectors.

00:52:16.607 --> 00:52:21.800
You hear from the question dependent input
representation to classify the output.

00:52:21.800 --> 00:52:27.841
And you can do that multiple times and
once they don't change the input and

00:52:27.841 --> 00:52:33.492
the start and the end tokens are the same
from the previous time step,

00:52:33.492 --> 00:52:35.655
you'll basically stop.

00:52:35.655 --> 00:52:40.431
So the reason we call this dynamic, here,
is that you do this multiple times, and

00:52:40.431 --> 00:52:42.660
your first iteration might be wrong.

00:52:42.660 --> 00:52:50.168
But you give that input so the argmax
is the highest resulting hidden state.

00:52:50.168 --> 00:52:54.980
This could be the 51st time step for
instance, the word turbine.

00:52:54.980 --> 00:52:59.840
You give that as input to this LSTM,
which was then given again,

00:52:59.840 --> 00:53:04.790
its output given to the input of
another iteration of this attempt

00:53:04.790 --> 00:53:07.769
at predicting the start and end token.

00:53:07.769 --> 00:53:11.799
Now In a simpler world, let's say when
you eventually get to this model,

00:53:11.799 --> 00:53:15.829
but you might implement the whole thing
and you might be very optimistic,

00:53:15.829 --> 00:53:19.770
just implement the whole thing and
then it doesn't work.

00:53:19.770 --> 00:53:21.130
What do you do to debug?

00:53:21.130 --> 00:53:23.797
Well, you just take out all
the different things and

00:53:23.797 --> 00:53:25.494
you try to do the simplest thing,

00:53:25.494 --> 00:53:29.626
which starts exactly at that pseudo code
I had in the very beginning of the class.

00:53:29.626 --> 00:53:33.630
You just have LSTM for
input, LSTM for question.

00:53:33.630 --> 00:53:38.730
And then you pipe each state of
the input into a neural network and

00:53:38.730 --> 00:53:40.895
you try to classify start and end token.

00:53:40.895 --> 00:53:44.840
And you might have some outer products
between them, and you plug those

00:53:44.840 --> 00:53:48.760
into a straight up neural network and
you classify start and end token.

00:53:48.760 --> 00:53:52.410
Then you might concatenate
these two outer products and

00:53:52.410 --> 00:53:55.400
just classify those start end token.

00:53:55.400 --> 00:54:00.393
If you eventually have that whole
coattention encoder you could then say,

00:54:00.393 --> 00:54:04.356
all right, now I just classify
independently the start and

00:54:04.356 --> 00:54:09.605
the end tokens from that representation
of the question dependent encorder.

00:54:09.605 --> 00:54:12.014
Just independent one classifier for
the start token,

00:54:12.014 --> 00:54:14.260
one classifier for the end token.

00:54:14.260 --> 00:54:15.190
And then you can go on.

00:54:15.190 --> 00:54:20.620
And each time it will take some time and
you run some experiment but as long as you

00:54:20.620 --> 00:54:26.405
sort of incrementally improve each step
you know that you didn't introduce a bug.

00:54:26.405 --> 00:54:30.442
And so whenever there is sort of general
bug fixing, you wanna have you wanna try

00:54:30.442 --> 00:54:34.437
to identify where your bugs might be as
you build the larger and larger system.

00:54:34.437 --> 00:54:39.145
And so if you start from something simple
that you know works reasonable well and

00:54:39.145 --> 00:54:42.110
is bug free then each time
you add something to it.

00:54:42.110 --> 00:54:47.325
And it improves the accuracy you can be
fairly certain that there's no new part,

00:54:47.325 --> 00:54:49.483
not always but for the most part.

00:54:49.483 --> 00:54:53.320
And so this is a you know in the end
a very complex system that puts a lot of

00:54:53.320 --> 00:54:55.390
these simpler steps together.

00:54:55.390 --> 00:55:00.463
We actually again have sort of
introduced all of the basic components,

00:55:00.463 --> 00:55:05.885
basically of this but there again,
sort of put together in a very novel way.

00:55:05.885 --> 00:55:08.609
And you already know
the Stanford Question Answering Dataset,

00:55:08.609 --> 00:55:11.743
unless of course you're doing a project
that has nothing to do with PA 4.

00:55:11.743 --> 00:55:14.696
So I'll just describe it
a little bit briefly, sorry for

00:55:14.696 --> 00:55:18.709
the folks who are doing PA 4 and are
intricately familiar with this already.

00:55:18.709 --> 00:55:23.446
So the Stanford Question Answering Dataset
is a really great dataset of 100,000 plus

00:55:23.446 --> 00:55:25.820
question answer input triplets.

00:55:25.820 --> 00:55:31.510
And the way it's constructed is that for
each question the answer

00:55:31.510 --> 00:55:36.825
has to be a particular span in
the input paragraph for the most part.

00:55:36.825 --> 00:55:39.337
Sort of short documents but
really mostly paragraphs.

00:55:39.337 --> 00:55:44.156
So when you ask, what is Donald Davis
credited with what's great also is

00:55:44.156 --> 00:55:48.580
they actually have multiple people
answering the same question,

00:55:48.580 --> 00:55:50.793
cuz sometimes it's ambiguous.

00:55:50.793 --> 00:55:54.329
So one ground truth answer might be Davis
is credited with coining the modern name

00:55:54.329 --> 00:55:58.540
packet switching and inspiring numerous
packet switching networks in Europe.

00:55:58.540 --> 00:56:02.830
Another person might just say he's
credited with just coining the modern name

00:56:02.830 --> 00:56:06.540
Packet switching and inspiring
numerous packet switching networks, or

00:56:06.540 --> 00:56:09.907
even shorter, just coining
the modern name Packet Switching.

00:56:09.907 --> 00:56:14.627
And we would assume that all of
them are reasonably correct and

00:56:14.627 --> 00:56:20.180
close enough, and if your model
predicts one, that it's good enough.

00:56:23.280 --> 00:56:28.856
Great data set, now again these whenever
you put a results table in it's already

00:56:28.856 --> 00:56:34.770
deprecated, actually one thing that was
really great to see, I just noticed today.

00:56:36.355 --> 00:56:41.298
Let's see if I can find this,
is the model now,

00:56:41.298 --> 00:56:44.150
this is the SQUAD website.

00:56:45.470 --> 00:56:52.560
Again, sorry to bore the folks who are
working on PA4 and not on the problem set.

00:56:52.560 --> 00:56:57.040
It's a really great new
phenomenon that I think we'll see

00:56:57.040 --> 00:57:01.887
also as we push the limits of not
just deep learning for NLP, but

00:57:01.887 --> 00:57:06.010
I think in general,
Of machine learning and AI.

00:57:06.010 --> 00:57:09.590
So have proper trained dev test splits,
nobody sees the test set.

00:57:09.590 --> 00:57:11.520
You have to submit your code, so

00:57:11.520 --> 00:57:13.580
that makes it more reproducible
in the future too,

00:57:13.580 --> 00:57:17.569
if people are willing to open source their
codes, of course, you don't have to here.

00:57:18.710 --> 00:57:24.060
And it's I think in general
a great way to improve the science

00:57:24.060 --> 00:57:29.630
of what is mostly an engineering
discipline, we're creating new systems and

00:57:29.630 --> 00:57:34.740
so you see here different systems and now
you also can see when they were submitted.

00:57:34.740 --> 00:57:37.290
So some groups were super active.

00:57:37.290 --> 00:57:40.793
Now there's kind of,
this is my group, submit it.

00:57:40.793 --> 00:57:43.000
&gt;&gt; [LAUGH]
&gt;&gt; Four months ago and

00:57:43.000 --> 00:57:48.080
this is when that paper came out,
and when this table happened.

00:57:48.080 --> 00:57:51.100
And so since the last four months
we worked on other things,

00:57:51.100 --> 00:57:53.420
and now this is not the state
of the art anymore.

00:57:53.420 --> 00:57:57.490
And there are lots of people who
are just this week submitted more, but

00:57:57.490 --> 00:58:01.704
at the time of submission this was kind
of this dynamic co-attention network,

00:58:01.704 --> 00:58:05.985
was the best model on squad,
the first one sort of push it above 80.

00:58:05.985 --> 00:58:08.600
What's also great is to actually
have human baseline and

00:58:08.600 --> 00:58:12.090
that is something that will make sense for
you too sometimes.

00:58:12.090 --> 00:58:13.930
And I have had several
students groups also and

00:58:13.930 --> 00:58:18.920
in their problem set work on a task, and
then they say, I look at my errors now,

00:58:18.920 --> 00:58:21.210
which is great,
always do careful error analysis,

00:58:21.210 --> 00:58:24.900
something we would definitely want to
see in your report in the posters.

00:58:24.900 --> 00:58:28.420
When does your model fail,
what can it not capture yet?

00:58:28.420 --> 00:58:31.928
And sometimes, you look at your errors and
you actually say,

00:58:31.928 --> 00:58:35.233
I actually agree more with my
model than with the data set,

00:58:35.233 --> 00:58:38.627
the official ground truth label
is actually kind of wrong.

00:58:38.627 --> 00:58:43.603
There's also just people, they were busy,
they had to make money on AMT or

00:58:43.603 --> 00:58:46.067
something, Crowd workers, right?

00:58:46.067 --> 00:58:48.440
Maybe they weren't properly filtered and
so on.

00:58:48.440 --> 00:58:50.840
And eventually you might hit

00:58:50.840 --> 00:58:53.900
an upper limit of just what that
data set can ever give you.

00:58:53.900 --> 00:58:57.030
And so it's good to have
this kind of human baseline.

00:58:57.030 --> 00:59:02.460
Here the human baseline is
sort of 91 in terms of F1,

00:59:02.460 --> 00:59:05.560
or the exact match of 82.

00:59:05.560 --> 00:59:08.300
And you know once you push above that,

00:59:08.300 --> 00:59:11.760
really you're just fitting to the noise
of that data set in some sense.

00:59:11.760 --> 00:59:15.750
And so
that is good if you're at that level, and

00:59:15.750 --> 00:59:20.570
it also helps to feel less bad if you have
a new data set, you created it yourself.

00:59:20.570 --> 00:59:23.840
It's good to know that
it's okay to be at 85,

00:59:23.840 --> 00:59:28.130
because if I ask two people they
would only agree in 85% of the cases.

00:59:28.130 --> 00:59:31.850
So this inter-annotator
agreement is pretty important

00:59:31.850 --> 00:59:35.150
to consider as your pushing your
numbers sort of higher and higher.

00:59:37.220 --> 00:59:40.765
Any questions on SQUADs,
the dynamic content neural network, yeah?

00:59:44.861 --> 00:59:47.729
I don't actually know all
the details of who they asked,

00:59:47.729 --> 00:59:49.719
it may have been just the first author.

00:59:52.681 --> 00:59:55.016
It's the Turkers and
their interannotator agreement.

00:59:55.016 --> 01:00:00.442
So maybe, okay, so
then if that's the case then basically

01:00:00.442 --> 01:00:05.008
you can look at how often do these,
training set.

01:00:07.694 --> 01:00:08.759
Explore.

01:00:10.482 --> 01:00:17.260
So how often do people actually agree
with when they write their answers?

01:00:17.260 --> 01:00:21.860
So here, there's perfect agreement between
the humans, but here, it might not be.

01:00:21.860 --> 01:00:26.320
So one might say, what did the church
claim could be avoided with money?

01:00:26.320 --> 01:00:30.850
God's punishment for sin or
versus just God's punishment.

01:00:30.850 --> 01:00:33.671
Or late medieval Catholic church
versus just a Catholic church.

01:00:33.671 --> 01:00:37.830
So they're different, sometimes
different people agree differently.

01:00:37.830 --> 01:00:41.237
And it doesn't make sense for your model
to try to agree more with any single

01:00:41.237 --> 01:00:43.196
human, than humans between one another.

01:00:49.682 --> 01:00:52.600
How do you say its performance
exceeds human performance?

01:00:54.410 --> 01:00:58.610
So you can try to do that by
basically saying, all right,

01:00:58.610 --> 01:01:01.750
humans agree this often with other humans.

01:01:01.750 --> 01:01:07.090
You can create an output that other
humans would be more likely to agree with

01:01:07.090 --> 01:01:08.440
than with one another that's one way.

01:01:08.440 --> 01:01:12.868
Or you say I will take five or ten experts
in the world about a certain thing.

01:01:12.868 --> 01:01:16.568
This actually becomes more important for
like medical diagnosis, when you wanna

01:01:16.568 --> 01:01:20.360
also make those kinds of claims or
just train really accurate algorithms.

01:01:20.360 --> 01:01:25.000
You could basically take a group
of experts, and you only select

01:01:25.000 --> 01:01:29.040
those where the majority of the experts
agree on what the output should be.

01:01:29.040 --> 01:01:33.747
And if you then agree more often with
the majority than any single doctor

01:01:33.747 --> 01:01:38.458
would agree with that majority then
you can claim super human accuracy.

01:01:57.954 --> 01:02:01.960
So what are the principles behind
sort of claiming a novel algorithm?

01:02:10.817 --> 01:02:13.471
So I guess in some ways it's kind of
out of the scope of the question,

01:02:13.471 --> 01:02:14.720
cuz it's a legal question.

01:02:14.720 --> 01:02:20.337
I think in general, novelty of algorithms
is something that is also in the eyes

01:02:20.337 --> 01:02:26.058
of the reader so, that's not really
a good scientific answer to the question.

01:02:29.560 --> 01:02:34.095
No, I guess in general, a lot of these
papers are submitted to conferences and so

01:02:34.095 --> 01:02:38.566
the question, whether they're novel
enough, kind of often is subjective and

01:02:38.566 --> 01:02:40.790
in the eyes of the reviewer.

01:02:40.790 --> 01:02:42.700
Which can also not always
be the right thing,

01:02:42.700 --> 01:02:45.340
because two or
three reviewers can also be wrong.

01:02:48.340 --> 01:02:50.370
So then here's a nice visualization,
again,

01:02:50.370 --> 01:02:55.920
something I would encourage you all to
do for your projects and problem sets.

01:02:55.920 --> 01:03:00.740
I would just basically in this case trying
to understand if this dynamic encoder

01:03:00.740 --> 01:03:04.771
having an extra LSTM layer on top of
just predicting a single start and

01:03:04.771 --> 01:03:07.360
end token once will actually help.

01:03:07.360 --> 01:03:09.680
And here we can kind of see it helping.

01:03:09.680 --> 01:03:13.980
So as you go through this, it's kind
of hard to read, but basically this is

01:03:13.980 --> 01:03:19.640
an input, and then you see the outputs of
the classifier of this highway network.

01:03:19.640 --> 01:03:23.732
And how certain it is that
a certain word is a start token, so

01:03:23.732 --> 01:03:28.650
66, end token 66 with just
a single word as a start token,

01:03:28.650 --> 01:03:32.450
versus having the start token be 84 and
the end token be 94.

01:03:32.450 --> 01:03:38.770
And actually it switches from the first
attempt at classifying the right span

01:03:38.770 --> 01:03:44.894
to the second, and
in this case more correct.

01:03:44.894 --> 01:03:49.740
All right,
now the second to last obstacle, one thing

01:03:49.740 --> 01:03:54.470
you've noticed in a lot of things and
in a lot of these more complex models is,

01:03:54.470 --> 01:04:00.490
that we actually use recurrent neural
networks as the basic building block for

01:04:00.490 --> 01:04:03.950
a lot of the different deep
learning NLP systems that we have.

01:04:03.950 --> 01:04:09.890
And sadly, those recurrent neural
network blocks are usually quite slow.

01:04:09.890 --> 01:04:14.720
And unlike convolutional neural networks,
they can't be parallelized as easily.

01:04:14.720 --> 01:04:20.180
And so the idea here is to basically
take the best and parallelizable

01:04:20.180 --> 01:04:24.460
parts from RNNs and convolutional
neural networks, respectively.

01:04:24.460 --> 01:04:28.047
And try to combine them in one model,
and this resulted in

01:04:28.047 --> 01:04:33.450
the Quasi-Recurrent Neural Network by
James, Stephen and Caiming and me.

01:04:33.450 --> 01:04:40.020
And this is essentially the description
of this quasi-recurrent neural network.

01:04:40.020 --> 01:04:44.501
So, in general,
the very first layer of an LSTM,

01:04:44.501 --> 01:04:50.375
where you just pipe something
through the single-word vector,

01:04:50.375 --> 01:04:53.489
you might be able to parallelize.

01:04:53.489 --> 01:04:56.835
But then as soon as you actually
take into consideration

01:04:56.835 --> 01:04:59.754
the previous time step
HT-1 in your LSTM cell,

01:04:59.754 --> 01:05:04.320
you have to wait until that's computed
before you can compute your new one.

01:05:04.320 --> 01:05:06.310
And so you can parallelize that.

01:05:06.310 --> 01:05:11.620
On the other hand, in the convolutional
neural network, you can parallelize

01:05:11.620 --> 01:05:16.220
the convolution really well because it
only depends on two consecutive inputs.

01:05:16.220 --> 01:05:17.250
But then,

01:05:17.250 --> 01:05:21.880
with the max pulling you don't actually
get a hidden state at every time step.

01:05:22.930 --> 01:05:27.214
But for many things like sequence
classification or identifying spans and

01:05:27.214 --> 01:05:28.302
things like that,

01:05:28.302 --> 01:05:32.863
we would actually like to have such a
hidden representation at every time step.

01:05:32.863 --> 01:05:36.616
And so the idea on a high
level of the QRNN is to have

01:05:36.616 --> 01:05:40.170
a parallelizable convolutional layer.

01:05:40.170 --> 01:05:44.970
And then have a parallelizable
element-wise pooling layer

01:05:44.970 --> 01:05:48.600
that just looks independently
at each feature I mentioned and

01:05:48.600 --> 01:05:52.050
computes these gates that we already know.

01:05:52.050 --> 01:05:56.648
So in some ways in
combines the CNN that we

01:05:56.648 --> 01:06:01.805
looked at with the gated and
LSTM type gates.

01:06:01.805 --> 01:06:06.338
And so we can write this as
a very simple description, right?

01:06:06.338 --> 01:06:08.941
This is something that
should look familiar to you.

01:06:08.941 --> 01:06:14.480
But instead of having Xht- 1 here,
you just have Xt- 1 and Xt.

01:06:15.545 --> 01:06:19.860
So you don't have to wait until you
computed the previous hidden time step.

01:06:19.860 --> 01:06:25.340
You're just making these gating decisions
based on two consecutive word vectors.

01:06:27.660 --> 01:06:34.230
And you have multiple layers of these so
this is just the first layer here.

01:06:34.230 --> 01:06:38.670
And you basically just have
a standard neural network.

01:06:38.670 --> 01:06:43.350
It's not recurrent, just concatenating
two input vectors at a time.

01:06:45.380 --> 01:06:49.430
And you sum up after that and
you have tanh or sigmoids,

01:06:49.430 --> 01:06:52.200
depending on what kind of gates you have.

01:06:53.830 --> 01:06:58.730
So now, this you can rewrite as
a convolutional operator where

01:06:58.730 --> 01:07:02.439
you have a set of weights,
Wz over your input X.

01:07:02.439 --> 01:07:06.373
You just basically multiply,
it's a pretty discreet computation.

01:07:06.373 --> 01:07:12.598
Once you write it like this, you can also
think of larger filter sizes, or windows.

01:07:12.598 --> 01:07:17.378
You could actually have Xt- 2, Xt- 1, and

01:07:17.378 --> 01:07:21.188
Xt in each time step, for instance.

01:07:21.188 --> 01:07:24.577
Does this make sense as an operator?

01:07:24.577 --> 01:07:26.970
Can't you just can't you just
compute the gates at each time step?

01:07:51.794 --> 01:07:54.563
So the question is you're
splitting a cell and

01:07:54.563 --> 01:07:57.835
then you're parallelizing
across each dimensions.

01:07:57.835 --> 01:07:59.748
&gt;&gt; I don't see what's parallel about this.

01:07:59.748 --> 01:08:00.914
&gt;&gt; So good question.

01:08:00.914 --> 01:08:05.062
So why's this parallel and
why can we parallelize this?

01:08:05.062 --> 01:08:10.588
Let's say you have these
five word vectors here,

01:08:10.588 --> 01:08:14.895
x1, x2, x3, x4, and x5.

01:08:14.895 --> 01:08:21.660
Now, at each time step what you do
is you basically take two as input.

01:08:21.660 --> 01:08:26.170
Take these two as input and
you compute a vector such as z, all right.

01:08:28.310 --> 01:08:33.640
And now, you do this basically for
all these, for all these pairs.

01:08:33.640 --> 01:08:35.842
Basically, you just move one over.

01:08:35.842 --> 01:08:40.613
Now, the reason we can parallelize this

01:08:40.613 --> 01:08:45.386
is basically because we can concatenate

01:08:45.386 --> 01:08:49.735
a large matrix that just has x1 x2,

01:08:49.735 --> 01:08:54.665
and then x2 x3, and x3 x4, and so on.

01:08:54.665 --> 01:08:58.615
And we can basically preprocess
our input in this format and

01:08:58.615 --> 01:09:02.254
then just multiply that same
matrix with all of those.

01:09:02.254 --> 01:09:04.044
And hence we can parallelize all of them.

01:09:04.044 --> 01:09:07.684
None of these computations depend
on the previous hidden state.

01:09:10.610 --> 01:09:15.416
So that's why this can be parallelize
across the time dimensions by

01:09:15.416 --> 01:09:19.053
basically just smartly
preprocessing the input.

01:09:19.053 --> 01:09:23.971
And then, the element-wise gate here can
also be parallelized across channels.

01:09:23.971 --> 01:09:28.235
So all of these are just elements-wise
multiplications of these gates and

01:09:28.235 --> 01:09:29.910
of the hidden states.

01:09:29.910 --> 01:09:33.084
And so you just multiply,
let's say you have 100 features,

01:09:33.084 --> 01:09:38.410
100 of these computations over time can
be done independently of one another.

01:09:38.410 --> 01:09:43.996
So ht here, for
the first dimension of my feature channel,

01:09:43.996 --> 01:09:51.721
can be multiplied independently of h2 of
the feature channel and h3, and so on.

01:09:51.721 --> 01:09:56.039
The ith element of the feature channel is

01:09:56.039 --> 01:10:01.378
independent of all
the non-i feature channels.

01:10:01.378 --> 01:10:02.236
Yeah?

01:10:06.982 --> 01:10:11.784
That's right, so here you can
parallelize this part across time,

01:10:11.784 --> 01:10:14.657
but this only across feature channels.

01:10:14.657 --> 01:10:19.924
So here,
you have the ith element of ht depends

01:10:19.924 --> 01:10:24.400
only on the ith element of f, h, and z.

01:10:24.400 --> 01:10:27.902
But now, you parallelize differently, you
parallelize across the feature channels,

01:10:27.902 --> 01:10:28.610
not across time.

01:10:30.440 --> 01:10:33.408
So you basically parallelize here,
parallelize this.

01:10:33.408 --> 01:10:36.400
Then once you have all of these,
you parallelize this again and

01:10:36.400 --> 01:10:37.992
you can parallelize this again.

01:10:37.992 --> 01:10:39.184
But you have to wait,

01:10:39.184 --> 01:10:43.632
you can't compute the third layer before
you compute the first and the second.

01:10:47.670 --> 01:10:53.466
So what's great about this is it turns
out to sometimes actually be better

01:10:53.466 --> 01:10:59.088
than LSTM for a couple of parameters and
settings and tasks that we ran.

01:10:59.088 --> 01:11:03.957
And it's certainly a lot faster,
especially once you're implemented

01:11:03.957 --> 01:11:08.440
properly with cuDNN kernels and
CUDA kernels and really dig in.

01:11:08.440 --> 01:11:11.073
If you just kind of multiply it in Python,

01:11:11.073 --> 01:11:14.850
you might not be able to optimize
the architecture as well.

01:11:15.970 --> 01:11:18.860
And you won't get these
kinds of speed ups.

01:11:18.860 --> 01:11:23.178
So depending on your batch size,
each of your mini-batches, and

01:11:23.178 --> 01:11:28.207
depending on the sequence lengths,
you can get up to sort of 16x speed ups.

01:11:28.207 --> 01:11:33.211
But if you have very large batch sizes and
very short sequences, then of course,

01:11:33.211 --> 01:11:38.385
that parallelization will buy you less and
you'll only get a 1.4 speed up or so.

01:11:38.385 --> 01:11:41.025
When you look at how much
of the computation for

01:11:41.025 --> 01:11:44.350
this kind of model is now spent
on what kind of operation.

01:11:44.350 --> 01:11:49.471
What's amazing is basically if the Q-RNN,
the recurrent types

01:11:49.471 --> 01:11:54.975
of multiplications and just computation
is actually very small now.

01:11:54.975 --> 01:12:00.404
This is language modeling, so
we have large vocabulary in our softmax.

01:12:00.404 --> 01:12:06.190
The majority of time here is spent
on the softmax classification only.

01:12:06.190 --> 01:12:09.380
And then there's a little bit of just
optimization overhead getting things onto

01:12:09.380 --> 01:12:15.550
the GPU reading, and getting the word
vectors and all of that stuff.

01:12:15.550 --> 01:12:20.670
Sometimes, they're also sort of easier to
interpret cuz we have this now independent

01:12:20.670 --> 01:12:21.880
feature dimensions.

01:12:21.880 --> 01:12:29.204
And so it can actually go into this demo,
okay can't see it,

01:12:35.923 --> 01:12:37.670
Where we re-visualize this.

01:12:37.670 --> 01:12:39.910
And this is also,
this is kind of nice to have.

01:12:39.910 --> 01:12:42.270
You don't have to do this,
but if you have extra time,

01:12:42.270 --> 01:12:44.880
you already have good
performance on your models,

01:12:44.880 --> 01:12:48.400
it's always nice to have some
interactive plots to interact.

01:12:48.400 --> 01:12:50.950
If you have your question
answering system figured out,

01:12:50.950 --> 01:12:55.480
you can write a little JavaScript or
some maybe even just command line thing.

01:12:55.480 --> 01:12:56.510
Type in a question and

01:12:56.510 --> 01:13:00.590
see what the answer is, give it a new
kind of input, a new kind of document.

01:13:00.590 --> 01:13:04.170
See if you can break it, how it breaks or
what the activations look like and

01:13:04.170 --> 01:13:05.620
things like that.

01:13:05.620 --> 01:13:10.920
So here we basically had a QRNN
trained on sentiment analysis.

01:13:10.920 --> 01:13:16.230
And then looked at the activations
as it goes over a very large and

01:13:16.230 --> 01:13:17.800
very long document.

01:13:17.800 --> 01:13:22.400
So here you can see what is
there to say about this movie,

01:13:22.400 --> 01:13:25.510
this movie is simply gorgeous.

01:13:25.510 --> 01:13:30.610
So once you hit gorgeous, you see that
a lot of the different activations for

01:13:30.610 --> 01:13:34.760
several of the neurons really strongly
change in their activation, so

01:13:34.760 --> 01:13:37.500
right at this location here.

01:13:37.500 --> 01:13:40.140
Simply gorgeous,
a true feast for the eyes.

01:13:40.140 --> 01:13:43.330
And now, some of these hidden activations,

01:13:43.330 --> 01:13:47.545
stay the same no matter what
other sort of content there is.

01:13:47.545 --> 01:13:51.370
So game set standard for
3D role playing games seven years ago,

01:13:51.370 --> 01:13:54.300
this movie sets the standard for
future CG movies.

01:13:54.300 --> 01:13:56.960
And then you've got these trailers,
blah blah blah.

01:13:56.960 --> 01:14:00.160
So this is kind of this idea that I
mentioned in the very beginning about

01:14:00.160 --> 01:14:03.010
having these different gates and
now this is for sentiment,

01:14:03.010 --> 01:14:04.550
and nothing changes much.

01:14:04.550 --> 01:14:08.210
Just kind of talks about content, but

01:14:08.210 --> 01:14:14.080
then when the movie does not disappoint,
some of the neurons will switch again.

01:14:14.080 --> 01:14:20.160
And then there's another sort of seemingly
pretty important change in this review so

01:14:20.160 --> 01:14:22.920
it's not exactly a bad story.

01:14:22.920 --> 01:14:29.080
That doesn't sound super positive so a lot
of these neurons again will switch around.

01:14:29.080 --> 01:14:34.080
And then, at the end here I
recommend this movie to everyone and

01:14:34.080 --> 01:14:38.210
you see okay, several of the neurons
again turning on very strongly and

01:14:38.210 --> 01:14:42.730
eventually classified this
as a positive review.

01:14:43.990 --> 01:14:50.020
So those are super nice to have if you
can try to visualize your model that way.

01:14:50.020 --> 01:14:53.780
And now in the last five
minutes I want to talk about

01:14:53.780 --> 01:14:58.750
very recent paper from Quoc Le,
also graduate

01:14:58.750 --> 01:15:04.250
from Stanford now in the Google Brain
team, actually a founding member of it.

01:15:04.250 --> 01:15:10.270
Working with somebody else,
Zoph here, where, basically,

01:15:10.270 --> 01:15:15.940
they realized, and this is very good
insight, more and more of our time as

01:15:15.940 --> 01:15:20.960
researchers is spent on creating complex,
neural network architectures.

01:15:20.960 --> 01:15:26.330
And in some ways, if ideally,
it would be better if we could

01:15:26.330 --> 01:15:31.810
actually have an AI select the right
architectures for what we do.

01:15:33.160 --> 01:15:38.620
And again, putting more A back into AI and

01:15:38.620 --> 01:15:42.720
have less human ingenuity and
human architecture design.

01:15:42.720 --> 01:15:47.550
So in some ways, and this is kind of an
introspective sort of end thought also for

01:15:47.550 --> 01:15:48.590
the class.

01:15:48.590 --> 01:15:51.200
We've moved from something in
the beginning where we said we do all this

01:15:51.200 --> 01:15:53.730
feature engineering back
in the day in the field.

01:15:53.730 --> 01:15:57.380
And now look everything is much better
cause now we have these architectures that

01:15:57.380 --> 01:16:00.050
end to end trainable and
they learn all these features.

01:16:00.050 --> 01:16:04.450
But as were now trying to improve numbers
more and more and performance and

01:16:04.450 --> 01:16:07.880
create new kinds of capabilities for
our deep learning NLP models.

01:16:07.880 --> 01:16:10.910
We catch ourselves designing
architectures more and

01:16:10.910 --> 01:16:13.060
more just like we used to
design features anymore.

01:16:13.060 --> 01:16:17.080
We're humans, we want to use our
intelligence in some positive way.

01:16:17.080 --> 01:16:20.660
And so basically, the idea here is

01:16:20.660 --> 01:16:24.565
to use artificial intelligence to
find the right architecture for

01:16:24.565 --> 01:16:28.735
a whole host of different kinds of
problems or for a very specific problem.

01:16:28.735 --> 01:16:33.615
And without going into too many details,
the main basic

01:16:33.615 --> 01:16:37.815
controller that we'll have is also
going to be a recurrent neural network.

01:16:37.815 --> 01:16:41.420
But the outputs of that recurrent
neural network are actually

01:16:41.420 --> 01:16:43.950
architecture hyper parameters,
if you will.

01:16:45.010 --> 01:16:48.760
So how many hidden layers should I have,
or how big should my hidden layer be?

01:16:48.760 --> 01:16:51.700
At each time step of
the recurrent neural network,

01:16:51.700 --> 01:16:53.900
I will output those kinds of features.

01:16:53.900 --> 01:16:58.660
And then whenever it outputs that,
it will try to train a child network

01:16:59.830 --> 01:17:03.870
with that kind of architecture to get
a certain accuracy on a single task.

01:17:05.040 --> 01:17:06.710
And then feed that back.

01:17:06.710 --> 01:17:11.815
Now it's very hard to actually,
it's not differential because

01:17:11.815 --> 01:17:15.755
as you make these various discreet kinds
of decisions about the whole architecture,

01:17:15.755 --> 01:17:18.895
so they use reinforcement learning,
which we haven't really covered in class.

01:17:18.895 --> 01:17:22.825
And in the last two minutes of class we're
not going to able to really do it any

01:17:22.825 --> 01:17:29.685
justice, so it's just a different learning
regime than the standard back propagation.

01:17:29.685 --> 01:17:34.522
This is kind of we trained the CNN what
these outputs would look like at one

01:17:34.522 --> 01:17:35.326
time step.

01:17:35.326 --> 01:17:39.121
It might predict a number of filters and
then the filter height and

01:17:39.121 --> 01:17:43.680
filter width and the stride size, how far
do you skip to either the next words?

01:17:43.680 --> 01:17:48.230
Or in computer vision, how many
pixels do you jump over, and so on?

01:17:48.230 --> 01:17:52.780
So basically, this kind of architecture
selects its own architecture for

01:17:52.780 --> 01:17:56.110
specific problem that
you say is its reward.

01:17:56.110 --> 01:18:00.430
And, remember when I told you the numbers
are getting better and better,

01:18:00.430 --> 01:18:05.098
this is again this data set on language
modeling where just a couple months ago,

01:18:05.098 --> 01:18:09.230
we're super excited to have these
pointers, and when we got to 70.

01:18:09.230 --> 01:18:13.654
And then we're super excited cuz
we're tied to word vectors, and

01:18:13.654 --> 01:18:15.400
we got to 66, or a 68.

01:18:15.400 --> 01:18:20.650
Now, with this incredible
new idea to actually

01:18:20.650 --> 01:18:23.600
human ingenuity is still important,
they also share the embedding.

01:18:23.600 --> 01:18:25.820
So it's sharing word vectors and
soft mix outputs.

01:18:25.820 --> 01:18:30.070
It's not something that that model could
have ever predicted and it's something

01:18:30.070 --> 01:18:34.440
that helps a lot, but in general,
they choose different kinds of cells.

01:18:34.440 --> 01:18:40.290
Instead of LSTMs, they learn what kinds of
cells should be used at every recurrent

01:18:40.290 --> 01:18:44.840
time step and with that,
get to an amazing 62 perplexity.

01:18:44.840 --> 01:18:49.090
It's really incredible how quickly
these numbers have plummeted.

01:18:49.090 --> 01:18:54.730
And just when you thought you had a good
intuition of why LSTMs work well,

01:18:54.730 --> 01:18:59.640
this is basically the kind of architecture
that this model comes up with.

01:18:59.640 --> 01:19:04.858
And there is no more like this gate, and
what happens when this gate happens.

01:19:04.858 --> 01:19:09.421
This kind of the model figured
out how to do it well and

01:19:09.421 --> 01:19:12.441
did it in the end incredibly well.

01:19:12.441 --> 01:19:17.458
All right, so basically there
are still a lot of limits that we need

01:19:17.458 --> 01:19:23.195
to tackle as a field, still can't do
general purpose, question answering.

01:19:23.195 --> 01:19:26.370
We still can't really do
complex multitask learning,

01:19:26.370 --> 01:19:29.610
where we might have the same architecture,
do machine translation and

01:19:29.610 --> 01:19:32.490
question answering, and
sentiment analysis, and so on.

01:19:32.490 --> 01:19:36.300
They're all still very
specialized architectures.

01:19:36.300 --> 01:19:39.228
We don't have systems that
can do multi-modal reasoning.

01:19:39.228 --> 01:19:43.017
So over images or
speech together with logical reasoning,

01:19:43.017 --> 01:19:48.106
together with memory based retrieval,
there's still a lot of work to be done.

01:19:48.106 --> 01:19:52.357
And really, all the systems right
now require us to have tons of data,

01:19:52.357 --> 01:19:56.930
but I can introduce a new word to you
like Jane hit John with an uftcha.

01:19:56.930 --> 01:20:01.160
I just made up the word uftcha, but now
you can make lots of various assumptions

01:20:01.160 --> 01:20:04.940
about how heavy it could be, is it a
physical object and not a mental concept.

01:20:06.160 --> 01:20:08.470
How big it would be, how heavy, and

01:20:08.470 --> 01:20:13.322
all these different logical kind of
conclusions, from a single example.

01:20:13.322 --> 01:20:17.729
All the systems we've looked at in this
class require ton of different kinds of

01:20:17.729 --> 01:20:22.290
examples and lot's of statistical patterns
that we essentially need to match.

01:20:23.380 --> 01:20:26.310
All right, with that congratulations,
you've made it.

01:20:26.310 --> 01:20:29.150
Good luck on your last couple
of days of your projects.

01:20:29.150 --> 01:20:32.028
Thanks to all the TAs, thanks to Chris.

01:20:32.028 --> 01:20:33.892
Good time, good luck.

01:20:33.892 --> 01:20:38.669
&gt;&gt; [APPLAUSE]

