WEBVTT
Kind: captions
Language: en

00:00:00.360 --> 00:00:02.620
How do we learn the parameters for
naive Bayes?

00:00:02.620 --> 00:00:05.760
The simplest way to learn
the multinomial naive Bayes model

00:00:05.760 --> 00:00:07.990
is to use maximum likelihood estimates.

00:00:07.990 --> 00:00:10.250
Simply use the frequencies in the data.

00:00:10.250 --> 00:00:13.020
So if we're trying to compute
the prior probability of a particular

00:00:13.020 --> 00:00:17.860
document being in a class, class j, we
have the count of all the documents, and

00:00:17.860 --> 00:00:20.680
out of those how many of
the documents are in class j.

00:00:20.680 --> 00:00:24.530
That's our prior that a random
document will be in class j.

00:00:24.530 --> 00:00:29.570
For the likelihood the probability
of words have i given class sub j,

00:00:29.570 --> 00:00:34.310
we count the number of times words
of i occurs in documents of class j.

00:00:35.325 --> 00:00:39.930
And we normalize by the total number of
words in class j, so the sum over all

00:00:39.930 --> 00:00:44.700
the words in the vocabulary of the count
of those words in documents of class j.

00:00:44.700 --> 00:00:47.900
So out of all the words
in documents of class j,

00:00:47.900 --> 00:00:52.942
how many of them are our particular
word words sub i we're looking at?

00:00:52.942 --> 00:00:56.910
So we're going to compute the fraction
of times a word wi appears

00:00:56.910 --> 00:01:00.730
among all the words in the document
of this particular topic c sub j.

00:01:00.730 --> 00:01:04.320
We're going to do this by creating
a kind of mega document for

00:01:04.320 --> 00:01:09.310
topic j by concatenating all the documents
that have that topic together.

00:01:09.310 --> 00:01:13.089
And then we're going to use the frequency
of w in this mega document.

00:01:14.260 --> 00:01:18.356
So for sentiment analysis we might have a
document sub positive, a mega document for

00:01:18.356 --> 00:01:19.972
all the positive documents, and

00:01:19.972 --> 00:01:23.909
we're just going to concatenate them all
together into some big mega document.

00:01:23.909 --> 00:01:27.369
We might have a document sub neg and
so on.

00:01:29.949 --> 00:01:33.594
Now I've been lying to you, we don't
in fact use maximum likelihood for

00:01:33.594 --> 00:01:34.328
naive Bayes.

00:01:34.328 --> 00:01:38.327
And the reason is the following, imagine
we're looking at the word fantastic.

00:01:38.327 --> 00:01:42.063
We're interested in the word fantastic,
might occur in the test set, but

00:01:42.063 --> 00:01:46.280
it happens not to appear in the training
set in the topic positive, let's say.

00:01:46.280 --> 00:01:51.160
So the probability of fantastic given
the class positive in our training

00:01:51.160 --> 00:01:56.240
set by maximum likelihood is the count
of fantastic occurring in positive,

00:01:56.240 --> 00:01:59.780
normalized by the sum over all words of
the count of those words in positive.

00:01:59.780 --> 00:02:02.820
But fantastic never occurs so
that count is 0.

00:02:02.820 --> 00:02:04.908
So the maximum likely estimate for

00:02:04.908 --> 00:02:08.300
the likelihood of fantastic
given positive will be 0.

00:02:08.300 --> 00:02:09.880
Why is that bad?

00:02:09.880 --> 00:02:12.670
Because these 0 probabilities
can never be conditioned away.

00:02:12.670 --> 00:02:15.740
If we're looking for
the most likely class,

00:02:15.740 --> 00:02:19.620
that's the argmax over all classes
of the prior times the likelihood.

00:02:19.620 --> 00:02:23.970
And if one of those likelihood terms is 0,
then this whole thing is 0,

00:02:23.970 --> 00:02:25.610
and we're never going to pick that class.

00:02:27.140 --> 00:02:31.190
The solution is very simple,
add-1 smoothing.

00:02:31.190 --> 00:02:33.580
So here's the computation
without smoothing.

00:02:33.580 --> 00:02:37.810
And add-1 smoothing,
we simply add 1 to each of those counts.

00:02:37.810 --> 00:02:40.684
So we'll add 1 to the count
when it's in the numerator and

00:02:40.684 --> 00:02:44.930
every time that count occurs in
the denominator we add a1 to that too.

00:02:44.930 --> 00:02:48.400
And we can rewrite that in
the form we've seen before

00:02:48.400 --> 00:02:53.130
where we're taking the total number
of tokens that were in class c.

00:02:53.130 --> 00:02:56.784
And we've added the vocabulary
size because we added 1 from every

00:02:56.784 --> 00:02:59.120
vocabulary item into the denominator.

00:02:59.120 --> 00:03:02.494
So classic Laplace or add-1 smoothing.

00:03:04.573 --> 00:03:07.400
So let's walk through
the calculation of these parameters.

00:03:07.400 --> 00:03:10.270
First from the training corpus,
we're going to extract the vocabulary,

00:03:10.270 --> 00:03:11.109
the list of words.

00:03:12.540 --> 00:03:15.950
Next we're going to calculate
the priors for each class c sub j.

00:03:15.950 --> 00:03:21.240
So for each class, we're going to get the
set of all documents that have that class.

00:03:21.240 --> 00:03:22.710
We'll call that set docs sub j.

00:03:23.780 --> 00:03:29.320
And the number of documents in that set
divided by the total number of documents,

00:03:29.320 --> 00:03:32.790
that will give us our prior
probability of that particular class.

00:03:34.100 --> 00:03:38.017
Now for the likelihood, we're going to
want to compute the likelihood for

00:03:38.017 --> 00:03:40.508
every word w sub k given every topic,
c sub j.

00:03:40.508 --> 00:03:44.353
So first we're going to create our
mega document by concatenating all

00:03:44.353 --> 00:03:47.664
the documents sub j into a mega
document called text sub j.

00:03:47.664 --> 00:03:52.255
And now for each word w sub k in our
vocabulary, we're going to count

00:03:52.255 --> 00:03:56.867
the number of times w sub k occurs
in this mega document text sub j.

00:03:56.867 --> 00:03:58.288
So that will be n sub k.

00:03:58.288 --> 00:04:03.485
And now, the probability, the likelihood
of word sub k given class sub j,

00:04:03.485 --> 00:04:05.630
is just the add-1 smooth, or

00:04:05.630 --> 00:04:11.015
I've shown you the add-alpha smooth
version of the naive Bayes algorithm.

00:04:11.015 --> 00:04:15.485
So we've added alpha to n sub k and
then the denominator we have n,

00:04:15.485 --> 00:04:18.344
the total number of tokens in the class j.

00:04:21.376 --> 00:04:23.390
What about unknown words?

00:04:23.390 --> 00:04:26.860
A simple way to deal with unknown words
is simply to add an extra word to

00:04:26.860 --> 00:04:27.640
the vocabulary.

00:04:27.640 --> 00:04:30.748
You might call that w sub u,
the unknown word.

00:04:30.748 --> 00:04:33.803
And now, what does that do
with our likelihood equation?

00:04:33.803 --> 00:04:37.287
Here's the likelihood
equation written out again.

00:04:37.287 --> 00:04:41.621
We've added a new word, w sub u, so
I've increased our vocabulary size by 1.

00:04:41.621 --> 00:04:45.976
Now what is the count of w sub
u in any particular class c?

00:04:45.976 --> 00:04:49.485
Well, the word was unknown, so
it never occurred in our training set so

00:04:49.485 --> 00:04:50.490
that count is zero.

00:04:51.900 --> 00:04:58.197
So the likelihood term for that unknown
word is going to be simply 1 over n,

00:04:58.197 --> 00:05:03.274
that's the number of tokens
in the class c, + the V + 1.

00:05:03.274 --> 00:05:08.310
So each of our unknown words is going to
be modeled by this very simplistic model

00:05:08.310 --> 00:05:09.890
of the probability of an unknown word.

00:05:11.635 --> 00:05:15.785
So learning the parameters of naÃ¯ve Bayes,
a simple computation for the prior and

00:05:15.785 --> 00:05:18.305
a slightly more complex computation for
the likelihood in

00:05:18.305 --> 00:05:21.305
which we can use add-1 smoothing and
we can deal with unknown words.

