WEBVTT
Kind: captions
Language: en

00:00:01.150 --> 00:00:03.510
The models that we've been looking at

00:00:03.510 --> 00:00:06.950
to motivate maxent models
have been joint models.

00:00:06.950 --> 00:00:09.320
We just had some data and

00:00:09.320 --> 00:00:13.710
we've been putting a probability
distribution over that observed data.

00:00:13.710 --> 00:00:17.040
So what do these joint models
of the probability of x

00:00:17.040 --> 00:00:20.410
have to do with the conditional
model of the form probability?

00:00:20.410 --> 00:00:23.770
Of c given d that we
were building beforehand.

00:00:23.770 --> 00:00:28.750
The answer of this question is
that we can think of the c cross d

00:00:28.750 --> 00:00:33.680
space as a complex x and,
in particular for those applications,

00:00:33.680 --> 00:00:37.950
we are working with the set of
classes is generally small.

00:00:37.950 --> 00:00:41.950
It may be there's something
like 2 to 100 topic classes,

00:00:41.950 --> 00:00:44.980
part of speech tags, named entity labels.

00:00:44.980 --> 00:00:48.180
Whereas on the other hand,
d is generally huge.

00:00:48.180 --> 00:00:51.850
So d is the space of
all possible documents,

00:00:51.850 --> 00:00:55.540
which is minimally humongous and
possibly infinite.

00:00:57.510 --> 00:01:02.390
In principle, we can build models
over the joint space of CxD.

00:01:03.690 --> 00:01:07.910
This will involve calculating
expectations of features over CxD

00:01:07.910 --> 00:01:10.240
such as is shown in the equation here.

00:01:11.360 --> 00:01:16.720
But in this equation,
we're having to sum over the joint space.

00:01:16.720 --> 00:01:19.390
And in general,
that's impractical because we can't

00:01:19.390 --> 00:01:22.280
enumerate all the members
of X effectively.

00:01:22.280 --> 00:01:23.870
It's just far too big a space.

00:01:25.950 --> 00:01:32.570
So d may be huge or infinite, but only
a few d occur in our actual training data.

00:01:32.570 --> 00:01:38.210
If at the end of the day, we're training
on a million words of training data or

00:01:38.210 --> 00:01:41.680
a million documents in a document
classification system,

00:01:41.680 --> 00:01:44.870
then we have at most one
million different D, and

00:01:44.870 --> 00:01:48.650
often practice will have quite
a few less because of repeats.

00:01:48.650 --> 00:01:55.250
So something we could try doing is
adding an extra feature to our model for

00:01:55.250 --> 00:02:00.970
each D, and constrain its expectation
to match our empirical data.

00:02:00.970 --> 00:02:07.550
So we're saying that the probability
of each d is it's observed probability.

00:02:07.550 --> 00:02:12.590
And what that will end up
doing is giving all of

00:02:12.590 --> 00:02:17.530
the probability matched in the observe
documents and saying that all the rest of

00:02:17.530 --> 00:02:22.900
the entries of p c d will be zero and
we are showing that here.

00:02:22.900 --> 00:02:28.040
So now all probability mass is
going to the observed d and

00:02:28.040 --> 00:02:31.040
all of the rest of them
are given probability zero.

00:02:32.440 --> 00:02:35.290
That seems a slightly
crude thing to do but

00:02:35.290 --> 00:02:39.950
it has a clear practical benefit,
because now we have a much easier sum.

00:02:39.950 --> 00:02:45.230
because now we only have to sum over
the cases which we saw in the data.

00:02:45.230 --> 00:02:48.820
And so then we have to iterate it over
the different possible classes, but

00:02:48.820 --> 00:02:52.650
we don't have to iterate anymore
over all possible data items.

00:02:54.850 --> 00:02:59.230
So if we've constrained the d
marginals in this way, then in

00:02:59.230 --> 00:03:04.690
estimating our model the only thing that
can vary is the conditional distributions.

00:03:04.690 --> 00:03:09.390
So, we're rewriting it as this form,
and then we're saying that

00:03:09.390 --> 00:03:14.190
the probability of d has to be
the observed probabilities of d.

00:03:14.190 --> 00:03:18.010
And so to maximize the likelihood
that the model gives to the data,

00:03:18.010 --> 00:03:20.520
the only degree of freedom
we're left with is

00:03:20.520 --> 00:03:23.310
adjusting this conditional
probability distribution here.

00:03:25.300 --> 00:03:27.140
This is the connection between joint and

00:03:27.140 --> 00:03:31.350
conditional maxent entropy or
exponential models.

00:03:31.350 --> 00:03:36.300
Conditional models can be thought of as
joint models with marginal constraints

00:03:36.300 --> 00:03:41.550
where we exactly match
the distribution of the observed data.

00:03:41.550 --> 00:03:45.890
In this constricted model form,
maximizing joint likelihood and

00:03:45.890 --> 00:03:49.380
conditional likelihood of
the data are actually equivalent.

00:03:49.380 --> 00:03:54.080
Because the joint likelihood
with the constraint of matching

00:03:54.080 --> 00:03:59.640
the marginals is the same as
maximizing the conditional likelihood.

00:03:59.640 --> 00:04:05.210
Okay, so I hope that's enough of
an introduction to have made sense of

00:04:05.210 --> 00:04:10.940
how you can view exponential or log linear
models as models that maximize entropy.

