WEBVTT
Kind: captions
Language: en

00:00:02.780 --> 00:00:07.500
Let's talk about cases where we need
to interpolate between, or back off,

00:00:07.500 --> 00:00:09.260
from one language modal to another one.

00:00:09.260 --> 00:00:10.830
And we'll also touch on the web today.

00:00:12.550 --> 00:00:16.245
These are cases where it helps to
use Less context rather than more.

00:00:17.325 --> 00:00:23.065
The intuition is, suppose,
that you have a very confident trigram.

00:00:23.065 --> 00:00:25.225
You've seen the trigram
a very large number of times,

00:00:25.225 --> 00:00:27.605
you're very confident this
trigram is a good estimator.

00:00:27.605 --> 00:00:29.120
Well we should use the trigram.

00:00:29.120 --> 00:00:31.430
But, suppose you only saw it once.

00:00:31.430 --> 00:00:34.360
Well, maybe you don't really trust that
trigram, so you might want to back off and

00:00:34.360 --> 00:00:35.470
use the bigram instead.

00:00:35.470 --> 00:00:37.560
And maybe you haven't seen
the bigram either, so

00:00:37.560 --> 00:00:38.880
you might back off to the unigram.

00:00:38.880 --> 00:00:43.320
So the idea of back off is, sometimes,
if you don't have a large count or

00:00:43.320 --> 00:00:47.740
a trustworthy evidence for
a larger order n-gram,

00:00:47.740 --> 00:00:53.310
we might back off to a smaller one.,
A related idea

00:00:53.310 --> 00:00:58.400
is interpolation, interpolation says, well
sometimes the trigram may not be useful,

00:00:58.400 --> 00:01:01.100
and in that case if we mixed trigrams,
and bigrams, and

00:01:01.100 --> 00:01:04.770
unigrams, well then we may get more
information from the unigrams and bigrams.

00:01:04.770 --> 00:01:08.320
But other times trigrams will be more
useful, and so interpolation suggests that

00:01:08.320 --> 00:01:14.410
we jut mix all three all the time,
and get the benefits of all of them.

00:01:14.410 --> 00:01:18.390
And it turns out in practice ,that
interpolation works better than backoff,

00:01:18.390 --> 00:01:22.090
so most of the time in language modeling,
we'll be dealing with interpolation.

00:01:22.090 --> 00:01:25.530
There are two kinds of interpolation.

00:01:27.720 --> 00:01:34.590
Simple linear interpolation,
we have our unigram, our bigram.

00:01:34.590 --> 00:01:38.690
And our trigram, and we simply add them
together with three weights: lambda one,

00:01:38.690 --> 00:01:40.570
lambda two, lambda three.

00:01:40.570 --> 00:01:42.940
The lambdas just sum to one
to make this a probability.

00:01:44.490 --> 00:01:49.610
And we can compute our new probability,
we'll call it p hat,

00:01:49.610 --> 00:01:55.130
of a word given the previous two words by
interpolating these three Language models.

00:01:57.170 --> 00:01:59.850
We can do something
slightly more complicated.

00:01:59.850 --> 00:02:02.590
We can condition our
Lambdas on the context.

00:02:02.590 --> 00:02:06.340
So, we can say still mix our tri gram,
our bi gram, and our uni gram, but

00:02:06.340 --> 00:02:10.360
now, the lambdas are dependent on
what the previous two words were.

00:02:10.360 --> 00:02:14.180
So, we can train even richer and
more complex.

00:02:14.180 --> 00:02:17.390
Context conditioning for
deciding how to mix our trigrams,

00:02:17.390 --> 00:02:19.430
and our bigrams, and our unigrams.

00:02:19.430 --> 00:02:21.350
So where do the lambdas come from?

00:02:23.480 --> 00:02:27.450
The normal way to set lambdas
is to use a held-out corpus.

00:02:27.450 --> 00:02:30.400
So, we've talked before about
having a training corpus, and

00:02:30.400 --> 00:02:31.800
here's our training corpus.

00:02:31.800 --> 00:02:32.850
And our test corpus.

00:02:34.190 --> 00:02:39.040
A held out corpus is yet another piece
that we set out, set aside from our data.

00:02:39.040 --> 00:02:44.670
And we use a held out corpus, sometimes we
use a held out corpus called a dev set,

00:02:44.670 --> 00:02:47.830
a development set, or
other kinds of held out data.

00:02:47.830 --> 00:02:51.360
We use them to set meta-parameters and
check for things.

00:02:51.360 --> 00:02:56.100
So in this, we can use the held-out corpus
to set out lambdas, and the idea is that

00:02:56.100 --> 00:03:00.933
we're going to chose lambdas that maximize
the likelihood of this held-out data.

00:03:00.933 --> 00:03:05.758
So here's what we do: we take our
training data and we train some engrams.

00:03:09.171 --> 00:03:14.910
Now we say which lambdas would I use
to interpolate those engrams,such

00:03:14.910 --> 00:03:19.998
that it gives me the highest
probability of this held out data?

00:03:19.998 --> 00:03:25.084
So we ask find the set
of probabilities Such

00:03:25.084 --> 00:03:30.312
that the log probability
of the actual words

00:03:30.312 --> 00:03:35.975
that appear in
the Held-Out Data are highest.

00:03:44.624 --> 00:03:47.640
Now we've talked about cases
where there are zeros.

00:03:47.640 --> 00:03:49.960
So we haven't seen some
bi-brand before and

00:03:49.960 --> 00:03:52.890
we have to replace that zero
count with some other count.

00:03:52.890 --> 00:03:53.810
That's smoothing.

00:03:53.810 --> 00:03:58.080
But what do we do if the actual word
itself has never been seen before?

00:03:58.080 --> 00:04:02.780
Now sometimes that doesn't happen in
tasks where, let's say a menu based

00:04:02.780 --> 00:04:08.070
task,where we have fixed set of commands,
then no other words can ever be said.

00:04:08.070 --> 00:04:12.180
Our vocabulary is fixed and we have what
is called a closed vocabulary task, but

00:04:12.180 --> 00:04:18.260
lots of times language modelling is
applied in cases where we don't know

00:04:18.260 --> 00:04:22.120
any word could be used and could be words
we have never seen in our training set.

00:04:22.120 --> 00:04:26.230
So we call these words, OOV or
out of vocabulary words.

00:04:26.230 --> 00:04:29.520
And one way of dealing with out of
vocabulary words is as follows.

00:04:29.520 --> 00:04:31.840
We create a special token called UNK.

00:04:31.840 --> 00:04:38.010
And the way we train the UNK probabilities
is we create a fixed lexicon.

00:04:38.010 --> 00:04:43.630
So we take our training data And we first
decide which we hold out a few words,

00:04:43.630 --> 00:04:48.410
the very rare words or the unimportant
words and we take all those words and

00:04:48.410 --> 00:04:50.540
we change those words to UNK.

00:04:52.890 --> 00:04:56.465
Now we train the probabilities of UNK
like like a normal, any normal word.

00:04:56.465 --> 00:05:00.280
So,we have our training corpus,
it has word word word and

00:05:00.280 --> 00:05:03.220
it has a low probability word Word,
word, word.

00:05:03.220 --> 00:05:05.360
And we'll take that word,
and we'll change it to UNK.

00:05:07.420 --> 00:05:08.580
And now we train our bigram.

00:05:08.580 --> 00:05:12.780
Word, word, word, UNK, word, word, word
just as if UNK had been a word in there.

00:05:12.780 --> 00:05:14.620
And now at decoding time,

00:05:14.620 --> 00:05:18.670
if you see a new word you haven't seen,
you replace that word with UNK, and

00:05:18.670 --> 00:05:23.860
treat it like its bigram probabilities and
its trigram probabilities.

00:05:23.860 --> 00:05:25.430
From the UNK word in the training set.

00:05:27.870 --> 00:05:33.050
Another important issue in n-grams has to
do with web-scale or very large n-grams.

00:05:33.050 --> 00:05:36.260
So we introduced
the Google N-grams corpus earlier.

00:05:36.260 --> 00:05:40.399
How do we deal with computing
probabilities in such large spaces?

00:05:41.690 --> 00:05:44.330
So, one answer is pruning.

00:05:44.330 --> 00:05:47.640
We only store N-grams that
have a very large count.

00:05:47.640 --> 00:05:51.440
So, for example,
the very high order N-grams,we

00:05:51.440 --> 00:05:55.310
might want to remove all of those
singletons, all things with count 1.

00:05:55.310 --> 00:06:00.600
Because by Zipf's law, there's going
to be a lot of those singleton counts.

00:06:00.600 --> 00:06:04.150
And we can also use other kinds of more
sophisticated versions of this where we

00:06:04.150 --> 00:06:07.560
don't just remove things with counts,
we actually compute the entropy or

00:06:07.560 --> 00:06:09.720
the complexity on a test set and remove

00:06:10.860 --> 00:06:14.680
counts that are contributing less to the
probability on a particulary held out set.

00:06:16.120 --> 00:06:17.480
So that's pruning.

00:06:17.480 --> 00:06:20.140
And we can do a number of
other efficiency things.

00:06:20.140 --> 00:06:23.300
We can use efficient data
structures like tries.

00:06:23.300 --> 00:06:27.280
We can use approximate language models,
which are very efficient, but

00:06:27.280 --> 00:06:30.750
are not guaranteed to give you
the exact same probability We have

00:06:30.750 --> 00:06:35.530
to do efficient things, like, don't store
the actual strings but just store indexes.

00:06:35.530 --> 00:06:37.760
We can use Huffman coding.

00:06:37.760 --> 00:06:42.590
And often, instead of storing our
probabilities as these big 8-byte-floats,

00:06:42.590 --> 00:06:45.140
we might just do some kind
of quantinization and

00:06:45.140 --> 00:06:47.630
just store small number or
bits for our probabilities.

00:06:51.170 --> 00:06:54.900
What about smoothing for
Web-scale N-grams?

00:06:54.900 --> 00:06:56.820
The most popular smoothing method for

00:06:56.820 --> 00:07:00.780
these very large N-grams is
an algorithm called stupid backoff.

00:07:02.170 --> 00:07:05.170
Stupid backoff is called stupid
because it's very simple, but

00:07:05.170 --> 00:07:07.460
it works very well at
the very large scale.

00:07:07.460 --> 00:07:10.880
And in fact, it's been shown to work as
well as any more complicated algorithm

00:07:10.880 --> 00:07:15.370
when you have very large amounts of data
And the intuition of stupid backoff is,

00:07:15.370 --> 00:07:20.770
if I want to compute the stupid
backoff probability of a word given

00:07:20.770 --> 00:07:26.780
some previous set of words,
I use the maximum likelihood estimator.

00:07:26.780 --> 00:07:30.680
And this is the count of the words
divided by the count of the prefix

00:07:30.680 --> 00:07:33.860
If that count is greater than zero and
if not,

00:07:33.860 --> 00:07:39.660
I just back off to
the probability of the previous,

00:07:39.660 --> 00:07:44.420
the lower border n-gram prefix
with some constant weight.

00:07:44.420 --> 00:07:49.890
So it's, if the trigram let's say occurs,
I just use the account of the trigram.

00:07:49.890 --> 00:07:52.650
If it doesn't, I take the bigram
probability, multiply it by 44,

00:07:52.650 --> 00:07:54.450
and just use that.

00:07:54.450 --> 00:07:57.405
And then when I get down to the unigrams,
if I don't have anything at all,

00:07:57.405 --> 00:07:58.230
I just use the unigram.

00:07:58.230 --> 00:08:03.810
I just use the unigram probability.

00:08:03.810 --> 00:08:07.390
So, we call this S instead of P

00:08:07.390 --> 00:08:09.820
because stupid backoff doesn't
produce probabilities.

00:08:09.820 --> 00:08:12.540
Because, to produce probabilities,

00:08:12.540 --> 00:08:17.160
we would have to actually use
various clever kinds of waiting.

00:08:18.390 --> 00:08:21.640
A backoff algorithm has to
discount this probability

00:08:21.640 --> 00:08:26.260
to leave some mass left over to
use the bigram probabilities.

00:08:26.260 --> 00:08:28.590
Otherwise, we're going to end up with
numbers that are greater than one and

00:08:28.590 --> 00:08:29.690
we won't have probabilities.

00:08:31.000 --> 00:08:37.110
So stupid backoff produces something
like scores, rather than probabilities.

00:08:37.110 --> 00:08:39.550
But, it turns out that
this works quite well.

00:08:42.790 --> 00:08:47.170
So, in summary for smoothing so
far, Add-1 smoothing is okay for

00:08:47.170 --> 00:08:50.610
text categorization, but it's not
recommended for language modeling.

00:08:50.610 --> 00:08:54.140
The most commonly used method we'll
discuss in the advanced section

00:08:54.140 --> 00:08:57.400
of this week,
is the Kneser-Ney algorithm or

00:08:57.400 --> 00:09:00.180
the extended interpolated
Kneser-Ney algorithm.

00:09:00.180 --> 00:09:03.800
But for very large N-grams,
like situations where you're using the web

00:09:03.800 --> 00:09:07.110
Simplistic algorithms like stupid
backoff actually work quite well.

00:09:10.920 --> 00:09:12.860
How about
Advanced Language Modeling issues.

00:09:13.960 --> 00:09:17.510
Recent research has focused on
things like Discriminative models.

00:09:17.510 --> 00:09:21.740
So, here the idea is pick the n-gram
weights, instead of picking them

00:09:21.740 --> 00:09:25.930
to fit some training data,
whether it's maximum likelihood or smooth.

00:09:25.930 --> 00:09:29.830
Instead, choose your n-gram
weights to improve some task.

00:09:29.830 --> 00:09:32.630
So, we'll pick a whatever
task we're doing.

00:09:32.630 --> 00:09:34.950
Machine translation or
speech recognition and

00:09:34.950 --> 00:09:37.540
choose whatever n-gram weights
make that task more likely.

00:09:39.820 --> 00:09:43.580
Another thing we can do is, instead of
just using n-grams we can use parsers and

00:09:43.580 --> 00:09:47.030
we'll see the use of parsers and
statistical parsers later in the course.

00:09:49.670 --> 00:09:51.610
Or we can use cashing models.

00:09:51.610 --> 00:09:52.730
In a cashing model,

00:09:52.730 --> 00:09:56.750
we assume that a word that's been used
recently is more likely to appear again.

00:09:56.750 --> 00:10:00.715
So, the cash probability of
a word given given some history,

00:10:00.715 --> 00:10:06.395
we mix the probability of the word with
some function of the history, how often

00:10:06.395 --> 00:10:10.845
the word occurred in the history, and we
weight those two probabilities together.

00:10:10.845 --> 00:10:14.157
It turns out that cash models don't
don't work in certain situations, and

00:10:14.157 --> 00:10:18.137
in particularly they perform poorly for
speech recognition.

00:10:18.137 --> 00:10:22.687
So, you should think about why that might
be, that a cache model performs poorly for

00:10:22.687 --> 00:10:23.477
speech recognition.

