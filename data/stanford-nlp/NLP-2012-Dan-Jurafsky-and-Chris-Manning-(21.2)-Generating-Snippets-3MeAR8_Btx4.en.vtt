WEBVTT
Kind: captions
Language: en

00:00:00.540 --> 00:00:03.100
Let's now see how to
generate these snippets or

00:00:03.100 --> 00:00:05.120
other summaries from single documents.

00:00:07.460 --> 00:00:09.490
Now here's an example of a snippet.

00:00:09.490 --> 00:00:11.690
This is again coming from Google.

00:00:11.690 --> 00:00:12.560
I've asked a question.

00:00:12.560 --> 00:00:16.330
Was cast-metal moveable
type invented in Korea?

00:00:16.330 --> 00:00:19.700
And Google's giving me three
little snippets with the answer.

00:00:19.700 --> 00:00:24.310
And you can see that it's bold-faced
cases where words in my query

00:00:24.310 --> 00:00:25.790
occurred in the snippet.

00:00:25.790 --> 00:00:30.100
And you can see the use of dot, dot,
dots in the snippets telling you that it's

00:00:30.100 --> 00:00:36.960
combining pieces from different pages,
excuse me, different places in the page.

00:00:36.960 --> 00:00:38.960
So let's see how these
kind of snippets and

00:00:38.960 --> 00:00:41.610
other kinds of summaries based
on single documents are built.

00:00:42.620 --> 00:00:46.115
We can think of any summarization
algorithm as having three stages.

00:00:46.115 --> 00:00:48.796
The first stage is content selection,

00:00:48.796 --> 00:00:52.787
extracting the sentences that
we need from the document.

00:00:52.787 --> 00:00:54.756
So we have some document as input.

00:00:54.756 --> 00:00:55.951
And we're going to need
to extract sentences.

00:00:55.951 --> 00:00:58.680
So we might segment our sentences off.

00:00:58.680 --> 00:01:01.780
Maybe we use sentences with periods,
full sentences.

00:01:01.780 --> 00:01:03.420
Maybe we use some kind of moving window.

00:01:04.900 --> 00:01:08.400
So we've extracted some kind of
little pieces, little sentences.

00:01:08.400 --> 00:01:13.080
And now,
from this segmented set of sentences.

00:01:13.080 --> 00:01:15.250
We want to pick the ones
that are important.

00:01:15.250 --> 00:01:16.920
So I've marked those with
little black dots here.

00:01:16.920 --> 00:01:20.770
We've picked some set
of extracted sentences.

00:01:20.770 --> 00:01:22.780
And our next task, information ordering,

00:01:22.780 --> 00:01:25.970
is we're going to decide what
order the sentences go in.

00:01:25.970 --> 00:01:28.990
So we have some now ordered
set of important sentences.

00:01:28.990 --> 00:01:31.820
And finally we might do some
modifications to the sentences,

00:01:31.820 --> 00:01:33.710
perhaps we are going to simplify them or
something else.

00:01:33.710 --> 00:01:35.420
So that's sentence realization.

00:01:35.420 --> 00:01:38.300
And the result of these
four steps our summary.

00:01:39.890 --> 00:01:43.680
Now, the most basic summarization
algorithm, or one that comes up a lot,

00:01:43.680 --> 00:01:48.880
really only uses one of these three steps,
the content selection step.

00:01:48.880 --> 00:01:53.660
So in the simplest possible algorithm,
we don't worry about what ordering

00:01:53.660 --> 00:01:56.900
the sentences come in and
we don't modify the sentences at all.

00:01:56.900 --> 00:02:01.810
We simply segment our document into
sentences or maybe their windows.

00:02:01.810 --> 00:02:05.020
We pick the important ones and we leave
them in the same order they came in.

00:02:06.170 --> 00:02:09.300
So we're going to use what we call
document order with the original

00:02:09.300 --> 00:02:09.800
sentences.

00:02:09.800 --> 00:02:13.460
So this is a very simple ace line for
summarization and

00:02:13.460 --> 00:02:17.099
one that most web-based snippet
generation algorithms certainly use.

00:02:18.530 --> 00:02:20.100
The most commonly used algorithm for

00:02:20.100 --> 00:02:24.140
content selection dates back to the very
earliest paper in the field, from 1915.

00:02:24.140 --> 00:02:27.340
It's pretty exciting that
these ideas came out so early.

00:02:27.340 --> 00:02:29.680
And the intuition is really fairly simple.

00:02:29.680 --> 00:02:33.410
Choose sentences that have salient or
informative words.

00:02:33.410 --> 00:02:34.110
Well, what's that mean?

00:02:34.110 --> 00:02:37.350
Well you've seen tf-idf,
that's a way of picking words that

00:02:37.350 --> 00:02:42.350
are particularly frequent, and then don't
contain words that occur in all documents.

00:02:42.350 --> 00:02:46.200
So that's one way we might define
saliency or informativity.

00:02:46.200 --> 00:02:49.180
Turns out in summarization,
we tend to use another approach,

00:02:49.180 --> 00:02:54.110
the log-likelihood ratio or,
sometimes called topic signature approach.

00:02:54.110 --> 00:02:56.990
And that differs from tf-idf in two ways.

00:02:56.990 --> 00:02:59.760
One that we use a slightly different
statistic for picking, for

00:02:59.760 --> 00:03:01.200
weighing each of the words.

00:03:01.200 --> 00:03:04.540
And second, instead of picking
all the words, we'll choose only

00:03:04.540 --> 00:03:08.940
the words whose weight is above some
threshold, the very salient words.

00:03:08.940 --> 00:03:12.280
Now log-likelihood ratio gives
us a statistic called lambda.

00:03:12.280 --> 00:03:16.180
I'm not going to go into details,
but they're in some lovely papers.

00:03:16.180 --> 00:03:18.548
And, we're going to choose all words for

00:03:18.548 --> 00:03:22.510
who the value of 2 log lambda is
greater than this cutoff of 10.

00:03:22.510 --> 00:03:24.434
So that gives us a threshold for

00:03:24.434 --> 00:03:29.600
which we can pick words that are
particularly salient by this statistic.

00:03:29.600 --> 00:03:30.820
So, we're going to weight every word.

00:03:30.820 --> 00:03:34.531
And the weight of word I is going to be 1
if the word is especially associated with

00:03:34.531 --> 00:03:38.471
that document, meaning occurs more times
in that document than in the background

00:03:38.471 --> 00:03:42.380
corpus by some threshold otherwise than
that we're going to give it a weight of 0.

00:03:42.380 --> 00:03:47.011
And again and for details about how to
compute the log likelihood of ratio and

00:03:47.011 --> 00:03:51.787
the intuition about the statistics you
can see this lovely Ted Dunning paper or

00:03:51.787 --> 00:03:55.790
the Lin and Hovy paper that proposed
using it for summarization.

00:03:58.860 --> 00:04:02.610
Now, we want to modify this algorithm for
dealing with query focus summarization.

00:04:02.610 --> 00:04:06.300
Again, we're not interested so much in
pure summarization in today's lecture, but

00:04:06.300 --> 00:04:09.620
how to use summarization techniques for
question answering.

00:04:09.620 --> 00:04:14.070
So, this is topics signature based,
topic signature meaning

00:04:14.070 --> 00:04:18.120
pick the words that are particularly
associated with a document.

00:04:18.120 --> 00:04:21.840
Content selection choosing
the sentences where we've got queries.

00:04:21.840 --> 00:04:24.780
All right, and so we're going to
modify the algorithm very slightly.

00:04:24.780 --> 00:04:28.810
We're going to choose words that are
informative either in log likelihood ratio

00:04:28.810 --> 00:04:32.200
or words that happen to
appear in the query.

00:04:32.200 --> 00:04:34.800
So we're going to weight
every word in a document,

00:04:34.800 --> 00:04:36.550
we're going to give it a weight of 1.

00:04:36.550 --> 00:04:40.320
If it meets the log likelihood threshold
that passes the threshold of about 10,

00:04:40.320 --> 00:04:44.700
we're going to give it a weight of 1
also if that word happens to appear in

00:04:44.700 --> 00:04:48.230
the query or question, and otherwise,
we're going to give the word weight of 0.

00:04:48.230 --> 00:04:52.450
And these weights are very simple, 1,
1, 0, you could imagine learning more

00:04:52.450 --> 00:04:56.550
complex weights, and some research has
gone into coming up with very powerful

00:04:56.550 --> 00:05:00.770
ways to learn detailed ways but one one
zero works pretty well it turns out.

00:05:00.770 --> 00:05:02.790
And now,
we're just going to weigh a sentence, or

00:05:02.790 --> 00:05:05.342
perhaps its a window if you
don't have actual sentences.

00:05:05.342 --> 00:05:09.050
We're going to weigh it by the weight of
the words so we're just going to sum over

00:05:09.050 --> 00:05:11.420
all of the words in our sentence
of the weight of the words and

00:05:11.420 --> 00:05:12.400
then we are going to take the average.

00:05:14.620 --> 00:05:18.010
Now the content selection algorithm
we just described is unsupervised.

00:05:18.010 --> 00:05:20.880
We didn't have any label
training set of which,

00:05:20.880 --> 00:05:24.170
of summaries to learn weights from or
things like that.

00:05:24.170 --> 00:05:27.060
So that's an alternative approach.

00:05:27.060 --> 00:05:31.900
Supervised content selection, so
now if we had a labelled training set,

00:05:31.900 --> 00:05:37.020
where for each document we had a good
summary and we had an alignment.

00:05:37.020 --> 00:05:40.770
For every sentence in the summary, we knew
what sentence had came from the document,

00:05:40.770 --> 00:05:42.800
we had the matching sentences.

00:05:42.800 --> 00:05:44.550
Now we can extract all sorts of features.

00:05:44.550 --> 00:05:47.950
We can extract the position of
the sentence in the document,

00:05:47.950 --> 00:05:52.320
first sentences are very likely to be
good summary sentences, how long it is,

00:05:52.320 --> 00:05:55.570
we can have all of the features we
had before, word informativeness and

00:05:55.570 --> 00:05:59.560
things like that we could add other
kinds of features based on discourse,

00:05:59.560 --> 00:06:01.350
information that we might have.

00:06:01.350 --> 00:06:04.830
We might associate every sentence
with some vector of features.

00:06:04.830 --> 00:06:06.970
And now we can just train
a binary classifier.

00:06:06.970 --> 00:06:09.330
Should I put this sentence in the summary,
yes or no?

00:06:09.330 --> 00:06:11.400
And it might learn weights for
all these features and

00:06:11.400 --> 00:06:12.710
any other features that
we can come up with.

00:06:14.660 --> 00:06:18.710
Now this algorithm sounds, good, but in
practice it turns out to be very hard to

00:06:18.710 --> 00:06:23.100
get label training data of this type,
when people actually write abstract first

00:06:23.100 --> 00:06:28.010
sentences, the authors don't
always use exact words and

00:06:28.010 --> 00:06:31.890
phrases, they certainly don't use entire
sentences that come from the document.

00:06:31.890 --> 00:06:39.530
So, finding perfectly labeled abstracts
with extracts from the document is hard.

00:06:39.530 --> 00:06:42.650
It's hard to do the alignment because
they don't pick entire sentences.

00:06:42.650 --> 00:06:44.850
They may be picking words,
or phrases, or chunks.

00:06:44.850 --> 00:06:47.650
It's hard to figure out where those
words came from even when they did

00:06:47.650 --> 00:06:49.160
pick them from the document.

00:06:49.160 --> 00:06:51.710
And it turns out, surprisingly perhaps,

00:06:51.710 --> 00:06:56.410
that the performance is simply not much
better than unsupervised algorithms.

00:06:56.410 --> 00:07:01.850
So in practice, unsupervised content
selection just using log likelihood ratio

00:07:01.850 --> 00:07:09.230
or other simple measures of how salient or
informative a word,

00:07:09.230 --> 00:07:13.170
and hence a sentence is, are the most
common method for content selection.

00:07:15.440 --> 00:07:19.520
So we've seen how to generate
summaries from a single document.

00:07:19.520 --> 00:07:23.900
And the baseline algorithm we pick is
simply come up with a simple statistical

00:07:23.900 --> 00:07:28.040
way to find a sentence that is
very informative by looking for

00:07:28.040 --> 00:07:29.260
informative words.

00:07:29.260 --> 00:07:31.880
And we talked about
the log-likelihood ratio

00:07:31.880 --> 00:07:34.190
an in important way of
finding these sentences.

