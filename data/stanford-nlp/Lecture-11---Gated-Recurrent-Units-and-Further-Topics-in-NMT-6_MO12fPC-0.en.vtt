WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.673
[MUSIC]

00:00:04.673 --> 00:00:07.669
Stanford University.

00:00:07.669 --> 00:00:08.759
&gt;&gt; Okay hi everyone.

00:00:08.759 --> 00:00:11.250
Let's get started again.

00:00:11.250 --> 00:00:19.640
We're back with we're into
week six now and Lecture 11.

00:00:20.740 --> 00:00:25.390
This is basically the third
now last of our lectures.

00:00:25.390 --> 00:00:30.910
It's sort of essentially concentrating on
what we can do with recurrent models and

00:00:30.910 --> 00:00:33.630
sequence to sequence architectures.

00:00:33.630 --> 00:00:39.490
I thought what I'd do in the first
part of the lecture is have one

00:00:39.490 --> 00:00:44.440
more attempt at explaining some
of the ideas about GRUs and

00:00:44.440 --> 00:00:48.140
LSTMs and where do they come from and
how do they work?

00:00:48.140 --> 00:00:51.680
I'd sort of decide to do
that anyway on the weekend,

00:00:51.680 --> 00:00:57.560
just because I know that when I first
started seeing some of these gated models,

00:00:57.560 --> 00:01:02.240
that it took a long time for
them to make much sense to me, and

00:01:02.240 --> 00:01:05.140
not just seem like a complete surprise and
mystery.

00:01:05.140 --> 00:01:06.540
That's the way they work so

00:01:06.540 --> 00:01:10.950
I hope I can do a bit of good at
explaining that one more time.

00:01:10.950 --> 00:01:15.410
That feeling was reconfirmed when we
started seeing some of the people

00:01:15.410 --> 00:01:20.135
who've filled in the midterm survey so
thanks to all the people who filled it in.

00:01:20.135 --> 00:01:21.255
For people who haven't,

00:01:21.255 --> 00:01:25.295
I'm still happy to have you fill it
in over the last couple of days.

00:01:25.295 --> 00:01:29.975
While there were a couple of people
who put LSTMs in the list of

00:01:29.975 --> 00:01:33.290
concepts they felt that they
understood really well.

00:01:33.290 --> 00:01:36.160
Dozens of people put LSTMs and

00:01:36.160 --> 00:01:41.050
GRUs into the list of concepts
they felt kind of unsure about.

00:01:41.050 --> 00:01:46.400
This first part is for you and if you're
one of the ones that already understand

00:01:46.400 --> 00:01:50.620
it really well, I guess you'll just
have to skip ahead to the second part.

00:01:50.620 --> 00:01:53.870
Then we'll have the research
highlight which should be fun today.

00:01:53.870 --> 00:01:57.820
And then, so
moving on from that it's then completing,

00:01:57.820 --> 00:02:00.610
saying a bit more about
machine translation.

00:02:00.610 --> 00:02:05.390
It's a bit that we sort of had skipped and
probably should have explained earlier

00:02:05.390 --> 00:02:08.610
which is how do people evaluate
machine translation systems?

00:02:08.610 --> 00:02:12.553
Because we've been showing you numbers and
graphs and so on and never discussed that.

00:02:12.553 --> 00:02:17.706
And then I wanna sort of say a bit
more about a couple of things that

00:02:17.706 --> 00:02:22.870
come up when trying to build new
machines translation systems.

00:02:22.870 --> 00:02:26.920
And in some sense, this is sort of
done on the weed stuff it's not

00:02:26.920 --> 00:02:31.520
that this is sort of one central concept
that you can possibly finish your

00:02:31.520 --> 00:02:34.504
neural networks class
without having learned.

00:02:34.504 --> 00:02:38.956
But on the other hand, I think that all of
these sort of kind of things that come up

00:02:38.956 --> 00:02:43.341
if you are actually trying to build
something where you've actually got a deep

00:02:43.341 --> 00:02:46.995
learning system that you can use to
do useful stuff in the world and

00:02:46.995 --> 00:02:50.950
that they're useful, good,
new concepts to know.

00:02:50.950 --> 00:02:52.370
Okay.

00:02:52.370 --> 00:02:54.940
Lastly just the reminders and
various things.

00:02:54.940 --> 00:02:57.740
The midterm, we have got it all graded.

00:02:57.740 --> 00:03:00.600
And our plan is that we are going to

00:03:00.600 --> 00:03:03.990
return it to the people
who are here after class.

00:03:03.990 --> 00:03:08.740
Where in particular, there's another
event that's on here after class,

00:03:08.740 --> 00:03:13.240
so where we're going to return it
after class is outside the door.

00:03:13.240 --> 00:03:17.065
That you should be able to find
TAs with boxes of midterms and

00:03:17.065 --> 00:03:18.606
be able to return them.

00:03:18.606 --> 00:03:23.206
Assignment three, yeah so this has
been a little bit of a stretch for

00:03:23.206 --> 00:03:26.193
everybody on assignment three I realized,

00:03:26.193 --> 00:03:30.572
because sort of the midterm got
in the way and people got behind.

00:03:30.572 --> 00:03:35.408
And we've also actually we're hoping
to be sort of right ready to go with

00:03:35.408 --> 00:03:40.325
giving people GPU resources on Azure and
that's kinda've gone behind,

00:03:40.325 --> 00:03:44.845
they're trying to work on that right
now so with any luck maybe by the end

00:03:44.845 --> 00:03:49.020
of today we might have the GPU
resources part in place.

00:03:49.020 --> 00:03:53.770
I mean, at any rate, you should absolutely
be getting start on the assignment and

00:03:53.770 --> 00:03:55.740
writing the code.

00:03:55.740 --> 00:04:00.010
But we also do really hope that
before you finish this assignment,

00:04:00.010 --> 00:04:04.400
you take a chance to try out Azure,
Docker and

00:04:04.400 --> 00:04:08.120
getting stuff working on GPUs because
that's really good experience to have.

00:04:09.440 --> 00:04:15.010
Then final projects,
the thing that we all noticed about our

00:04:15.010 --> 00:04:20.360
office hours last week after the midterm
is that barely anybody came to them.

00:04:20.360 --> 00:04:26.276
We'd really like to urge for this week,
please come along to office hours again.

00:04:26.276 --> 00:04:30.732
And especially if you're doing
a final project, we'd really,

00:04:30.732 --> 00:04:35.349
really like you to turn up and
talk to us about your final projects and

00:04:35.349 --> 00:04:40.048
in particular tonight after class and
a bit of dinner which is again,

00:04:40.048 --> 00:04:43.309
we're going be doing
unlimited office hours.

00:04:43.309 --> 00:04:45.017
Feel free to come and see him, and

00:04:45.017 --> 00:04:48.799
possibly even depending on how you feel
about it, you might even go off and

00:04:48.799 --> 00:04:52.950
have dinner first and then come back and
see him to spread things out a little bit.

00:04:54.230 --> 00:04:57.570
Are there any questions
people are dying to know,

00:04:57.570 --> 00:04:59.770
or do I head straight into
content at that point?

00:05:03.870 --> 00:05:05.230
I'll head straight into content.

00:05:06.340 --> 00:05:10.900
Basically I wanted to sort of spend
a bit of time going through, again,

00:05:10.900 --> 00:05:16.750
the sort of ideas of where did these
kinds of fancy recurrent units come from?

00:05:16.750 --> 00:05:20.700
What are they going to try and achieve and
how do they go about doing it?

00:05:21.700 --> 00:05:26.336
Our starting point is, what we have
with a recurrent neural network is that

00:05:26.336 --> 00:05:29.735
we've got something that's
evolving through time.

00:05:29.735 --> 00:05:37.398
And at the end of that we're at some
point in that here where time t plus n.

00:05:37.398 --> 00:05:41.842
And then what we want to do
is have some sense of well,

00:05:41.842 --> 00:05:48.790
this stuff that we saw at time t, is that
affecting what happens at time t plus n?

00:05:48.790 --> 00:05:55.280
That's the kind of thing of is it
the fact that we saw at time t

00:05:55.280 --> 00:06:02.940
this verb squash that is having
some effect on the n words later,

00:06:02.940 --> 00:06:08.470
that this is being someone saying
the word window because this is some

00:06:08.470 --> 00:06:13.900
kind of association between squashing and
windows or is that completely irrelevant?

00:06:13.900 --> 00:06:17.780
We wanna sort of measure
how what you're doing here

00:06:17.780 --> 00:06:22.690
affects what's happening maybe six,
eight, ten words later.

00:06:22.690 --> 00:06:28.020
And so the question is how can we
achieve that and how can we achieve it?

00:06:28.020 --> 00:06:32.400
And what Richard discussed and
there was some sort of complex math here

00:06:32.400 --> 00:06:36.280
which I'm not going to explain,
again, in great detail.

00:06:36.280 --> 00:06:41.250
But what we found is if we had a basic
recurrent neural network what we're

00:06:41.250 --> 00:06:46.160
doing at each time step in the basic
recurrent neural network is

00:06:46.160 --> 00:06:50.570
we've got some hidden state and
we're multiplying it by matrix and

00:06:50.570 --> 00:06:54.360
then we're adding some stuff to do with
the input and then we go onto next

00:06:54.360 --> 00:06:59.360
time stamp where we're multiplying that
hidden state by the same matrix again and

00:06:59.360 --> 00:07:03.520
adding some input stuff and then we
go onto the time step and we model.

00:07:03.520 --> 00:07:07.446
Multiplying that,
hidden stuff by the same matrix again.

00:07:07.446 --> 00:07:12.340
It keeping on doing these matrix
multiplies and when you keep on doing

00:07:12.340 --> 00:07:18.170
these matrix multiplies you can
potentially get into trouble.

00:07:18.170 --> 00:07:23.620
And the trouble you get into is
if your gradient is going to zero

00:07:23.620 --> 00:07:28.440
you kind of can't tell whether that
means that actually what happened

00:07:28.440 --> 00:07:33.650
in words ago is having no effect
on what you're seeing now.

00:07:33.650 --> 00:07:38.962
Or whether it is you hadn't set all
of the things in your matrixes norm

00:07:38.962 --> 00:07:45.204
exactly right and so that the gradient
is going to zero because it's vanishing.

00:07:50.594 --> 00:07:55.517
This is where the stuff about eigenvalues
and stuff like that comes in.

00:07:55.517 --> 00:07:59.360
But kind of the problem is with.

00:07:59.360 --> 00:08:03.681
Basic RNA, sort of a bit too much
like having to land your aircraft

00:08:03.681 --> 00:08:06.916
on the aircraft carrier or
something like that.

00:08:06.916 --> 00:08:10.014
That if you can get things
just the right size,

00:08:10.014 --> 00:08:13.112
things you can land on
the aircraft carrier but

00:08:13.112 --> 00:08:18.611
if somehow your eigenvalues are a bit too
small then you have vanishing gradients.

00:08:18.611 --> 00:08:23.752
And if they're a bit too large
you have exploding gradients and

00:08:23.752 --> 00:08:27.729
you sort of,
it's very hard to get it right and so

00:08:27.729 --> 00:08:34.437
this this naive transition function seems
to be the cause of a lot of the problems.

00:08:34.437 --> 00:08:37.612
With the naive transition
function in particular,

00:08:37.612 --> 00:08:42.210
what it means is that sorta we're doing
this sequence of matrix multipliers.

00:08:42.210 --> 00:08:46.110
So we're keeping on multiplying
by matrix at each time step.

00:08:46.110 --> 00:08:49.650
And so, that means that when
we're then trying to learn.

00:08:49.650 --> 00:08:53.715
How much effect things have
on our decisions up here.

00:08:53.715 --> 00:08:57.992
We're doing that by backpropagating
through this whole sequence of

00:08:57.992 --> 00:09:00.105
intermediate nodes.

00:09:00.105 --> 00:09:06.167
And so, the whole idea of all of these
gated recurrent models is to say,

00:09:06.167 --> 00:09:11.725
well, somehow, we'd like to be
able to get more direct evidence

00:09:11.725 --> 00:09:16.675
of the effect of early time
steps on much later time steps,

00:09:16.675 --> 00:09:24.083
without having to do this long sequence
matrix multiplies, which almost certainly.

00:09:24.083 --> 00:09:27.820
Give us the danger of
killing off the evidence.

00:09:27.820 --> 00:09:30.880
So essentially what we wanna have is,

00:09:30.880 --> 00:09:35.070
we want to kinda consider the time
sequence that's our straight line.

00:09:35.070 --> 00:09:40.176
We also want to allow these shortcut
connections so ht can directly

00:09:40.176 --> 00:09:46.102
affect ht +2 because if we could do
that we then when we're backpropagating

00:09:46.102 --> 00:09:52.134
we'll then be able to measure in the
backward phase the effect of ht on ht + 2.

00:09:52.134 --> 00:09:53.147
And therefore,

00:09:53.147 --> 00:09:57.280
we would be much more likely to
learn these long term dependencies.

00:09:58.790 --> 00:10:00.920
So that seems a good idea.

00:10:03.590 --> 00:10:06.470
So I'm gonna do the kinda gated
recurrent units first, and

00:10:06.470 --> 00:10:10.170
then kinda build onto LSTMs,
which are even more complex.

00:10:10.170 --> 00:10:14.240
So essentially that's what we're
doing in the gated recurrent unit.

00:10:14.240 --> 00:10:18.740
And we're only making it a little
bit more complex by saying, well,

00:10:18.740 --> 00:10:23.863
rather than just uniformly
putting in stuff from time -1 and

00:10:23.863 --> 00:10:29.900
time -2, maybe we can have adaptive
shortcut connections where we're

00:10:30.920 --> 00:10:35.760
deciding how much attention to pay to
the past, as well as to the present.

00:10:35.760 --> 00:10:39.580
And so, that's essentially what you
get with the gated recurrent unit.

00:10:39.580 --> 00:10:45.250
So the key equation of the gated
recurrent unit is this first one.

00:10:45.250 --> 00:10:49.990
So it's sort of saying, well, we're
going to do the normal neural network

00:10:49.990 --> 00:10:53.060
recurrent units stuff,
that's the stuff in green.

00:10:53.060 --> 00:10:58.600
So for the stuff in green, we take the
current input and multiply it by a matrix.

00:10:58.600 --> 00:11:01.810
We take the previous hidden statement and
multiply it by a matrix.

00:11:01.810 --> 00:11:04.200
We add all of those things with a bias and

00:11:04.200 --> 00:11:09.450
put it through a tanh, that's exactly the
standard recurrent neural network update.

00:11:09.450 --> 00:11:16.742
So we're going to do that candidate
update just like a regular RNN.

00:11:16.742 --> 00:11:20.840
But to actually work out what
function we're computing,

00:11:20.840 --> 00:11:25.774
we're then going to adaptively learn
how much and on which dimensions

00:11:25.774 --> 00:11:30.873
to use that candidate update and
how much that we just gonna shortcut it,

00:11:30.873 --> 00:11:35.530
and just stick with what we had
from the previous time step.

00:11:35.530 --> 00:11:39.860
And while that stuff in the previous
time step will have been to some

00:11:39.860 --> 00:11:44.590
extent computed by this regular and
updated the previous time step.

00:11:44.590 --> 00:11:47.580
But of course, that was also a mixture, so

00:11:47.580 --> 00:11:52.480
to some extent, it will have been directly
inherited from the time step before that.

00:11:52.480 --> 00:11:56.480
And so,
we kind of adaptively allowing things from

00:11:56.480 --> 00:12:00.900
far past time steps just to
be passed straight through,

00:12:00.900 --> 00:12:05.520
with no further multiplications
into the current time step.

00:12:05.520 --> 00:12:09.070
So a lot of the key to is it
that we have this plus here.

00:12:09.070 --> 00:12:13.628
The stuff that is on this side
of the plus, we're just saying,

00:12:13.628 --> 00:12:18.272
just move along the stuff you had
before onto the next time step,

00:12:18.272 --> 00:12:23.174
which has the effect that we're
directly having stuff from the past

00:12:23.174 --> 00:12:26.367
be present to affect further on decisions.

00:12:26.367 --> 00:12:30.281
So that's most of what
we have in a GRU and

00:12:30.281 --> 00:12:37.420
a GRU is then just a little bit more
complex than that because if we do this,

00:12:37.420 --> 00:12:43.890
it's sort of all additive,
you kinda kick stuff around forever.

00:12:43.890 --> 00:12:46.782
You're deciding which to pay attention to,
but

00:12:46.782 --> 00:12:50.319
once you've paid attention to it,
it's around forever.

00:12:50.319 --> 00:12:54.951
And that's because you're sort
of just adding stuff on here.

00:12:54.951 --> 00:13:00.327
And so, the final step is to say well
actually, maybe we want to sort of prune

00:13:00.327 --> 00:13:06.010
away some of the past stuff adaptively so
it doesn't hang around forever.

00:13:06.010 --> 00:13:10.290
And so, to do that, we're adding
this second gate, the reset gate.

00:13:10.290 --> 00:13:16.542
And so, the reset gate gives you a vector
of, again, numbers between zero and

00:13:16.542 --> 00:13:21.967
one, which is calculated like a kind
of a standard recurrent unit.

00:13:21.967 --> 00:13:27.099
But it's sort of saying,
well to some extent, what we want to do is

00:13:27.099 --> 00:13:33.850
be able to delete some of the stuff that
was in ht- 1 when it's no longer relevant.

00:13:33.850 --> 00:13:36.230
And so,
we doing this sort of hadamard product,

00:13:36.230 --> 00:13:40.560
the element wise product of the reset
gate and the previous hidden state.

00:13:40.560 --> 00:13:43.530
And so,
we can forget parts of the hidden state.

00:13:43.530 --> 00:13:47.600
And the parts that we're forgetting is
embedded in this kind of candidate update.

00:13:47.600 --> 00:13:52.644
The part that's being just
passed along from the past to

00:13:52.644 --> 00:13:58.025
have direct updates is still
just exactly as it was before.

00:13:58.025 --> 00:14:02.310
So to have one attempt to
be more visual at that.

00:14:02.310 --> 00:14:06.442
So if we have a basic vanilla tanh-RNN,

00:14:06.442 --> 00:14:13.003
one way that you could think about
that is we have a hidden state,

00:14:13.003 --> 00:14:18.470
and what our execution of our
unit is doing as a program

00:14:18.470 --> 00:14:23.330
is saying you read the whole
of that register h,

00:14:23.330 --> 00:14:29.433
you do your RNN update, and
you write the whole thing back.

00:14:29.433 --> 00:14:33.240
So you've got this one memory register.

00:14:33.240 --> 00:14:38.110
You read it all, do a standard recurrent
update, and write it all back.

00:14:38.110 --> 00:14:40.920
So that's sort of very inflexible.

00:14:40.920 --> 00:14:45.240
And you're just sort of repeating that
over and over again at each time step.

00:14:45.240 --> 00:14:50.170
So in contrast to that,
when you have a GRU unit, that is then,

00:14:50.170 --> 00:14:55.230
allowing you to sort of learn
this adaptive flexibility.

00:14:55.230 --> 00:14:59.990
So first of all,
with the reset gate, you can learn

00:14:59.990 --> 00:15:05.370
a subset of the hidden state that
you want to read and make use of.

00:15:05.370 --> 00:15:07.750
And the rest of it will
then get thrown away.

00:15:07.750 --> 00:15:10.255
So you have an ability to forget stuff.

00:15:10.255 --> 00:15:15.325
And then,
once you've sort of read your subset,

00:15:15.325 --> 00:15:20.032
you'll then going to do
on it your standard RNN

00:15:20.032 --> 00:15:23.910
computation of how to update things.

00:15:23.910 --> 00:15:27.820
But then secondly,
you're gonna select the writable subset.

00:15:27.820 --> 00:15:29.550
So this is saying,

00:15:29.550 --> 00:15:33.310
some of the hidden state we're
just gonna carry on from the past.

00:15:33.310 --> 00:15:37.218
We're only now going to
edit part of the register.

00:15:37.218 --> 00:15:41.851
And saying part of the register,
I guess is a lying and simplifying a bit,

00:15:41.851 --> 00:15:45.665
because really,
you've got this vector of real numbers and

00:15:45.665 --> 00:15:50.601
some said the part of the register is
70% updating this dimension and 20%

00:15:50.601 --> 00:15:56.300
updating this dimension that values could
be one or zero but normally they won't be.

00:15:56.300 --> 00:15:58.890
So I choose the writable subset And

00:15:58.890 --> 00:16:03.970
then it's that part of it that I'm
then updating with my new candidate

00:16:03.970 --> 00:16:07.630
update which is then written back,
adding on to it.

00:16:08.820 --> 00:16:11.785
And so
both of those concepts in the gating,

00:16:11.785 --> 00:16:16.210
the one gate is selecting what to read for
your candidate update.

00:16:16.210 --> 00:16:23.252
And the other gate is saying, which
parts of the hidden state to overwrite?

00:16:23.252 --> 00:16:26.234
Does that sort of make
sense how that's a useful,

00:16:26.234 --> 00:16:30.007
more powerful way of thinking
about having a recurrent model?

00:16:34.286 --> 00:16:35.486
Yes, a question?

00:16:43.306 --> 00:16:48.770
Yeah, so how you select the readable
subset is based on this reset gate?

00:16:48.770 --> 00:16:53.413
So, the reset gate decides
which parts of the hidden

00:16:53.413 --> 00:16:57.116
state to read to update the hidden state.

00:16:57.116 --> 00:17:03.132
So, the reset gate calculates which parts
to read based on the current input and

00:17:03.132 --> 00:17:05.380
the previous hidden state.

00:17:05.380 --> 00:17:12.875
So it's gonna say, okay, I wanna pay a lot
of attention to dimensions 7 and 52.

00:17:12.875 --> 00:17:16.030
And so, those are the ones and
a little to others.

00:17:16.030 --> 00:17:20.503
And so those are the ones that
will be being read here and

00:17:20.503 --> 00:17:24.881
used in the calculation of
the new candidate update,

00:17:24.881 --> 00:17:30.931
which is then sort of mixed together
with carrying on what you had before.

00:17:30.931 --> 00:17:32.284
Any, yes.

00:17:46.050 --> 00:17:50.288
So, the question was explain this again.

00:17:50.288 --> 00:17:51.068
I'll try.

00:17:51.068 --> 00:17:53.894
[LAUGH] I will try.

00:17:53.894 --> 00:17:55.390
I will try and do that.

00:17:55.390 --> 00:17:59.600
Let me go back to this slide first,
cuz this has most of that,

00:17:59.600 --> 00:18:01.840
except the last piece, right.

00:18:01.840 --> 00:18:09.990
So here, what we want to do is we're
carrying along a hidden state over time.

00:18:09.990 --> 00:18:14.843
And at each point in time,
we're going to say, well,

00:18:14.843 --> 00:18:19.590
based on the new input and
the previous hidden state,

00:18:19.590 --> 00:18:24.125
we want to try and
calculate a new hidden state, but

00:18:24.125 --> 00:18:28.893
we don't fully want to
calculate a new hidden state.

00:18:28.893 --> 00:18:33.930
Sometimes, it will be useful just to
carry over information from further back.

00:18:33.930 --> 00:18:39.007
That's how we're going to get longer term
memory into our current neural network.

00:18:39.007 --> 00:18:44.027
Cuz if we kind of keep on doing
multiplications at each time step

00:18:44.027 --> 00:18:48.880
along a basic RNN,
we lose any notion of long-term memory.

00:18:48.880 --> 00:18:54.145
And essentially, we can't remember things
for more than seven to ten time steps.

00:18:54.145 --> 00:19:01.167
So that is sort of the top level equation
to say, well, what we gonna calculate.

00:19:01.167 --> 00:19:07.216
We want to calculate a mixture
of a candidate update and

00:19:07.216 --> 00:19:13.780
keeping what we had there before and
how do we do that?

00:19:13.780 --> 00:19:19.370
Well, what we're going to learn is
this ut vector, the update gate and

00:19:19.370 --> 00:19:23.610
the elements of that vector
are gonna be between zero and one.

00:19:23.610 --> 00:19:26.387
And if they're close to one,
it's gonna say,

00:19:26.387 --> 00:19:30.847
overwrite the current hidden state with
what we calculated this time step.

00:19:30.847 --> 00:19:33.718
And if they're close to zero,
it's gonna say,

00:19:33.718 --> 00:19:37.180
keep this element vector
just what it used to be.

00:19:37.180 --> 00:19:42.039
And so how we calculate the update
gate is using our regular kind

00:19:42.039 --> 00:19:46.439
of recurrent unit where it
looks at the current input and

00:19:46.439 --> 00:19:51.663
it looks at the recent history and
it calculates a value with the only

00:19:51.663 --> 00:19:56.522
difference that we use here sigmoid,
so that's between 0 and

00:19:56.522 --> 00:20:01.230
1 rather than tanh that puts
that at between minus 1 and 1.

00:20:01.230 --> 00:20:06.986
And so the kind of hope
here intuitively is suppose

00:20:06.986 --> 00:20:12.193
we have a unit that is
sort of sensitive to what

00:20:12.193 --> 00:20:17.674
verb we're on,
then what we wanna say is well,

00:20:17.674 --> 00:20:24.820
we're going through this sentence and
we've seen a verb.

00:20:24.820 --> 00:20:30.215
We wanted that unit, well, sorry,
these dimension of the vector.

00:20:30.215 --> 00:20:33.951
Let's say, their five dimensions of the
vector that sort of record what kind of

00:20:33.951 --> 00:20:35.350
verb it's just seen.

00:20:35.350 --> 00:20:42.074
We want those dimensions of the vector
to just stay recording what verb was

00:20:42.074 --> 00:20:47.901
seen until such time as in the input,
a band new verb appears.

00:20:47.901 --> 00:20:53.480
And it's at precisely that point, we wanna
say, okay, now is the time to update.

00:20:53.480 --> 00:20:56.767
Forget about what used to be
stored in those five dimensions.

00:20:56.767 --> 00:21:00.810
Now, you should store
a representation of the new verb.

00:21:00.810 --> 00:21:04.212
And so, that's exactly what
the update gate could do here.

00:21:04.212 --> 00:21:10.210
It could be looking at the input and
say, okay, I found a new verb.

00:21:10.210 --> 00:21:15.617
So dimensions 47 to 52 should
be being given a value of 1 and

00:21:15.617 --> 00:21:22.860
that means that they'll be storing a value
calculated from this candidate update,

00:21:22.860 --> 00:21:27.057
and ignoring what they
used to store in the past.

00:21:27.057 --> 00:21:30.817
But if the update gate finds
it's looking at a preposition or

00:21:30.817 --> 00:21:34.441
at a term in our It'll say,
no, not interested in those.

00:21:34.441 --> 00:21:38.459
So it'll make the update
value close to 0 and

00:21:38.459 --> 00:21:43.430
that means that dimensions
47 to 52 will continue to

00:21:43.430 --> 00:21:48.736
store the verb that you last saw
even if it was ten words ago.

00:21:48.736 --> 00:21:49.986
I haven't quite finish.

00:21:49.986 --> 00:21:52.480
So that was that part of it, so yes.

00:21:52.480 --> 00:21:54.364
So, the candidate update.

00:21:54.364 --> 00:21:55.790
So, that's the update gate.

00:21:55.790 --> 00:22:00.747
And when we do update, the candidate
update is just exactly the same as

00:22:00.747 --> 00:22:05.704
it always was in our current new
network that you're calculating this

00:22:05.704 --> 00:22:09.655
function of the important
the previous hidden state and

00:22:09.655 --> 00:22:13.207
put it through a tanh
together from minus 1 to 1.

00:22:13.207 --> 00:22:17.069
Then the final idea here is that well,

00:22:17.069 --> 00:22:23.222
if you just have this,
if you're doing a candidate update,

00:22:23.222 --> 00:22:28.411
you're always using
the previous hidden state and

00:22:28.411 --> 00:22:32.770
the new input word in
exactly the same way.

00:22:33.880 --> 00:22:39.240
Whereas really for my example, what I was
saying was if you have detected a new

00:22:39.240 --> 00:22:45.015
verb in the input, you should be storing
that new verb in dimensions 47 to 52 and

00:22:45.015 --> 00:22:48.916
you should just be ignoring
what you used to have there.

00:22:48.916 --> 00:22:52.393
And so it's sort of seems like
at least in some circumstances

00:22:52.393 --> 00:22:56.006
what you'd like to do is throw
away your current hidden state,

00:22:56.006 --> 00:22:59.350
so you could replace it
with some new hidden state.

00:22:59.350 --> 00:23:03.117
And so that's what this second gate,
the reset gate does.

00:23:03.117 --> 00:23:07.829
So the reset gate can also look at
the current import in the previous hidden

00:23:07.829 --> 00:23:11.520
state and
it choses a value between zero, and one.

00:23:11.520 --> 00:23:15.288
And if the reset gate choses
a value close to zero,

00:23:15.288 --> 00:23:20.587
you're essentially just throwing
away the previous hidden state and

00:23:20.587 --> 00:23:24.283
calculating something
based on your new input.

00:23:24.283 --> 00:23:28.456
And the suggestion there for
language analogy is well,

00:23:28.456 --> 00:23:35.316
if it's something like you're recording,
the last seen verb in dimensions 47 to 52.

00:23:35.316 --> 00:23:40.175
When you see a new verb, well, the right
thing to do is to throw away what you

00:23:40.175 --> 00:23:45.115
have in your history from 47 to 52 and
just calculate something new based

00:23:45.115 --> 00:23:49.694
on the input, but that's not always
gonna be what you want to do.

00:23:49.694 --> 00:23:54.618
For example, in English, English is
famous for having a lot of verb particle

00:23:54.618 --> 00:23:59.406
combinations which cause enormous
difficulty to non-native speakers.

00:23:59.406 --> 00:24:05.894
So that's all of these things
like make up, make out, take up.

00:24:05.894 --> 00:24:08.121
All of these combinations of a verb and

00:24:08.121 --> 00:24:11.890
a preposition have a special
meaning that you just have to know.

00:24:11.890 --> 00:24:17.300
It isn't really, you can't tell
from the words most of the time.

00:24:17.300 --> 00:24:22.549
So if you are wanting to work out
what the meaning of make out is,

00:24:22.549 --> 00:24:28.415
so you've seen make and
you put in that into dimensions 47 to 52.

00:24:28.415 --> 00:24:33.114
But if dimensions 47 to 52 are really
storing main predicate meaning,

00:24:33.114 --> 00:24:38.261
if you see the word out coming next, you
don't wanna throw away make because it's

00:24:38.261 --> 00:24:43.968
a big difference in meaning whether
it's make out or take out will give out.

00:24:43.968 --> 00:24:47.880
What you wanna do is you wanna combine
both of them together to try and

00:24:47.880 --> 00:24:49.770
calculate the predicate's meaning.

00:24:49.770 --> 00:24:55.110
So in that case, you want your reset
gate to have a value near one so you're

00:24:55.110 --> 00:24:59.490
still keeping it and you're keeping the
new import and calculating another value.

00:25:02.170 --> 00:25:05.364
Okay, that was my attempt to explain GRUs,
and now the question.

00:25:18.954 --> 00:25:23.882
So the question is okay, but
why this gated recurrent

00:25:23.882 --> 00:25:28.930
unit not suffer from
the vanishing gradient problem?

00:25:29.990 --> 00:25:35.833
And really the secret is
right here in this plus sign.

00:25:39.044 --> 00:25:44.063
If you allowed me to simplify slightly,

00:25:44.063 --> 00:25:51.750
and this is actually a version
of a network that has been used.

00:25:51.750 --> 00:25:57.210
It's essentially, not more details,
but this aspect of it actually

00:25:57.210 --> 00:26:02.499
corresponds to the very original
form of an LSTM that was proposed.

00:26:02.499 --> 00:26:10.500
Suppose I just delete this this- ut here,
so this just was 1.

00:26:10.500 --> 00:26:16.240
So what we have here is ht- 1,
so kind of like the reset gate,

00:26:16.240 --> 00:26:21.030
the update gate is only
being used on this side.

00:26:21.030 --> 00:26:25.892
It's saying should you pay any
attention to the new candidate,

00:26:25.892 --> 00:26:29.325
but you're always plussing it with ht-1.

00:26:29.325 --> 00:26:33.955
If you'll imagine that
slightly simplified form,

00:26:33.955 --> 00:26:37.741
well, if you think about your gradients,

00:26:37.741 --> 00:26:43.105
then what we've got here is when
we're kind of working at h,

00:26:43.105 --> 00:26:46.277
this has been used to calculate ht.

00:26:46.277 --> 00:26:51.053
Ht-1 is being used to calculate ht, so

00:26:51.053 --> 00:26:54.284
ht equals a plus ht-1, so

00:26:54.284 --> 00:26:59.482
there's a completely linear relationship

00:26:59.482 --> 00:27:05.247
with a coefficient of one between ht and
ht-1.

00:27:05.247 --> 00:27:09.108
Okay, and so
therefore when you do your calculus and

00:27:09.108 --> 00:27:13.781
you back prop that, right,
you have something with slope 1.

00:27:13.781 --> 00:27:19.415
That ht is just directly reflecting ht-1.

00:27:19.415 --> 00:27:24.100
And that's the perfect case for
gradients to flow beautifully.

00:27:24.100 --> 00:27:28.910
Nothing is lost, it's just going
straight back down the line.

00:27:28.910 --> 00:27:34.240
And so that's why it can carry
information for a very long time.

00:27:34.240 --> 00:27:40.015
So once we put in this update gate,
what we're having is the providing

00:27:40.015 --> 00:27:45.770
ut is close to zero,
this is gonna be approximately one,

00:27:45.770 --> 00:27:50.190
and so the gradients are just gonna flow
straight back to the line in an arbitrary

00:27:50.190 --> 00:27:54.650
distance and
you can have long distance dependencies.

00:27:54.650 --> 00:27:58.490
Crucially, it's not like you're
multiplying by a matrix every time,

00:27:58.490 --> 00:28:01.730
which causes all with vanishing gradients.

00:28:01.730 --> 00:28:07.454
It's just almost one there,
straight linear sequence.

00:28:07.454 --> 00:28:14.308
Now of course, if at some point ut is
close to 1, so this is close to zero,

00:28:14.308 --> 00:28:19.235
well then almost nothing
is flowing in from ht-1.

00:28:19.235 --> 00:28:22.320
But that's then saying there
is no long term dependency.

00:28:22.320 --> 00:28:24.720
That's what the model learn.

00:28:24.720 --> 00:28:29.507
So nothing flows a long way back.

00:28:29.507 --> 00:28:30.432
Is that a question?

00:28:30.432 --> 00:28:31.270
Yeah.

00:28:39.236 --> 00:28:46.710
So the question is,
isn't ht tilted ut both dependent on ht-1.

00:28:46.710 --> 00:28:47.976
And yeah, they are.

00:28:47.976 --> 00:28:54.882
Just like the ut you're calculating
it here in terms of ht-1.

00:28:54.882 --> 00:29:00.931
So in some sense the answer is yeah,
you are right but

00:29:00.931 --> 00:29:06.040
it's sort of turns out not matter, right?

00:29:06.040 --> 00:29:10.390
So the thing I think is If I put words
in to your mouth, the thing that you're

00:29:10.390 --> 00:29:15.570
thinking about is well, this ut
look right down at the bottom here,

00:29:15.570 --> 00:29:20.054
you'll calculate it by matrix
vector multiply from ht-1.

00:29:20.054 --> 00:29:24.830
And well then, where the ht-1 come from,

00:29:24.830 --> 00:29:29.960
it came from ht-2 and there was some
more matrix vector multiplies here,

00:29:29.960 --> 00:29:34.220
so there is a pathway going
through the gates where

00:29:34.220 --> 00:29:38.610
you're keep on doing matrix vector
multiplies, and that is true.

00:29:38.610 --> 00:29:42.189
But, it turns out that sort
of doesn't really matter,

00:29:42.189 --> 00:29:47.143
because of the fact that there is this
direct pathway, where you're getting

00:29:47.143 --> 00:29:51.886
this straight linear flow of gradient
information, going back in time.

00:29:54.335 --> 00:29:55.373
Any other question?

00:29:55.373 --> 00:30:00.141
Yes, I don't think I'll get any further
in this class if I'm not careful.

00:30:10.658 --> 00:30:12.200
I'm sorry if that's true.

00:30:13.550 --> 00:30:18.900
So the question was, why when you
Is before ut and one, one is ut.

00:30:18.900 --> 00:30:19.763
We swapped.

00:30:19.763 --> 00:30:23.180
&gt;&gt; [INAUDIBLE]
&gt;&gt; Yeah, if that's true, sorry about that.

00:30:23.180 --> 00:30:25.415
That was bad, boo boo mistake,

00:30:25.415 --> 00:30:27.925
cuz obviously we should be
trying to be consistent.

00:30:27.925 --> 00:30:31.889
But, it totally doesn't matter.

00:30:31.889 --> 00:30:36.433
This is sort of, in some sense, whether
you're thinking of it as the forget

00:30:36.433 --> 00:30:40.790
gate or a remember gate, and
you can kind of have it either way round.

00:30:40.790 --> 00:30:44.849
And that doesn't effect how the math and
the learning works.

00:30:48.104 --> 00:30:49.726
Any other questions?

00:30:51.829 --> 00:30:56.458
I'm happy to talk about this because I do
actually think it's useful to understand

00:30:56.458 --> 00:31:00.754
this stuff cuz in some sense these kind
of gated units have been the biggest and

00:31:00.754 --> 00:31:04.940
most useful idea for making practical
systems in the last couple of years.

00:31:04.940 --> 00:31:05.624
Yes.

00:31:11.282 --> 00:31:16.522
I actually have a picture for
an LSTM later on.

00:31:16.522 --> 00:31:20.296
It depends on a lot of particularities,
but

00:31:20.296 --> 00:31:24.536
it sort of seems like
somewhere around 100.

00:31:24.536 --> 00:31:29.671
Sorry the question was how long does a GRU
actually end up remembering for and I

00:31:29.671 --> 00:31:35.500
kind of think order of magnitude the kind
number you want in your head is 100 steps.

00:31:35.500 --> 00:31:40.826
So they don't remember forever I think
that's something people also get wrong.

00:31:40.826 --> 00:31:46.685
If we go back to the other one,
that I hope to get to eventually,

00:31:46.685 --> 00:31:49.570
the name is kind of a mouthful.

00:31:49.570 --> 00:31:54.401
I think it was actually very
deliberately named, where it was called,

00:31:54.401 --> 00:31:56.330
long short term memory.

00:31:56.330 --> 00:32:01.140
Right there was no idea in people's
heads that this was meant to be

00:32:01.140 --> 00:32:05.640
the model of long term
memory in the human brain.

00:32:05.640 --> 00:32:08.740
Long term memory is
fundamentally different and

00:32:08.740 --> 00:32:11.770
needs to be modeled in other ways and
maybe later in the class,

00:32:11.770 --> 00:32:16.340
we'll say a little a bit about the kind
of ideas people thinking about this.

00:32:16.340 --> 00:32:19.556
What this was about was saying okay,

00:32:19.556 --> 00:32:24.860
well people have a short term memory and
it lasts for a while.

00:32:24.860 --> 00:32:29.729
Whereas the problem was our current
neural networks are losing all of there

00:32:29.729 --> 00:32:31.435
memory in ten time steps.

00:32:31.435 --> 00:32:36.336
So if we could get that pushed out
another order of magnitude during

00:32:36.336 --> 00:32:40.362
100 time steps that would
be really useful to give us

00:32:40.362 --> 00:32:43.707
a more human like sense
of short term memory.

00:32:43.707 --> 00:32:44.207
Sorry, yeah?

00:32:49.930 --> 00:32:55.603
So the question is,
do GRUs train faster than LSTMs?

00:32:55.603 --> 00:32:59.625
I don't think that's true,
does Richard have an opinion?

00:32:59.625 --> 00:33:05.292
&gt;&gt; [INAUDIBLE]
&gt;&gt; Yes,

00:33:05.292 --> 00:33:10.426
so Richard says less computation
the computational cost is faster,

00:33:10.426 --> 00:33:15.393
but I sort of feel that sometimes
LSTMs have a slight edge on speed.

00:33:15.393 --> 00:33:17.375
No huge difference,
let's say that's the answer.

00:33:17.375 --> 00:33:23.270
Any other, was there another
question that people want to ask?

00:33:24.770 --> 00:33:26.260
Okay, I'll go on.

00:33:26.260 --> 00:33:31.743
You can ask them again in a minute and
I go on.

00:33:31.743 --> 00:33:36.551
Okay, so then finally I wanted to sort

00:33:36.551 --> 00:33:40.619
of say a little bit about LSTMs.

00:33:40.619 --> 00:33:45.795
So LSTMs are more complex because there
are more equations down the right side.

00:33:45.795 --> 00:33:52.630
And there's more gates but they're barely
different when it comes down to it.

00:33:52.630 --> 00:34:00.170
And to some extent, they look more
different than they are because of

00:34:00.170 --> 00:34:05.810
certain arbitrary choices of notation
that was made when LSTMs were introduced.

00:34:05.810 --> 00:34:10.853
So when LSTMs were introduced,
Hochreiter &amp; Schmidhuber

00:34:10.853 --> 00:34:15.480
sort of decided to say, well,
we have this privileged notion of

00:34:15.480 --> 00:34:20.250
memory in the LSTM,
which we're going to call the cell.

00:34:20.250 --> 00:34:24.517
And so people use C for
the cell of the LSTM.

00:34:24.517 --> 00:34:30.099
But the crucial thing to notice
Is that the cell of the LSTM

00:34:30.099 --> 00:34:35.680
is behaving like the hidden
state of the GRU, so really,

00:34:35.680 --> 00:34:41.330
the h of the GRU is equivalent
to the c of the LSTM.

00:34:41.330 --> 00:34:45.380
Whereas the h of the LSTM is

00:34:45.380 --> 00:34:49.320
something different that's related
to sort what's exposed to the world.

00:34:49.320 --> 00:34:55.980
So the center of the LSTM,
this equation for updating the cell.

00:34:55.980 --> 00:35:01.810
Is do a first approximation exactly
the same as this most crucial equation for

00:35:01.810 --> 00:35:04.690
updating the hidden state of the GRU.

00:35:04.690 --> 00:35:09.119
Now, if you stare a bit,
they're not quite the same,

00:35:09.119 --> 00:35:12.880
the way they are different is very small.

00:35:12.880 --> 00:35:18.156
So in the LSTM you have two gates
a forget gate and then an input gate so

00:35:18.156 --> 00:35:23.818
both of those for each of the dimension
have a value between zero and one.

00:35:23.818 --> 00:35:28.276
So you can simultaneously keep
everything from the past and

00:35:28.276 --> 00:35:32.111
keep everything from your
new calculated value and

00:35:32.111 --> 00:35:36.290
sum them together which is
a little bit different.

00:35:36.290 --> 00:35:41.720
To the GRU where you're sort of doing
this tradeoff as to how much to take

00:35:41.720 --> 00:35:47.180
directly, copy across the path versus
how much to use your candidate update.

00:35:47.180 --> 00:35:51.317
So it split those into two functions,
so you get the sum of them both.

00:35:51.317 --> 00:35:54.621
But other than that,
it's exactly the same, right?

00:35:54.621 --> 00:35:57.453
Where's my mouse?

00:35:57.453 --> 00:36:02.066
The candidate update is
exactly the same as what's

00:36:02.066 --> 00:36:06.140
being listed in terms of c tilde and
h tilde but

00:36:06.140 --> 00:36:10.644
the candidate update is exactly,
well, sorry,

00:36:10.644 --> 00:36:15.792
it's not quite I guess it's
the reset gate the candidate

00:36:15.792 --> 00:36:21.680
update is virtually the same as
the standard LSTM style unit.

00:36:21.680 --> 00:36:25.944
And then for the gates,
the gates are sort of the same,

00:36:25.944 --> 00:36:28.911
that they're using these sort of R and

00:36:28.911 --> 00:36:34.864
N style calculations to get a value
between zero for one for each dimension.

00:36:34.864 --> 00:36:39.822
So the differences
are that we added one more

00:36:39.822 --> 00:36:44.244
gate because we kinda having forget and

00:36:44.244 --> 00:36:49.202
input gates here and
the other difference is

00:36:49.202 --> 00:36:54.428
to have the ability to
sort of that the GRUs sort

00:36:54.428 --> 00:36:59.252
of has this reset gate where it's saying,

00:36:59.252 --> 00:37:07.670
I might ignore part of the past when
calculating My candidate update.

00:37:07.670 --> 00:37:11.310
The LSTM is doing it
a little bit differently.

00:37:11.310 --> 00:37:17.390
So the LSTM in the candidate update,
it's always using the current input.

00:37:17.390 --> 00:37:22.397
But for this other half here, it's not

00:37:22.397 --> 00:37:28.029
using ct minus 1, it's using ht minus 1.

00:37:28.029 --> 00:37:33.560
So the LSTM has this extra
ht which is derived from ct.

00:37:34.780 --> 00:37:39.767
And the way that it's derived from ct
is that there's an extra tanh here but

00:37:39.767 --> 00:37:42.786
then you're scaling with this output gate.

00:37:42.786 --> 00:37:49.500
So the output gate is sort of equivalent
of the reset gate of the GRU.

00:37:49.500 --> 00:37:53.691
But effectively,
it's one one time step earlier,

00:37:53.691 --> 00:37:57.691
cuz on the LSTM side,
on the preceding time step,

00:37:57.691 --> 00:38:03.215
you also calculate an ht by ignoring
some stuff with the output gate,

00:38:03.215 --> 00:38:07.025
whereas in the GRU, for
the current time step,

00:38:07.025 --> 00:38:13.149
you're multiplying with the reset gate
times your previous hidden state.

00:38:13.149 --> 00:38:14.194
That sorta makes sense?

00:38:14.194 --> 00:38:15.320
A question.

00:38:29.960 --> 00:38:31.496
Right, yes, the don't forget gate.

00:38:31.496 --> 00:38:36.390
[LAUGH] You're right, so
it's the question about was the ft.

00:38:36.390 --> 00:38:37.830
Is it really a forget gate?

00:38:37.830 --> 00:38:41.320
No, as presented here,
it's a don't forget gate.

00:38:41.320 --> 00:38:45.750
Again, you could do the 1 minus trick if
you wanted to and call this 1 minus f1,

00:38:45.750 --> 00:38:50.060
but yeah, as presented here,
if the value is close to 1,

00:38:50.060 --> 00:38:52.700
it means don't forget, yeah, absolutely.

00:39:03.280 --> 00:39:09.071
So this one here is genuinely
an update gate because if If the value

00:39:09.071 --> 00:39:15.330
of it is close to 1,
you're updating with the candidate update.

00:39:15.330 --> 00:39:17.490
And if the value is close to zero,

00:39:17.490 --> 00:39:20.145
you're keeping the previous
contents of the hidden state.

00:39:20.145 --> 00:39:25.977
&gt;&gt; [INAUDIBLE]
reset.

00:39:25.977 --> 00:39:29.966
&gt;&gt; Right, so the reset gate is
sort of a don't reset gate.

00:39:29.966 --> 00:39:30.733
[LAUGH] Yeah, okay.

00:39:30.733 --> 00:39:36.712
[LAUGH] I'm having a hard time
with the terminology here [LAUGH].

00:39:36.712 --> 00:39:39.334
You are right.

00:39:39.334 --> 00:39:42.540
Another question?

00:40:03.349 --> 00:40:09.424
So okay, so the question was
sometimes you're using ct-1,

00:40:09.424 --> 00:40:13.098
and sometimes you're using ht-1.

00:40:13.098 --> 00:40:14.632
What's going on there?

00:40:14.632 --> 00:40:22.380
And the question is in what sense
is ct less exposed in the LSTM?

00:40:22.380 --> 00:40:27.200
Right, so there was something I glossed
over in my LSTM presentation, and

00:40:27.200 --> 00:40:28.620
I'm being called on it.

00:40:28.620 --> 00:40:33.740
Is look, actually for the LSTM, it's ht-1

00:40:33.740 --> 00:40:39.531
that's being used everywhere for
all three gates.

00:40:39.531 --> 00:40:44.258
So really, when I sort of said
that what we're doing here,

00:40:44.258 --> 00:40:49.850
calculating ht, that's sort of
similar to the reset gate in the GRU.

00:40:51.180 --> 00:40:53.600
I kind of glossed over that a little.

00:40:53.600 --> 00:40:58.770
It's sort of true in terms of thinking of
the calculation of the candidate update

00:40:58.770 --> 00:41:03.945
cuz this ht- 1 will then go
into the candidate update.

00:41:03.945 --> 00:41:09.370
But's a bit more than that, cuz actually,
stuff that you throw away with your

00:41:09.370 --> 00:41:14.910
output gate at one time step is
then also gonna be thrown away

00:41:14.910 --> 00:41:20.420
in the calculation of every
gate at the next time step.

00:41:20.420 --> 00:41:28.822
Yeah, and so then the second question is
in what sense is the cell less exposed?

00:41:28.822 --> 00:41:30.529
And that's sort of the answer to that.

00:41:30.529 --> 00:41:35.524
The sense in which the cell
is less exposed is

00:41:35.524 --> 00:41:40.789
the only place that
the cell is directly used,

00:41:40.789 --> 00:41:46.594
is to sort of linearly add
on the cell at the previous

00:41:46.594 --> 00:41:51.290
time step plus its candidate update.

00:41:51.290 --> 00:41:53.700
For all the other computations,

00:41:53.700 --> 00:41:58.430
you're sort of partially hiding
the cell using this output gate.

00:42:01.220 --> 00:42:02.798
Another question, sure.

00:42:28.964 --> 00:42:32.339
Hm, okay, so the question is, gee,

00:42:32.339 --> 00:42:38.143
why do you need this tanh here,
couldn't you just drop that one?

00:42:42.068 --> 00:42:42.718
Whoops.

00:42:51.190 --> 00:42:52.369
Hm.

00:42:54.697 --> 00:42:58.506
I'm not sure I have such a good
answer to that question.

00:42:58.506 --> 00:43:08.506
&gt;&gt; [INAUDIBLE]

00:43:17.112 --> 00:43:19.802
&gt;&gt; Okay, so Richard's suggestion is,

00:43:19.802 --> 00:43:23.078
well this ct is kind of
like a linear layer, and

00:43:23.078 --> 00:43:28.970
therefore it's kind of insured if you
should add a non linearity after it.

00:43:28.970 --> 00:43:31.580
And that gives you a bit more power.

00:43:32.750 --> 00:43:36.217
Maybe that's right.

00:43:36.217 --> 00:43:40.540
Well, we could try it both ways and
see if it makes a difference, or

00:43:40.540 --> 00:43:43.670
maybe Shane already has,
I'm not sure [LAUGH].

00:43:43.670 --> 00:43:45.890
Any other questions?

00:43:45.890 --> 00:43:49.276
Make them a softball
one that I can answer.

00:43:49.276 --> 00:43:56.398
&gt;&gt; [LAUGH]
&gt;&gt; Okay,

00:43:56.398 --> 00:44:02.250
so I had a few more
pictures that went through

00:44:02.250 --> 00:44:08.450
the parts of the LSTM
with one more picture.

00:44:08.450 --> 00:44:12.520
I'm starting to think I should maybe
not dwell on this in much detail.

00:44:12.520 --> 00:44:17.000
Cuz we've sort of talked about
the fact that there are the gates for

00:44:17.000 --> 00:44:18.680
all the things.

00:44:18.680 --> 00:44:25.696
We're working out the candidate update,
just like an RNN.

00:44:25.696 --> 00:44:30.234
The only bit that I just wanna
say one more time is I think

00:44:30.234 --> 00:44:34.870
it's fair to say that the whole
secret of these things,

00:44:34.870 --> 00:44:41.190
is that you're doing this addition
where you're adding together.

00:44:41.190 --> 00:44:44.590
When in the addition,
it's sort of a weighted addition.

00:44:44.590 --> 00:44:46.170
But in the addition,

00:44:46.170 --> 00:44:51.740
one choice is you're just copying
stuff from the previous time step.

00:44:51.740 --> 00:44:56.794
And to the extent that you're copying
stuff from the previous time step,

00:44:56.794 --> 00:45:00.640
you have a gradient of 1,
which you're just pushing.

00:45:00.640 --> 00:45:04.149
So you can push error directly
back across that, and

00:45:04.149 --> 00:45:08.120
you can keep on doing that for
any number of time steps.

00:45:08.120 --> 00:45:13.074
So it's that plus, having that plus
with the previous time step rather

00:45:13.074 --> 00:45:16.260
than having it all multiplied by matrix.

00:45:16.260 --> 00:45:22.839
That is the central idea that makes LSTMs
be able to have long short-term memory.

00:45:22.839 --> 00:45:27.715
And I mean, that has proven to
be an incredibly powerful idea,

00:45:27.715 --> 00:45:32.131
and so in general,
it doesn't sound that profound, but

00:45:32.131 --> 00:45:37.467
that idea has been sort of driving
a lot of the developments of what's

00:45:37.467 --> 00:45:42.163
been happening in deep learning
in the last couple of years.

00:45:42.163 --> 00:45:49.455
So we don't really talk about,
in this class, about vision systems.

00:45:49.455 --> 00:45:52.431
You can do that next quarter in 231N.

00:45:52.431 --> 00:45:57.979
But one of the leading ideas and has
been used recently in better systems for

00:45:57.979 --> 00:46:02.910
doing kind of vision systems with
deep learning has been the idea of

00:46:02.910 --> 00:46:07.700
residual networks,
commonly shortened as ResNets.

00:46:07.700 --> 00:46:14.060
And to a first approximation, so

00:46:14.060 --> 00:46:18.800
ResNets is saying gee,
we want to be able to build 100 layer

00:46:18.800 --> 00:46:23.400
deep neural networks and
be able to train those successfully.

00:46:23.400 --> 00:46:25.310
And to a first approximation,

00:46:25.310 --> 00:46:31.490
the way ResNets are doing that is exactly
the same idea here with the plus sign.

00:46:31.490 --> 00:46:34.215
It's saying, as you go up each layer,

00:46:34.215 --> 00:46:40.340
we're going to calculate some non-linear
function using a regular neural net layer.

00:46:40.340 --> 00:46:42.471
But will offer the alternative,

00:46:42.471 --> 00:46:46.504
which is that you can just shunt
stuff up from the layer before,

00:46:46.504 --> 00:46:51.430
add those two together, and
repeat over again and go up 100 layers.

00:46:51.430 --> 00:46:56.336
And so this plus sign,
you may have learned in third grade, but

00:46:56.336 --> 00:47:02.274
turns out plus signs have been a really
useful part of modern deep learning.

00:47:02.274 --> 00:47:09.726
Okay, Yeah, here is my little picture,
which I'll just show.

00:47:09.726 --> 00:47:14.305
I think you'll have to sort
of then slow it down to

00:47:14.305 --> 00:47:19.222
understand that this is sort
of going backwards from

00:47:19.222 --> 00:47:24.472
Time 128 as to how long
information lasts in an LSTM,

00:47:24.472 --> 00:47:29.020
and it sort of looks
like this if I play it.

00:47:29.020 --> 00:47:33.525
And so if we then try and drag it back,
I think, then I can play it more slowly.

00:47:33.525 --> 00:47:38.467
All right, so that almost instantaneously,
the RNN has less

00:47:38.467 --> 00:47:44.308
information because of
the Matrix multiply.

00:47:44.308 --> 00:47:47.780
But as you go back,
that by the time you've gone back so

00:47:47.780 --> 00:47:52.900
at ten times steps, the RNN is
essentially lost the information.

00:47:52.900 --> 00:47:57.030
Whereas the LSTM even be going back,

00:47:57.030 --> 00:48:01.220
it starts loose information, but you know
you sort of gain back this sort of more

00:48:01.220 --> 00:48:06.150
like, time step 30 or
something before it's kind of

00:48:06.150 --> 00:48:09.970
lost all of its information which is sort
of the intuition I suggested before.

00:48:09.970 --> 00:48:16.346
But something like 100 time
steps you can get out of a LSTM.

00:48:16.346 --> 00:48:21.375
Almost up for a halftime break,
and the research highlight,

00:48:21.375 --> 00:48:25.632
but before that couple other
things I wanted to say,

00:48:25.632 --> 00:48:29.620
here's just a little bit
of practical advice.

00:48:29.620 --> 00:48:36.840
So both for assignment for or
for many people's final projects.

00:48:36.840 --> 00:48:40.829
They're gonna be wanting
to train recurrent neural

00:48:40.829 --> 00:48:44.150
networks with LSTMs on a largest scale.

00:48:44.150 --> 00:48:46.890
So here is some of the tips
that you should know, yes.

00:48:46.890 --> 00:48:50.407
So if you wanna build a big
recurrent new network,

00:48:50.407 --> 00:48:53.099
definitely use either GRU or an LSTM.

00:48:53.099 --> 00:48:56.959
So for any of these recurrent networks,

00:48:56.959 --> 00:49:01.622
initialization is really,
really important.

00:49:01.622 --> 00:49:07.117
That if your net, recurrent your network
should work, if your network isn't

00:49:07.117 --> 00:49:12.780
working, often times it's because
the initial initialization is bad.

00:49:12.780 --> 00:49:17.960
So what are the kind of initialization
ideas that often tend to be important?

00:49:17.960 --> 00:49:23.230
It's turned to be really useful for
the recurrent matrices, that's the one

00:49:23.230 --> 00:49:27.320
where you're multiplying by the previous
hidden state of previous cell state.

00:49:27.320 --> 00:49:29.580
It's really useful to
make that one orthogonal.

00:49:29.580 --> 00:49:33.600
So there's chance to use your good
old-fashioned linear algebra.

00:49:33.600 --> 00:49:37.310
There aren't actually that many
parameters in a recurrent neural net.

00:49:37.310 --> 00:49:41.635
And giving an orthogonal
initialization has proved to

00:49:41.635 --> 00:49:46.437
be a better way to kinda get
them learning something useful.

00:49:46.437 --> 00:49:49.576
Even with sort of these
ideas with GRUs and LSTMs,

00:49:49.576 --> 00:49:54.750
you're gonna kinda keep multiplying
things in a recurrent neural network.

00:49:54.750 --> 00:49:58.810
So normally, you wanna have
your initialization is small.

00:49:58.810 --> 00:50:02.250
If you start off with two large
values that can destroy things,

00:50:02.250 --> 00:50:05.830
try making the numbers smaller.

00:50:05.830 --> 00:50:08.140
Here's a little trick, so

00:50:08.140 --> 00:50:13.840
a lot of the times we initialize
things near zero, randomly.

00:50:13.840 --> 00:50:19.413
An exception to that is when you're
setting the bias of a forget gate,

00:50:19.413 --> 00:50:24.135
it normally works out much better
if you set the bias gate for

00:50:24.135 --> 00:50:28.764
the forget gate to a decent size
positive number like one or

00:50:28.764 --> 00:50:32.079
two or
a random number close to one or two.

00:50:32.079 --> 00:50:36.992
That's sort of effectively saying
you should start off paying

00:50:36.992 --> 00:50:40.060
a lot of attention to the distant past.

00:50:40.060 --> 00:50:43.380
That's sort of biasing it
to keep long term memory.

00:50:43.380 --> 00:50:46.030
And that sort of encourages
you to get a good model.

00:50:46.030 --> 00:50:48.620
Which effectively uses long term memory.

00:50:48.620 --> 00:50:53.626
And if the long term past stuff isn't
useful, it can shrink that down.

00:50:53.626 --> 00:50:57.283
But if the forget gate starts
off mainly forgetting stuff,

00:50:57.283 --> 00:51:01.320
it'll just forget stuff and
never change to any other behavior.

00:51:02.730 --> 00:51:06.190
In general, these algorithms work much

00:51:06.190 --> 00:51:08.670
better with modern adaptive
learning rate algorithms.

00:51:08.670 --> 00:51:10.880
We've already been using
Adam in the assignments.

00:51:10.880 --> 00:51:16.787
The ones like Adam, AdaDelta,
RMSprop work a lot better than basic SGD.

00:51:16.787 --> 00:51:19.490
You do wanna clip
the norms of the gradients.

00:51:19.490 --> 00:51:22.999
You can use a number like five,
that'll work fine.

00:51:22.999 --> 00:51:25.711
And so,
we've used dropout in the assignments, but

00:51:25.711 --> 00:51:28.797
we haven't actually ever talked
about it much in lectures.

00:51:28.797 --> 00:51:34.810
For RNNs of any sort,
it's trivial to do dropout vertically.

00:51:34.810 --> 00:51:38.280
And that usually improves performance.

00:51:38.280 --> 00:51:39.470
It doesn't work and

00:51:39.470 --> 00:51:44.660
I either do drop out horizontally
along the recurrent connections.

00:51:44.660 --> 00:51:48.359
Because if you have reasonable
percentage of drop out and

00:51:48.359 --> 00:51:52.829
you run it horizontally then within
the few time steps, almost every

00:51:52.829 --> 00:51:57.936
dimension will be dropped in one of them,
and so you have no information flow.

00:51:57.936 --> 00:52:03.044
There have been more recent work
that's talked about ways that you

00:52:03.044 --> 00:52:08.243
can successfully do horizontal
dropout in recurrent networks in,

00:52:08.243 --> 00:52:13.167
including orthongal's PhD student
in England who did work on so

00:52:13.167 --> 00:52:16.933
called base in drop out
that works well for that.

00:52:16.933 --> 00:52:20.935
But quite commonly, it's still the case
that people just drop out vertically and

00:52:20.935 --> 00:52:23.220
don't drop out at all horizontally.

00:52:23.220 --> 00:52:27.340
The final bit of advice is be
patient if you're running,

00:52:27.340 --> 00:52:31.580
if you're learning recurrent
nets over large data sets,

00:52:31.580 --> 00:52:34.370
it often takes quite a while and
you don't wanna give up.

00:52:34.370 --> 00:52:37.720
Sometimes if you just train them
long enough start to learn stuff.

00:52:37.720 --> 00:52:42.641
This is one of the reasons why we
really want to get you guys started

00:52:42.641 --> 00:52:45.862
using GPUs because the fact of the matter,

00:52:45.862 --> 00:52:50.785
if you're actually trying to do
things on decent size data sets,

00:52:50.785 --> 00:52:56.353
you just don't wanna be trying to train
in LSTM or GRU without Using a GPU.

00:52:56.353 --> 00:53:02.330
One other last tip that we should
mention some time is ensembling.

00:53:02.330 --> 00:53:08.190
If you'd like your numbers to be 2%
higher, very effective strategy,

00:53:08.190 --> 00:53:12.390
which again, makes it good to have a GPU,
is don't train just one model,

00:53:12.390 --> 00:53:16.250
train ten models and
you average their predictions and

00:53:16.250 --> 00:53:19.640
that that normally gives you
quite significant gains.

00:53:19.640 --> 00:53:24.378
So here are some results
from MT Systems trained.

00:53:24.378 --> 00:53:26.436
Montreal again.

00:53:26.436 --> 00:53:30.318
So it's different language pairs.

00:53:30.318 --> 00:53:32.661
The red ones is a single model.

00:53:32.661 --> 00:53:37.610
The purple ones are training 8 models,
and in this case,

00:53:37.610 --> 00:53:42.165
it's actually just majority
voting them together.

00:53:42.165 --> 00:53:45.643
But you can also sort of
average their predictions and

00:53:45.643 --> 00:53:50.898
you can see it's just giving very nice
gains in performance using the measure for

00:53:50.898 --> 00:53:54.392
mt performance which I'll
explain after the break.

00:53:54.392 --> 00:54:02.260
But we're now gonna have Michael up
to talk about the research highlight.

00:54:02.260 --> 00:54:04.073
And I'll quickly explain
it until the video is in

00:54:04.073 --> 00:54:04.634
there-
&gt;&gt; Okay.

00:54:04.634 --> 00:54:06.009
&gt;&gt; After the picture.

00:54:06.009 --> 00:54:07.500
&gt;&gt; Okay.

00:54:08.520 --> 00:54:09.192
Hi, everyone.

00:54:09.192 --> 00:54:13.380
I'm gonna be presenting the paper
Lip Reading Sentences in the Wild.

00:54:14.630 --> 00:54:19.070
So our task is basically taking a video,
which we preprocessed into

00:54:19.070 --> 00:54:23.900
a sequence of lip-centered images,
with or without audio.

00:54:23.900 --> 00:54:28.439
And we're trying to predict like the words
that are being said in the video.

00:54:28.439 --> 00:54:31.275
&gt;&gt; Just slide after that one.

00:54:34.277 --> 00:54:36.515
Maybe it doesn't

00:55:03.061 --> 00:55:06.060
&gt;&gt; The government will pay for both sides.

00:55:06.060 --> 00:55:09.260
&gt;&gt; We have to look at whether it
&gt;&gt; Not.

00:55:09.260 --> 00:55:12.352
Said security had been
stepped up in Britain.

00:55:29.494 --> 00:55:34.152
&gt;&gt; Cool, so anyway,
it's hard to do lip reading.

00:55:34.152 --> 00:55:38.340
So anyway, and for the rest of this I'll
talk about what architecture they use,

00:55:38.340 --> 00:55:42.323
which is, they deem the watch,
listen, attend, and spell model.

00:55:42.323 --> 00:55:45.746
&gt;&gt; Gonna talk about some of these training
strategies that might also be helpful for

00:55:45.746 --> 00:55:46.711
your final projects.

00:55:46.711 --> 00:55:48.915
There's also the dataset and

00:55:48.915 --> 00:55:54.124
the results was actually surpassing
like a professional lip reader.

00:55:54.124 --> 00:55:58.210
So, the architecture basically
breaks down into three components.

00:55:58.210 --> 00:56:03.298
We have a watch component which takes
in the visual and the listening

00:56:03.298 --> 00:56:09.367
component which takes in the audio and
these feed information to the attend, and

00:56:09.367 --> 00:56:14.470
spell module which outputs
the prediction one character at a time.

00:56:15.696 --> 00:56:20.410
And they also use this with like, just the
watch module or just the listen module.

00:56:22.510 --> 00:56:26.610
To go into slightly more detail,
for the watch module,

00:56:26.610 --> 00:56:32.130
we take a sliding window over
like the face centered images and

00:56:32.130 --> 00:56:35.450
feed that into a CNN,
which then the output of

00:56:35.450 --> 00:56:41.240
the CNN gets fed into an LSTM
much size over the time steps.

00:56:41.240 --> 00:56:46.760
We output a single state vector S of v,
as well as the set of

00:56:46.760 --> 00:56:52.517
output vectors L of v and
the listen module is very similar.

00:56:52.517 --> 00:56:55.388
We take the pre-processed speech and

00:56:55.388 --> 00:57:00.591
we again site over using the LSTM,
and we have another state vector,

00:57:00.591 --> 00:57:05.630
and another set of output vectors,
and then in the decoding step.

00:57:06.940 --> 00:57:11.620
So we have an LSTM as a really
steps of during the decoding and

00:57:11.620 --> 00:57:16.660
the initial hidden state is initialized
as the concatenation of the two hidden

00:57:16.660 --> 00:57:21.520
states from the two previous
modules as well as we have

00:57:21.520 --> 00:57:26.340
like a dual attention mechanism
which takes in the output

00:57:26.340 --> 00:57:31.030
vectors from each of their respective
modules, and we take those together, and

00:57:31.030 --> 00:57:35.210
we make our prediction using a softmax
over a multi-layer procepteron.

00:57:37.320 --> 00:57:40.360
And so, one strategy that uses
called curriculum learning.

00:57:40.360 --> 00:57:45.971
So ordinarily, when you're training
this sequence to sequence models,

00:57:45.971 --> 00:57:50.165
you might be tend to just use
one full sentence at a time.

00:57:50.165 --> 00:57:55.780
Tip by what they do on curriculum learning
is you start with the word length like

00:57:55.780 --> 00:58:01.652
segment and then you can slowly increase
the length of your training sequences and

00:58:01.652 --> 00:58:07.013
what happens is you're actually like
the idea is you're trying to learn,

00:58:07.013 --> 00:58:10.587
like slowly build up the learning for
the model and

00:58:10.587 --> 00:58:16.414
what happens is it ends up converging
faster as well as decreasing overfitting.

00:58:16.414 --> 00:58:20.253
Another thing that they use
is called scheduled sampling.

00:58:20.253 --> 00:58:24.649
So ordinarily during training,
you'll be using

00:58:24.649 --> 00:58:29.359
the ground truth input like
character sequence, but

00:58:29.359 --> 00:58:34.593
during the test time you
wouldn't be using that you'd just

00:58:34.593 --> 00:58:40.190
be using your previous prediction
after every time step.

00:58:40.190 --> 00:58:43.980
So what you do in scheduled sampling is
kind of like bridge the difference in

00:58:43.980 --> 00:58:47.440
scenarios between training and
testing is that you actually just for

00:58:47.440 --> 00:58:51.470
a random small probability,
like sample from the previous input

00:58:51.470 --> 00:58:55.140
instead of the ground truth input for
that time step during training.

00:58:58.180 --> 00:59:04.936
So the dataset was taken from the authors
collected it from the BBC News and

00:59:04.936 --> 00:59:09.620
they have like dataset that's much
larger than the previous ones

00:59:09.620 --> 00:59:13.790
out there with over 17,000
vocabulary words and

00:59:13.790 --> 00:59:18.160
the other the quite a bit like processing
to like some other things on the lips, and

00:59:18.160 --> 00:59:21.120
do like the alignment of the audio,
and the visuals.

00:59:23.810 --> 00:59:29.067
So, just to talk about the results, I
guess the most eye popping result is that

00:59:29.067 --> 00:59:34.169
they gave the test set to actually like
a company that does like professional

00:59:34.169 --> 00:59:39.428
lip reading and they're only able to get
about like one in four words correct or

00:59:39.428 --> 00:59:44.549
as this model was able to get one in two,
roughly, based on word error rate.

00:59:44.549 --> 00:59:48.984
And they also did some other
experience as well with looking at,

00:59:48.984 --> 00:59:52.041
if you combine the lips
version with the audio,

00:59:52.041 --> 00:59:57.469
you get like a slightly better model which
shows that using both modalities improves

00:59:57.469 --> 01:00:02.149
the model as well as looking at what
happens if you add noise to the model.

01:00:02.149 --> 01:00:03.450
Great.
Thanks.

01:00:03.450 --> 01:00:09.398
&gt;&gt; [APPLAUSE]
&gt;&gt; Thanks, Michael.

01:00:09.398 --> 01:00:11.379
Yeah, so obviously,
a lot of details there.

01:00:11.379 --> 01:00:16.211
But again, that's kind of an example of
what's been happening with deep learning

01:00:16.211 --> 01:00:21.111
where you're taking this basic model
architecture, things like LSTM and saying,

01:00:21.111 --> 01:00:24.492
here's another problem,
let's try it on that as well and

01:00:24.492 --> 01:00:26.933
it turns out to work fantastically well.

01:00:26.933 --> 01:00:28.365
Let's say, 20 minutes left.

01:00:28.365 --> 01:00:31.869
I'll see how high I can get in teaching
everything else about it on machine

01:00:31.869 --> 01:00:32.613
translation.

01:00:32.613 --> 01:00:36.828
So it's something I did just want
to explain is so, back here and

01:00:36.828 --> 01:00:41.450
in general, when we've been showing
machine translation results.

01:00:41.450 --> 01:00:44.212
We've been divvying these
graphs that up is good and

01:00:44.212 --> 01:00:48.347
what it's been measuring with these
numbers are things called blue scores.

01:00:48.347 --> 01:00:54.051
So, I wanted to give you some idea of how
and why we evaluate machine translation.

01:00:54.051 --> 01:01:00.266
So the central thing to know about machine
translation is if you take a paragraph or

01:01:00.266 --> 01:01:04.410
text and give it to ten
different humans translators,

01:01:04.410 --> 01:01:08.340
you'll get back ten
different translations.

01:01:08.340 --> 01:01:13.068
There's no correct answer
as to how to translate

01:01:13.068 --> 01:01:16.537
a sentence into another language.

01:01:16.537 --> 01:01:21.007
And in practice, most of the time
all translations are imperfect and

01:01:21.007 --> 01:01:26.091
it's kind of deciding what you wanna pay
most attention to is that do you want to

01:01:26.091 --> 01:01:31.179
maximally preserve the metaphor that
the person used in the source language or

01:01:31.179 --> 01:01:34.956
do you wanna more directly
convey the meaning it conveys,

01:01:34.956 --> 01:01:40.666
because that metaphor won't really be
familiar to people in the target language.

01:01:40.666 --> 01:01:43.475
Do you want to choose sort
of short direct words,

01:01:43.475 --> 01:01:46.362
because it's written in a short,
direct style?

01:01:46.362 --> 01:01:48.565
Or do you more want to sort of,

01:01:48.565 --> 01:01:52.984
you choose a longer word that's
a more exact translation?

01:01:52.984 --> 01:01:56.872
There's all of these decisions and
things and in some sense a translator is

01:01:56.872 --> 01:01:59.710
optimizing over if we do it
in machine learning terms,

01:01:59.710 --> 01:02:02.194
but the reality is it's
sort of not very clear.

01:02:02.194 --> 01:02:03.980
There are a lot of choices.

01:02:03.980 --> 01:02:07.784
You have lots of syntactic choices
as whether you make it a passive or

01:02:07.784 --> 01:02:09.864
an active and word order, and so on.

01:02:09.864 --> 01:02:10.796
No right answer.

01:02:10.796 --> 01:02:14.638
So we just can't have it like a lot
things of saying, here's the accuracy,

01:02:14.638 --> 01:02:16.680
that was what you were meant to use.

01:02:16.680 --> 01:02:18.273
So, how do you do it?

01:02:18.273 --> 01:02:22.450
So, one way to do MT evaluation
is to do it manually.

01:02:22.450 --> 01:02:26.654
You get human beings to look
at translations and to say,

01:02:26.654 --> 01:02:28.089
how good they are.

01:02:28.089 --> 01:02:29.901
And to this day, basically,

01:02:29.901 --> 01:02:34.540
that's regarded as the gold standard
of machine translation evaluation,

01:02:34.540 --> 01:02:38.740
because we don't have a better
way to fully automate things.

01:02:38.740 --> 01:02:43.212
So one way of doing that is things
like Likert scales where you're

01:02:43.212 --> 01:02:46.791
getting humans to judge
translations to adequacy,

01:02:46.791 --> 01:02:52.077
which is how well they convey the meaning
of the source and fluency which is for

01:02:52.077 --> 01:02:56.492
how natural the output sentence
sounds in the target language.

01:02:56.492 --> 01:03:02.275
Commonly, a way that's more easily
measurable that people prefer is actually

01:03:02.275 --> 01:03:08.058
if you're comparing systems for goodness
is that you directly ask human beings

01:03:08.058 --> 01:03:13.603
to do pairwise judgments of which is
better translation A or translation B.

01:03:13.603 --> 01:03:17.587
I mean, it turns out that even
that is incredibly hard for

01:03:17.587 --> 01:03:23.071
humans to do as someone who has sat around
doing this task of human evaluation.

01:03:23.071 --> 01:03:26.028
I mean, all the time, it's kind of okay,

01:03:26.028 --> 01:03:30.793
this one made a bad word choice here and
this one got the wrong verb form

01:03:30.793 --> 01:03:34.178
there which of these do I
regard as a worse error.

01:03:34.178 --> 01:03:38.730
So it's a difficult thing, but
we use the data we can from human beings.

01:03:38.730 --> 01:03:41.820
Okay, that's still the best
thing that we can do.

01:03:41.820 --> 01:03:43.530
It has problems.

01:03:43.530 --> 01:03:50.770
Basically, it's slow and expensive to get
human beings to judge translation quality.

01:03:50.770 --> 01:03:52.540
So what else could we do?

01:03:52.540 --> 01:03:57.454
Well, another obvious idea is to say,
well, If we can embed machine

01:03:57.454 --> 01:04:02.992
translation into some task, we can just
see which is more easily a valuable.

01:04:02.992 --> 01:04:09.360
We could just see which MT system
lets us do the final task better.

01:04:09.360 --> 01:04:13.600
So, we'd like to do question answering
over foreign language documents.

01:04:13.600 --> 01:04:17.317
We'll just to get our question
answers correct score, and

01:04:17.317 --> 01:04:19.633
they'll be much easier to measure.

01:04:19.633 --> 01:04:22.405
And that's something that you can do, but

01:04:22.405 --> 01:04:25.877
it turns out that that often
isn't very successful.

01:04:25.877 --> 01:04:29.726
Cuz commonly your accuracy
on the downstream task is

01:04:29.726 --> 01:04:34.453
very little affected by many of
the fine points of translation.

01:04:34.453 --> 01:04:38.940
An extreme example of that is sort of
like cross-lingual information retrieval.

01:04:38.940 --> 01:04:41.690
When you're just wanting
to retrieve relevant

01:04:41.690 --> 01:04:44.000
documents to a query in another language.

01:04:44.000 --> 01:04:48.113
That providing you can kind of produce
some of the main content words in

01:04:48.113 --> 01:04:53.131
the translation, it really doesn't matter
how you screw up the details of syntax and

01:04:53.131 --> 01:04:54.265
verb inflection.

01:04:54.265 --> 01:04:55.910
It's not really gonna affect your score.

01:04:57.650 --> 01:05:00.650
Okay, so what people have
wanted to have is a direct

01:05:02.070 --> 01:05:06.340
metric that is fast and cheap to apply.

01:05:06.340 --> 01:05:11.270
And for a long time, I think no one
thought there was such a thing.

01:05:11.270 --> 01:05:15.960
And so
then starting in the very early 2000s,

01:05:15.960 --> 01:05:20.920
people at IBM suggested
this first idea of, hey,

01:05:20.920 --> 01:05:26.740
here's a cheap way in which we can
measure word translation quality.

01:05:26.740 --> 01:05:29.560
And so they called it the BLEU metric.

01:05:29.560 --> 01:05:34.324
And so
here was the idea of how they do that.

01:05:34.324 --> 01:05:39.540
What they said is let us
produce reference translations.

01:05:39.540 --> 01:05:44.440
We know that there are many, many possible
ways that something can be translated.

01:05:44.440 --> 01:05:53.150
But let's get a human being to
produce a reference translation.

01:05:53.150 --> 01:05:57.974
So what we are going to do is then we're
going to have a reference translation by

01:05:57.974 --> 01:06:01.940
a human, and
we're going to have a machine translation.

01:06:01.940 --> 01:06:06.622
And to a first approximation we're
going to say that the machine

01:06:06.622 --> 01:06:11.490
translation is good to the extent
that you can find word n-grams.

01:06:11.490 --> 01:06:15.691
So sequences of words like three
words in a row, two words in a row,

01:06:15.691 --> 01:06:20.310
which also appear in the reference
translation anywhere.

01:06:20.310 --> 01:06:21.946
So what are the elements of this?

01:06:21.946 --> 01:06:28.780
So by having multi-word sequences,
that's meant to be trying to

01:06:28.780 --> 01:06:33.660
judge whether you have some understanding
of the sort of right syntax and arguments.

01:06:33.660 --> 01:06:37.270
Because you're much more likely
to match a four word sequence

01:06:37.270 --> 01:06:40.530
if it's not just you've
got a bag of keywords.

01:06:40.530 --> 01:06:44.090
You actually understand something
of the syntax of the sentence.

01:06:44.090 --> 01:06:48.710
The fact that you can match it anywhere
is meant to be dealing with the fact that

01:06:48.710 --> 01:06:51.860
human languages normally have
quite flexible word order.

01:06:51.860 --> 01:06:57.539
So it's not adequate to insist that
the phrases appear in the same word order.

01:06:57.539 --> 01:07:02.503
Of course, in general in English, a lot of
the time you can say, last night I went

01:07:02.503 --> 01:07:07.040
to my friend's place, or,
I went to my friend's place last night.

01:07:07.040 --> 01:07:09.890
And it seems like you should
get credit for last night

01:07:09.890 --> 01:07:12.440
regardless of whether you put it at
the beginning or the end of the sentence.

01:07:13.910 --> 01:07:17.833
So, that was the general idea
in slightly more detail.

01:07:17.833 --> 01:07:20.570
The BLEU measure is a precision score.

01:07:20.570 --> 01:07:25.025
So it's looking at whether
n-grams that are in the machine

01:07:25.025 --> 01:07:29.312
translation also appear in
the reference translation.

01:07:29.312 --> 01:07:31.710
There are a couple of fine points then.

01:07:31.710 --> 01:07:36.740
You are only allowed to count for
a certain n and n-gram once.

01:07:36.740 --> 01:07:41.800
So if in your translation,
the airport appears three times,

01:07:41.800 --> 01:07:44.660
but there's only one
the airport in the reference,

01:07:44.660 --> 01:07:48.640
you're only allowed to count one of
them as correct, not all three of them.

01:07:48.640 --> 01:07:53.900
And then there's this other trick that we
have, this thing called a brevity penalty.

01:07:53.900 --> 01:07:57.576
Because if it's purely
a precision-oriented measure,

01:07:57.576 --> 01:08:01.932
saying is what appears in the machine
translation in the reference.

01:08:01.932 --> 01:08:03.772
There are games you could play,

01:08:03.772 --> 01:08:07.398
like you could just translate
every passage with the word the.

01:08:07.398 --> 01:08:11.287
Because if it's English the word the is
pretty sure to appear somewhere in

01:08:11.287 --> 01:08:14.122
the reference translation,
and get precision one.

01:08:14.122 --> 01:08:16.130
And that seems like it's cheating.

01:08:16.130 --> 01:08:21.475
So if you're making what your translation
is shorter than the human translations,

01:08:21.475 --> 01:08:25.040
you'll lose.

01:08:25.040 --> 01:08:31.250
Okay, so more formally, so you're doing
this with n-grams up to a certain size.

01:08:31.250 --> 01:08:35.330
Commonly it's four so you use single
words, pairs of words, triples,

01:08:35.330 --> 01:08:36.670
and four words.

01:08:36.670 --> 01:08:38.896
You work out this kind
of precision of each.

01:08:38.896 --> 01:08:43.075
And then you're working out a kind
of a weighted geometric mean

01:08:43.075 --> 01:08:44.855
of those precisions.

01:08:44.855 --> 01:08:47.753
And you multiplying that
by brevity penalty.

01:08:47.753 --> 01:08:52.702
And the brevity penalty penalizes
you if your translation

01:08:52.702 --> 01:08:56.546
is shorter than the reference translation.

01:08:56.546 --> 01:09:00.590
There are some details here, but
maybe I'll just skip them and go ahead.

01:09:00.590 --> 01:09:06.610
So there's one other idea then which is,
well, what about this big problem that,

01:09:06.610 --> 01:09:11.091
well, there are a lot of different
ways to translate things.

01:09:11.091 --> 01:09:15.339
And there's no guarantee that your
translation could be great, and

01:09:15.339 --> 01:09:18.870
it might just not match
the human's translation.

01:09:18.870 --> 01:09:24.226
And so the answer to that that
the original IBM paper suggested

01:09:24.226 --> 01:09:30.097
was what we should do is collect
a bunch of reference translations.

01:09:30.097 --> 01:09:34.090
And the suggested number that's
been widely used was four.

01:09:34.090 --> 01:09:39.680
And so then, most likely,
if you're giving a good translation,

01:09:39.680 --> 01:09:43.070
it'll appear in one of
the reference translations.

01:09:43.070 --> 01:09:45.892
And then, you'll get a matching n-gram.

01:09:45.892 --> 01:09:49.110
Now, of course,
that's the sort of a statistical argument.

01:09:49.110 --> 01:09:51.781
Cuz you might have a really
good translation and

01:09:51.781 --> 01:09:53.999
none of the four translators chose it.

01:09:53.999 --> 01:09:57.785
And the truth is then in
that case you just lose.

01:09:57.785 --> 01:10:02.745
And indeed what's happened in more
recent work is quite a lot of the time,

01:10:02.745 --> 01:10:07.718
actually, the BLEU measure is only
run with one reference translation.

01:10:07.718 --> 01:10:10.133
And that's seems a little bit cheap.

01:10:10.133 --> 01:10:14.922
And it's certainly the case that if you're
running with one reference translation,

01:10:14.922 --> 01:10:19.448
you're either just lucky or unlucky as to
whether you guessed to translate the way

01:10:19.448 --> 01:10:21.270
the translator translates.

01:10:21.270 --> 01:10:25.643
But you can make a sort of a statistical
argument which by and large is valid.

01:10:25.643 --> 01:10:28.549
That if you're coming up
with good translations,

01:10:28.549 --> 01:10:33.359
providing there's no correlation somehow
between one system and the translator.

01:10:33.359 --> 01:10:38.015
That you'd still expect on balance that
you'll get a higher score if you're

01:10:38.015 --> 01:10:40.950
consistently giving better translations.

01:10:40.950 --> 01:10:43.260
And broadly speaking, that's right.

01:10:43.260 --> 01:10:49.445
Though this problem of correlation does
actually start to rear its head, right?

01:10:49.445 --> 01:10:54.124
That if the reference translator always
translated the things as US, and

01:10:54.124 --> 01:10:59.336
one system translates with US, and the
other one translates with United States.

01:10:59.336 --> 01:11:01.211
Kind of one person will get lucky, and

01:11:01.211 --> 01:11:04.147
the other one will get unlucky
in a kind of a correlated way.

01:11:04.147 --> 01:11:07.004
And that can create problems.

01:11:07.004 --> 01:11:12.655
So even though it was very simple
when BLEU was initially introduced,

01:11:12.655 --> 01:11:17.445
it seemed to be miraculously
good that it just corresponded

01:11:17.445 --> 01:11:22.705
really well with human judgments
of translation quality.

01:11:22.705 --> 01:11:26.885
Rarely do you see an empirical
data set that's as linear as that.

01:11:26.885 --> 01:11:29.595
And so this seemed really awesome.

01:11:29.595 --> 01:11:33.755
Like many things that
are surrogate metrics,

01:11:33.755 --> 01:11:38.770
there are a lot of surrogate metrics that
work really well If no one is trying

01:11:38.770 --> 01:11:43.860
to optimize them but don't work so well
once people are trying to optimize them.

01:11:43.860 --> 01:11:45.620
So what happen then was,

01:11:45.620 --> 01:11:49.690
everyone evaluated their systems
on BLEU scores and so therefore,

01:11:49.690 --> 01:11:53.970
all researchers worked on how to make
their systems have better BLEU scores.

01:11:53.970 --> 01:11:58.850
And then what happened is this
correlation graph went way down.

01:11:58.850 --> 01:12:03.820
And so the truth is now that current,
and this relates to the sort of

01:12:03.820 --> 01:12:06.900
when I was saying the Google
results were exaggerated.

01:12:06.900 --> 01:12:14.380
The truth is that current MT systems
produce BLEU scores that are very similar

01:12:14.380 --> 01:12:18.440
to human translations for many language
pairs which reflects the fact that

01:12:18.440 --> 01:12:23.540
different human beings are quite creative
and vary in how they translate sensors.

01:12:23.540 --> 01:12:27.770
But in truth, the quality of machine
translation is still well below

01:12:27.770 --> 01:12:31.651
the quality of human translation.

01:12:31.651 --> 01:12:35.195
Okay, few minutes left to
say a bit more about MT.

01:12:35.195 --> 01:12:37.595
I think I can't get through
all this material, but

01:12:37.595 --> 01:12:41.185
let me just give you a little
bit of a sense of some of it.

01:12:42.200 --> 01:12:46.910
Okay, so one of the big problems you
have if you've tried to build something,

01:12:46.910 --> 01:12:49.170
any kind of generation system,

01:12:49.170 --> 01:12:55.070
where you're generating words is you have
a problem that there are a lot of words.

01:12:55.070 --> 01:12:58.170
Languages have very large vocabularies.

01:12:58.170 --> 01:13:02.480
So from the hidden state,
what we're doing is multiplying by this

01:13:02.480 --> 01:13:07.740
matrix of Softmax parameters,
which is the size of the vocabulary

01:13:07.740 --> 01:13:12.470
times the size of the hidden
state doing this Softmax.

01:13:12.470 --> 01:13:14.680
And that's giving us
the probability of different words.

01:13:16.020 --> 01:13:19.080
And so the problem is if you wanna
have a very large vocabulary,

01:13:19.080 --> 01:13:24.630
you spend a huge amount of time just doing
these Softmaxes over, and over again.

01:13:24.630 --> 01:13:29.640
And so, for instance, you saw that in
the kind of pictures of the Google system,

01:13:29.640 --> 01:13:34.190
that over half of their
computational power was just going

01:13:34.190 --> 01:13:38.620
into calculating these Softmax so
that's being a real problem.

01:13:38.620 --> 01:13:42.730
So something people have worked
on quite a lot is how can we

01:13:42.730 --> 01:13:45.130
string the cost of that computation.

01:13:46.240 --> 01:13:49.650
Well one thing we can do is say,
ha, let's use a smaller vocabulary.

01:13:49.650 --> 01:13:54.350
Let's only use a 50,000 word
vocabulary for our MT system, and

01:13:54.350 --> 01:13:56.990
some of the early MT
work did precisely that.

01:13:56.990 --> 01:14:01.970
But the problem is, that if you do that,
you start with lively sentences.

01:14:01.970 --> 01:14:06.650
And instead what you get is unk,
unk, unk because all of

01:14:06.650 --> 01:14:12.270
the interesting words in the sentence fall
outside of your 50,000 word vocabulary.

01:14:12.270 --> 01:14:18.190
And those kind of sentences are not very
good ones to show that human beings,

01:14:18.190 --> 01:14:20.600
because they don't like them very much.

01:14:20.600 --> 01:14:25.740
So, it seems like we need to
somehow do better than that.

01:14:25.740 --> 01:14:30.540
So, there's been work on, well, how can
we more effectively do the softmaxes

01:14:30.540 --> 01:14:33.470
without having to do as much computation.

01:14:33.470 --> 01:14:36.400
And so,
there have been some ideas on that.

01:14:36.400 --> 01:14:41.420
One idea is to sort of have a hierarchical
Softmax where we do the standard

01:14:41.420 --> 01:14:44.510
computer scientist trick
of putting a tree structure

01:14:44.510 --> 01:14:46.830
to improve our amount of computation.

01:14:46.830 --> 01:14:50.770
So if you can sort of divide the
vocabulary into sort of tree pieces and

01:14:50.770 --> 01:14:54.850
divide down branches of the tree,
we can do less computation.

01:14:56.070 --> 01:15:00.167
Remember, we did noise
contrast to destination for

01:15:00.167 --> 01:15:03.998
words of that was a way
of avoiding computation.

01:15:03.998 --> 01:15:06.860
Those are possible ways to do things.

01:15:06.860 --> 01:15:09.360
They are not very
GPU-friendly unfortunately.

01:15:09.360 --> 01:15:13.920
Once you start taking branches down
the tree, you then can't do the kind

01:15:13.920 --> 01:15:17.820
of nice just bang bang bang type
of computations down to GPU.

01:15:17.820 --> 01:15:21.930
So there's been on work on coming
up with alternatives to that, and

01:15:21.930 --> 01:15:24.930
I wanted to mention one example of this.

01:15:24.930 --> 01:15:29.862
And an idea of this is well,
maybe we can actually

01:15:29.862 --> 01:15:34.815
sort of just work with small
vocabularies at any one time.

01:15:34.815 --> 01:15:39.625
So when we're training our models,
we could train using subsets of

01:15:39.625 --> 01:15:45.060
the vocabulary because there's a lot
of rare words but they're rare.

01:15:45.060 --> 01:15:51.220
So if you pick any slice of the training
data most rare words won't be in it.

01:15:51.220 --> 01:15:55.010
Commonly if you look at your
whole vocabulary about 40% of

01:15:55.010 --> 01:15:57.770
your word types occur only once.

01:15:57.770 --> 01:16:02.030
That means if you cut your
data set into 20 pieces,

01:16:02.030 --> 01:16:04.690
19 of those 20 will not contain that word.

01:16:04.690 --> 01:16:09.140
And then,
we also wanna be smart on testing.

01:16:09.140 --> 01:16:15.470
So we wanna be able to, at test time
as well, generate sort of a smaller

01:16:15.470 --> 01:16:20.990
set of words for our soft max, and so we
can be fast at both train and test time.

01:16:20.990 --> 01:16:22.290
Well, how can you do that?

01:16:22.290 --> 01:16:28.630
Well, so at training time,
we want to have a small vocabulary.

01:16:28.630 --> 01:16:33.730
And so we can do that by partitioning the
vocab, for partitioning the training data,

01:16:33.730 --> 01:16:38.449
each slice of the training data,
we'll have a much lower vocabulary.

01:16:39.580 --> 01:16:44.896
And then we could partition randomly or
we could even smarter and

01:16:44.896 --> 01:16:49.532
we can cut it into pieces
that have similar vocabulary.

01:16:49.532 --> 01:16:52.634
If we put all the basketball
articles in one file and

01:16:52.634 --> 01:16:57.775
all the foot walled articles in another
pile, will shrink the vocabulary further.

01:16:57.775 --> 01:17:03.235
And so they look at ways of doing that,
so in practice that they can get down and

01:17:03.235 --> 01:17:07.855
order a magnitude or more in the size
of the vocab that they need for

01:17:07.855 --> 01:17:10.720
each slice of the data, that's great.

01:17:13.040 --> 01:17:15.980
Okay, so what do we do at test time?

01:17:15.980 --> 01:17:18.800
Well, what we wanna do
it at test time as well,

01:17:18.800 --> 01:17:23.800
when we're actually translating,
we want to use as much smaller vocabulary.

01:17:23.800 --> 01:17:26.250
Well, here's an idea of
how you could do that.

01:17:26.250 --> 01:17:28.890
Firstly, we say, they're are just common

01:17:28.890 --> 01:17:32.300
function words that we always
gonna want to have available.

01:17:32.300 --> 01:17:35.030
So we pick the K most frequent words and

01:17:35.030 --> 01:17:37.420
say we're always gonna
have them in our Softmax.

01:17:37.420 --> 01:17:39.930
But then for the rest of it,

01:17:39.930 --> 01:17:44.730
what we're actually gonna do is
sort of have a lexicon on the side

01:17:44.730 --> 01:17:49.500
where we're gonna know about likely
translations for each source word.

01:17:49.500 --> 01:17:54.300
So that we'll have stored ways that would
be reasonable to translate she loves

01:17:54.300 --> 01:17:56.440
cats into French.

01:17:56.440 --> 01:17:59.900
And so when we're translating a sentence,
we'll look out for

01:17:59.900 --> 01:18:04.520
each word in the source sentence what
are likely translations of it and

01:18:04.520 --> 01:18:07.770
throw those into our candidates for
the Softmax.

01:18:09.540 --> 01:18:14.750
And so then we've got a sort
of a candidate list of words.

01:18:14.750 --> 01:18:17.660
And when translating
a particular soft sentence,

01:18:17.660 --> 01:18:21.110
we'll only run our
Softmax over those words.

01:18:21.110 --> 01:18:28.290
And then again, we can save well over
an order of magnitude computations.

01:18:28.290 --> 01:18:34.230
So, K prime is about 10 or 20 and
K is sort of a reasonable size vocab.

01:18:34.230 --> 01:18:39.001
We can again, sort of cut at least in
the order of magnitude the size of

01:18:39.001 --> 01:18:42.721
our soft mixers and
act as if we had large vocabulary.

01:18:44.558 --> 01:18:50.060
There are other ways to do that too,
which are on this slide.

01:18:50.060 --> 01:18:54.950
And what I was then going to go on,
and we'll decide whether it does or

01:18:54.950 --> 01:18:57.120
doesn't happen based on the syllabus.

01:18:57.120 --> 01:19:02.930
I mean, you could sort of say,
well, that's still insufficient

01:19:02.930 --> 01:19:07.760
because I sort of said that you have
to deal with a large vocabulary.

01:19:07.760 --> 01:19:13.960
And you've sort of told us how to deal
with a large vocabulary more efficiently.

01:19:13.960 --> 01:19:18.690
But you've still got problems, because
in any new piece of text you give it,

01:19:18.690 --> 01:19:23.460
you're going to have things like new
names turn up, new numbers turn up, and

01:19:23.460 --> 01:19:26.530
you're going to want to
deal with those as well.

01:19:26.530 --> 01:19:30.620
And so
it seems like somehow we want to be able

01:19:30.620 --> 01:19:35.720
to just deal with new stuff at test time,
at translation time.

01:19:35.720 --> 01:19:39.270
Which effectively means that
kind of theoretically we have

01:19:39.270 --> 01:19:40.590
an infinite vocabulary.

01:19:40.590 --> 01:19:44.260
And so, there's also been a bunch of
work on newer machine translation and

01:19:44.260 --> 01:19:45.470
dealing with that.

01:19:45.470 --> 01:19:49.090
But unfortunately, this class time is not

01:19:49.090 --> 01:19:53.740
long enough to tell you about it right
now, so I'll stop here for today.

01:19:53.740 --> 01:19:58.700
And don't forget, outside you can
collect your midterm on the way out.

