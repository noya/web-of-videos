WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.787
[MUSIC]

00:00:04.787 --> 00:00:07.657
Stanford University.

00:00:07.657 --> 00:00:09.071
&gt;&gt; All right, hello everybody.

00:00:10.530 --> 00:00:14.340
Welcome to Lecture seven or
maybe it's eight.

00:00:14.340 --> 00:00:17.550
Definitely today is the beginning of

00:00:18.620 --> 00:00:23.100
where we talk about models that
really matter in practice.

00:00:23.100 --> 00:00:27.930
We'll talk today about the simplest
recurrent neural network model

00:00:27.930 --> 00:00:29.030
one can think of.

00:00:29.030 --> 00:00:29.690
But in general,

00:00:29.690 --> 00:00:34.450
this model family is what most people
now use in real production settings.

00:00:34.450 --> 00:00:36.770
So it's really exciting.

00:00:36.770 --> 00:00:39.830
We only have a little bit
of math in between and

00:00:39.830 --> 00:00:43.880
a lot of it is quite applied and
should be quite fun.

00:00:43.880 --> 00:00:47.040
Just one organizational
item before we get started.

00:00:47.040 --> 00:00:50.330
I'll have an extra office
hour today right after class.

00:00:50.330 --> 00:00:52.610
I'll be again on Queuestatus 68 or so.

00:00:53.960 --> 00:00:56.300
Last week we had to end at 8:30.

00:00:56.300 --> 00:00:59.704
And there's still a lot of
people who had a question, so

00:00:59.704 --> 00:01:03.552
I'll be here after class for
probably another two hours or so.

00:01:03.552 --> 00:01:05.880
Try to get through everybody's questions.

00:01:05.880 --> 00:01:10.705
Are there any questions around projects?

00:01:10.705 --> 00:01:12.845
&gt;&gt; [LAUGH]
&gt;&gt; And organizational stuff?

00:01:18.075 --> 00:01:21.350
All right, then let's take a look
at the overview for today.

00:01:21.350 --> 00:01:26.520
So to really appreciate the power of
recurrent neural networks it makes sense

00:01:26.520 --> 00:01:29.000
to get a little bit of background
on traditional language models.

00:01:29.000 --> 00:01:33.628
Which will have huge RAM requirements and
won't be quite feasible in

00:01:33.628 --> 00:01:38.518
their best kinds of settings where
they obtain the highest accuracies.

00:01:38.518 --> 00:01:41.440
And then we'll motivate recurrent
neural networks with language modeling.

00:01:41.440 --> 00:01:44.320
It's a very important
kind of fundamental task

00:01:44.320 --> 00:01:46.560
in NLP that tries to
predict the next word.

00:01:46.560 --> 00:01:49.720
Something that sounds quite simple but
is really powerful.

00:01:49.720 --> 00:01:53.560
And then we'll dive a little bit into
the problems that you can actually quite

00:01:53.560 --> 00:01:58.610
easily understand once
you have figured out

00:01:58.610 --> 00:02:03.060
how to take gradients and you actually
understand what backpropagation does.

00:02:03.060 --> 00:02:07.700
And then we can go and
see how to extend these models and

00:02:07.700 --> 00:02:10.897
apply them to real sequence tasks
that people really run in practice.

00:02:11.970 --> 00:02:14.500
All right, so let's dive right in.

00:02:14.500 --> 00:02:15.360
Language models.

00:02:15.360 --> 00:02:16.680
So basically,

00:02:16.680 --> 00:02:20.490
we want to just compute the probability
of an entire sequence of words.

00:02:20.490 --> 00:02:22.110
And you might say,
well why is that useful?

00:02:22.110 --> 00:02:25.750
Why should we be able to compute
how likely a sequence is?

00:02:25.750 --> 00:02:28.620
And actually comes up for
a lot of different kinds of problems.

00:02:28.620 --> 00:02:31.820
So one, for instance,
in machine translation,

00:02:31.820 --> 00:02:36.720
you might have a bunch of potential
translations that a system gives you.

00:02:36.720 --> 00:02:40.690
And then you might wanna understand
which order of words is the best.

00:02:40.690 --> 00:02:46.370
So "the cat is small" should get a higher
probability than "small the is cat".

00:02:46.370 --> 00:02:49.790
But based on another language
that you translate from,

00:02:49.790 --> 00:02:52.110
it might not be as obvious.

00:02:52.110 --> 00:02:56.060
And the other language might have
a reversed word order and whatnot.

00:02:56.060 --> 00:02:58.480
Another one is when you do speech
recognition, for instance.

00:02:58.480 --> 00:03:00.670
It also comes up in the machine
translation a little bit,

00:03:00.670 --> 00:03:02.250
where you might have,

00:03:02.250 --> 00:03:05.335
well this particular example is clearly
more a machine translation example.

00:03:05.335 --> 00:03:10.570
But comes up also in speech
recognition where you might wanna

00:03:10.570 --> 00:03:14.740
understand which word might be the better
choice given the rest of the sequence.

00:03:14.740 --> 00:03:18.740
So "walking home after school" sounds
a lot more natural than "walking

00:03:18.740 --> 00:03:20.280
house after school".

00:03:20.280 --> 00:03:25.210
But home and
house have the same translation or

00:03:25.210 --> 00:03:28.870
same word in German which is haus,
H A U S.

00:03:28.870 --> 00:03:32.000
And you want to know which one is
the better one for that translation.

00:03:32.000 --> 00:03:35.670
So comes up in a lot of
different kinds of areas.

00:03:36.840 --> 00:03:42.850
Now basically it's hard to compute
the perfect probabilities for

00:03:42.850 --> 00:03:45.900
all potential sequences 'cause
there are a lot of them.

00:03:45.900 --> 00:03:50.940
And so what we usually end up doing is
we basically condition on just a window,

00:03:50.940 --> 00:03:56.000
we try to predict the next word based
on the just the previous n words

00:03:56.000 --> 00:03:58.100
before the one that
we're trying to predict.

00:03:58.100 --> 00:04:01.560
So this is, of course,
an incorrect assumption.

00:04:01.560 --> 00:04:06.970
The next word that I will utter will
depend on many words in the past.

00:04:06.970 --> 00:04:10.030
But it's something that had to be done

00:04:10.030 --> 00:04:13.740
to use traditional count based
machine learning models.

00:04:13.740 --> 00:04:19.070
So basically we'll approximate this
overall sequence probability here

00:04:19.070 --> 00:04:22.470
with just a simpler version.

00:04:22.470 --> 00:04:28.360
In the perfect sense this would basically
be the product here of each word,

00:04:28.360 --> 00:04:31.040
given all preceding words
from the first one all

00:04:31.040 --> 00:04:34.200
the way to the one just
before the i_th one.

00:04:34.200 --> 00:04:39.360
But in practice, this probability with
traditional machine learning models

00:04:39.360 --> 00:04:40.590
we couldn't really compute so

00:04:40.590 --> 00:04:46.690
we actually approximate that with some
number of n words just before each word.

00:04:46.690 --> 00:04:50.780
So this is a simple Markov assumption
just assuming the next action or

00:04:50.780 --> 00:04:55.060
next word that is uttered just
depends on n previous words.

00:04:55.060 --> 00:05:00.490
And now if we wanted to use traditional
methods that are just basically

00:05:00.490 --> 00:05:04.850
based on the counts of words and not
using our fancy word vectors and so on.

00:05:04.850 --> 00:05:09.646
Then the way we would compute and estimate
these probabilities is essentially just by

00:05:09.646 --> 00:05:13.928
counting how often does, if you want to
get the probability for the second word,

00:05:13.928 --> 00:05:15.203
given the first word.

00:05:15.203 --> 00:05:21.016
We would just basically count up how often
do these two words co-occur in this order,

00:05:21.016 --> 00:05:25.450
divided by how often the first
word appears in the whole corpus.

00:05:25.450 --> 00:05:28.150
Let's say we have a very large corpus and
we just collect all these counts.

00:05:30.100 --> 00:05:35.290
And now if we wanted to condition not just
on the first and the previous word but

00:05:35.290 --> 00:05:39.700
on the two previous words, then we'd
have to compute all these counts.

00:05:39.700 --> 00:05:42.610
And now you can kind of sense that well,

00:05:42.610 --> 00:05:47.170
if we want to ideally condition on as
many n-grams as possible before but

00:05:47.170 --> 00:05:54.102
we have a large vocabulary of say 100,000
words, then we'll have a lot of counts.

00:05:54.102 --> 00:05:56.599
Essentially 100,000 cubed,

00:05:56.599 --> 00:06:01.441
many numbers we would have to store
to estimate all these probabilities.

00:06:03.674 --> 00:06:04.840
Does that make sense?

00:06:04.840 --> 00:06:07.259
Are there any questions for
these traditional methods?

00:06:13.233 --> 00:06:17.898
All right, now, the problem with
that is that the performance

00:06:17.898 --> 00:06:22.222
usually improves as we have more and
more of these counts.

00:06:22.222 --> 00:06:26.980
But, also,
you now increase your RAM requirements.

00:06:26.980 --> 00:06:30.900
And so,
one of the best models of this traditional

00:06:31.980 --> 00:06:38.400
type actually required 140 gigs of RAM for
just computing all these counts

00:06:38.400 --> 00:06:43.470
when they wanted to compute them for
126 billion token corpus.

00:06:44.650 --> 00:06:49.570
So it's very,
very inefficient in terms of RAM.

00:06:49.570 --> 00:06:53.470
And you would never be able
to put a model that basically

00:06:54.930 --> 00:06:57.500
stores all these different n-gram counts.

00:06:57.500 --> 00:07:00.210
You could never store it in a phone or
any small machine.

00:07:02.550 --> 00:07:06.930
And now, of course, once computer
scientists struggle with a problem like

00:07:06.930 --> 00:07:09.050
that, they'll find ways to deal with it,
and so,

00:07:09.050 --> 00:07:11.830
there are a lot of different
ways you can back off.

00:07:11.830 --> 00:07:16.340
You say, well, if I don't find the 4-gram,
or I didn't store it,

00:07:16.340 --> 00:07:20.160
because it was not frequent enough,
then maybe I'll try the 3-gram.

00:07:20.160 --> 00:07:24.170
And if I can't find that or I don't have
many counts for that, then I can back off

00:07:24.170 --> 00:07:29.260
and estimate my probabilities with fewer
and fewer words in the context size.

00:07:29.260 --> 00:07:32.745
But in general you want
to have at least tri or

00:07:32.745 --> 00:07:37.130
4-grams that you store and the RAM
requirements for those are very large.

00:07:38.630 --> 00:07:41.710
So that is actually something
that you'll observe in a lot of

00:07:41.710 --> 00:07:43.790
comparisons between deep
learning models and

00:07:43.790 --> 00:07:49.290
traditional NLP models that are based on
just counting words for specific classes.

00:07:49.290 --> 00:07:51.331
The more powerful your models are,

00:07:51.331 --> 00:07:55.277
sometimes the RAM requirements can
get very large very quickly, and

00:07:55.277 --> 00:07:59.305
there are a lot of different ways
people tried to combat these issues.

00:07:59.305 --> 00:08:02.855
Now our way will be to use
recurrent neural networks.

00:08:04.390 --> 00:08:08.700
Where basically, they're similar to
the normal neural networks that we've seen

00:08:08.700 --> 00:08:13.600
already, but they will actually tie
the weights between different time steps.

00:08:13.600 --> 00:08:15.765
And as you go over it, you keep using,

00:08:15.765 --> 00:08:21.001
re-using essentially the same
linear plus non-linearity layer.

00:08:22.260 --> 00:08:26.620
And that will at least in theory,
allow us to actually condition

00:08:26.620 --> 00:08:30.040
what we're trying to predict
on all the previous words.

00:08:30.040 --> 00:08:34.141
And now here the RAM requirements will
only scale with the number of words not

00:08:34.141 --> 00:08:37.746
with the length of the sequence
that we might want to condition on.

00:08:39.652 --> 00:08:42.200
So now how's this really defined?

00:08:42.200 --> 00:08:44.980
Again, they're you'll see different
kinds of visualizations and

00:08:44.980 --> 00:08:46.680
I'm introducing you to a couple.

00:08:46.680 --> 00:08:52.470
I like sort of this unfolded one where we
have here a abstract hidden time step t

00:08:52.470 --> 00:08:59.240
and we basically, it's conditioned on
H_t-1, and then here you compute H_t+1.

00:08:59.240 --> 00:09:02.020
But in general,
the equations here are quite intuitive.

00:09:03.220 --> 00:09:05.960
We assume we have a list of word vectors.

00:09:05.960 --> 00:09:09.100
For now,
let's assume the word vectors are fixed.

00:09:09.100 --> 00:09:15.050
Later on we can actually loosen
that assumption and get rid of it.

00:09:15.050 --> 00:09:18.930
And now, at each time step
to compute the hidden state.

00:09:18.930 --> 00:09:24.070
At that time step will essentially
just have these two matrices,

00:09:24.070 --> 00:09:28.470
these two linear layers,
matrix vector products and we sum them up.

00:09:28.470 --> 00:09:33.009
And that's essentially similar to
saying we concatenate h_t-1 and

00:09:33.009 --> 00:09:37.820
the word vector at time step t, and
we also concatenate these two matrices.

00:09:39.550 --> 00:09:41.610
And then we apply
an element-wise non-linearity.

00:09:41.610 --> 00:09:46.610
So this is essentially just a standard
single layer neural network.

00:09:48.910 --> 00:09:53.050
And then on top of that we can
use this as a feature vector, or

00:09:53.050 --> 00:09:57.810
as our input to our standard
softmax classification layer.

00:09:57.810 --> 00:10:02.030
To get an output probability for
instance over all the words.

00:10:03.320 --> 00:10:07.450
So now the way we would write
this out in this formulation is

00:10:07.450 --> 00:10:12.780
basically the probability that
the next word is of this specific,

00:10:12.780 --> 00:10:16.900
at this specific index j conditioned
on all the previous words

00:10:16.900 --> 00:10:21.266
is essentially the j_th element
of this large output vector.

00:10:21.266 --> 00:10:21.766
Yes?

00:10:25.153 --> 00:10:25.800
What is s?

00:10:25.800 --> 00:10:32.110
So here you can have different
ways to define your matrices.

00:10:32.110 --> 00:10:35.030
Some people just use u, v,
and w or something like that.

00:10:35.030 --> 00:10:39.990
But here we basically use the superscript
just identify which matrix we have.

00:10:39.990 --> 00:10:44.960
And these are all different matrices, so
W_(hh), the reason we call it hh is it's

00:10:44.960 --> 00:10:51.290
the W that computes the hidden
layer h given the input h t- 1.

00:10:51.290 --> 00:10:56.470
And then you have an h_x here,
which essentially maps x

00:10:56.470 --> 00:11:01.180
into the same vector space that we have.

00:11:01.180 --> 00:11:05.110
Our hidden states in and
then s is just our softmax w.

00:11:05.110 --> 00:11:07.410
The weights of the softmax classifier.

00:11:09.200 --> 00:11:12.620
And so let's look at the dimensions here.

00:11:12.620 --> 00:11:13.790
It's again very important.

00:11:13.790 --> 00:11:14.458
You have another question?

00:11:18.610 --> 00:11:21.620
So why do we concatenate and
not add is the question.

00:11:22.840 --> 00:11:24.340
So they're the same.

00:11:24.340 --> 00:11:29.551
So when you write W_(h) using

00:11:29.551 --> 00:11:35.534
same notation plus W_(hx) times x

00:11:35.534 --> 00:11:42.120
then this is actually the same thing.

00:11:42.120 --> 00:11:48.060
And so this will now basically be
a vector, and we are feed in linearity but

00:11:48.060 --> 00:11:53.230
it doesn't really change things, so
let's just look at this inside part here.

00:11:53.230 --> 00:11:58.860
Now if we concatenated h and
x together we're now have,

00:11:58.860 --> 00:12:05.240
and let's say, x here has a certain
dimensionality which we'll call d.

00:12:06.370 --> 00:12:11.735
So x is in R_d and
our h will define to be in for

00:12:11.735 --> 00:12:16.670
having the dimensionality R_(Dh).

00:12:18.750 --> 00:12:24.660
Now, what would the dimensionality be
if we concatenated these two matrices?

00:12:24.660 --> 00:12:30.760
So we have here the output has to be,
again a Dh matrix.

00:12:30.760 --> 00:12:32.977
And now this vector here is a,

00:12:32.977 --> 00:12:38.408
what dimensionality does this factor
have when we concatenate the two?

00:12:41.244 --> 00:12:42.250
That's right.

00:12:42.250 --> 00:12:48.120
So this is a d plus Dh times one and

00:12:48.120 --> 00:12:52.610
here we have Dh times our matrix.

00:12:52.610 --> 00:12:56.393
It has to be the same dimensionality, so

00:12:56.393 --> 00:13:00.624
d plus Dh and
that's why we could essentially

00:13:00.624 --> 00:13:06.270
concatenate here W_h in this way,
and W_hx here.

00:13:06.270 --> 00:13:08.610
And now we could basically multiply these.

00:13:08.610 --> 00:13:12.640
And if you, again if this is confusing,
you can write out all the indices.

00:13:12.640 --> 00:13:14.846
And you realize that these
two are exactly the same.

00:13:18.731 --> 00:13:19.231
Does that make sense?

00:13:23.429 --> 00:13:28.092
Right, so as you sum up all the values
here, It'll essentially just get

00:13:28.092 --> 00:13:32.320
summed up also, it doesn't matter
if you do it in one go or not.

00:13:32.320 --> 00:13:35.935
Just a single layer and that worked
where you compact in two inputs but

00:13:35.935 --> 00:13:39.750
it's in many cases for recurrent
neutral networks is written this way.

00:13:51.083 --> 00:13:52.180
All right.

00:13:52.180 --> 00:13:56.420
So now, here are two other ways
you'll often see these visualized.

00:13:56.420 --> 00:14:04.220
This is kind of a not unrolled version of
a hidden, of a recurrent neural network.

00:14:04.220 --> 00:14:08.690
And sometimes you'll also see
sort of this self loop here.

00:14:08.690 --> 00:14:14.530
I actually find these kinds of
unrolled versions the most intuitive.

00:14:18.038 --> 00:14:18.760
All right.

00:14:18.760 --> 00:14:21.380
Now when you start and you.

00:14:21.380 --> 00:14:21.880
Yup?

00:14:25.979 --> 00:14:26.700
Good question.

00:14:26.700 --> 00:14:29.420
So what is x[t]?

00:14:29.420 --> 00:14:33.134
It's essentially the word vector for

00:14:33.134 --> 00:14:37.310
the word that appears
at the t_th time step.

00:14:40.184 --> 00:14:47.410
As opposed to x_t and intuitively here
x_t you could define it in any way.

00:14:47.410 --> 00:14:51.410
It's really just like as you go through
the lectures you'll actually observe

00:14:51.410 --> 00:14:57.670
different versions but intuitively
here x_t is just a vector at xt but

00:14:57.670 --> 00:15:02.700
here xt is already an input, and
what it means in practice is you

00:15:02.700 --> 00:15:07.710
actually have to now go at that t time
step, find the word identity and pull

00:15:07.710 --> 00:15:13.242
that word vector from your glove or word
to vec vectors, and get that in there.

00:15:13.242 --> 00:15:19.270
So x_t we used in previous
lectures as the t_th element for

00:15:19.270 --> 00:15:24.290
instance in the whole embedding matrix,
all our word vectors.

00:15:24.290 --> 00:15:27.790
So this is just to make it very explicit
that we look up the identity of the word

00:15:27.790 --> 00:15:30.853
at the tth time step and then get
the word vector for that identity,

00:15:30.853 --> 00:15:33.112
like the vector in all our word vectors.

00:15:38.620 --> 00:15:39.132
Yep.

00:15:49.826 --> 00:15:53.606
So I'm showing here a single layer
neural network at each time step, and

00:15:53.606 --> 00:15:57.640
then the question is whether that
is standard or just for simplicity?

00:15:57.640 --> 00:16:02.260
It is actually the simplest and
still somewhat useful.

00:16:02.260 --> 00:16:06.300
Variant of a recurrent neural network,
though we'll see a lot of extensions even

00:16:06.300 --> 00:16:11.040
in this class, and then in the lecture
next week we'll go to even better versions

00:16:11.040 --> 00:16:12.650
of these kinds of
recurrent neural networks.

00:16:12.650 --> 00:16:15.285
But this is actually a somewhat
practical neural network,

00:16:15.285 --> 00:16:16.979
though we can improve it in many ways.

00:16:21.074 --> 00:16:25.313
Now, you might be curious when
you just start your sequence, and

00:16:25.313 --> 00:16:29.300
this is age 0 here and
there isn't any previous words.

00:16:29.300 --> 00:16:33.332
What you would do and the simplest thing
is you just initialize the vector for

00:16:33.332 --> 00:16:37.561
the first hidden layer at the first or the
0 time step as just a vector of all 0s.

00:16:42.176 --> 00:16:46.732
Right and this is the X[t] definition
you had just describe through the column

00:16:46.732 --> 00:16:51.164
vector of L which is our embedding matrix
at index [t] which the time step t.

00:16:51.164 --> 00:16:57.220
All right so it's very important to keep
track properly of all our dimensionality.

00:16:57.220 --> 00:17:03.170
Here, W(S) to Softmax actually goes
over the size of our vocabulary,

00:17:03.170 --> 00:17:05.170
V times the hidden state.

00:17:06.430 --> 00:17:10.350
So the output here is the same
as the vector of the length

00:17:10.350 --> 00:17:12.620
of the number of words that we
might wanna to be able to predict.

00:17:19.140 --> 00:17:20.483
All right, any questions for

00:17:20.483 --> 00:17:23.234
the feed forward definition of
a recurrent neural network?

00:17:28.685 --> 00:17:31.050
All right, so how do we train this?

00:17:31.050 --> 00:17:31.930
Well fortunately,

00:17:31.930 --> 00:17:37.510
we can use all the same machinery we've
already introduced and carefully derived.

00:17:37.510 --> 00:17:40.900
So basically here we have probability
distribution over the vocabulary and

00:17:40.900 --> 00:17:46.080
we're going to use the same exact cross
entropy loss function that we had before,

00:17:46.080 --> 00:17:50.770
but now the classes are essentially
just the next word.

00:17:50.770 --> 00:17:53.710
So this actually sometimes
creates a little confusion on

00:17:53.710 --> 00:17:56.800
the nomenclature that we have
'cause now technically this is

00:17:56.800 --> 00:18:00.360
unsupervised in the sense that
you just give it raw text.

00:18:00.360 --> 00:18:05.370
But this is the same kind of objective
function we use when we have supervised

00:18:05.370 --> 00:18:09.450
training where we have a specific
class that we're trying to predict.

00:18:09.450 --> 00:18:13.683
So the class at each time step is
just a word index of the next word.

00:18:17.280 --> 00:18:19.285
And you're already familiar with that,

00:18:19.285 --> 00:18:23.248
here we're just summing over the entire
vocabulary for each of the elements of Y.

00:18:26.902 --> 00:18:29.150
And now, in theory, you could just.

00:18:29.150 --> 00:18:33.927
To evaluate how well you can predict
the next word over many different words in

00:18:33.927 --> 00:18:38.925
longer sequences, you could in theory just
take this negative of the average log

00:18:38.925 --> 00:18:42.000
probability is over this entire dataset.

00:18:42.000 --> 00:18:47.900
But for maybe historical reasons, and also
reasons like information theory and so

00:18:47.900 --> 00:18:52.650
on that we don't need to get into, what's
more common is actually to use perplexity.

00:18:52.650 --> 00:18:56.250
So that's just 2 to
the power of this value and,

00:18:56.250 --> 00:18:59.790
hence, we want to basically
be less perplexed.

00:18:59.790 --> 00:19:03.510
So the lower our perplexity is,
the less the model is perplexed or

00:19:03.510 --> 00:19:06.030
confused about what the next word is.

00:19:06.030 --> 00:19:10.076
And we essentially, ideally we'll assign
a higher probability to the word that

00:19:10.076 --> 00:19:13.062
actually appears in the longer
sequence at each time step.

00:19:20.322 --> 00:19:20.946
Yes?

00:19:24.670 --> 00:19:26.990
Any reason why 2 to the J?

00:19:28.582 --> 00:19:32.632
Yes, but it's sort of a rat hole
we can go down, maybe after class.

00:19:32.632 --> 00:19:36.584
Information theory bits and
so on, it's not necessary.

00:19:36.584 --> 00:19:37.288
All right.

00:19:37.288 --> 00:19:41.910
&gt;&gt; [LAUGH]
&gt;&gt; All right, so

00:19:41.910 --> 00:19:46.880
now you would think, well this is pretty
simple, we have a single set of W

00:19:46.880 --> 00:19:53.300
matrices, and training should
be relatively straightforward.

00:19:53.300 --> 00:19:56.927
Sadly, and this is really the main
drawback of this and a reason of why we

00:19:56.927 --> 00:20:00.799
introduce all these other more powerful
recurrent neural network models,

00:20:00.799 --> 00:20:03.961
training these kinds of models
is actually incredibly hard.

00:20:06.402 --> 00:20:07.988
And we can now analyze,

00:20:07.988 --> 00:20:12.443
using the tools of back propagation and
chain rule and all of that.

00:20:12.443 --> 00:20:16.570
Now we can analyze and
understand why that is.

00:20:16.570 --> 00:20:20.840
So basically we're multiplying here,
the same matrix at each time step, right?

00:20:20.840 --> 00:20:24.610
So you can kind of think of
this matrix multiplication

00:20:24.610 --> 00:20:29.430
as amplifying certain patterns over and
over again at every single time step.

00:20:29.430 --> 00:20:32.240
And so, in a perfect world,

00:20:32.240 --> 00:20:36.090
we would want the inputs from many time
steps ago to actually be able to still

00:20:36.090 --> 00:20:41.630
modify what we're trying to predict
at a later, much later, time step.

00:20:41.630 --> 00:20:44.840
And so, one thing I would like
to encourage you to do is

00:20:44.840 --> 00:20:49.130
to try to take the derivatives
with respect to these Ws,

00:20:49.130 --> 00:20:51.700
if you just had a two or
three word sequence.

00:20:51.700 --> 00:20:55.200
It's a great exercise,
great preparation for the midterm.

00:20:55.200 --> 00:20:58.790
And it'll give you some
interesting insights.

00:20:58.790 --> 00:21:04.000
Now, as we multiply the same matrix
at each time step during foreprop,

00:21:04.000 --> 00:21:09.030
we have to do the same thing during
back propagation We have, remember,

00:21:09.030 --> 00:21:14.590
our deltas, our air signals and sort of
the global elements of the gradients.

00:21:14.590 --> 00:21:18.720
They will essentially at each time step
flow through this network backwards.

00:21:18.720 --> 00:21:22.306
So when we take our cross-entropy
loss here, we take derivatives,

00:21:22.306 --> 00:21:24.559
we back propagate we compute our deltas.

00:21:24.559 --> 00:21:29.223
Now the first time step here that just
happened close to that output would make

00:21:29.223 --> 00:21:32.738
a very good update and
will probably also make a good update

00:21:32.738 --> 00:21:36.250
to the word vector here if
we wanted to update those.

00:21:36.250 --> 00:21:36.964
We'll talk about that later.

00:21:36.964 --> 00:21:41.977
But then as you go backwards in
time what actually will happen

00:21:41.977 --> 00:21:46.710
is your signal might get either too weak,
or too strong.

00:21:47.720 --> 00:21:52.090
And that is essentially called
the vanishing gradient problem.

00:21:52.090 --> 00:21:56.820
As you go backwards through time, and you
try to send the air signal at time step t,

00:21:56.820 --> 00:22:01.210
many time steps into the past, you'll
have the vanishing gradient problem.

00:22:01.210 --> 00:22:04.200
So, what does that mean and
how does it happen?

00:22:04.200 --> 00:22:09.280
Let's define here a simpler, but
similar recurrent neural network that

00:22:09.280 --> 00:22:13.830
will allow us to give you an intuition and
simplify the math downstream.

00:22:13.830 --> 00:22:18.740
So here we essentially just say, all
right, instead of our original definition

00:22:18.740 --> 00:22:22.170
where we had some kind of f
some kind of non-linearity,

00:22:22.170 --> 00:22:24.390
here we use the sigma function,
you could use other one.

00:22:24.390 --> 00:22:28.918
First introduce the rectified linear units
and so on instead of applying it here,

00:22:28.918 --> 00:22:34.190
we'll apply it in the definition
just right in here.

00:22:35.430 --> 00:22:37.440
So it's the same thing.

00:22:37.440 --> 00:22:40.360
And then let's assume, for now,
we don't have the softmax.

00:22:40.360 --> 00:22:44.460
We just have here, a standard,
a bunch of un-normalized scores.

00:22:44.460 --> 00:22:47.891
Which really doesn't matter for
the math, but it'll simplify the math.

00:22:47.891 --> 00:22:53.676
Now if you want to compute the total
error with respect to an entire sequence,

00:22:53.676 --> 00:22:57.503
with respect to your W then
you basically have to sum

00:22:57.503 --> 00:23:00.444
up all the errors at all the time steps.

00:23:00.444 --> 00:23:01.415
At each time step,

00:23:01.415 --> 00:23:05.126
we have an error of how incorrect we
were about predicting the next word.

00:23:07.810 --> 00:23:10.563
And that's basically the sum here and

00:23:10.563 --> 00:23:15.590
now we're going to look at the element
at the t timestamp of that sum.

00:23:15.590 --> 00:23:20.700
So let's just look at a single time step,
a single error at a single time step.

00:23:20.700 --> 00:23:25.480
And now even computing that will
require us to have a very large

00:23:25.480 --> 00:23:30.251
chain rule application,
because essentially this error at

00:23:30.251 --> 00:23:34.430
time step t will depend on all
the previous time steps too.

00:23:34.430 --> 00:23:41.734
So you have here the delta or
dE_t over dy_t,

00:23:41.734 --> 00:23:46.640
so the t, the hidden state.

00:23:46.640 --> 00:23:51.122
Sorry, the soft max output or
here these unnormalized square output Yt.

00:23:51.122 --> 00:23:55.082
But then you have to multiply that
with the partial derivative of yt with

00:23:55.082 --> 00:23:57.160
respect to the hidden state.

00:23:57.160 --> 00:24:01.990
So that's just That's just this guy
right here, or this guy for ht.

00:24:01.990 --> 00:24:07.280
But now, that one depends on,
of course, the previous one, right?

00:24:07.280 --> 00:24:10.140
This one here, but it also depends
on that one, and that one, and

00:24:10.140 --> 00:24:11.551
the one before that, and so on.

00:24:11.551 --> 00:24:15.245
And so that's why you have to sum over
all the time step from the first one,

00:24:15.245 --> 00:24:19.190
all the way to the current one, where
you're trying to predict the next word.

00:24:20.380 --> 00:24:24.930
And now, each of these was
also computed with a W, so

00:24:24.930 --> 00:24:27.990
you have to multiply partial of that,
as well.

00:24:30.720 --> 00:24:33.850
Now, let's dig into
this a little bit more.

00:24:33.850 --> 00:24:37.320
And you don't have to worry too
much if this is a little fast.

00:24:37.320 --> 00:24:39.916
You won't have to really
go through all of this, but

00:24:39.916 --> 00:24:43.053
it's very similar to a lot of
the math that we've done before.

00:24:43.053 --> 00:24:48.717
So you can kind of feel comfortable for
the most part going over it at this speed.

00:24:48.717 --> 00:24:52.424
So now, remember here,
our definition of h_t.

00:24:52.424 --> 00:24:56.909
We basically have all these partials
of all the h_t's with respect to

00:24:56.909 --> 00:25:01.880
the previous time steps,
the h's of the previous time steps.

00:25:01.880 --> 00:25:06.260
Now, to compute each of these,
we'll have to use the chain rule again.

00:25:06.260 --> 00:25:09.110
And now, what this means is essentially

00:25:09.110 --> 00:25:12.540
a partial derivative of a vector
with respect to another vector.

00:25:12.540 --> 00:25:16.200
Something that if we're clever with
our backprop definitions before,

00:25:16.200 --> 00:25:18.754
we never actually have to do in practice,
right?

00:25:18.754 --> 00:25:20.812
'cause this is a very large matrix, and

00:25:20.812 --> 00:25:25.812
we're combining the computation with the
flow graph, and our delta messages before

00:25:25.812 --> 00:25:30.338
such that we don't actually have to
compute explicitly, these Jacobians.

00:25:30.338 --> 00:25:33.155
But for the analysis of the math here,

00:25:33.155 --> 00:25:36.690
we'll basically look at
all the derivatives.

00:25:36.690 --> 00:25:41.470
So just because we haven't defined it,
what's the partial for each of

00:25:41.470 --> 00:25:46.650
these is essentially called the Jacobian,
where you have all the partial derivatives

00:25:46.650 --> 00:25:53.110
with respect to each element of the top
here ht with respect to the bottom.

00:25:54.210 --> 00:25:58.681
And so in general, if you have
a vector valued function output and

00:25:58.681 --> 00:26:03.722
a vector valued input, and you take
the partials here, you get this large

00:26:03.722 --> 00:26:08.389
matrix of all the partial derivatives
with respect to all outputs.

00:26:16.661 --> 00:26:17.618
Any questions?

00:26:20.509 --> 00:26:23.588
All right, so basically here,
a lot of chain rule.

00:26:23.588 --> 00:26:27.870
And now, we got this beast
which is essentially a matrix.

00:26:27.870 --> 00:26:30.450
And we multiply, for each partial here,

00:26:30.450 --> 00:26:32.640
we actually have to multiply all of these,
right?

00:26:32.640 --> 00:26:37.564
So this is a large product
of a lot of these Jacobians.

00:26:37.564 --> 00:26:41.320
Now, we can try to simplify this,
and just say, all right.

00:26:41.320 --> 00:26:43.154
Let's say, there is an upper bound.

00:26:43.154 --> 00:26:47.404
And we also,
the derivative of h with respect to h_j.

00:26:47.404 --> 00:26:54.978
Actually, with this simple definition of
each h actually can be computed this way.

00:26:54.978 --> 00:27:00.232
And now,
we can essentially upper bound the norm

00:27:00.232 --> 00:27:05.873
of this matrix with
the multiplication of basically

00:27:05.873 --> 00:27:10.889
these equation right here,
where we have W_t.

00:27:10.889 --> 00:27:14.402
And if you remember our
backprop equations,

00:27:14.402 --> 00:27:17.545
you'll see some common terms here, but

00:27:17.545 --> 00:27:22.823
we'll actually write this out as
not just an element wise product.

00:27:22.823 --> 00:27:26.911
But we can write the same thing as
a diagonal where we have instead of

00:27:26.911 --> 00:27:28.083
the element wise.

00:27:28.083 --> 00:27:31.756
Elements we basically just put them into
the diagonal of a larger matrix, and

00:27:31.756 --> 00:27:34.202
with zero path,
everything that is off diagonal.

00:27:34.202 --> 00:27:36.741
Now, we multiply these two norms here.

00:27:36.741 --> 00:27:41.846
And now, we just define beta, W and
beta h, as essentially the upper bounds.

00:27:41.846 --> 00:27:44.280
Some number, single scalar for

00:27:44.280 --> 00:27:48.529
each as like how large they
could maximally be, right?

00:27:48.529 --> 00:27:53.266
We have W, we could compute easily
any kind of norm for our W, right?

00:27:53.266 --> 00:27:57.813
It's just a matrix, computed matrix norm,
we get a single number out.

00:27:57.813 --> 00:28:02.180
And now, basically, when we write
this all, we put all this together,

00:28:02.180 --> 00:28:06.261
then we see that an upper bound for
this Jacobians is essentially for

00:28:06.261 --> 00:28:08.924
each one of these
elements as this product.

00:28:08.924 --> 00:28:14.735
And if we define each of the elements
here, in terms of their upper bounds beta,

00:28:14.735 --> 00:28:19.890
then we basically have this product
beta here taken to the t- k power.

00:28:19.890 --> 00:28:24.818
And so as the sequence gets longer and
longer, and t gets larger and

00:28:24.818 --> 00:28:30.362
larger, it really depends on the value
of beta to have this either blow up or

00:28:30.362 --> 00:28:32.744
get very, very small, right?

00:28:32.744 --> 00:28:35.947
If now the norms of this matrix,
for instance,

00:28:35.947 --> 00:28:40.124
that norm, and then you have
control over that norm, right?

00:28:40.124 --> 00:28:42.924
You initialize your wait matrix W with

00:28:42.924 --> 00:28:48.000
some small random values initially
before you start training.

00:28:48.000 --> 00:28:52.249
If you initialize this to a matrix that
has a norm that is larger than one,

00:28:52.249 --> 00:28:56.585
then at each back propagation step and
the longer the time sequence goes.

00:28:56.585 --> 00:29:00.735
You basically will get a gradient
that is going to explode,

00:29:00.735 --> 00:29:05.555
cuz you take some value that's larger
than one to a large power here.

00:29:05.555 --> 00:29:09.425
Say, you have 100 or something,
and your norm is just two,

00:29:09.425 --> 00:29:14.720
then you have two to the 100th as an upper
bound for that gradient and vice-versa.

00:29:14.720 --> 00:29:19.585
If you initialize your matrix W in
the beginning to a bunch of small

00:29:19.585 --> 00:29:24.894
random values such that the norm of
your W is actually smaller than one,

00:29:24.894 --> 00:29:30.379
then the final gradient that will be
sent from ht to hk could become a very,

00:29:30.379 --> 00:29:34.829
very small number, right,
half to the power of 100th.

00:29:34.829 --> 00:29:37.670
Basically, none of the errors will arrive.

00:29:37.670 --> 00:29:41.120
None of the error signal, we got small and
smaller as you go further and

00:29:41.120 --> 00:29:42.374
further backwards in time.

00:29:42.374 --> 00:29:43.128
Yeah.

00:29:56.202 --> 00:30:00.773
So if the gradient here is exploding, does
that mean a word that is further away has

00:30:00.773 --> 00:30:03.105
a bigger impact on a word that's closer?

00:30:03.105 --> 00:30:06.322
And the answer is when
it's exploding like that,

00:30:06.322 --> 00:30:08.792
you'll get to not a number in no time.

00:30:08.792 --> 00:30:13.832
And that doesn't even become a practical
issue because the numbers will

00:30:13.832 --> 00:30:18.803
literally become not a number,
cuz it's too large a value to compute.

00:30:18.803 --> 00:30:21.480
And we'll have to think
of ways to come back.

00:30:21.480 --> 00:30:25.819
It turns out the exploding gradient
problem has some really great hacks that

00:30:25.819 --> 00:30:29.705
make them easier to deal with than
the vanishing gradient problem.

00:30:29.705 --> 00:30:31.116
And we'll get to those in a second.

00:30:37.880 --> 00:30:41.677
All right, so now,
you might say this could be a problem.

00:30:41.677 --> 00:30:46.093
Now, why is the vanishing gradient
problem, an actual common practice?

00:30:46.093 --> 00:30:50.620
And again, it basically prevents
us from allowing a word that

00:30:50.620 --> 00:30:54.798
appears very much in the past
to have any influence on what

00:30:54.798 --> 00:30:58.382
we're trying to break in
terms of the next word.

00:31:00.360 --> 00:31:04.225
And so here a couple of examples from just
language modeling where that is a real

00:31:04.225 --> 00:31:04.760
problem.

00:31:05.850 --> 00:31:08.320
So let's say, for instance,
you have Jane walked into the room.

00:31:08.320 --> 00:31:09.490
John walked in too.

00:31:09.490 --> 00:31:10.990
It was late in the day.

00:31:10.990 --> 00:31:12.030
Jane said hi to.

00:31:13.130 --> 00:31:18.298
Now, you can put an almost
probability mass of one,

00:31:18.298 --> 00:31:23.127
that the next word in this blank is John,
right?

00:31:23.127 --> 00:31:25.974
But if now,
each of these words have the word vector,

00:31:25.974 --> 00:31:28.837
you type it in to the hidden state,
you compute this.

00:31:28.837 --> 00:31:32.289
And now, you want the model to pick up
the pattern that if somebody met somebody

00:31:32.289 --> 00:31:34.550
else, and your all this complex stuff.

00:31:34.550 --> 00:31:38.212
And then they said hi too, and
the next thing is the name.

00:31:38.212 --> 00:31:42.901
You wanna put a very high probability
on it, but you can't get your model to

00:31:42.901 --> 00:31:47.667
actually send that error signal way
back over here, to now modify the hidden

00:31:47.667 --> 00:31:51.850
state in a way that would allow you
to give John a high probability.

00:31:51.850 --> 00:31:56.834
And really, this is a large problem in
any kind of time sequence that you have.

00:31:56.834 --> 00:31:59.778
And many people might
intuitively say well,

00:31:59.778 --> 00:32:02.900
language is mostly a Sequence problem,
right?

00:32:02.900 --> 00:32:05.740
You have words that appear
from left to right or

00:32:05.740 --> 00:32:09.060
in some temporal order as we speak.

00:32:09.060 --> 00:32:10.900
And so this is a huge problem.

00:32:10.900 --> 00:32:13.910
And now we'll have a little bit
of code that we can look into.

00:32:13.910 --> 00:32:17.112
But before that we'll have
the awesome Shayne give

00:32:17.112 --> 00:32:20.392
us a little bit of an intercession,
intermission.

00:32:27.061 --> 00:32:30.767
&gt;&gt; Hi, so let's take a short break
from recurrent neural networks to

00:32:30.767 --> 00:32:33.746
talk about transition-based
dependency parsing,

00:32:33.746 --> 00:32:38.280
which is exactly what you guys saw
this time last week in lecture.

00:32:38.280 --> 00:32:43.224
So just as a recap, a transition-based
dependency parser is a method of

00:32:43.224 --> 00:32:47.460
taking a sentence and
turning it into dependence parse tree.

00:32:47.460 --> 00:32:51.484
And you do this by looking at
the state of the sentence and

00:32:51.484 --> 00:32:54.610
then predicting a transition.

00:32:54.610 --> 00:32:55.890
And you do this over and

00:32:55.890 --> 00:33:00.620
over again in a greedy fashion until
you have a full transition sequence

00:33:00.620 --> 00:33:05.050
which itself encodes, the dependency
parse tree for that sentence.

00:33:05.050 --> 00:33:09.771
So I wanna show you how to get from
the model that you'll be implementing in

00:33:09.771 --> 00:33:12.056
your assignment two question two,

00:33:12.056 --> 00:33:16.120
which you're hopefully working
on right now, to SyntaxNet.

00:33:16.120 --> 00:33:17.860
So what is SyntaxNet?

00:33:18.980 --> 00:33:24.040
SyntaxNet is a model that

00:33:24.040 --> 00:33:28.320
Google came out with and they claim
it's the world's most accurate parser.

00:33:29.700 --> 00:33:32.360
And it's new,
fast performant TensorFlow framework for

00:33:32.360 --> 00:33:35.240
syntactic parsing is available for
over 40 languages.

00:33:35.240 --> 00:33:38.535
The one in English is called
the Parse McParseface.

00:33:38.535 --> 00:33:43.210
&gt;&gt; [LAUGH]
&gt;&gt; So my slide seemed to have been jumbled

00:33:43.210 --> 00:33:47.210
a little bit here, but
hopefully you can read through it.

00:33:47.210 --> 00:33:51.620
So basically the baseline we're
gonna begin with is the Chen and

00:33:51.620 --> 00:33:53.400
Manning model which came out in 2014.

00:33:53.400 --> 00:33:59.089
And Chen and Manning are respectively
your head TA and instructor.

00:33:59.089 --> 00:34:04.145
And the models that produce SyntaxNet
in just two stages of improvements,

00:34:04.145 --> 00:34:07.370
those directly modified Chen and

00:34:07.370 --> 00:34:11.600
Manning's model, which is exactly what
you guys will be doing in assignment two.

00:34:11.600 --> 00:34:14.940
And so we're going to focus today
on the main bulk of these changes,

00:34:14.940 --> 00:34:19.640
modifications which were
introduced in 2015 by Weiss et al.

00:34:19.640 --> 00:34:24.730
So without further ado, I'm gonna look
at their three main contributions.

00:34:24.730 --> 00:34:28.195
So the first one is they leverage
unlabeled data using something called

00:34:28.195 --> 00:34:29.180
Tri-Training.

00:34:29.180 --> 00:34:32.280
The second is that they tuned
their neural network and

00:34:32.280 --> 00:34:34.550
made some slight modifications.

00:34:34.550 --> 00:34:38.860
And the last and probably most important
is that they added a final layer on top

00:34:38.860 --> 00:34:43.800
of the model involving a structured
perceptron with beam search.

00:34:43.800 --> 00:34:46.800
So each of these seeks to solve a problem.

00:34:46.800 --> 00:34:49.080
So the first one is tri-training.

00:34:49.080 --> 00:34:50.780
So as you know, in most supervised models,

00:34:50.780 --> 00:34:53.270
they perform better the more
data that they have.

00:34:53.270 --> 00:34:55.790
And this is especially the case for
dependency parsing,

00:34:55.790 --> 00:35:00.220
where as you can imagine there are an
infinite number of possible sentences with

00:35:00.220 --> 00:35:02.350
a ton of complexity and
you're never gonna see all of them,

00:35:02.350 --> 00:35:05.570
and you're gonna see even some
of them very, very rarely.

00:35:05.570 --> 00:35:07.680
So the more data you have, the better.

00:35:07.680 --> 00:35:10.670
So what they did is they took
a ton of unlabeled data and

00:35:10.670 --> 00:35:15.260
two highly performing dependency parsers
that were very different from each other.

00:35:15.260 --> 00:35:19.050
And when they agreed, independently
agreed on a dependency parse tree for

00:35:19.050 --> 00:35:22.950
a given sentence, then that would
be added to the labeled data set.

00:35:22.950 --> 00:35:25.680
And so now you have ten
million new tokens of data

00:35:27.140 --> 00:35:30.930
that you can use in addition
to what you already have.

00:35:30.930 --> 00:35:34.800
And this by itself improved
a highly performing network's

00:35:34.800 --> 00:35:38.940
performance by 1% using
the unlabeled attachment score.

00:35:38.940 --> 00:35:41.590
So the problem here was not having
enough data for the task and

00:35:41.590 --> 00:35:43.790
they improved it using this.

00:35:43.790 --> 00:35:48.582
The second augmentation they made
was by taking the existing model,

00:35:48.582 --> 00:35:50.080
which is the one you
guys are implementing,

00:35:50.080 --> 00:35:53.900
which has an input layer
consisting of the word vectors.

00:35:53.900 --> 00:35:58.590
The vectors for the part of speech tags
and the arc labels with one hidden layer

00:35:58.590 --> 00:36:04.620
and one soft max layer predicting which
transition and they changed it to this.

00:36:04.620 --> 00:36:09.830
Now this is actually pretty much the same
thing, except for three small changes.

00:36:11.190 --> 00:36:12.610
The first is that they added,

00:36:12.610 --> 00:36:15.100
there are two hidden layers
instead of one hidden layer.

00:36:15.100 --> 00:36:18.450
The second is that they used
a RELU nonlinearity function

00:36:18.450 --> 00:36:20.210
instead of the cube nonlinearity function.

00:36:20.210 --> 00:36:23.700
And the third and most important is
that they added a perceptron layer

00:36:23.700 --> 00:36:25.990
on top of the soft max layer.

00:36:25.990 --> 00:36:28.790
And notice that the arrows,
that it takes in

00:36:31.200 --> 00:36:37.080
as input the outputs from all
the previous layers in the network.

00:36:37.080 --> 00:36:41.420
So this perceptron layer wants
to solve one particular problem,

00:36:41.420 --> 00:36:45.308
and this problem is that greedy algorithms
aren't able to really look ahead.

00:36:45.308 --> 00:36:47.150
They make short term decisions and

00:36:47.150 --> 00:36:51.100
as a result they can't really
recover from one incorrect decision.

00:36:51.100 --> 00:36:56.050
So what they said is, let's allow
the network then to look ahead and

00:36:56.050 --> 00:36:59.850
so we're going to have a tree
which we can search over and

00:36:59.850 --> 00:37:04.610
this tree is the tree of all the possible
partial transition sequences.

00:37:04.610 --> 00:37:08.825
So each edge is a possible transition
form the state that you're at.

00:37:08.825 --> 00:37:09.655
As you can imagine,

00:37:09.655 --> 00:37:13.285
even with three transitions your tree
is gonna blossom very, very quickly and

00:37:13.285 --> 00:37:16.805
you can't look that far ahead and
explore all of the possible branches.

00:37:16.805 --> 00:37:19.815
So what you have to do
is prune some branches.

00:37:19.815 --> 00:37:21.415
And for that they use beam search.

00:37:21.415 --> 00:37:25.220
Now beam search is only
gonna keep track of the top

00:37:25.220 --> 00:37:28.180
K partial transition
sequences up to a depth of M.

00:37:28.180 --> 00:37:29.670
Now how do you decide which K?

00:37:29.670 --> 00:37:34.790
You're going to use a score computed
using the perceptron weights.

00:37:34.790 --> 00:37:38.870
You guys probably have a decent idea
at this point of how perceptron works.

00:37:38.870 --> 00:37:43.000
The exact function they used
is shown here, and I'm gonna

00:37:43.000 --> 00:37:46.320
leave up the annotations so you can take
a look at it later if you're interested.

00:37:47.730 --> 00:37:52.570
But basically those are the three
things that they did solve,

00:37:52.570 --> 00:37:55.610
the problems with the previous
Chen &amp; Manning model.

00:37:55.610 --> 00:38:00.920
So in summary, Chen &amp; Manning had
an unlabeled attachment score of 92%,

00:38:00.920 --> 00:38:03.230
already phenomenal performance.

00:38:03.230 --> 00:38:05.680
And with those three changes,
they boosted it to 94%,

00:38:05.680 --> 00:38:10.390
and then there's only 0.6%
left to get you to SyntaxNet,

00:38:10.390 --> 00:38:15.340
which is Google's 2016
state of the art model.

00:38:15.340 --> 00:38:19.310
And if you're curious what the did to get
that 0.6%, take a look at Andrew All's

00:38:19.310 --> 00:38:24.780
paper Which uses global normalization
instead of local normalization.

00:38:24.780 --> 00:38:28.130
So the main takeaway, and
it's pretty straight forward but

00:38:28.130 --> 00:38:32.810
I can't stress it enough, is when you're
trying to improve upon an existing model,

00:38:32.810 --> 00:38:35.900
you need to identify the specific
flaws that are in this model.

00:38:35.900 --> 00:38:41.080
In this case the greedy algorithm and
solved those problems specifically.

00:38:41.080 --> 00:38:45.970
In this case they did that
using semi-supervised method

00:38:45.970 --> 00:38:47.700
using unlabeled data.

00:38:47.700 --> 00:38:48.880
They tune the model better and

00:38:48.880 --> 00:38:51.650
they use the structured
perception with beam search.

00:38:52.660 --> 00:38:53.735
Thank you very much.

00:38:53.735 --> 00:39:00.335
&gt;&gt; [APPLAUSE]
&gt;&gt; Kind of awesome.

00:39:00.335 --> 00:39:03.489
You can now look at these
kinds of pictures and

00:39:03.489 --> 00:39:05.983
you totally know what's going on.

00:39:05.983 --> 00:39:10.923
And in like state of the art stuff
that the largest companies in

00:39:10.923 --> 00:39:12.728
the world publishes.

00:39:12.728 --> 00:39:13.718
Exciting times.

00:39:13.718 --> 00:39:15.780
All right, so

00:39:15.780 --> 00:39:21.210
we'll gonna through a little bit of
like a practical Python notebook sort

00:39:21.210 --> 00:39:25.840
of implementation that shows you a simple
version of the vanishing gradient problem.

00:39:25.840 --> 00:39:29.120
Where we don't even have a full recurrent
real network we just have a simple two

00:39:29.120 --> 00:39:32.580
layer neural network and even in
those kinds of networks you will see

00:39:32.580 --> 00:39:35.560
that the error that you start at
the top and the norm of the gradients

00:39:35.560 --> 00:39:40.040
as you go down through your network,
the norm is already getting smaller.

00:39:40.040 --> 00:39:43.620
And if you remember these were the two
equations where I said if you get

00:39:43.620 --> 00:39:47.330
to the end of those two equations you know
all the things that you need to know, and

00:39:47.330 --> 00:39:51.890
you'll actually see these three
equations in the code as well.

00:39:51.890 --> 00:39:55.657
So let's jump into this.

00:39:55.657 --> 00:39:56.809
I don't see it.

00:39:56.809 --> 00:39:59.120
Let me get out of the presentation

00:40:04.040 --> 00:40:07.722
All right, better, all right.

00:40:07.722 --> 00:40:10.842
Now, zoom in.

00:40:10.842 --> 00:40:15.753
So here, we're going to define
a super simple problem.

00:40:15.753 --> 00:40:19.856
This is a code that we started,
and 231N (with Andrej), and

00:40:19.856 --> 00:40:23.800
we just modified it to
make it even simpler.

00:40:23.800 --> 00:40:27.730
So let's say our data set,
to keep it also very simple,

00:40:27.730 --> 00:40:30.180
is just this kind of
classification data set.

00:40:30.180 --> 00:40:34.530
Where we have basically three classes,
the blue, yellow, and red.

00:40:34.530 --> 00:40:39.250
And they're basically in
the spiral clusterform.

00:40:39.250 --> 00:40:42.302
We're going to define our
simple nonlinearities.

00:40:42.302 --> 00:40:47.330
You can kind of see it as a solution
almost to parts of the problem set,

00:40:47.330 --> 00:40:49.810
which is why we're only showing it now.

00:40:49.810 --> 00:40:53.020
And we'll put this on the website too,
so no worries.

00:40:53.020 --> 00:40:54.840
You can visit later.

00:40:54.840 --> 00:40:58.945
But basically, you could define here f,
our different nonlinearities,

00:40:58.945 --> 00:41:04.300
element-wise, and the gradients for them.

00:41:04.300 --> 00:41:07.970
So this is f and
f prime if f is a sigmoid function.

00:41:07.970 --> 00:41:12.530
We'll also look at the relu, the other
nonlinearity that's very popular.

00:41:12.530 --> 00:41:18.580
And here, we just have the maximum between
0 and x, and very simple function.

00:41:18.580 --> 00:41:22.496
Now, this is a relatively
straight forward definition and

00:41:22.496 --> 00:41:26.500
implementation of this simple
three layer neural network.

00:41:26.500 --> 00:41:31.097
Has this input, here our nonlinearity,
our data x, just these points in two

00:41:31.097 --> 00:41:35.144
dimensional space, the class,
it's one of those three classes.

00:41:35.144 --> 00:41:39.855
We'll have this model here,
we have our step size for

00:41:39.855 --> 00:41:43.220
SDG, and our regularization value.

00:41:43.220 --> 00:41:47.313
Now, these are all our parameters,
w1, w2 and w3 for

00:41:47.313 --> 00:41:51.242
all the outputs, and
variables of the hidden states.

00:42:00.223 --> 00:42:01.768
Two sets is bigger, all right.

00:42:01.768 --> 00:42:08.164
&gt;&gt; [LAUGH]
&gt;&gt; All right, now, if our nonlinearity

00:42:08.164 --> 00:42:12.853
is the relu, then we have here relu,
and we just input x, multiply it.

00:42:12.853 --> 00:42:15.366
And in this case,
your x can be the entirety of the dataset,

00:42:15.366 --> 00:42:19.150
cuz the dataset's so small, each
mini-batch, we can essentially do a batch.

00:42:19.150 --> 00:42:23.391
Again, if you have realistic datasets,
you wouldn't wanna do full batch training,

00:42:23.391 --> 00:42:24.972
but we can get away with it here.

00:42:24.972 --> 00:42:26.640
It's a very tiny dataset.

00:42:26.640 --> 00:42:31.670
We multiply w1 times x
plus our bias terms, and

00:42:31.670 --> 00:42:35.770
then we have our element-wise
rectified linear units or relu.

00:42:35.770 --> 00:42:38.505
Then we've computed in layer two,
same idea.

00:42:38.505 --> 00:42:41.250
But now, it's input instead of
x is the previous hidden layer.

00:42:42.280 --> 00:42:47.020
And then we compute our scores this way.

00:42:47.020 --> 00:42:50.881
And then here, we'll normalize
our scores with the softmax.

00:42:50.881 --> 00:42:54.988
Just exponentiate our scores,
some of them.

00:42:54.988 --> 00:42:57.957
So very similar to the equations
that we walk through.

00:42:57.957 --> 00:43:00.540
And now,
it's just basically an if statement.

00:43:00.540 --> 00:43:05.490
Either we have used relu
as our activations, or

00:43:05.490 --> 00:43:10.704
we use a sigmoid, but
the math inside is the same.

00:43:10.704 --> 00:43:13.306
All right, now,
we're going to compute our loss.

00:43:13.306 --> 00:43:19.350
Our good friend, the simple average cross
entropy loss plus the regularization.

00:43:19.350 --> 00:43:22.760
So here,
we have negative log of the probabilities,

00:43:22.760 --> 00:43:25.140
we summed them up overall the elements.

00:43:25.140 --> 00:43:29.988
And then here, we have our regularization
as the L2, standard L2 regularization.

00:43:29.988 --> 00:43:35.654
And we just basically sum up the squares
of all the elements in all our parameters,

00:43:35.654 --> 00:43:38.379
and I guess it does cut off a little bit.

00:43:38.379 --> 00:43:40.860
Let me zoom in.

00:43:40.860 --> 00:43:44.350
All three have the same of
amount of regularization, and

00:43:44.350 --> 00:43:47.150
we add that to our final loss.

00:43:47.150 --> 00:43:50.230
And now, every 1,000 iterations,
we'll just print our loss and

00:43:50.230 --> 00:43:51.316
see what's happening.

00:43:51.316 --> 00:43:53.465
And this is something you
always want to do too.

00:43:53.465 --> 00:43:55.700
You always wanna visualize,
see what's going on.

00:43:55.700 --> 00:43:58.824
And hopefully,
a lot of this now looks very familiar.

00:43:58.824 --> 00:44:03.053
Maybe if implemented it not quite as
efficiently, as efficiently in problem set

00:44:03.053 --> 00:44:06.862
one, but maybe you have, and
then it's very, very straightforward.

00:44:06.862 --> 00:44:10.112
Now, that was the forward propagation,
we can compute our error.

00:44:10.112 --> 00:44:13.012
Now, we're going to go backwards, and

00:44:13.012 --> 00:44:17.379
we're computing our delta
messages first from the scores.

00:44:17.379 --> 00:44:19.529
Then we have here, back propagation.

00:44:19.529 --> 00:44:24.419
And now,
we have the hidden layer activations,

00:44:24.419 --> 00:44:29.760
transposed times delta
messages to compute w.

00:44:29.760 --> 00:44:35.596
Again, remember, we have always for
each w here, we have this outer product.

00:44:35.596 --> 00:44:38.032
And that's the outer
product we see right here.

00:44:38.032 --> 00:44:44.106
And now, the softmax was the same
regardless of whether we used a value or

00:44:44.106 --> 00:44:45.120
a sigmoid.

00:44:45.120 --> 00:44:46.810
Let's walk through the sigmoid here.

00:44:46.810 --> 00:44:52.390
We now, basically, have our delta scores,
and have here the product.

00:44:52.390 --> 00:44:56.565
So this is exactly computing delta for
the next layer.

00:44:56.565 --> 00:45:01.050
And that's exactly this equation here,
and just Python code.

00:45:01.050 --> 00:45:03.880
And then again,
we'll have our updates dw, which is,

00:45:03.880 --> 00:45:06.460
again, this outer product right there.

00:45:06.460 --> 00:45:10.325
So it's a very nice
sort of equations code,

00:45:10.325 --> 00:45:14.615
almost a nice one to one
mapping between the two.

00:45:14.615 --> 00:45:16.040
All right, now,

00:45:16.040 --> 00:45:22.030
we're going to go through the network
from the top down to the first layer.

00:45:23.510 --> 00:45:24.670
Again, here, our outer product.

00:45:25.670 --> 00:45:29.318
And now, we add the derivatives for
our regularization.

00:45:29.318 --> 00:45:31.513
In this case, it's very simple,

00:45:31.513 --> 00:45:35.004
just matrices themselves
times the regularization.

00:45:35.004 --> 00:45:40.969
And we combine all our gradients
in this data structure.

00:45:40.969 --> 00:45:45.237
And then we update all our parameters
with our step_size and SGD.

00:45:52.745 --> 00:45:57.774
All right, then we can evaluate how
well we do on the training set, so

00:45:57.774 --> 00:46:03.660
that we can basically print out
the training accuracy as we train us.

00:46:03.660 --> 00:46:08.310
All right, now, we're going to
initialize all the dimensionality.

00:46:08.310 --> 00:46:12.963
So we have there just our two
dimensional inputs, three classes.

00:46:12.963 --> 00:46:16.450
We compute our hidden sizes
of the hidden vectors.

00:46:16.450 --> 00:46:19.289
Let's say, they're 50, it's pretty large.

00:46:19.289 --> 00:46:22.170
And now, we can run this.

00:46:22.170 --> 00:46:25.501
All right, we'll train it with both
sigmoids and rectify linear units.

00:46:25.501 --> 00:46:30.820
And now,
once we wanna analyze what's going on,

00:46:30.820 --> 00:46:38.290
we can essentially now plot some of
the magnitudes of the gradients.

00:46:38.290 --> 00:46:43.368
So those are essentially the updates as we
do back propagation through the snap work.

00:46:43.368 --> 00:46:49.150
And what we'll see here is
the some of the gradients for

00:46:49.150 --> 00:46:56.580
the first and the second layer when
we use sigmoid non-linearities.

00:46:56.580 --> 00:47:01.759
And basically here, the main takeaway
messages that blue is the first layer,

00:47:01.759 --> 00:47:03.810
and green is the second layer.

00:47:03.810 --> 00:47:06.251
So the second layer is
closer to the softmax,

00:47:06.251 --> 00:47:08.510
closer to what we're trying to predict.

00:47:08.510 --> 00:47:12.480
And hence, it's gradient is
usually had larger in magnitude

00:47:12.480 --> 00:47:15.390
than the one that arrives
at the first layer.

00:47:16.590 --> 00:47:19.975
And now, imagine you do this 100 times.

00:47:19.975 --> 00:47:23.612
And you have intuitively your vanishing
gradient problem in recurrent neural

00:47:23.612 --> 00:47:24.390
networks.

00:47:24.390 --> 00:47:25.917
They'll essentially be zero.

00:47:25.917 --> 00:47:29.476
They're already almost half in size

00:47:29.476 --> 00:47:34.308
over the iterations when
you just had two layers.

00:47:34.308 --> 00:47:38.870
And the problem is a little less strong
when you use rectified linear units.

00:47:38.870 --> 00:47:46.875
But even there, you're going to have
some decrease as you continue to train.

00:47:46.875 --> 00:47:52.265
All right,
any questions around this code snippet and

00:47:52.265 --> 00:47:55.439
vanishing creating problems?

00:48:00.875 --> 00:48:02.413
No, sure.

00:48:02.413 --> 00:48:11.760
[LAUGH] That's a good question.

00:48:11.760 --> 00:48:14.060
The question is why
are the gradings flatlining.

00:48:14.060 --> 00:48:15.770
And it's essentially
because the dataset is so

00:48:15.770 --> 00:48:19.700
simple that you actually just
perfectly fitted your training data.

00:48:19.700 --> 00:48:24.373
And then there's not much else to do
you're basically in a local optimum and

00:48:24.373 --> 00:48:26.427
then not much else is happening.

00:48:26.427 --> 00:48:31.555
So yeah, so these are the outputs where
if you visualize the decision boundaries,

00:48:31.555 --> 00:48:35.803
here at the relue and the relue you
see a little bit more sort of edges,

00:48:35.803 --> 00:48:39.907
because you have sort of linear
parts of your decision boundary and

00:48:39.907 --> 00:48:43.160
the sigmoid is a little smoother,
little rounder.

00:48:48.547 --> 00:48:52.978
All right, so now you can implement a very
quick versions to get an intuition for

00:48:52.978 --> 00:48:55.650
the vanishing gradient problem.

00:48:55.650 --> 00:49:00.380
Now the exploding gradient problem is,
in theory, just as bad.

00:49:00.380 --> 00:49:05.250
But in practice,
it turns out we can actually have a hack,

00:49:05.250 --> 00:49:09.740
that was first introduced by
Thomas Mikolov, and it's very

00:49:09.740 --> 00:49:13.660
unmathematical in some ways 'cause say,
all you have is a large gradient of 100.

00:49:13.660 --> 00:49:15.110
Let's just cap it to five.

00:49:17.030 --> 00:49:18.740
That's it,
you just define the threshold and

00:49:18.740 --> 00:49:23.190
you say whenever the value is larger
than a certain value, just cut it.

00:49:23.190 --> 00:49:27.210
Totally not the right
mathematical direction anymore.

00:49:27.210 --> 00:49:29.330
But turns out to work very
well in practice, yep.

00:49:31.270 --> 00:49:34.470
So vanishing creating problems,
how would you cap it?

00:49:34.470 --> 00:49:39.350
It's like it gets smaller and
smaller, and you just multiply it?

00:49:39.350 --> 00:49:41.360
But then it's like, it might overshoot.

00:49:41.360 --> 00:49:43.690
It might go in the completely
wrong direction.

00:49:43.690 --> 00:49:49.280
And you don't want to have the hundredth
word unless it really matters.

00:49:49.280 --> 00:49:53.040
You can't just make all
the hundred words or

00:49:53.040 --> 00:49:56.160
thousand words of the past
all matter the same amount.

00:49:56.160 --> 00:49:57.660
Right?
Intuitively.

00:49:57.660 --> 00:49:59.830
That doesn't make that much sense either.

00:49:59.830 --> 00:50:06.030
So this gradient clipping solution
is actually really powerful.

00:50:06.030 --> 00:50:11.567
And then a couple years after it
was introduced, Yoshua Bengio and

00:50:11.567 --> 00:50:16.360
one of his students Actually gained
a little bit of intuition and

00:50:16.360 --> 00:50:18.230
it's something I encourage
you always to do too.

00:50:18.230 --> 00:50:21.690
Not just in the equations, where you
can write out recurrent neural network,

00:50:21.690 --> 00:50:24.880
where everything's one dimensional,
and the math comes out easy and

00:50:24.880 --> 00:50:26.270
you gain intuition about it.

00:50:26.270 --> 00:50:30.340
But you can also, and this is what
they did here, implement a very simple

00:50:30.340 --> 00:50:33.950
recurrent neural network which
just had a single hidden unit.

00:50:33.950 --> 00:50:37.840
Not very useful for anything in practice
but now, with the single unit W.

00:50:37.840 --> 00:50:40.628
And you know, at still the bias term,

00:50:40.628 --> 00:50:45.610
they can actually visualize exactly
what the air surface looks like.

00:50:45.610 --> 00:50:50.428
So and oftentimes we call the air
surface or the energy landscape or so

00:50:50.428 --> 00:50:52.750
that the landscape of
our objective function.

00:50:53.910 --> 00:50:56.830
This error surface and basically.

00:50:56.830 --> 00:51:02.000
You can see here the size of
the z axis here is the error

00:51:02.000 --> 00:51:04.390
that you have when you trained
us on a very simple problem.

00:51:05.990 --> 00:51:07.320
I forgot what the problem here was but

00:51:07.320 --> 00:51:10.760
it's something very simple
like keep around this unit and

00:51:10.760 --> 00:51:14.460
remember the value and then just
return that value 50 times later.

00:51:14.460 --> 00:51:16.170
Something simple like that.

00:51:16.170 --> 00:51:20.550
And what they essentially observe
is that in this air surface or

00:51:20.550 --> 00:51:23.300
air landscape you have
these high curvature walls.

00:51:23.300 --> 00:51:25.800
And so as you do an update each

00:51:25.800 --> 00:51:29.970
little line here you can interpret as
what happens at an sg update step.

00:51:29.970 --> 00:51:31.650
You update your parameters.

00:51:31.650 --> 00:51:35.512
And you say, in order to minimize
my objective function right now,

00:51:35.512 --> 00:51:38.560
I'm going to change the value
of my one hidden unit and

00:51:38.560 --> 00:51:42.610
my bias term just like by this amount
to go over here, go over here.

00:51:42.610 --> 00:51:44.910
And all of a sudden you hit
these large curvature walls.

00:51:44.910 --> 00:51:50.750
And then your gradient basically blows up,
and it moves you somewhere way different.

00:51:50.750 --> 00:51:53.270
And so intuitively what happens here is,

00:51:53.270 --> 00:51:59.040
if you rescale to the thick size with
the special method, then essentially

00:51:59.040 --> 00:52:03.320
you're not going to jump to some crazy,
faraway place, but you're just going to

00:52:03.320 --> 00:52:07.310
stay in this general area that seemed
useful before you hit that curvature wall.

00:52:08.818 --> 00:52:09.318
Yeah?

00:52:30.568 --> 00:52:33.876
So the question is, intuitively,
why wouldn't such a trick work for

00:52:33.876 --> 00:52:37.817
the vanishing grading problem but it does
work for the exploding grading problem.

00:52:42.828 --> 00:52:44.063
Why does the reason for

00:52:44.063 --> 00:52:47.591
the vanishing does not apply to
the exploding grading problem.

00:52:47.591 --> 00:52:51.790
So intuitively,
this is exactly the issue here.

00:52:51.790 --> 00:52:55.630
So the exploding,
as you move way too far away,

00:52:55.630 --> 00:53:00.250
you basically jump out of the area
where you, in this case here for

00:53:00.250 --> 00:53:03.460
instance, we're getting closer and
closer to a local optimum, but

00:53:03.460 --> 00:53:06.590
the local optimum was very
close to high curvature wall.

00:53:06.590 --> 00:53:09.790
And without the gradient problem,
without the clipping trick,

00:53:09.790 --> 00:53:11.280
you would go way far away.

00:53:12.300 --> 00:53:16.910
Right, now, on the vanishing grading
problem, it get's smaller and smaller.

00:53:16.910 --> 00:53:19.150
So in general clipping doesn't make sense,
but

00:53:19.150 --> 00:53:21.650
let's say, so that's the obvious answer.

00:53:21.650 --> 00:53:25.490
You can't, something gets smaller and
smaller, it doesn't help to have a maximum

00:53:25.490 --> 00:53:28.940
and then make it, you know cut it to that
maximum 'cause that's not the problem.

00:53:28.940 --> 00:53:30.800
It goes in the opposite direction.

00:53:30.800 --> 00:53:32.010
And so.

00:53:32.010 --> 00:53:34.130
That's kind of most
obvious intuitive answers.

00:53:34.130 --> 00:53:35.420
Now, you could say.

00:53:35.420 --> 00:53:39.930
Why couldn't you, if it gets below
a certain threshold, blow it up?

00:53:39.930 --> 00:53:41.490
But then that would mean that.

00:53:41.490 --> 00:53:43.490
Let's say you had.

00:53:43.490 --> 00:53:44.950
You wanted to predict the word.

00:53:44.950 --> 00:53:46.650
And now you're 50 time steps away.

00:53:46.650 --> 00:53:50.670
And really,
the 51st doesn't actually impact

00:53:50.670 --> 00:53:52.650
the word you're trying to
predict at time step T, right?

00:53:52.650 --> 00:53:55.850
So you're 50 times to 54 and
it doesn't really modify that word.

00:53:55.850 --> 00:54:00.400
And now you're artificially going to
blow up and make it more important.

00:54:00.400 --> 00:54:02.433
So that's less intuitive than saying,

00:54:02.433 --> 00:54:06.333
I don't wanna jump into some completely
different part of my error surface.

00:54:11.890 --> 00:54:15.150
The wall just comes from this is what
the error surface looks like for

00:54:15.150 --> 00:54:18.984
a very very simple recurrent node network
with a very simple kind of problem that

00:54:18.984 --> 00:54:20.555
it tries to solve.

00:54:20.555 --> 00:54:24.555
And you can actually use most
of the networks that you have,

00:54:24.555 --> 00:54:27.245
you can try to make them
have just two parameters and

00:54:27.245 --> 00:54:29.245
then you can visualize
something like this too.

00:54:29.245 --> 00:54:31.530
In fact it's very intuitive
sometimes to do that.

00:54:31.530 --> 00:54:36.490
When you try different optimizers,
we'll get to those in a later lecture

00:54:36.490 --> 00:54:39.790
like Adam or SGD or achieve momentum,
we'll talk about those soon.

00:54:39.790 --> 00:54:44.280
You can actually always try to visualise
that in some simple kind of landscape.

00:54:44.280 --> 00:54:48.927
This just happens to be the landscape that
this particular recurrent neural network

00:54:48.927 --> 00:54:51.776
problem has with one-hidden unit and
just a bias term.

00:55:06.876 --> 00:55:09.310
So the question is, how could we know for

00:55:09.310 --> 00:55:13.620
sure that this happens with non-linear
actions and multiple weight.

00:55:13.620 --> 00:55:17.160
So you also have some
non-linearity here in this.

00:55:17.160 --> 00:55:22.660
So that intuitively wouldn't prevent
us from transferring that knowledge.

00:55:22.660 --> 00:55:24.090
Now, in general, it's very hard.

00:55:24.090 --> 00:55:28.150
We can't really visualize
a very high dimensional spaces.

00:55:28.150 --> 00:55:35.030
There is actually now an interesting
new idea that was introduced,

00:55:35.030 --> 00:55:39.900
I think by Ian Goodfellow
where you can actually try to,

00:55:39.900 --> 00:55:44.190
let's say you have your parameter space,
inside your parameter space,

00:55:44.190 --> 00:55:45.520
you have some kind of cross function.

00:55:45.520 --> 00:55:51.120
So you say my w matrices are at this value
and so on, and I have some error when

00:55:51.120 --> 00:55:54.900
all my values are here, and then I start
to optimize and I end up somewhere here.

00:55:54.900 --> 00:55:57.400
Now the problem is, we can't
visualize it because it's usually in

00:55:57.400 --> 00:56:00.740
realistic settings,
you have the 100 million.

00:56:00.740 --> 00:56:01.380
Workflow.

00:56:01.380 --> 00:56:05.010
At least a million or so
parameters, sometimes 100 million.

00:56:05.010 --> 00:56:09.590
And so, something crazy might be going
on as you optimize between this.

00:56:09.590 --> 00:56:12.050
And so, because we can't visualize it and

00:56:12.050 --> 00:56:15.230
we can't even sub-sample it because
it's such a high-dimensional space.

00:56:15.230 --> 00:56:18.890
What they do is they actually
draw a line between the point

00:56:18.890 --> 00:56:23.050
from where they started with their random
initialization before optimization.

00:56:23.050 --> 00:56:27.920
And end the line all the way to the point

00:56:27.920 --> 00:56:30.620
where you actually
finished the optimization.

00:56:30.620 --> 00:56:35.110
And then you can evaluate along
this line at a certain intervals,

00:56:35.110 --> 00:56:39.960
you can evaluate how big your area is.

00:56:39.960 --> 00:56:43.845
And if that area changes between
two such intervals a lot,

00:56:43.845 --> 00:56:47.560
then that means we have very
high curvature in that area.

00:56:47.560 --> 00:56:52.600
So that's one trick of how
you might use this idea and

00:56:52.600 --> 00:56:55.470
gain some intuition of
the curvature of the space.

00:56:55.470 --> 00:56:58.940
But yeah, only in two dimensions can we
get such nice intuitive visualizations.

00:57:00.070 --> 00:57:00.575
Yeah.

00:57:05.775 --> 00:57:09.260
So the question is why don't
we just have less dependence?

00:57:09.260 --> 00:57:13.570
And the question of course,
it's a legit question, but

00:57:13.570 --> 00:57:15.740
ideally we'll let
the model figure this out.

00:57:15.740 --> 00:57:17.990
Ideally we're better at
optimizing the model, and

00:57:17.990 --> 00:57:20.650
the model has in theory these
long range dependencies.

00:57:20.650 --> 00:57:22.400
In practice, they rarely ever do.

00:57:23.400 --> 00:57:26.817
In fact when you implement these, and
you can start playing around with this and

00:57:26.817 --> 00:57:28.867
this is something I
encourage you all to do too.

00:57:28.867 --> 00:57:33.138
As you implement your models you can try
to make it a little bit more interactive.

00:57:33.138 --> 00:57:35.899
Have some IPython Notebook,
give it a sequence and

00:57:35.899 --> 00:57:38.153
look at the probability of the next word.

00:57:38.153 --> 00:57:41.752
And then give it a different sequence
where you change words like ten time

00:57:41.752 --> 00:57:44.320
steps away, and
look again at the probabilities.

00:57:44.320 --> 00:57:48.714
And what you'll often observe is that
after seven words or so, the words before

00:57:48.714 --> 00:57:53.583
actually don't matter, especially not for
these simple recurrent neural networks.

00:57:53.583 --> 00:57:55.529
But because this is a big problem,

00:57:55.529 --> 00:57:58.910
there are actually a lot of
different kinds of solutions.

00:57:58.910 --> 00:58:03.980
And so the biggest and
best one is one we'll introduce next week.

00:58:03.980 --> 00:58:07.540
But a simpler one is to use
rectified linear units and

00:58:07.540 --> 00:58:12.070
to also initialize both of your w's
to ones from hidden to hidden and

00:58:12.070 --> 00:58:16.620
the ones from the input to the hidden
state with the identity matrix.

00:58:16.620 --> 00:58:19.808
And this is a trick that I
introduced a couple years ago and

00:58:19.808 --> 00:58:23.077
then it was sort of combined
with rectified linear units.

00:58:23.077 --> 00:58:27.480
And applied to recurrent
neural networks by Quoc Le.

00:58:27.480 --> 00:58:32.455
And so the main idea here is if
you move around in your space.

00:58:32.455 --> 00:58:35.352
Let's say you have your h, and

00:58:35.352 --> 00:58:40.378
usually we have here our whh times h,
plus whx plus x.

00:58:40.378 --> 00:58:46.378
And let's assume for now that h and
x have the same dimensionality.

00:58:46.378 --> 00:58:52.168
So then all these
are essentially square matrices.

00:58:52.168 --> 00:58:54.360
And we have here our different vectors.

00:58:56.050 --> 00:59:01.260
Now, in the standard initialization,
what you would do is you'd

00:59:01.260 --> 00:59:06.150
just have a bunch of small random values
and all the different elements of w.

00:59:06.150 --> 00:59:09.840
And what that means is
as you start optimizing,

00:59:09.840 --> 00:59:12.890
whatever x is you have some random
projection into the hidden space.

00:59:14.110 --> 00:59:18.180
Instead, the idea here is we actually
have identity initialization.

00:59:18.180 --> 00:59:23.107
Maybe you can scale it, so instead
you have a half times the identity,

00:59:23.107 --> 00:59:24.704
and what does that do?

00:59:24.704 --> 00:59:28.656
Intuitively when you combine
the hidden state and the word vector?

00:59:37.381 --> 00:59:38.633
That's exactly right.

00:59:38.633 --> 00:59:41.160
If this is an identity initialized matrix.

00:59:41.160 --> 00:59:43.983
So it's just, 1, 1, 1,
1, 1, 1 on the diagonal.

00:59:43.983 --> 00:59:46.510
And you multiply all of these by one half.

00:59:46.510 --> 00:59:49.880
Same as just having a half,
a half, a half, and so on.

00:59:49.880 --> 00:59:52.336
And you multiply this with this vector and
you do the same thing here.

00:59:52.336 --> 00:59:56.132
What essentially that means is that
you have a half, times that vector,

00:59:56.132 --> 00:59:58.010
plus half times that other vector.

00:59:59.580 --> 01:00:02.692
And intuitively that means in
the beginning, if you don't know anything.

01:00:02.692 --> 01:00:05.842
Let's not do a crazy random projection
into the middle of nowhere in our

01:00:05.842 --> 01:00:07.535
parameter space, but just average.

01:00:07.535 --> 01:00:12.078
And say, well as I move through the space
my hidden state is just a moving

01:00:12.078 --> 01:00:13.987
average of the word vectors.

01:00:13.987 --> 01:00:15.550
And then I start making some updates.

01:00:16.600 --> 01:00:19.890
And it turns out when you look here and

01:00:19.890 --> 01:00:22.545
you apply this to the very
tight problem of MNIST.

01:00:22.545 --> 01:00:25.061
Which we don't really have to go into,
but its a bunch of small digits.

01:00:25.061 --> 01:00:27.832
And they're trying to basically predict

01:00:27.832 --> 01:00:32.330
what digit it is by going over
all the pixels in a sequence.

01:00:32.330 --> 01:00:33.140
Instead of using

01:00:33.140 --> 01:00:35.970
other kinds of neural networks like
convolutional neural networks.

01:00:35.970 --> 01:00:38.650
And basically we look
at the test accuracy.

01:00:38.650 --> 01:00:41.060
These are very long time sequences.

01:00:41.060 --> 01:00:44.310
And the test accuracy for
these is much, much higher.

01:00:44.310 --> 01:00:49.750
When you use this identity initialization
instead of random initialization,

01:00:49.750 --> 01:00:52.600
and also using rectified linear units.

01:00:52.600 --> 01:00:57.760
Now more importantly for
real language modeling,

01:00:57.760 --> 01:01:01.370
we can compare recurrent neural
networks in this simple form.

01:01:01.370 --> 01:01:05.370
So we had the question before like,
do these actually matter or

01:01:05.370 --> 01:01:11.340
did I just kind of describe single
layer recurrent neural networks for

01:01:11.340 --> 01:01:13.670
the class to describe the concept.

01:01:13.670 --> 01:01:17.880
And here we actually have these
simple recurrent neural networks, and

01:01:17.880 --> 01:01:19.410
we basically compare.

01:01:19.410 --> 01:01:24.483
This one is called Kneser-Ney with 5
grams, so a lot of counts, and some clever

01:01:24.483 --> 01:01:29.573
back off and smoothing techniques which
we won't need to get into for the class.

01:01:29.573 --> 01:01:33.377
And we compare these on
two different corpora and

01:01:33.377 --> 01:01:36.460
we basically look at the perplexity.

01:01:36.460 --> 01:01:40.881
So these are all perplexity numbers,
and we look at the neural network or

01:01:40.881 --> 01:01:44.362
the neural network that's
combined with Kneser-Ney,

01:01:44.362 --> 01:01:46.549
assuming probability estimates.

01:01:46.549 --> 01:01:49.720
And of course when you combine the two
then you don't really get the advantage of

01:01:49.720 --> 01:01:50.660
having less RAM.

01:01:50.660 --> 01:01:53.901
So ideally this by itself would do best,
but

01:01:53.901 --> 01:01:58.028
in general combining the two
used to still work better.

01:01:58.028 --> 01:02:02.310
These are results from five years ago,
and they failed most very quickly.

01:02:02.310 --> 01:02:07.870
I think the best results now are pure
neural network language models.

01:02:07.870 --> 01:02:12.690
But basically we can see
that compared to Kneser-Ney,

01:02:12.690 --> 01:02:17.910
even back then, the neural
network actually works very well.

01:02:17.910 --> 01:02:25.145
And has much lower perplexity than just
the Kneser-Ney or just account based.

01:02:28.379 --> 01:02:32.438
Now one problem that you'll
observe in a lot of cases,

01:02:32.438 --> 01:02:36.640
is that the softmax is really,
really large.

01:02:36.640 --> 01:02:40.630
So your word vectors are one
set of parameters, but

01:02:40.630 --> 01:02:43.090
your softmax is another set of parameters.

01:02:43.090 --> 01:02:45.785
And if your hidden state is 1000, and

01:02:45.785 --> 01:02:49.335
let's say you have
100,000 different words.

01:02:49.335 --> 01:02:54.086
Then that's 100,000 times 1000 dimensional
matrix that you'd have to multiply with

01:02:54.086 --> 01:02:57.030
the hidden state at
every single time step.

01:02:57.030 --> 01:02:59.460
So that's not very efficient, and so

01:02:59.460 --> 01:03:05.690
one way to improve this is with
a class-based word prediction.

01:03:05.690 --> 01:03:09.422
Where we first try to predict some
class that we can come up, and

01:03:09.422 --> 01:03:11.700
there are different kinds
of things we can do.

01:03:11.700 --> 01:03:16.050
In many cases you can sort,
just the words by how frequent they are.

01:03:16.050 --> 01:03:18.305
And say the thousand most frequent
words are in the first class,

01:03:18.305 --> 01:03:20.725
the next thousand most frequent
words in the second class and so on.

01:03:20.725 --> 01:03:28.347
And so you first basically classify, try
to predict the class based on the history.

01:03:28.347 --> 01:03:32.099
And then you predict the word inside
that class, based on that class.

01:03:32.099 --> 01:03:34.986
And so this one is only
a thousand dimensional, and so

01:03:34.986 --> 01:03:36.471
you can basically do this.

01:03:36.471 --> 01:03:39.541
And now the more classes
the better the perplexity, but

01:03:39.541 --> 01:03:43.270
also the slower the speed
the less you gain from this.

01:03:43.270 --> 01:03:46.990
And especially at training time
which is what we see here,

01:03:48.170 --> 01:03:49.480
this makes a huge difference.

01:03:49.480 --> 01:03:54.730
So if you have just very few classes,
you can actually reduce

01:03:54.730 --> 01:03:59.010
the number here of seconds
that each eproc takes.

01:03:59.010 --> 01:04:02.780
By almost 10x compared to
having more classes or

01:04:03.780 --> 01:04:06.610
even more than 10x if you
have the full softmax.

01:04:09.050 --> 01:04:13.791
And even the test time, is faster cuz now
you only essentially evaluate the word

01:04:13.791 --> 01:04:18.059
probabilities for the classes that
have a very high probability here.

01:04:21.988 --> 01:04:27.266
All right, one last trick and
this is maybe obvious to some but

01:04:27.266 --> 01:04:34.090
it wasn't obvious to others even in
the past when people published on this.

01:04:34.090 --> 01:04:37.290
But you essentially only need
to do a single backward's pass

01:04:37.290 --> 01:04:38.580
through the sequence.

01:04:38.580 --> 01:04:45.200
Once you accumulate all the deltas
from each error at each time set.

01:04:45.200 --> 01:04:50.340
So looking at this figure,
really quick again.

01:04:51.540 --> 01:04:54.550
Here, essentially you have
one forward pass where you

01:04:54.550 --> 01:04:58.850
compute all the hidden states and
all your errors, and

01:04:58.850 --> 01:05:01.870
then you only have a single
backwards pass, and as you go

01:05:01.870 --> 01:05:06.370
backwards in time you keep accumulating
all the deltas of each time step.

01:05:06.370 --> 01:05:11.010
And so originally people said, for this
time step I'm gonna go all the way back,

01:05:11.010 --> 01:05:14.010
and then I go to the next time step,
and then I go all the way back, and

01:05:14.010 --> 01:05:16.910
then the next step, and all the way back,
which is really inefficient.

01:05:16.910 --> 01:05:19.680
And is essentially same as combining

01:05:20.750 --> 01:05:24.190
all the deltas in one clean
back propagation step.

01:05:24.190 --> 01:05:27.240
And again, it's kind of is intuitive.

01:05:27.240 --> 01:05:29.337
An intuitive sort of
implementation trick but

01:05:29.337 --> 01:05:31.980
people gave that the term back
propagation through time.

01:05:38.553 --> 01:05:43.009
All right, now that we have these
simple recurrent neural networks,

01:05:43.009 --> 01:05:45.890
we can use them for
a lot of fun applications.

01:05:45.890 --> 01:05:48.930
In fact, the name entity recognition
that we're gonna use in example with

01:05:48.930 --> 01:05:50.150
the Window.

01:05:50.150 --> 01:05:56.220
In the Window model, you could only
condition the probability of this being

01:05:56.220 --> 01:06:00.840
a location, a person, or an organization
based on the words in that Window.

01:06:00.840 --> 01:06:03.800
The recurrent neural network
you can in theory take and

01:06:03.800 --> 01:06:08.700
condition these probabilities
on a lot larger context sizes.

01:06:08.700 --> 01:06:11.060
And so
you can do Named Entity Recognition (NER),

01:06:11.060 --> 01:06:15.030
you can do entity level sentiment in
context, so for instance you can say.

01:06:15.030 --> 01:06:18.480
I liked the acting, but
the plot was a little thin.

01:06:18.480 --> 01:06:22.570
And you can say I want to now for
acting say positive, and

01:06:22.570 --> 01:06:25.000
predict the positive class for that word.

01:06:25.000 --> 01:06:29.774
Predict the null class, and
all sentiment for all the other words,

01:06:29.774 --> 01:06:33.146
and then plot should get
negative class label.

01:06:33.146 --> 01:06:39.157
Or you can classify opinionated
expressions, and this is what researchers

01:06:39.157 --> 01:06:44.654
at Cornell where they
essentially used RNNs for

01:06:44.654 --> 01:06:49.694
opinion mining and essentially wanted
to classify whether each word in

01:06:49.694 --> 01:06:55.406
a relatively smaller purpose here is
either the direct subjective expression or

01:06:55.406 --> 01:07:00.703
the expressive subjective expression,
so either direct or expressive.

01:07:00.703 --> 01:07:05.630
So basically this is direct
subjective expressions,

01:07:05.630 --> 01:07:11.770
explicitly mention some private state or
speech event, whereas the ESEs just

01:07:11.770 --> 01:07:16.950
indicate the sentiment or emotion without
explicitly stating or conveying them.

01:07:16.950 --> 01:07:17.764
So here's an example,

01:07:17.764 --> 01:07:22.330
like the committee as usual has
refused to make any statements.

01:07:22.330 --> 01:07:25.920
And so you want to classify
as usual as an ESE, and

01:07:25.920 --> 01:07:29.580
basically give each of these
words here a certain label.

01:07:29.580 --> 01:07:33.970
And this is something you'll actually
observe a lot in sequence tagging paths.

01:07:33.970 --> 01:07:37.280
Again, all the same models
the recurrent neural network.

01:07:37.280 --> 01:07:39.350
You have the soft max at every time step.

01:07:39.350 --> 01:07:43.330
But now the soft max actually
has a set of classes

01:07:43.330 --> 01:07:48.160
that indicate whether a certain
expression begins or ends.

01:07:48.160 --> 01:07:52.990
And so here you would basically
have this BIO notation

01:07:52.990 --> 01:07:57.820
scheme where you have the beginning or
the end, or a null token.

01:07:57.820 --> 01:08:00.570
It's not any of the expressions
that I care about.

01:08:00.570 --> 01:08:05.030
So here you would say for instance,
as usual is an overall ESE expression, so

01:08:05.030 --> 01:08:07.800
it begins here, and
it's in the middle right here.

01:08:09.500 --> 01:08:12.340
And then these are neither ESEs or DSEs.

01:08:12.340 --> 01:08:18.120
All right, now they started with
the standard recurrent neural network, and

01:08:18.120 --> 01:08:23.530
I want you to at some point be able
to glance over these equations,

01:08:23.530 --> 01:08:26.150
and just say I've seen this before.

01:08:26.150 --> 01:08:29.240
It doesn't have to be W superscript HH,
and so on.

01:08:29.240 --> 01:08:34.510
But whenever you see, the summation
order of course, doesn't matter either.

01:08:35.530 --> 01:08:39.720
But here, they use W, V, and
U, but then they defined,

01:08:39.720 --> 01:08:43.310
instead of writing out softmax,
they write g here.

01:08:43.310 --> 01:08:46.580
But once you look at these equations,
I hope that eventually you're just like

01:08:46.580 --> 01:08:48.070
it's just a recurrent neural network,
right?

01:08:48.070 --> 01:08:51.450
You have here,
are your hidden to hidden matrix.

01:08:51.450 --> 01:08:57.490
You have your input to hidden matrix, and
here you have your softmax waits you.

01:08:57.490 --> 01:09:02.380
So same idea, but these are the actual
equations from this real paper that you

01:09:02.380 --> 01:09:06.990
can now kind of read and immediately sort
of have the intuition of what happens.

01:09:08.302 --> 01:09:13.570
All right, you need directional
recurrent neural network where we,

01:09:13.570 --> 01:09:17.670
if we try to make the prediction here,
of whether this is an ESE or

01:09:17.670 --> 01:09:21.820
whatever name entity recognition,
any kind of sequence labelling task,

01:09:21.820 --> 01:09:25.190
what's the problem with
this kind of model?

01:09:25.190 --> 01:09:28.440
What do you think as we go
from left to right only?

01:09:28.440 --> 01:09:31.846
What do you think could be a problem for
making the most accurate predictions?

01:09:40.505 --> 01:09:41.920
That's right.

01:09:41.920 --> 01:09:45.080
Words that come after
the current word can't be

01:09:45.080 --> 01:09:48.410
helping us to make accurate
predictions at that time step, right?

01:09:48.410 --> 01:09:50.750
Cuz we only went from left to right.

01:09:50.750 --> 01:09:55.120
And so one of the most common
extensions of recurrent neural networks

01:09:55.120 --> 01:09:58.460
is actually to do bidirectional
recurrent neural networks

01:09:58.460 --> 01:10:03.210
where instead of just going from left to
right, we also go from right to left.

01:10:03.210 --> 01:10:05.040
And it's essentially the exact same model.

01:10:05.040 --> 01:10:08.420
In fact, you could implement it by
changing your input and just reversing all

01:10:08.420 --> 01:10:11.070
the words of your input, and
then it's exactly the same thing.

01:10:12.100 --> 01:10:15.380
And now, here's the reason why they
don't have superscripts with WHH,

01:10:15.380 --> 01:10:19.100
cuz now they have these
arrows that indicate

01:10:19.100 --> 01:10:21.770
whether you're going from left to right,
or from right to left.

01:10:23.440 --> 01:10:27.690
And now, they basically have
this concatenation here, and

01:10:27.690 --> 01:10:33.740
in order to make a prediction at a certain
time step t they essentially concatenate

01:10:33.740 --> 01:10:37.860
the hidden states from both the left
direction and the right direction.

01:10:37.860 --> 01:10:39.940
And those are now the feature vectors.

01:10:39.940 --> 01:10:44.760
And this vector ht coming from the left,
has all the context ordinal,

01:10:44.760 --> 01:10:48.010
again seven plus words,
depending on how well you train your RNN.

01:10:49.080 --> 01:10:50.704
From all the words on the left,

01:10:50.704 --> 01:10:54.327
ht from the right has all the contacts
from the words on the right, and

01:10:54.327 --> 01:10:58.785
that is now your feature vector to make an
accurate prediction at a certain time set.

01:11:00.612 --> 01:11:03.850
Any questions around bidirectional
recurrent neural networks?

01:11:03.850 --> 01:11:06.930
You'll see these a lot in all
the recent papers you'll be learning,

01:11:08.200 --> 01:11:09.200
in various modifications.

01:11:09.200 --> 01:11:09.700
Yeah.

01:11:14.855 --> 01:11:17.670
Have people tried
Convolutional Neural Networks?

01:11:17.670 --> 01:11:20.677
They have, and we have a special lecture
also we will talk a little bit about

01:11:20.677 --> 01:11:22.076
Convolutional Neural Networks.

01:11:31.457 --> 01:11:33.410
So you don't necessarily have a cycle,
right?

01:11:33.410 --> 01:11:37.890
You just go, basically as you implement
this, you go once all the way for

01:11:37.890 --> 01:11:38.610
your the left, and

01:11:38.610 --> 01:11:41.720
you don't have any interactions with
the step that goes from the right.

01:11:41.720 --> 01:11:46.030
You can compute your
feet forward HTs here for

01:11:46.030 --> 01:11:49.140
that direction,
are only coming from the left.

01:11:49.140 --> 01:11:51.330
And the HT from the other direction,
you can compete,

01:11:51.330 --> 01:11:55.050
in fact you could paralyze this if
you want to be super efficient and.

01:11:55.050 --> 01:11:57.160
Have one core,
implement the left direction, and

01:11:57.160 --> 01:11:58.730
one core implement the right direction.

01:11:58.730 --> 01:12:02.530
So in that sense it doesn't make
the vanishing create any problem worse.

01:12:04.120 --> 01:12:07.433
But, of course,
just like any recurring neural network,

01:12:07.433 --> 01:12:10.217
it does have the vanishing
creating problem, and

01:12:10.217 --> 01:12:14.725
the exploding creating problems and it has
to be clever about flipping it and so,

01:12:14.725 --> 01:12:19.674
yeah We call them standard

01:12:19.674 --> 01:12:25.250
feedforward neural networks or
Window based feedforward neural networks.

01:12:25.250 --> 01:12:27.290
And now we have recurrent neural networks.

01:12:27.290 --> 01:12:30.400
And this is really one of
the most powerful family and

01:12:30.400 --> 01:12:31.620
we'll see lots of extensions.

01:12:31.620 --> 01:12:34.880
In fact, if there's no other
question we can go even deeper.

01:12:34.880 --> 01:12:37.420
It is after all deep learning.

01:12:37.420 --> 01:12:42.500
And so, now you'll observe [LAUGH] we
definitely had to skip that superscript.

01:12:42.500 --> 01:12:47.815
And we have different, Characters here for

01:12:47.815 --> 01:12:52.495
each of our matrices, because,
instead of just going from left to right,

01:12:52.495 --> 01:12:56.080
you can also have a deep neural
network at each time step.

01:12:57.230 --> 01:13:02.680
And so now, to compute the ith
layer at a given time step, you

01:13:02.680 --> 01:13:07.450
essentially again, have only the things
coming from the left that modify it but,

01:13:07.450 --> 01:13:13.430
you just don't take in the vector from the
left, you also take the vector from below.

01:13:13.430 --> 01:13:20.590
So, in the simplest definition that is
just your x, your input vector right?

01:13:20.590 --> 01:13:26.203
But as you go deeper you now also have
the previous hidden layers input.

01:13:36.409 --> 01:13:38.203
Instead of why are the,

01:13:50.648 --> 01:13:51.927
So the question is,

01:13:51.927 --> 01:13:57.010
why do we feed the hidden layer into
another hidden layer instead of the y?

01:13:57.010 --> 01:14:00.280
In fact, you can actually have so
called short circuit connections,

01:14:00.280 --> 01:14:05.040
too, where each of these h's can
go directly to the y as well.

01:14:05.040 --> 01:14:09.890
And so here in this figure you see
that only the top ones go into the y.

01:14:09.890 --> 01:14:14.670
But you can actually have short circuit
connections where y here has as input

01:14:14.670 --> 01:14:19.310
not just ht from the top layer,
noted here as capital L, but

01:14:19.310 --> 01:14:21.520
the concatenation of all the h's.

01:14:21.520 --> 01:14:25.298
It's just another way to make
this monster even more monstrous.

01:14:28.029 --> 01:14:32.424
And in fact there a lot of modifications,
in fact, Shayne has a paper,

01:14:32.424 --> 01:14:36.970
an ArXiv right now on a search based
odyssey type thing where you have so

01:14:36.970 --> 01:14:41.979
many different kinds of knobs that you can
tune for even more sophisticated recurrent

01:14:41.979 --> 01:14:46.879
neural networks of the type that we'll
introduce next week that, it gets a little

01:14:46.879 --> 01:14:51.421
unwieldy and it turns out a lot of
the things don't matter that much, but

01:14:51.421 --> 01:14:55.260
each can kind of give you a little
bit of a boost in many cases.

01:14:55.260 --> 01:14:56.880
So if you have three layers,
you have four layers,

01:14:56.880 --> 01:14:59.070
what's the dimensionality
of all the layers and

01:14:59.070 --> 01:15:02.310
the various different kinds of connections
and short circuit connections.

01:15:02.310 --> 01:15:07.900
We'll introduce some of these, but
in general this like a pretty decent model

01:15:07.900 --> 01:15:12.300
and will eventually extract away from
how we compute that hidden state, and

01:15:12.300 --> 01:15:17.850
that will be a more complex kind of cell
type that we'll introduce next Tuesday.

01:15:19.490 --> 01:15:23.600
Do we have one more question?

01:15:23.600 --> 01:15:25.690
So now how do we evaluate this?

01:15:25.690 --> 01:15:30.720
It's very important to evaluate
your problems correctly,

01:15:30.720 --> 01:15:34.510
and we actually talked about this before.

01:15:34.510 --> 01:15:39.830
When you have a very imbalanced data set,
where some of the classes appear

01:15:39.830 --> 01:15:42.870
very frequently and others are not very
frequent, you don't wanna use accuracy.

01:15:42.870 --> 01:15:47.950
In fact, in these kinds of sentences,
you often observe, this is an extreme one

01:15:47.950 --> 01:15:53.610
where you have a lot of ESEs and
DSEs but in many cases, just content.

01:15:53.610 --> 01:15:59.210
Standard sort of non-sentiment context and

01:15:59.210 --> 01:16:03.590
words, and so a lot of these
are actually O, have no label.

01:16:03.590 --> 01:16:06.670
And so it's very important to use F1 and

01:16:06.670 --> 01:16:10.720
we basically had this question also after
class, but it's important for all of you

01:16:10.720 --> 01:16:15.450
to know because the F1 metric is really
one of the most commonly used metrics.

01:16:15.450 --> 01:16:18.510
And it's essentially just the harmonic
mean of precision and recall.

01:16:18.510 --> 01:16:21.390
Precision is just the true
positives divided by

01:16:21.390 --> 01:16:24.790
true positives plus false positives and

01:16:24.790 --> 01:16:28.600
recall is just true positives divided
by true positives plus false negatives.

01:16:28.600 --> 01:16:31.890
And then you have here the harmonic
mean of these two numbers.

01:16:33.120 --> 01:16:38.190
So intuitively, you can be very
accurate by always saying something or

01:16:38.190 --> 01:16:41.160
have a very high recall for
a certain class but

01:16:41.160 --> 01:16:45.040
if you always miss another class
That would hurt you a lot.

01:16:45.040 --> 01:16:50.020
And now here's an evaluation
that you should also be familiar

01:16:50.020 --> 01:16:55.110
with where basically this is something
I would like to see in a lot of your

01:16:55.110 --> 01:16:59.560
project reports too as you analyze the
various hyper parameters that you have.

01:16:59.560 --> 01:17:03.990
And so one thing they found here is they
have two different data set sizes that

01:17:03.990 --> 01:17:12.580
they train on,
in many cases if you train with more data,

01:17:12.580 --> 01:17:17.410
you basically do better but then also it's
not always the case that more layers.

01:17:17.410 --> 01:17:22.650
So this is the depth that we had here, the
number l for all these different layers.

01:17:22.650 --> 01:17:25.430
It's not always the case
that more layers are better.

01:17:25.430 --> 01:17:29.500
In fact here, the highest performance
they get is with three layers,

01:17:29.500 --> 01:17:30.280
instead of four or five.

01:17:31.770 --> 01:17:32.940
All right, so let's recap.

01:17:32.940 --> 01:17:34.340
Recurring neural networks,

01:17:34.340 --> 01:17:39.400
best deep learning model family that
you'll learn about in this class.

01:17:39.400 --> 01:17:41.420
Training them can be very hard.

01:17:41.420 --> 01:17:44.240
Fortunately, you understand
back propagation now.

01:17:44.240 --> 01:17:48.200
You can gain an intuition of
why that might be the case.

01:17:48.200 --> 01:17:52.920
We'll in the next lecture extend
them some much more powerful models

01:17:52.920 --> 01:17:56.980
the Gated Recurring Units or LSTMs,
and those are the models you'll see

01:17:56.980 --> 01:18:00.590
all over the place in all the state
of the art models these days.

01:18:00.590 --> 01:18:01.210
All right.

01:18:01.210 --> 01:18:01.710
Thank you.

