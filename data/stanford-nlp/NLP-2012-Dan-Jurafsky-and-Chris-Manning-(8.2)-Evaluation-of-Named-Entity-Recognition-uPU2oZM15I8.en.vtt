WEBVTT
Kind: captions
Language: en

00:00:00.650 --> 00:00:04.978
Okay let me now introduce how we
evaluate named entity recognition.

00:00:04.978 --> 00:00:10.420
In the named entity recognition task,
we have a sequence of word tokens.

00:00:10.420 --> 00:00:13.360
And what we're going to want
to do is predict entities.

00:00:13.360 --> 00:00:18.090
So we're going to want to predict that
this is an organization, these two words.

00:00:18.090 --> 00:00:23.100
These two words are a person,
this one word is an organization.

00:00:23.100 --> 00:00:28.090
So in general, we can have entity
names that are several tokens long and

00:00:28.090 --> 00:00:32.580
we want to identify both
the boundaries of the entity and

00:00:32.580 --> 00:00:35.870
then also its class, if this is a person.

00:00:35.870 --> 00:00:42.120
Now you can think of that as making a
classification for each token in sequence.

00:00:42.120 --> 00:00:44.410
But in a way that doesn't
terribly make sense.

00:00:44.410 --> 00:00:49.100
because really, our unit of
interest is these whole entities,

00:00:49.100 --> 00:00:52.130
the person and the organizations.

00:00:52.130 --> 00:00:57.150
And so the standard and better task
motivated evaluation that's used for

00:00:57.150 --> 00:01:02.790
named entity recognition is to
evaluate per entity, not per token.

00:01:02.790 --> 00:01:06.576
And so
when we're working out our two-by-two,

00:01:06.576 --> 00:01:09.561
contingency table of true positives.

00:01:11.257 --> 00:01:12.470
And so on.

00:01:12.470 --> 00:01:15.250
And here's our system guess.

00:01:16.990 --> 00:01:20.810
What we're going to do,
is do it at the level of entities.

00:01:20.810 --> 00:01:28.120
So in this data there are three entities,
and we could imagine perhaps,

00:01:29.720 --> 00:01:35.130
that our system identified
this one as a person name,

00:01:35.130 --> 00:01:39.260
and identified this one as an organization
name, but missed this one.

00:01:41.160 --> 00:01:47.740
So what we'll be doing is saying that
there are two true positives and

00:01:47.740 --> 00:01:53.370
one false negative out
of the three tokens and

00:01:53.370 --> 00:01:58.480
so the precision of our
system here is 100%.

00:01:58.480 --> 00:02:03.653
Everything it says is right and
its recall is two thirds.

00:02:06.393 --> 00:02:08.661
Okay, so that looks okay but

00:02:08.661 --> 00:02:14.970
when we get into the details it gets
a little bit trickier than that.

00:02:14.970 --> 00:02:19.005
So the problem is that recall and
precision are straightforward for

00:02:19.005 --> 00:02:23.750
tasks like web search information
retrieval, or text categorization, where

00:02:23.750 --> 00:02:29.090
there is only one grain size that you're
putting a classification on a document.

00:02:29.090 --> 00:02:32.920
But in this case, what we're doing
is putting classifications on

00:02:32.920 --> 00:02:37.430
subsequences of words and the precision

00:02:37.430 --> 00:02:41.510
recall on F-measures actually behave
a bit funnily when that happens.

00:02:41.510 --> 00:02:44.850
So here's an example to give you
a good sense of the problems

00:02:44.850 --> 00:02:47.560
which actually occur commonly in systems.

00:02:47.560 --> 00:02:52.510
So here's the piece of text,
First Bank of Chicago announced earnings.

00:02:52.510 --> 00:02:54.970
And the correct entity is right here.

00:02:54.970 --> 00:02:58.930
First Bank of Chicago which is
a single organization name.

00:03:00.140 --> 00:03:03.330
However, our system made
a little bit of a booboo.

00:03:03.330 --> 00:03:09.660
Our system has said, Bank of Chicago
is the name of an organization,

00:03:09.660 --> 00:03:12.250
and so
that means it's made a boundary error.

00:03:12.250 --> 00:03:16.320
It's got the right boundary
of the entity correct, but

00:03:16.320 --> 00:03:19.350
it's got the left boundary
of the entity wrong.

00:03:19.350 --> 00:03:25.200
And this is the kind of error
that NER systems make a lot.

00:03:25.200 --> 00:03:29.650
And it's very easy to see in this case why
it's made the error, because first is also

00:03:29.650 --> 00:03:33.800
a common noun, and at the start of a
sentence it's perfectly reasonable to have

00:03:33.800 --> 00:03:40.070
a common noun of first Apple announced
this, and then Microsoft announced this.

00:03:41.270 --> 00:03:45.100
So, intuitively you
might feel like really,

00:03:45.100 --> 00:03:49.730
in this case the named entity recognized
should be counted as mostly correct.

00:03:49.730 --> 00:03:54.240
It identified that there was
an organization name here, and

00:03:54.240 --> 00:03:56.650
it labeled three of the four tokens.

00:03:56.650 --> 00:04:02.250
But that's not how things work using
the set based measures of false positives,

00:04:02.250 --> 00:04:07.650
false negatives, true positives and true
negatives when you're working on sequences

00:04:07.650 --> 00:04:12.760
because what we say is that the true

00:04:12.760 --> 00:04:18.250
annotation is that there's
an organization that spans

00:04:18.250 --> 00:04:23.550
from token one through
token four of the text.

00:04:23.550 --> 00:04:27.790
Whereas what our system guessed
is that there was an organization

00:04:27.790 --> 00:04:33.090
that spanned from token two
through token four of the text.

00:04:33.090 --> 00:04:37.705
And each of these claims
is taken as a unit and

00:04:37.705 --> 00:04:42.620
is put into a set of claims and then
we count the number of matching claims,

00:04:42.620 --> 00:04:48.020
that's the true positives and
then we count the set differences

00:04:48.020 --> 00:04:53.850
in both directions and that gives us the
false positives and the false negatives.

00:04:53.850 --> 00:04:57.610
So what we end up with
in our classification,

00:04:57.610 --> 00:05:02.170
in this case is that
this is a false negative.

00:05:02.170 --> 00:05:05.730
And that this one here
is a false positive.

00:05:05.730 --> 00:05:13.910
And so actually our system will be scored
as having made two errors if it does this.

00:05:13.910 --> 00:05:18.830
And so actually the system would have
scored better in an F1 evaluation

00:05:18.830 --> 00:05:22.630
of named entity recognition
by having labeled nothing.

00:05:22.630 --> 00:05:25.010
Now that can be easily seem
kind of wrong to you, and

00:05:25.010 --> 00:05:29.260
it has seemed wrong to other people,
so there have been various suggestions

00:05:29.260 --> 00:05:33.640
to provide measures for
evaluating the entity recognition systems.

00:05:33.640 --> 00:05:36.880
But you get partial credit for
doing things like this, for

00:05:36.880 --> 00:05:38.580
getting entity almost right.

00:05:38.580 --> 00:05:43.410
So for example, the MUC scorer that was
used in some of the prominent early

00:05:43.410 --> 00:05:45.870
evaluations of entity recognition,

00:05:45.870 --> 00:05:51.420
it had algorithm that gave partial
credit for cases like this.

00:05:51.420 --> 00:05:54.760
But once you do that then
there are these complicated

00:05:54.760 --> 00:05:57.940
questions of how much partial
credit to give in which cases,

00:05:57.940 --> 00:06:03.510
and it's not exactly clear and
you have various arbitrary parameters.

00:06:03.510 --> 00:06:07.920
So really most of the rest of the field
hasn't gone there and has ended up

00:06:07.920 --> 00:06:12.840
using a straightforward F1 measure for
named entity recognition despite

00:06:12.840 --> 00:06:16.840
the complexities with boundary errors
that I've just tried to illustrated.

00:06:16.840 --> 00:06:22.130
Okay, so that should give you a good sense
of what these measures of precision,

00:06:22.130 --> 00:06:23.597
recall and F-measure are.

00:06:24.720 --> 00:06:29.370
Why they're useful, how we use them for
named entity recognition, but

00:06:29.370 --> 00:06:30.990
also a slight sense for

00:06:30.990 --> 00:06:34.580
how you have to be a little bit careful
interpreting the numbers in that case.

