WEBVTT
Kind: captions
Language: en

00:00:00.620 --> 00:00:04.900
In this segment I briefly discuss
how we evaluate parsing results.

00:00:07.190 --> 00:00:11.070
So how do we go about evaluating whether
our parsers are doing a good job or not?

00:00:11.070 --> 00:00:13.003
One measure would just be to say,

00:00:13.003 --> 00:00:18.330
is the parse that we produce exactly the
right parse according to the tree bank?

00:00:18.330 --> 00:00:22.600
That's a possible standard, and actually
close to what the probabilistic models do,

00:00:22.600 --> 00:00:27.000
but it's rather a tough job to get the
entire structure of the sentence right.

00:00:27.000 --> 00:00:31.736
And so the most used evaluation measures
have divided up the pieces of a parse so

00:00:31.736 --> 00:00:34.410
you can get partial credit for
being right.

00:00:34.410 --> 00:00:36.630
So let's go through how that works.

00:00:36.630 --> 00:00:40.430
So the idea is we start with
the gold standard parse tree.

00:00:40.430 --> 00:00:43.640
This is the correct
parse of this sentence.

00:00:43.640 --> 00:00:45.505
And so then we run our parser, and

00:00:45.505 --> 00:00:50.370
our parser is going to choose some parse
tree for the sentence and return it.

00:00:50.370 --> 00:00:55.591
So in the example here,
the parser is mostly right.

00:00:55.591 --> 00:00:58.610
It's got the noun phrase sales executives.

00:00:58.610 --> 00:01:01.980
It's got this verb structure
of were examining and

00:01:01.980 --> 00:01:03.810
the noun phrase the figures right.

00:01:03.810 --> 00:01:06.130
It's actually made one
little mistake at the end.

00:01:06.130 --> 00:01:09.480
So yesterday should be this kind of weird,

00:01:09.480 --> 00:01:14.620
kind of temporal noun phrase in English
has where you have bare noun phrases like

00:01:14.620 --> 00:01:19.349
yesterday or next week to express a time
that can otherwise be expressed as

00:01:19.349 --> 00:01:22.936
a prepositional phrase like
on Sunday in the spring.

00:01:22.936 --> 00:01:27.550
But the parser has wrongly just
stuck it on as an extra noun

00:01:27.550 --> 00:01:32.660
at the end of the noun phrase great care,
and so that's an error.

00:01:32.660 --> 00:01:34.970
So how are we going to
go about measuring that?

00:01:34.970 --> 00:01:37.600
Well, the way that we're
going to do it is we're going to

00:01:37.600 --> 00:01:40.860
look at individual constituency claims.

00:01:40.860 --> 00:01:43.860
So here is a noun phrase, and

00:01:43.860 --> 00:01:48.950
we're going to say that it goes from
position 0 to 2 in the sentence, where I'm

00:01:48.950 --> 00:01:54.840
putting these fence post marker numbers
in between the words of the sentence.

00:01:54.840 --> 00:02:00.310
And so then we look down at our
guessed parse and say, well,

00:02:00.310 --> 00:02:06.470
it also has a noun phrase that
goes from 0 to 2 in the sentence.

00:02:06.470 --> 00:02:09.440
And so that particular constituency claim

00:02:09.440 --> 00:02:14.970
in the gold tree is correctly
captured in the parse tree.

00:02:14.970 --> 00:02:19.145
And so they way we do that for all
the cases is we convert the gold tree into

00:02:19.145 --> 00:02:22.973
a set of constituency claims where
we leave out the root nodes, so

00:02:22.973 --> 00:02:25.571
I've written claims across the root node.

00:02:25.571 --> 00:02:30.751
So these are our constituency claims.

00:02:30.751 --> 00:02:35.513
So we have a sentence that's
going from 0 to 11, and

00:02:35.513 --> 00:02:39.561
then the various other
constituency claims.

00:02:39.561 --> 00:02:43.084
Then we do exactly the same thing for

00:02:43.084 --> 00:02:47.670
the candidate parse
suggested by the parser.

00:02:47.670 --> 00:02:53.531
And so it's also suggested that the whole
thing is a sentence from 0 to 11,

00:02:53.531 --> 00:02:57.750
it's also suggested the NP from 0 to 2,
and so on.

00:02:57.750 --> 00:03:00.460
So it's got a stead of
constituency claims.

00:03:00.460 --> 00:03:04.771
And then what we do is we simply
treat each of these as a unit,

00:03:04.771 --> 00:03:08.240
as an atom,
that you either get right or wrong.

00:03:08.240 --> 00:03:12.982
And then we use exactly the same precision
recall F measure that you've already seen

00:03:12.982 --> 00:03:15.010
several times before.

00:03:15.010 --> 00:03:19.530
So here are the gold
standard label bracketings.

00:03:19.530 --> 00:03:21.960
Here the candidate label bracketings.

00:03:21.960 --> 00:03:26.594
And I've shown in bold the three
that the candidate agrees with

00:03:26.594 --> 00:03:28.800
the correct parse on.

00:03:28.800 --> 00:03:32.580
And so in total,
there are eight bracketings

00:03:34.250 --> 00:03:38.260
in the gold parse and
seven in the proposed parse.

00:03:38.260 --> 00:03:44.810
So the label precision is three-sevenths,
42.9%, recall 37.5%,

00:03:44.810 --> 00:03:50.323
and the F1 that combines those in
the usual way comes out as 40%.

00:03:50.323 --> 00:03:55.424
Now what that partly means is when
before we had this, there were these

00:03:55.424 --> 00:04:00.800
extra nodes down here for the parts of
speech that we talked about before.

00:04:00.800 --> 00:04:05.591
We don't in the parsing measure of value
evaluate the part of speech taking

00:04:05.591 --> 00:04:10.750
even though most parses do part of
speech taking as part of their work.

00:04:10.750 --> 00:04:15.377
So that's reported separately,
and in the example I've got here,

00:04:15.377 --> 00:04:19.621
the tagging is completely correct,
and so that's 100%.

00:04:19.621 --> 00:04:24.990
Something that you might notice here is
that these scores are actually pretty low.

00:04:24.990 --> 00:04:28.830
And it's worth thinking a bit
about why they're so low.

00:04:28.830 --> 00:04:34.527
And what we find is that even though
the candidate parse was mostly correct,

00:04:34.527 --> 00:04:38.990
we actually get a lot of brackets wrong,
so what happened?

00:04:38.990 --> 00:04:44.945
The only thing that was wrong in
the candidate parse was that this noun,

00:04:44.945 --> 00:04:49.711
yesterday, was wrongly fit
into this noun phrase rather

00:04:49.711 --> 00:04:53.990
than being a sentential
modifier as a temporal NP.

00:04:53.990 --> 00:04:57.400
But as a consequence of
making that mistake,

00:04:57.400 --> 00:05:01.886
the parser is scored as doing
a lot of other things wrong, so

00:05:01.886 --> 00:05:05.926
that this verb phrase is
scored as wrong because it's

00:05:05.926 --> 00:05:10.910
wrongly extending to cover yesterday,
whereas it shouldn't.

00:05:10.910 --> 00:05:14.390
This verb phrase is wrong for
exactly the same reason.

00:05:14.390 --> 00:05:19.270
This prepositional phrase is wrong for
same reason, and so is this noun phrase.

00:05:19.270 --> 00:05:24.989
So this label precision recall
measure suffers from these cascading

00:05:24.989 --> 00:05:31.901
errors whenever you attach something very
low that should be high, or vice versa.

00:05:31.901 --> 00:05:37.830
So many people think this is actually not
such a good measure of parser performance.

00:05:37.830 --> 00:05:42.498
And people, including me, have argued
instead we should use dependency measures

00:05:42.498 --> 00:05:45.710
of parsing performance even for
constituency parsers.

00:05:45.710 --> 00:05:51.070
But that just isn't the current practice
of what you see everywhere in research.

00:05:51.070 --> 00:05:57.270
The measure that you see is this
labeled precision, labeled recall F1.

00:05:57.270 --> 00:06:01.295
And so that's a measure
that we've presented here.

00:06:01.295 --> 00:06:05.425
I'll just briefly mention you can
also do an unlabeled version of

00:06:05.425 --> 00:06:09.805
these measures where you simply
look at the constituency claim as

00:06:09.805 --> 00:06:14.175
a sequence of words without worrying about
the label, but that's much less used.

00:06:15.840 --> 00:06:22.000
Okay, so now that we know how to
evaluate PCFGs, how well do they do?

00:06:22.000 --> 00:06:26.494
Well, if you just train a PCFG
off the Penn tree bank and

00:06:26.494 --> 00:06:32.259
run it on some Penn tree bank test
data and evaluate with this measure,

00:06:32.259 --> 00:06:36.198
the answer is you get
about 73% F1 measure.

00:06:36.198 --> 00:06:37.590
So that's not terrible.

00:06:37.590 --> 00:06:39.740
You get quite a lot of the brackets right.

00:06:39.740 --> 00:06:43.598
But remember,
it is counting each bracket separately.

00:06:43.598 --> 00:06:48.475
So if you're getting over a quarter of
the brackets wrong, that means that

00:06:48.475 --> 00:06:52.420
you're tending to make several bad
attachment errors in every sentence.

00:06:52.420 --> 00:06:58.800
So it's not a great result, and we'll
talk about how to improve that very soon.

00:06:58.800 --> 00:07:00.970
So let's just summarize, though,

00:07:00.970 --> 00:07:05.440
a little bit the properties of
the PCFGs that we've seen up until now.

00:07:05.440 --> 00:07:09.414
So the good things about PCFGs
is that they're very robust.

00:07:09.414 --> 00:07:13.145
Normally when estimating
a PCFG off a tree bank,

00:07:13.145 --> 00:07:17.514
you get the result,
though with smooth probabilities,

00:07:17.514 --> 00:07:22.640
that every possible string of
words is included in the grammar.

00:07:22.640 --> 00:07:26.490
So there's no categorical grammar
constraint whatsoever anymore.

00:07:26.490 --> 00:07:30.431
All the action is in the probabilities,
what's high probability and

00:07:30.431 --> 00:07:31.510
low probability.

00:07:31.510 --> 00:07:34.403
And that's useful because
the parser is robust.

00:07:34.403 --> 00:07:36.397
You can give anything at all and

00:07:36.397 --> 00:07:41.170
it'll do its best job to give you
the most likely sentence parse for that.

00:07:41.170 --> 00:07:45.911
Another good thing about PCFGs is
that they give us at least part of

00:07:45.911 --> 00:07:48.423
a solution to grammar ambiguity.

00:07:48.423 --> 00:07:52.668
So like in the example of people
fishing that I showed earlier,

00:07:52.668 --> 00:07:56.670
the PCFG tells you which parse for
a sentence to choose, and

00:07:56.670 --> 00:08:00.350
at least sometimes it
chooses the correct one.

00:08:00.350 --> 00:08:05.543
So it gives some ideas of the plausibility
of a parse, but as we'll see more later,

00:08:05.543 --> 00:08:09.405
and you've already got a hint of,
that because of the strong independence

00:08:09.405 --> 00:08:15.855
assumptions of PCFG, it's not actually
very good that at doing that.

00:08:15.855 --> 00:08:21.070
A third good property is that PCFGs give
us a probabilistic language model so

00:08:21.070 --> 00:08:25.710
we say how likely or
unlikely sentences are in a language.

00:08:25.710 --> 00:08:29.558
It's important to point out that
you shouldn't just now go and

00:08:29.558 --> 00:08:32.396
run off and think, aha,
I've got a grammar.

00:08:32.396 --> 00:08:36.165
I'll use a PCFG as a better
language model than the bigrams and

00:08:36.165 --> 00:08:38.240
trigrams we saw earlier.

00:08:38.240 --> 00:08:42.222
Because actually a PCFG,
as we've shown it so far,

00:08:42.222 --> 00:08:47.199
doesn't work as well as a bigram or
trigram as a language model for

00:08:47.199 --> 00:08:51.560
tasks like spelling correction or
speech recognition.

00:08:51.560 --> 00:08:55.680
And the reason that seems
to be is that PCFGs

00:08:55.680 --> 00:09:00.620
lack the lexicalization
of a trigram model.

00:09:00.620 --> 00:09:02.600
So what do we mean by that?

00:09:02.600 --> 00:09:07.320
Well, what we meant by that is that
if you're looking at this PCFG,

00:09:07.320 --> 00:09:11.650
most of the PCFG is of the nature VP

00:09:11.650 --> 00:09:16.730
rewrites as VBD, VP,
that rules that are expanding

00:09:16.730 --> 00:09:21.500
without any reference to what words
are actually used in the sentence.

00:09:21.500 --> 00:09:25.040
In fact,
the only rewrite rules that consider words

00:09:25.040 --> 00:09:29.380
are the ones that rewrite from
a preterminal to the word itself.

00:09:29.380 --> 00:09:34.154
And so this limits the ability
of a plain PCFG to act as a very

00:09:34.154 --> 00:09:36.500
effective language model.

00:09:36.500 --> 00:09:39.540
That's a problem we'll
address again very soon.

00:09:39.540 --> 00:09:44.260
But for the moment, you now know how to
go about evaluating constituency parsers.

