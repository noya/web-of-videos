WEBVTT
Kind: captions
Language: en

00:00:00.530 --> 00:00:03.728
The Naive Bayes algorithm is one of
the most important algorithms for

00:00:03.728 --> 00:00:04.838
text classification.

00:00:07.338 --> 00:00:10.990
The intuition of a Naive Bayes
algorithm is really quite simple.

00:00:10.990 --> 00:00:13.640
It's based on Bayes rule,
which we'll see in a second.

00:00:13.640 --> 00:00:16.510
And relies on very simple
representation of the document

00:00:16.510 --> 00:00:19.530
called the bag of words representation.

00:00:19.530 --> 00:00:22.070
Let's see the intuition of
the Bag of Words representation.

00:00:23.540 --> 00:00:26.720
Imagine I have some document that says,
I love this movie!

00:00:26.720 --> 00:00:29.480
It's sweet, but
satirical humor, and so on.

00:00:29.480 --> 00:00:30.180
And or job

00:00:31.840 --> 00:00:36.800
is to build this function gamma which
takes the document and returns a class.

00:00:39.670 --> 00:00:43.600
The class could be positive, or
the class could be negative,

00:00:43.600 --> 00:00:46.680
in the case of sentiment analysis.

00:00:46.680 --> 00:00:47.180
Which is it?

00:00:47.180 --> 00:00:47.970
Positive or negative?

00:00:50.490 --> 00:00:51.730
In order to solve this task,

00:00:52.930 --> 00:00:57.140
one thing we might do is look at
individual words in the document.

00:00:57.140 --> 00:00:59.860
Like love, or satirical or great.

00:00:59.860 --> 00:01:02.850
We might look at all the words,
in some kinds of text classification,

00:01:02.850 --> 00:01:04.110
we're going to look at all the words.

00:01:04.110 --> 00:01:05.790
We're going to look at every single word.

00:01:05.790 --> 00:01:08.290
In other cases,
we'll look at just some subset.

00:01:08.290 --> 00:01:13.100
If we were to look at a subset,
we might imagine that

00:01:13.100 --> 00:01:16.710
the document looks something like this,
it just look like it has the word love and

00:01:16.710 --> 00:01:19.350
the word satirical, and the word great,
and all the other words have disappeared.

00:01:21.160 --> 00:01:23.200
Whether we use a subset of words or

00:01:23.200 --> 00:01:26.790
all the words in the document,
the bag of words representation

00:01:28.010 --> 00:01:31.280
loses all the information about
the order of the words in the document.

00:01:31.280 --> 00:01:34.890
And all we represent about the document
is the set of words that occurred

00:01:34.890 --> 00:01:35.610
in their accounts.

00:01:37.640 --> 00:01:41.600
So for example, for the previous document,
we might represent the document as just

00:01:41.600 --> 00:01:47.090
the vector of words, great, love,
recommend, laugh, happy and

00:01:47.090 --> 00:01:52.140
for each one account, great occur twice,
love occur twice, recommend occur once.

00:01:52.140 --> 00:01:54.920
And again, we can keep all of the words
in the document and we'll often do that,

00:01:54.920 --> 00:01:58.130
or we can keep just some of the words in
the document if we have an idea that some

00:01:58.130 --> 00:02:00.740
of the words are particularly
indicative cues.

00:02:02.370 --> 00:02:05.440
So the idea of the bag
of words representation

00:02:05.440 --> 00:02:09.770
is that we're going to represent our
document just by a list of words and

00:02:09.770 --> 00:02:13.040
their counts and throw away
everything else about the document-

00:02:13.040 --> 00:02:17.360
which order the words occurred in,
what font they were in, anything else.

00:02:17.360 --> 00:02:20.236
And our function gamma, or classifier,

00:02:20.236 --> 00:02:25.428
will take that representation and
assign us a class positive or negative.

00:02:28.048 --> 00:02:29.740
And this applies, I've shown it to you for

00:02:29.740 --> 00:02:31.640
the two class problem
of sentiment analysis.

00:02:31.640 --> 00:02:33.780
Positive or negative sentiment.

00:02:33.780 --> 00:02:36.928
But this supplies for all sorts
of document classification tasks.

00:02:36.928 --> 00:02:41.960
So I might have some document I need to
classify into a different computer science

00:02:41.960 --> 00:02:46.696
topic because I'm building an online
library of computer science papers or

00:02:46.696 --> 00:02:49.740
I'm giving advice on
computer science topics.

00:02:51.210 --> 00:02:55.530
So I have some text, some document here,
with words like parser,

00:02:55.530 --> 00:02:57.810
or language or label or translation.

00:02:57.810 --> 00:03:00.700
And I want to know which aspect of
computer science it should go in so

00:03:00.700 --> 00:03:03.090
I can file my paper automatically.

00:03:03.090 --> 00:03:07.739
And a good text classifier should
automatically figure out that's a natural

00:03:07.739 --> 00:03:09.538
language processing paper.

00:03:13.538 --> 00:03:18.178
So that's the intuition of
the Naive Bayes classifier.

