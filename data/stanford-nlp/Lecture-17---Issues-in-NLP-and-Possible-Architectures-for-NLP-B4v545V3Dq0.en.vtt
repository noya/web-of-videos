WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.830
[MUSIC]

00:00:04.830 --> 00:00:09.109
Stanford University.

00:00:09.109 --> 00:00:12.683
&gt;&gt; Okay, hi everyone.

00:00:12.683 --> 00:00:13.640
It's exciting, right?

00:00:13.640 --> 00:00:17.880
We're in week ten, everyone's really
excited for the end of the class.

00:00:17.880 --> 00:00:19.170
Well, thank you so much for

00:00:19.170 --> 00:00:24.010
the people who did turn up in person to
see this last week's lecture I guess.

00:00:24.010 --> 00:00:29.528
Rich and me both have one lecture each,
with me doing the Tuesday and Thursday.

00:00:29.528 --> 00:00:34.472
I suspect that there probably a few
people who we won't be seeing today.

00:00:34.472 --> 00:00:37.186
Who are trying to catch up
on their final projects.

00:00:37.186 --> 00:00:40.210
I'm sure every one of them
will watch the video and

00:00:40.210 --> 00:00:43.230
catch up on what they
miss during spring break.

00:00:43.230 --> 00:00:46.527
But, for those of you who are here or
watching.

00:00:46.527 --> 00:00:49.712
Here's what we're gonna have today.

00:00:49.712 --> 00:00:54.064
I wanted to sort of try and
say a little bit about

00:00:54.064 --> 00:00:58.870
sort of bigger picture
solving needs of language.

00:00:58.870 --> 00:01:02.151
And where things might be heading or
should be heading.

00:01:02.151 --> 00:01:04.041
I wanted to kind of come back and

00:01:04.041 --> 00:01:08.010
say a bit more about some of
the tree structured model ideas.

00:01:08.010 --> 00:01:12.762
And how those might be reinterpreted in
a different context at the high level of

00:01:12.762 --> 00:01:13.482
language.

00:01:13.482 --> 00:01:16.243
And then something that's in
the same vein as that is for

00:01:16.243 --> 00:01:17.868
our research highlight today.

00:01:17.868 --> 00:01:22.270
Zhedi is then gonna be talking about this
kind of interesting model out of Berkeley.

00:01:22.270 --> 00:01:25.471
For learning to compose for
question answering.

00:01:25.471 --> 00:01:30.268
And then for the end part of it, there
are kind of a couple of things that we

00:01:30.268 --> 00:01:33.189
haven't said very much about in the class.

00:01:33.189 --> 00:01:37.704
Which I thought I should just say a bit
more about before the class ends so

00:01:37.704 --> 00:01:40.500
we'll talk about character based models.

00:01:41.990 --> 00:01:43.940
Okay, quickly the reminders.

00:01:43.940 --> 00:01:48.090
Well, best of luck in finishing your
assignment four or your final project.

00:01:49.350 --> 00:01:52.774
So, I mean,
there's sort of a subtle balance.

00:01:52.774 --> 00:01:55.031
But in terms of what you're doing, I mean,

00:01:55.031 --> 00:01:59.430
you should be making sure you kind of have
solid baselines and well-trained models.

00:01:59.430 --> 00:02:03.372
It's obviously good to be
investigating fancy architectures.

00:02:03.372 --> 00:02:08.999
But if they're sort of not working at
all or it seems like they're working

00:02:08.999 --> 00:02:14.007
very badly it's sort of always
hard to work out where things are.

00:02:14.007 --> 00:02:17.610
So especially if it's a different problem,
well you sort of wanna know, well.

00:02:17.610 --> 00:02:21.032
How much would you get if you just ran
a bigram logistic regression on this?

00:02:21.032 --> 00:02:25.530
And then, is there reasonable evidence
that this neural model was well trained?

00:02:25.530 --> 00:02:28.581
So it's not that you have to sort of
infinitely tune your hyper parameters.

00:02:28.581 --> 00:02:33.340
But we sort of hope that you have
sort of reasonably solid models.

00:02:33.340 --> 00:02:36.200
If you're having any problems do try and
get some help.

00:02:36.200 --> 00:02:40.250
I know there are a lot of people
going to office hours today.

00:02:40.250 --> 00:02:45.280
So tonight Richard is again having
infinite project office hours so

00:02:45.280 --> 00:02:47.440
feel free to come by.

00:02:47.440 --> 00:02:51.520
Depending, you can either go home and have
dinner and come back at 10 PM probably and

00:02:51.520 --> 00:02:53.308
he'll still be there.

00:02:53.308 --> 00:02:59.090
[LAUGH]
Make sure you're in the queue.

00:02:59.090 --> 00:03:03.940
And then sort of, finally,
for Microsoft Azure, so

00:03:03.940 --> 00:03:07.900
I mean Microsoft's really been super
generous at giving us more GPU hours.

00:03:07.900 --> 00:03:11.603
And so we've got a fresh,
new allotment of GPU hours.

00:03:11.603 --> 00:03:16.050
And I guess this afternoon I was trying
to top up various people's accounts.

00:03:16.050 --> 00:03:18.558
But in general it's kind of easiest for

00:03:18.558 --> 00:03:22.133
everybody if your account
doesn't run out of credits.

00:03:22.133 --> 00:03:25.419
Because then it gets cancelled,
and you get locked out, and

00:03:25.419 --> 00:03:28.713
we have to reactivate and
blah, blah, blah, blah, blah.

00:03:28.713 --> 00:03:33.540
Really if you sort of know you want to do
something that takes more credits than

00:03:33.540 --> 00:03:34.220
you have.

00:03:34.220 --> 00:03:37.910
Feel free to get in contact
on piazza beforehand.

00:03:37.910 --> 00:03:41.750
Preferably don't just email
me cuz in the good case,

00:03:41.750 --> 00:03:45.690
one of the TAs, James or
Nish do it rather than me.

00:03:47.200 --> 00:03:47.921
Okay, so

00:03:47.921 --> 00:03:53.522
a kind of interesting thing that's
happened in the last couple of years.

00:03:53.522 --> 00:03:58.314
Is that a lot of the key deep learning
people have really sort of redirected

00:03:58.314 --> 00:03:59.630
their mission.

00:03:59.630 --> 00:04:02.416
With this idea they could
sort of solve language.

00:04:02.416 --> 00:04:07.367
So historically, most of deep
learning came out of vision, and

00:04:07.367 --> 00:04:11.110
vision really dominated in deep learning.

00:04:11.110 --> 00:04:13.876
But for a couple of interesting reasons,

00:04:13.876 --> 00:04:18.865
sort of the same people that were the sort
of most key deep learning people.

00:04:18.865 --> 00:04:22.029
Such as Yann LeCun and
the others, and Geoff Hinton.

00:04:22.029 --> 00:04:22.568
Yoshua Bengio,

00:04:22.568 --> 00:04:28.630
really they've all considerably redirected
their research programs towards language.

00:04:28.630 --> 00:04:32.605
And so here's just sort of one
random sort of business mag quote

00:04:32.605 --> 00:04:34.564
from businessinsider.com.

00:04:34.564 --> 00:04:38.123
Which shows how this is sort
of panning out in practice.

00:04:38.123 --> 00:04:40.630
So Yann LeCun is quoted as saying.

00:04:40.630 --> 00:04:45.581
The role of FAIR, Facebook AI Research, is
to advance the science and the technology

00:04:45.581 --> 00:04:50.467
of AI and do experiments that demonstrate
that technology for new applications like

00:04:50.467 --> 00:04:55.354
computer vision, dialogue systems,
virtual assistants, speech recognition,

00:04:55.354 --> 00:04:59.449
natural language understanding,
translation, things like that.

00:04:59.449 --> 00:05:02.740
And the thing that's sort of
really surprising about that list.

00:05:02.740 --> 00:05:07.868
Is, well okay the first application
he mentions is computer vision.

00:05:07.868 --> 00:05:12.020
And he is a 20 year computer vision guy so
that may be not too surprising.

00:05:12.020 --> 00:05:15.730
But if you keep on reading
after the first application.

00:05:15.730 --> 00:05:20.290
Every other application he mentions
is a natural language application.

00:05:20.290 --> 00:05:23.794
Dialogue systems, virtual assistants,
speech recognition,

00:05:23.794 --> 00:05:26.543
natural language understanding,
translation.

00:05:26.543 --> 00:05:30.808
They're all language applications.

00:05:30.808 --> 00:05:35.529
And I think coming off of that
that sort of many people who

00:05:35.529 --> 00:05:39.750
haven't been long term language people.

00:05:39.750 --> 00:05:43.835
Kind of feel that language will
have a kind of an image net moment.

00:05:43.835 --> 00:05:50.420
Where somehow someone builds a big
enough complex enough neural network.

00:05:50.420 --> 00:05:55.738
That kind of the task will just be
sort of measurably solved in one step.

00:05:55.738 --> 00:05:58.890
I kind of think actually that's
probably not gonna happen.

00:05:58.890 --> 00:06:03.795
Because I kinda think there are so many
different language phenomena and tasks.

00:06:03.795 --> 00:06:06.020
That there's sort of just no one task.

00:06:06.020 --> 00:06:09.013
That can really sort of have
the same kind of seminal, okay,

00:06:09.013 --> 00:06:11.484
we've solved natural
language understanding.

00:06:11.484 --> 00:06:16.486
And so I wanted to,
with respect to that, sort of mention

00:06:16.486 --> 00:06:21.506
a couple of things of what has
been lost from old NLP work.

00:06:21.506 --> 00:06:25.024
So I think there's actually kind
of an interesting contrast.

00:06:25.024 --> 00:06:29.364
If you go back to the sort of good old
fashioned AI work in natural language

00:06:29.364 --> 00:06:32.980
processing versus a lot of
what you see more recently.

00:06:32.980 --> 00:06:38.750
And that is if you sort of look back and
sort of 80s, 70s,

00:06:38.750 --> 00:06:43.690
natural language processing work,
that these people had really lofty goals.

00:06:43.690 --> 00:06:49.304
That their goal was to sort of really
have human level language understanding.

00:06:49.304 --> 00:06:53.990
Now, actually what they could do
is extremely, extremely modest.

00:06:53.990 --> 00:06:58.230
But nevertheless the contrast is,
it sort of seems like here we are today.

00:06:58.230 --> 00:07:03.389
We have much better realities as
to the kind of things we can do.

00:07:03.389 --> 00:07:06.820
But it's not always clear that
everyone's reaching for the stars.

00:07:06.820 --> 00:07:11.203
As opposed to just thinking, okay, well
we can run an LSTM on this language data.

00:07:11.203 --> 00:07:15.260
And that'll sort of work reasonably
well and not try to get beyond that.

00:07:15.260 --> 00:07:18.456
So I thought it might be good just to
have a few slides at the beginning.

00:07:18.456 --> 00:07:20.690
Sort of looking at a bit
of older NLP work.

00:07:20.690 --> 00:07:25.713
And seeing if there are things that we
could kind of see and learn from that.

00:07:25.713 --> 00:07:30.346
And so I chose as my example
Peter Norvig's Ph.D thesis.

00:07:30.346 --> 00:07:34.945
So probably most of you vaguely at
least know who Peter Norvig is.

00:07:34.945 --> 00:07:40.323
Because of the textbook on what
is Artificial Intelligence and

00:07:40.323 --> 00:07:44.792
the Modern Approach,
Russell and Norvig's book.

00:07:44.792 --> 00:07:48.449
He's worked for many years as
a director of research at Google.

00:07:48.449 --> 00:07:52.680
And so it's just past the 30th anniversary
of Peter's PhD thesis which was in

00:07:52.680 --> 00:07:54.885
natural language understanding.

00:07:54.885 --> 00:07:58.755
It was called A Unified Theory of
Inference for Text Understanding.

00:08:00.919 --> 00:08:04.266
Well, what you'll find if
you look in his thesis?

00:08:04.266 --> 00:08:08.335
I mean, in terms of the actual
language processing and

00:08:08.335 --> 00:08:11.972
language understanding
what's in the thesis.

00:08:11.972 --> 00:08:16.176
I mean, in some sense,
there's shockingly little of that.

00:08:16.176 --> 00:08:23.026
So for the work for his entire thesis,
there is only one piece of real,

00:08:23.026 --> 00:08:29.537
natural language that's analyzed
in the entire dissertation.

00:08:29.537 --> 00:08:34.168
All the rest of it is sort of toy
examples like Bill had a bicycle.

00:08:34.168 --> 00:08:38.691
John wanted it, he gave it to him,
and trying to analyze stuff like that.

00:08:38.691 --> 00:08:43.421
Actually, a bit reminiscent of the kind
of baby sentences that Facebook AI

00:08:43.421 --> 00:08:48.600
research lately came out with, but this is
the once piece of real natural language

00:08:48.600 --> 00:08:53.505
that's analyzed in the dissertation
which comes from a children's story.

00:08:53.505 --> 00:08:57.915
In a poor fishing village built on
an island not far from the coast of China,

00:08:57.915 --> 00:09:01.351
a young boy named Chang Lee
lived with his widowed mother.

00:09:01.351 --> 00:09:05.939
Every day, little Chang bravely set off
with his net, hoping to catch a few fish

00:09:05.939 --> 00:09:10.001
from the sea which they could sell and
have a little money to buy bread.

00:09:10.001 --> 00:09:15.302
But the interesting thing is that these
are sort of some of things that Norvig

00:09:15.302 --> 00:09:20.116
said, we should be able to get out of
this text and I think many of these or

00:09:20.116 --> 00:09:25.100
at least half of these are things
that we're still not very good at.

00:09:25.100 --> 00:09:26.674
Actually, the first half.

00:09:26.674 --> 00:09:29.120
So, he's hoping to get out.

00:09:29.120 --> 00:09:33.906
There is a sea, which surrounds
the island is used by the villagers for

00:09:33.906 --> 00:09:36.999
fishing and
forms part of the coast of China.

00:09:36.999 --> 00:09:39.869
So, there's a sort of interpretive

00:09:39.869 --> 00:09:44.089
understanding of the relations
that are involved.

00:09:44.089 --> 00:09:49.565
And in particular, the relationship
between the sea and China is quite

00:09:49.565 --> 00:09:54.960
distant in the text and sort of
mediated by the existence of the coast.

00:09:54.960 --> 00:09:57.285
That's the kind of stuff
we're still not very good at.

00:09:57.285 --> 00:10:01.906
Chang intends to trap fish in his net,
which is a fishing net.

00:10:01.906 --> 00:10:06.450
Again, that sort of interpretive stuff.

00:10:06.450 --> 00:10:11.060
So, there's sort of this leap of he's
set off with his net hoping to catch

00:10:11.060 --> 00:10:13.090
a few fish from the sea.

00:10:13.090 --> 00:10:18.030
So it never actually says that he's
gonna catch fish with the net, but

00:10:18.030 --> 00:10:21.222
any human being would
interpret it that way.

00:10:21.222 --> 00:10:25.859
And again, that's sort of the kind of
interpretive language that we still aren't

00:10:25.859 --> 00:10:27.394
very good at dealing with.

00:10:27.394 --> 00:10:30.902
The other two actually much easier.

00:10:30.902 --> 00:10:36.208
The word which refers to the fish, the
word they refers to Chang and his mother.

00:10:36.208 --> 00:10:38.020
We talked about things like that.

00:10:39.290 --> 00:10:42.392
So in terms of how he's gonna do this,
I mean,

00:10:42.392 --> 00:10:47.330
interestingly, the kind of perspective
in the 80s was just completely

00:10:47.330 --> 00:10:50.613
diametrically opposed to
what it is these days.

00:10:50.613 --> 00:10:53.670
I mean, in the 1980s,
it was sort of believed,

00:10:53.670 --> 00:10:58.255
sort of just as an assumption that was
beyond question that the only way that you

00:10:58.255 --> 00:11:02.354
were going to do any kind of natural
language understanding was to have

00:11:02.354 --> 00:11:05.640
a knowledge base that you
could work with and reason on.

00:11:05.640 --> 00:11:12.241
So on page four of the thesis, he takes
it as established as we have just seen.

00:11:12.241 --> 00:11:15.179
A suitable knowledge base
is a prerequisite for

00:11:15.179 --> 00:11:20.323
making proper inferences and that was sort
of just no more of an argument than look,

00:11:20.323 --> 00:11:24.070
we need to make inferences
between these facts to understand

00:11:24.070 --> 00:11:26.949
that the net is gonna be
used to catch the fish.

00:11:26.949 --> 00:11:30.328
Therefore, we need a knowledge
base to do it over.

00:11:30.328 --> 00:11:34.497
Where it's sort of what's happened in
the last two decades is to sort of show

00:11:34.497 --> 00:11:38.802
that there's actually a lot of natural
language that you can understand and

00:11:38.802 --> 00:11:43.461
do without having any knowledge base, and
just working on fairly surface forms.

00:11:43.461 --> 00:11:46.961
So in his thesis,
he outlines six forms of inference and

00:11:46.961 --> 00:11:50.759
tries to build a natural language
understanding system that

00:11:50.759 --> 00:11:55.093
embodies them and will be able to
make inferences over that passage.

00:11:55.093 --> 00:11:58.225
Two of them come in pairs, so
there are four basic types.

00:11:58.225 --> 00:12:00.552
So, one is elaboration.

00:12:00.552 --> 00:12:05.400
And so, that's working out
how to connect to entities.

00:12:05.400 --> 00:12:10.560
So this example is John got a piggy
bank for the reason of having money for

00:12:10.560 --> 00:12:15.721
the reason of buying a present, but
the example from the previous slide

00:12:15.721 --> 00:12:20.560
was he had the net with him for
the reason of helping catch the fish.

00:12:20.560 --> 00:12:24.733
And so, you have to sort of do
this contextual elaboration.

00:12:24.733 --> 00:12:27.417
The second one is reference resolution.

00:12:27.417 --> 00:12:30.780
That's really the only one of
these that we've talked about and

00:12:30.780 --> 00:12:34.279
we know how to do that one, and
we can do that one pretty well today.

00:12:34.279 --> 00:12:39.127
The third one is related
to the interpretation

00:12:39.127 --> 00:12:44.107
of metaphor and
abstract forms from language.

00:12:44.107 --> 00:12:47.927
So that if you have a sentence like
the Red Sox killed the Yankees,

00:12:47.927 --> 00:12:52.780
that doesn't mean that they went out
murdering them with knifes and guns.

00:12:52.780 --> 00:12:56.753
It means that they defeated
them convincingly.

00:12:56.753 --> 00:12:59.915
And so,
language is full of that kind of metaphor.

00:12:59.915 --> 00:13:02.785
But all the time,
we're sort of using physical

00:13:02.785 --> 00:13:07.036
metaphors which are having these
sort of abstract interpretations.

00:13:07.036 --> 00:13:12.022
And so another part of his dissertation
was sort of trying to work out how you

00:13:12.022 --> 00:13:16.774
derived those abstract interpretations,
then the third one is sort of

00:13:16.774 --> 00:13:21.998
an interesting one that a lot of the time
that I think we don't think of as much,

00:13:21.998 --> 00:13:26.701
but is actually I think quite
important and that's concretization.

00:13:26.701 --> 00:13:31.605
And so, that this is you solve goal
from a general description to much

00:13:31.605 --> 00:13:33.644
more specific form of that.

00:13:33.644 --> 00:13:38.548
So if you know someone is traveling in
automobile, then you know that the kind of

00:13:38.548 --> 00:13:43.260
traveling that at least one of the people
is doing is driving the automobile,

00:13:43.260 --> 00:13:46.453
at least until our autonomous
cars get a bit better.

00:13:46.453 --> 00:13:51.259
And so, there's that kind of
concretization that you'll need to be able

00:13:51.259 --> 00:13:55.300
to do more fine grained inferences
in the state of knowledge.

00:13:55.300 --> 00:13:58.621
So, that's kind of interesting.

00:13:58.621 --> 00:14:02.830
There are ways in which we've
made enormous progress.

00:14:02.830 --> 00:14:07.658
So in Norvig's thesis, the situation
is essentially that they don't

00:14:07.658 --> 00:14:12.566
have syntactic parsers that's good
enough that they can actually parse

00:14:12.566 --> 00:14:15.957
sentences they'd like to
parse like that story.

00:14:15.957 --> 00:14:20.515
So he describes how Wilensky,
that's his dissertation advisor's.

00:14:20.515 --> 00:14:24.491
PHRAN program was used where
possible to pause sentences.

00:14:24.491 --> 00:14:27.657
But for some input,
PHRAN was not up to the task, so

00:14:27.657 --> 00:14:31.721
we constructed the representation
to sentence by hand instead.

00:14:31.721 --> 00:14:33.189
Well, you know we're
actually better than that.

00:14:33.189 --> 00:14:36.579
We can actually parse
sentences pretty well now.

00:14:36.579 --> 00:14:41.904
But once you're starting to do the kind
of elaborations to do the sort of things

00:14:41.904 --> 00:14:46.909
that were being talked about as other
goals, these are kind of things that

00:14:46.909 --> 00:14:52.098
most of NLP still hasn't gotten to, and
maybe should be starting to get to.

00:14:52.098 --> 00:14:56.744
So, I think there's still sort of big
things that we still need to do in NLP.

00:14:56.744 --> 00:15:00.734
On the one hand, I mean,
it's just true that in the last few years,

00:15:00.734 --> 00:15:05.424
there's been these exciting times and a
lot of systems have gone a lot better and

00:15:05.424 --> 00:15:10.044
there's sort of this sense that BiLSTMs
with attention are sort of taking over

00:15:10.044 --> 00:15:14.874
the field of natural language processing,
because you can try them on any task and

00:15:14.874 --> 00:15:19.004
they work better than what people used
to do and you can even use them for

00:15:19.004 --> 00:15:20.274
surprising things.

00:15:20.274 --> 00:15:23.810
You can just use a BiLSTM for attention.

00:15:23.810 --> 00:15:28.429
As that's gonna be your natural language
parser, it works surprisingly well.

00:15:28.429 --> 00:15:30.211
So, they seem good.

00:15:30.211 --> 00:15:34.564
Another thing that's really exciting
that's happened is these neural

00:15:34.564 --> 00:15:39.361
methods have just clearly led to a
renaissance in language generation tasks.

00:15:39.361 --> 00:15:43.635
So any task where you actually have to
generate pieces of natural language where

00:15:43.635 --> 00:15:46.824
that's the sort of generation
side machine translation,

00:15:46.824 --> 00:15:49.190
the generation side of dialogue.

00:15:49.190 --> 00:15:53.050
Answering a question, doing summarization.

00:15:53.050 --> 00:15:57.650
That all of those feels the generation
sides of things were sort of

00:15:57.650 --> 00:16:02.344
Fairly moribund in the first decade of
the 2000s where now it just seems like,

00:16:02.344 --> 00:16:05.760
we have these fantastically
good neural language models.

00:16:05.760 --> 00:16:09.420
That we can configure in different
ways and we can do really exciting and

00:16:09.420 --> 00:16:10.960
nice language generation.

00:16:10.960 --> 00:16:13.680
All those fields have sort of
been springing to life lately.

00:16:15.410 --> 00:16:19.730
And it's also sorta super
interesting time, scientifically,

00:16:19.730 --> 00:16:24.130
because I think sorta most of
natural language processing

00:16:24.130 --> 00:16:28.550
has just sort of been this assumption
that we need to be building particular

00:16:28.550 --> 00:16:31.710
kinds of representations of language and
working with them.

00:16:31.710 --> 00:16:35.150
So we want to have sort of
syntactic representations.

00:16:35.150 --> 00:16:39.890
And often we want to have above those
things like semantic frames to represent

00:16:39.890 --> 00:16:42.010
events and relations in language.

00:16:42.010 --> 00:16:45.954
And so we're sort of building
explicit localist language knowledge

00:16:45.954 --> 00:16:47.114
representations.

00:16:47.114 --> 00:16:51.906
Where it's what some of the recent work
such as the stuff that Richard was talking

00:16:51.906 --> 00:16:56.626
about last week was showing is that it now
seems that there are a lot of situations

00:16:56.626 --> 00:17:01.276
in which we can build these end to end
deep learning systems that don't have any

00:17:01.276 --> 00:17:05.859
of these kind of explicit localist
knowledge representations that actually

00:17:05.859 --> 00:17:08.000
work very nicely.

00:17:08.000 --> 00:17:11.260
But on the other hand,
there are a lot of things that we

00:17:11.260 --> 00:17:14.610
still I think have barely
scratched the surface off.

00:17:14.610 --> 00:17:20.600
So one of those is we still kind
of have very primitive means for

00:17:20.600 --> 00:17:23.450
building and
accessing memories or knowledge.

00:17:23.450 --> 00:17:28.240
So yes, there's been a lot of work
with LSTMs where the M is memory and

00:17:28.240 --> 00:17:30.540
memory networks and
other things like that.

00:17:30.540 --> 00:17:36.028
But really all of those are models of
sort of very recent short term memory.

00:17:36.028 --> 00:17:38.560
That's the ST in the LSTM.

00:17:38.560 --> 00:17:42.631
They're not really models of
how like human beings that we

00:17:42.631 --> 00:17:46.960
can have years of experience of
our lives stored in our heads.

00:17:46.960 --> 00:17:51.853
And we can flexibly sort of bring forth
relevant facts at the right time,

00:17:51.853 --> 00:17:56.506
that all they're doing is a sort of
linear scan of what's happened in

00:17:56.506 --> 00:17:59.659
the last 100 words or something like that.

00:17:59.659 --> 00:18:01.379
So that's pretty poor.

00:18:01.379 --> 00:18:05.257
Another thing that's pretty poor
is we don't really have much

00:18:05.257 --> 00:18:09.440
in the way of models that let us
formulate goals or formulate plans.

00:18:09.440 --> 00:18:13.645
And really, if you're going to do a lot
of things in conversation like have

00:18:13.645 --> 00:18:18.050
a meaningful dialogue, you sort of have
to have some goals and plans that yes,

00:18:18.050 --> 00:18:20.150
you can just be shooting the breeze.

00:18:20.150 --> 00:18:21.040
But a lot of the time,

00:18:21.040 --> 00:18:24.730
you wanna accomplish things which leads
to sub tasks, and things like that.

00:18:26.160 --> 00:18:30.371
Another area where we're still really
bad is we're just sort of pretty

00:18:30.371 --> 00:18:32.981
bad at inter-sentential relationships.

00:18:32.981 --> 00:18:37.856
So once we're inside one sentence
the structure is usually pretty clear and

00:18:37.856 --> 00:18:39.595
we can work with that.

00:18:39.595 --> 00:18:44.585
But once we sort of try and reason
between sentences or between clauses and

00:18:44.585 --> 00:18:48.710
understand the relationships,
we're usually pretty poor at that still.

00:18:48.710 --> 00:18:53.490
And we still can't do many of
those things that Peter Norvig was

00:18:53.490 --> 00:18:54.470
talking about, right?

00:18:54.470 --> 00:18:58.000
That if we wanna do
elaborations in the situation

00:18:58.000 --> 00:19:01.673
using common sense knowledge that's not
really the kind of thing that we've been

00:19:01.673 --> 00:19:03.680
able to build deep learning
systems to do so far.

00:19:06.020 --> 00:19:08.730
Okay, so that's the end of part one.

00:19:08.730 --> 00:19:10.781
So, for part two of today,

00:19:10.781 --> 00:19:15.700
I wanted to sort of say a bit more
about tree-structured models.

00:19:15.700 --> 00:19:20.250
And then sort of talk about a bit of
recent work that was done by a recent

00:19:20.250 --> 00:19:23.939
student of mine, Sam Bowman,
along with Jean Gaultier

00:19:23.939 --> 00:19:28.825
on sort of having more efficient ways
of doing tree-structured models.

00:19:28.825 --> 00:19:33.240
So for my linguistic self,
I still think sort of having these sort of

00:19:33.240 --> 00:19:37.810
constituent pieces that you can
build representations of which gives

00:19:37.810 --> 00:19:42.250
you kind of a tree structure is
roughly the right kind of model.

00:19:42.250 --> 00:19:47.800
And so up until now, I'd sort of
shown some examples of bits of syntax

00:19:47.800 --> 00:19:51.150
and looking inside
the sentiment of clauses.

00:19:51.150 --> 00:19:53.640
Here's another nice
example that was some work

00:19:53.640 --> 00:19:57.880
that was done at Maryland by
Mojita Iya and his fellow students.

00:19:57.880 --> 00:20:02.570
And so what they were wanting to do,
is the learning models of

00:20:02.570 --> 00:20:06.310
the political ideology
of pieces of language.

00:20:06.310 --> 00:20:10.800
And so pieces of language could either
be mutual which is shown in gray or

00:20:10.800 --> 00:20:17.140
they could have liberal or conservative
pieces of political ideology and

00:20:17.140 --> 00:20:22.550
that wasn't just done at sort of
a whole paragraph or sentence level.

00:20:22.550 --> 00:20:25.930
In particular,
they were able to sort of build this

00:20:25.930 --> 00:20:30.755
hierarchical constituent model where
you could label pieces of rhetoric or

00:20:30.755 --> 00:20:34.790
ideology as conservative or
liberal inside sentences.

00:20:34.790 --> 00:20:36.800
So we have examples like this one.

00:20:36.800 --> 00:20:39.040
They dubbed it the death tax and

00:20:39.040 --> 00:20:43.260
created a big lie about its adverse
effects on small businesses.

00:20:43.260 --> 00:20:49.212
And so the model is picking out death
tax as a term of conservative ideology,

00:20:49.212 --> 00:20:52.839
and its adverse effects
on small businesses.

00:20:52.839 --> 00:20:56.340
So that's conservative ideology language.

00:20:56.340 --> 00:21:01.272
But then when you're sort of putting that
together, interestingly it seems to have

00:21:01.272 --> 00:21:04.397
learned that putting scare
quotes around death tax,

00:21:04.397 --> 00:21:07.466
it then regards that as
a piece of liberal ideology.

00:21:07.466 --> 00:21:12.031
[LAUGH] But beyond that once you get up
to the whole sentence representation,

00:21:12.031 --> 00:21:16.595
when it's they dubbed it the death tax,
that that's then being regarded as

00:21:16.595 --> 00:21:22.010
sort of a piece of liberal ideology, of
representing the opposite point of view.

00:21:22.010 --> 00:21:25.550
Here's one other example that
shows the same kind of thing.

00:21:25.550 --> 00:21:30.140
But taxpayers do know already that TARP,
so that was the recovery

00:21:30.140 --> 00:21:33.990
program beginning of the Obama
administration, was designed in a way that

00:21:33.990 --> 00:21:38.580
allowed the same corporations who were
saved by huge amounts of taxpayer money to

00:21:38.580 --> 00:21:43.428
continue to show the same arrogant traits
that should've destroyed their companies.

00:21:43.428 --> 00:21:44.612
So this bit here,

00:21:44.612 --> 00:21:50.182
the huge amounts of taxpayer money is
being identified as conservative ideology.

00:21:50.182 --> 00:21:54.249
And then it's being embedded in this
sentence which is again showing liberal

00:21:54.249 --> 00:21:55.570
ideology.

00:21:55.570 --> 00:22:03.922
Yeah I cannot explain that,

00:22:03.922 --> 00:22:06.944
I mean it seems like it should've
been colored gray, yeah.

00:22:06.944 --> 00:22:11.155
[LAUGH] These models
aren't always perfect,

00:22:11.155 --> 00:22:15.260
they try with some inference or something.

00:22:15.260 --> 00:22:16.200
Yeah, I don't know.

00:22:17.980 --> 00:22:22.450
And even saved, it's not very clear,
that's what we get out.

00:22:22.450 --> 00:22:26.370
Okay, so, those were our kind of
tree recursive neural networks.

00:22:26.370 --> 00:22:28.840
So I think that theoretically appealing.

00:22:28.840 --> 00:22:31.740
They can be empirically
competitive especially if

00:22:31.740 --> 00:22:34.780
you don't have 100 million
words of data to train on.

00:22:34.780 --> 00:22:38.850
But in most circles they've
sort of fallen out of favor,

00:22:38.850 --> 00:22:41.840
and that's because they have
some big disadvantages.

00:22:41.840 --> 00:22:45.560
So they're often prohibitively slow.

00:22:45.560 --> 00:22:48.180
Most often they've been used
with an external parser,

00:22:48.180 --> 00:22:50.680
although you can use them
to parse as you go, but

00:22:50.680 --> 00:22:55.190
I guess that contributes to
them being prohibitively slow.

00:22:55.190 --> 00:22:59.260
And you could also think that although
there's something nice about these

00:22:59.260 --> 00:23:00.812
tree-structured models,

00:23:00.812 --> 00:23:04.313
you might actually wonder if
are they missing out on something.

00:23:04.313 --> 00:23:08.948
Cuz even though it makes sense that
there's sort of this tree structure of

00:23:08.948 --> 00:23:12.550
language, language does also
have a linear structure.

00:23:12.550 --> 00:23:16.620
You do sort of say these strings
of words that go left to right.

00:23:16.620 --> 00:23:20.440
And something that people have
often observed is if you only

00:23:20.440 --> 00:23:24.730
have tree-structured models, you then
have words that should be very close to

00:23:24.730 --> 00:23:29.820
each other that end up very distant from
each other in any kind of tree structure.

00:23:29.820 --> 00:23:33.850
And so actually the model I'm about to
talk of ends up having both tree structure

00:23:33.850 --> 00:23:34.910
and linear structure.

00:23:38.230 --> 00:23:43.830
Why are the tree-structured models so
badly performing?

00:23:43.830 --> 00:23:47.590
And essentially, the reason is
they're not well suited to the kind of

00:23:47.590 --> 00:23:52.510
batch computations on GPUs, which is
really the sort of centerpiece of what's

00:23:52.510 --> 00:23:57.530
allowed sort of efficient training
of large deep learning models.

00:23:57.530 --> 00:24:01.070
So if you have a sequence model,

00:24:01.070 --> 00:24:03.880
a sequence model can
only have one structure.

00:24:03.880 --> 00:24:05.350
You're going from left to right, and

00:24:05.350 --> 00:24:08.470
you're computing hidden stuff
above each word in turn.

00:24:08.470 --> 00:24:12.410
And because of that, you can take a whole
bunch of sentences, preferably of

00:24:12.410 --> 00:24:17.330
similar lengths, and run them through and
lock step in the sequence model.

00:24:17.330 --> 00:24:18.515
And that's really efficient.

00:24:18.515 --> 00:24:23.616
And the problem is if you wanna do
that with TreeRNNs, that you get this

00:24:23.616 --> 00:24:29.253
input specific structure, that every
sentence has a different structure.

00:24:29.253 --> 00:24:33.696
And so that undermines your ability to do
batched computation because you're still

00:24:33.696 --> 00:24:38.560
trying to construct different structural
units in the different sentences.

00:24:38.560 --> 00:24:43.260
And what happens with GPU code,
if you've got a batch of sentences and

00:24:43.260 --> 00:24:48.340
each one has different structure,
what it does is of your 32 threads,

00:24:48.340 --> 00:24:52.538
one of them is computing and
the other one of 31 are idle.

00:24:52.538 --> 00:24:55.290
Every time that there's
something different being done

00:24:55.290 --> 00:24:59.320
on one thread versus the other threads,
and so that kills all your efficiency.

00:25:00.430 --> 00:25:05.440
So Sam and John and co were sort
of trying to then work out well,

00:25:05.440 --> 00:25:09.300
could we come up with
a different form of model,

00:25:09.300 --> 00:25:14.350
which is at least closer to
the efficiency of a sequence model,

00:25:14.350 --> 00:25:18.670
while still giving us the benefits
of tree-structured representation.

00:25:18.670 --> 00:25:22.626
And that led to the Shift-reduce
Parser-Interpreter Neural Network or

00:25:22.626 --> 00:25:23.990
SPINN model.

00:25:23.990 --> 00:25:28.130
So the base model is
equivalent to a TreeRNN, but

00:25:28.130 --> 00:25:30.450
because it's better for batch computation,

00:25:30.450 --> 00:25:34.110
you can't completely get rid of the fact
that they're different structures.

00:25:34.110 --> 00:25:35.683
It's better for batch computation.

00:25:35.683 --> 00:25:41.082
It can be sort of 25 times faster or
more, depending on the kind of data.

00:25:41.082 --> 00:25:45.035
And it also provide an opportunity
to do a sort of a mixed linear and

00:25:45.035 --> 00:25:49.540
tree-structured model that can
be used alone without a parser.

00:25:49.540 --> 00:25:53.740
So, it's kind of a nice integrated model,
and I just wanna show you a bit of that.

00:25:53.740 --> 00:25:58.265
And the starting point of it is
essentially exactly the same as

00:25:58.265 --> 00:26:03.146
the dependency parsers that we saw
in assignment two and in class.

00:26:03.146 --> 00:26:08.116
So for any piece of tree structure,
you can describe a tree structure

00:26:08.116 --> 00:26:11.720
as uniquely as a sequence of shifts and
reduces.

00:26:11.720 --> 00:26:15.890
So for this structure on the left,
all right, you're gonna take the and cat,

00:26:15.890 --> 00:26:17.430
shift on twice.

00:26:17.430 --> 00:26:20.060
Then you reduce them once
to put them together.

00:26:20.060 --> 00:26:23.730
You shift,
shift to get sat down on the stack.

00:26:23.730 --> 00:26:26.470
You reduce once,
you reduce the second time.

00:26:26.470 --> 00:26:30.870
And so this sequence of shifts and reduces
corresponds to this tree structure.

00:26:30.870 --> 00:26:34.190
And every other sequence
of shifting reduces,

00:26:34.190 --> 00:26:38.272
well, either corresponds to a different
tree structure or is invalid.

00:26:38.272 --> 00:26:40.250
All right, if you start off
trying to reduce before you've

00:26:40.250 --> 00:26:42.240
got nothing on the stack, you can't do it.

00:26:42.240 --> 00:26:47.222
And so what we can do is that we
can build a model that's sort of

00:26:47.222 --> 00:26:52.623
acting like a transition based
parser in shifting and reducing.

00:26:52.623 --> 00:26:55.190
And so it's gonna have a sequence
model in the middle of it.

00:26:55.190 --> 00:26:59.836
But that sequence model is then gonna
be looking at a buffer of words yet

00:26:59.836 --> 00:27:02.610
to be dealt with and maintaining a stack.

00:27:02.610 --> 00:27:06.620
And inside the stack,
there's gonna be composition,

00:27:06.620 --> 00:27:11.990
kind of like a TreeRNN, or not really,
"kinda like" exactly like a TreeRNN,

00:27:11.990 --> 00:27:15.500
which will build TreeRNN representations.

00:27:15.500 --> 00:27:19.620
So at each point,
our LSTM model is tracking along, and

00:27:19.620 --> 00:27:22.615
it's predicting which thing to do,
reduce or

00:27:22.615 --> 00:27:26.320
shift, kind of just like
the dependency parsers that you build.

00:27:26.320 --> 00:27:30.160
And so depending on what it
does when things reduce, you're

00:27:30.160 --> 00:27:34.830
then doing a composition operation on
the stack, which is reshaping the stack.

00:27:36.795 --> 00:27:41.387
And while having this tracking
LSTM is both a simple parser and

00:27:41.387 --> 00:27:45.712
it gives us this kind of sequence
context that we can just,

00:27:45.712 --> 00:27:49.000
sort of, also model the sequence of words.

00:27:50.000 --> 00:27:53.570
So the essence of getting
this to work well is,

00:27:53.570 --> 00:27:56.660
how can you implement
the stack efficiently?

00:27:56.660 --> 00:28:01.541
Because if the stack was just like this,
and you've sort of got a stack for

00:28:01.541 --> 00:28:04.315
each time step, and the stack changes.

00:28:04.315 --> 00:28:09.658
Well, then you'd use a vast amount
of memory in the stack because

00:28:09.658 --> 00:28:14.798
you've got a different complete
stack at each time step.

00:28:14.798 --> 00:28:19.690
And there would also be bad for
achieving this lock step advance that will

00:28:19.690 --> 00:28:23.610
make computation efficient on a GPU.

00:28:23.610 --> 00:28:27.830
So that's sort of bad in various ways, so
the secret of getting it to work pretty

00:28:27.830 --> 00:28:33.300
well is to say actually, for each
sentence, we're only gonna have one stack.

00:28:33.300 --> 00:28:36.260
And we're gonna incrementally
sort of build up representations

00:28:36.260 --> 00:28:38.110
on the stack as we go.

00:28:38.110 --> 00:28:40.966
And so it's using an efficient
kind of data structure,

00:28:40.966 --> 00:28:43.716
which is similar to data
structures used elsewhere.

00:28:43.716 --> 00:28:46.707
They're sometimes called zipper data
structures and things like that.

00:28:46.707 --> 00:28:49.580
They're also using programming
language techniques.

00:28:49.580 --> 00:28:53.510
So, the idea is like this, so we wanna
build up the structure of Spot sat down.

00:28:53.510 --> 00:28:57.913
And so when we start shifting,
we have this one stack,

00:28:57.913 --> 00:29:02.520
which we start shifting words on to,
Spot sat down.

00:29:02.520 --> 00:29:04.970
And so there's only ever one stack, but

00:29:04.970 --> 00:29:08.798
then on this side,
we maintain some backpointers.

00:29:08.798 --> 00:29:12.030
And these backpointers
read from right to left is

00:29:12.030 --> 00:29:14.290
sort of telling us what's
on top of the stack.

00:29:14.290 --> 00:29:17.372
So at the moment,
3 is on top of the stack, followed by 2.

00:29:17.372 --> 00:29:22.547
So then when do a reduce operation,
we don't delete things

00:29:22.547 --> 00:29:27.470
off the stack,
we just write a new thing on to the stack.

00:29:27.470 --> 00:29:33.320
So we compute using our composition
operation in the TreeRNN style.

00:29:33.320 --> 00:29:37.628
A representation of the sat down, and we
simply change our backpointers, and say,

00:29:37.628 --> 00:29:39.950
okay, now, we've got 1, 4 on the stack.

00:29:39.950 --> 00:29:44.600
So 4 is the top of a stack, and
1 is the other thing on the stack.

00:29:44.600 --> 00:29:46.933
And so then when we, again, do a reduce,

00:29:46.933 --> 00:29:50.291
we compute a representation
of the (Spot (sat down)).

00:29:50.291 --> 00:29:54.810
And our backpointers say 5 is
the only active part of the stack.

00:29:54.810 --> 00:29:59.202
And the crucial thing is in these
backpointers allow us to know what's where

00:29:59.202 --> 00:30:01.272
on the stack when we do operations.

00:30:01.272 --> 00:30:06.582
But in terms of our vectors that we're
using inside our forward pass and

00:30:06.582 --> 00:30:11.200
our backward pass,
all of the vectors are in this array here.

00:30:11.200 --> 00:30:14.000
And so we only have sort
of one array per sentence

00:30:14.000 --> 00:30:17.530
that we're sort of
incrementally building up.

00:30:17.530 --> 00:30:18.030
Yes?

00:30:24.626 --> 00:30:25.440
Yes, it does.

00:30:26.730 --> 00:30:27.360
Yeah.

00:30:30.723 --> 00:30:38.370
So, well I mean, it clearly needs to
back propagate through it, right?

00:30:38.370 --> 00:30:40.890
Cuz it's wanting to learn
composition functions, right?

00:30:40.890 --> 00:30:43.420
So it's going to be sort of

00:30:43.420 --> 00:30:48.260
learning a matrix representation as
to how to combine these two words.

00:30:48.260 --> 00:30:52.750
And then your question is how,
and at that point,

00:30:52.750 --> 00:30:58.790
I have to give a sort of an answer of,
gee, this was done with supervision.

00:30:58.790 --> 00:31:03.340
So the reason you'd think it wouldn't be
differentiable is, you'd think, okay,

00:31:03.340 --> 00:31:04.860
there's a choice at different points.

00:31:04.860 --> 00:31:06.176
That's where there's a shift or reduce.

00:31:06.176 --> 00:31:12.720
And if that's a hard decision, then
that introduces a non-differentiability.

00:31:12.720 --> 00:31:17.035
Now, the way we did things in this
was sort of to take it in roundabout

00:31:17.035 --> 00:31:17.910
round that.

00:31:17.910 --> 00:31:23.056
So if we train the model at training
time on sentences that had parses,

00:31:23.056 --> 00:31:24.460
we could compute.

00:31:24.460 --> 00:31:27.966
Then there's no non-differentiability,
and we could

00:31:27.966 --> 00:31:32.940
compute the composition functions, and
we could learn to predict the actions.

00:31:32.940 --> 00:31:37.292
And so in some sense,
that's the easy way to do it,

00:31:37.292 --> 00:31:41.170
which avoids the non-differentiability.

00:31:41.170 --> 00:31:43.988
If you didn't wanna do that and
you actually sorta wanted to say,

00:31:43.988 --> 00:31:46.397
let's have this uncertainty
while you're learning,

00:31:46.397 --> 00:31:48.460
then you'd have to do
something more complex.

00:31:48.460 --> 00:31:50.593
And that then leads into ideas,

00:31:50.593 --> 00:31:55.159
like reinforcement learning or Or
some of the other estimators that

00:31:55.159 --> 00:32:00.015
have been tried recently like the straight
through estimator where effectively,

00:32:00.015 --> 00:32:03.638
although the model makes hard
decisions on the forward pass.

00:32:03.638 --> 00:32:07.719
You're using a kind of a soft
logistic function in the backward

00:32:07.719 --> 00:32:09.879
pass to do the differentiation.

00:32:24.375 --> 00:32:28.737
But doing this kind of model compared
to a traditional LSTM, sorry,

00:32:28.737 --> 00:32:31.521
compared to a traditional
tree with recurrent

00:32:31.521 --> 00:32:34.925
neural network is actually
sort of super efficient.

00:32:34.925 --> 00:32:39.387
So, this sort of shows for
different batch sizes.

00:32:39.387 --> 00:32:44.347
And obviously, it's going well out in
the batch sizes, but sort of shows

00:32:44.347 --> 00:32:49.147
the general point that traditionally
sort of tree recursive models and

00:32:49.147 --> 00:32:53.867
just sort of really inefficient that
you don't kind of get a good batch

00:32:53.867 --> 00:32:58.507
speed up effect, because you got
different operations at every time

00:32:58.507 --> 00:33:03.260
whereas the dotted line is just
an LSTM which is super efficient.

00:33:03.260 --> 00:33:06.666
And although this model starts
to curve up to in the blue and

00:33:06.666 --> 00:33:11.199
it has to curve up, because you are doing
different operations on the stack.

00:33:11.199 --> 00:33:15.230
It's sort of just way, way more efficient
out to quite large batch sizes.

00:33:17.750 --> 00:33:22.049
So Sam was then working to use this
model on natural language inference and

00:33:22.049 --> 00:33:25.643
I think we haven't talked much about
natural language inference, but

00:33:25.643 --> 00:33:29.941
I know it's come up for some of you
that look at the fake news challenge and

00:33:29.941 --> 00:33:33.845
things like that, cuz I and
others have pointed people out of it.

00:33:33.845 --> 00:33:38.372
So the idea of the natural language
inference was you have a piece of text and

00:33:38.372 --> 00:33:42.535
then you have a hypothesis following
it and you're wanting to say,

00:33:42.535 --> 00:33:47.310
whether the hypothesis follows from
the piece of text is an entailment.

00:33:47.310 --> 00:33:50.105
So for
a man rides a bike on a snow covered road,

00:33:50.105 --> 00:33:52.991
then a man is outside,
that's an entailment.

00:33:52.991 --> 00:33:54.880
By the way, it's a contradiction.

00:33:54.880 --> 00:33:57.845
So for
man in an apron is shopping at a market,

00:33:57.845 --> 00:34:01.513
that contradicts a man in
an apron is preparing dinner or

00:34:01.513 --> 00:34:06.990
whether it's neutral which means it's
neither an entailment or contradiction.

00:34:06.990 --> 00:34:10.979
So two female babies eating
chips is neutral with respect

00:34:10.979 --> 00:34:14.883
to two female babies are enjoying chips,
cuz they may or

00:34:14.883 --> 00:34:18.720
may not be enjoying them
even if they're eating them.

00:34:18.720 --> 00:34:23.465
So we collected this large corpus
SNLI of these kind of sentences and

00:34:23.465 --> 00:34:28.464
the way we constructed that was we
started off with one of the vision and

00:34:28.464 --> 00:34:34.337
language databases, MS COCO where there
is a picture and the sentence describe.

00:34:34.337 --> 00:34:38.624
A man rides a bike on a snow covered road
and we wanted to sorta have something

00:34:38.624 --> 00:34:42.502
where the scene was a description
of a picture, cuz that meant there

00:34:42.502 --> 00:34:46.200
was sort of a concrete thing that
the sentences were described.

00:34:46.200 --> 00:34:50.104
So that would hope to avoid there
be uncertainty of reference, but

00:34:50.104 --> 00:34:53.521
then we collected this sort of
hypothesis from Turkers and

00:34:53.521 --> 00:34:56.391
the Turkers weren't
actually shown the photo.

00:34:56.391 --> 00:35:01.677
They were just shown the passage and then
they were meant to generate sentences or

00:35:01.677 --> 00:35:04.787
entailments, neutrals or
contradictions and

00:35:04.787 --> 00:35:09.762
then we're trying to build systems that
do that and this has been a quite good

00:35:09.762 --> 00:35:14.064
task that a whole bunch of other
groups that then try to do better on.

00:35:14.064 --> 00:35:18.337
For what we were doing, we're kind of
interested in this idea of coming up with

00:35:18.337 --> 00:35:22.750
sentence representations by building
them as recursive neural networks.

00:35:22.750 --> 00:35:27.311
And so, the models we were building
was using spin model rules to build

00:35:27.311 --> 00:35:32.572
representation for each sentence and
then running it through comparison neural

00:35:32.572 --> 00:35:38.082
network layers to find the whole sentence
meaning at the center's relationships.

00:35:38.082 --> 00:35:42.006
So, we have one spin model for
representation.

00:35:42.006 --> 00:35:43.799
This sentence one for that one and

00:35:43.799 --> 00:35:46.556
then we're sticking it
through a neural network,

00:35:46.556 --> 00:35:50.419
which is then giving a probability
distribution of the three actions.

00:35:50.419 --> 00:35:54.890
So after having being through the spin
model, we have a couple of fully connected

00:35:54.890 --> 00:35:59.310
neural network layers and then you've
got a Softmax over the three decisions.

00:36:00.810 --> 00:36:03.408
And so,
here are a couple of results from this.

00:36:03.408 --> 00:36:07.570
So there's sort of some previous results,
but

00:36:07.570 --> 00:36:12.667
the sort of things that are kind
of interesting here is this

00:36:12.667 --> 00:36:17.987
is LSTM RNN sequence model
that has accuracy of 80.6%.

00:36:17.987 --> 00:36:21.726
Some of surprisingly and
somewhat disappointingly,

00:36:21.726 --> 00:36:26.449
over this quite large dataset, so
it sort of half a million samples.

00:36:26.449 --> 00:36:33.333
The tree model actually barely does better
than that, so it's performance was 80.9%.

00:36:33.333 --> 00:36:36.879
Whereas relative my argument,
you'd hope for more gains.

00:36:36.879 --> 00:36:40.945
Something that's interesting is having
both at once, the sequence model and

00:36:40.945 --> 00:36:42.620
the tree structure model.

00:36:42.620 --> 00:36:45.492
That actually seems to be rather nice and

00:36:45.492 --> 00:36:48.453
give you kind of a nice
gain on this model.

00:36:48.453 --> 00:36:54.341
I should mention having whole sentence
representation isn't the best way

00:36:54.341 --> 00:36:59.675
to do task like the sort of SNLI,
entailment contradiction task.

00:36:59.675 --> 00:37:04.175
I mean, as comes up In a whole bunch of
these tasks in a lot of recent work that

00:37:04.175 --> 00:37:06.747
if you wanna do even
better at these tasks,

00:37:06.747 --> 00:37:11.250
you'd just do better using attention
models to make alignments at the word

00:37:11.250 --> 00:37:13.990
level between the different sentences.

00:37:13.990 --> 00:37:15.287
And if you do that,

00:37:15.287 --> 00:37:20.333
you can do several percent better as
people have shown in recent results.

00:37:20.333 --> 00:37:25.727
But nevertheless, I still do believe
in the sort of tree structured idea and

00:37:25.727 --> 00:37:30.787
you can sort of see places where
having the tree representations allows

00:37:30.787 --> 00:37:35.890
you to get things right that you just
don't get right in the LSTM model.

00:37:35.890 --> 00:37:40.540
So when there are finer grain
semantic facts, such as negation,

00:37:40.540 --> 00:37:46.140
gymnast completes her floor exercise,
gymnast cannot finish her exercise.

00:37:46.140 --> 00:37:48.429
Well, then then tree
structure model is better.

00:37:48.429 --> 00:37:53.335
It also turns out to be differentially
better when you just have very long

00:37:53.335 --> 00:37:56.463
examples, which the LSTM
gets less good at.

00:37:56.463 --> 00:37:57.780
I finished part one.

00:37:57.780 --> 00:38:01.730
And so, we now hand over
to the research highlight.

00:38:01.730 --> 00:38:02.697
[INAUDIBLE] Zhedi.

00:38:07.056 --> 00:38:12.290
&gt;&gt; So today, we'll discuss how to compose
neural networks for question answering.

00:38:12.290 --> 00:38:17.162
So as a high-level overview,
the papers talk about a compositional,

00:38:17.162 --> 00:38:22.766
attentional for answering questions about
a variety of world representations,

00:38:22.766 --> 00:38:26.685
including images and
also structured knowledge bases.

00:38:26.685 --> 00:38:29.790
The model has two components,
trained jointly.

00:38:29.790 --> 00:38:32.538
The first complement is
a collection of neural

00:38:32.538 --> 00:38:34.799
modules that can be freely composed.

00:38:34.799 --> 00:38:38.815
The figure shows actually four modules,
a lookup module,

00:38:38.815 --> 00:38:42.588
a relate module, an and
module and also a find module and

00:38:42.588 --> 00:38:47.089
the second component is a network
layout predictor that assembles

00:38:47.089 --> 00:38:52.130
modules into complete deep networks
tailored to each question.

00:38:52.130 --> 00:38:55.729
So, our current query is
what cities are in Georgia?

00:38:55.729 --> 00:39:02.752
And the figure shows the network
layout for that particular query.

00:39:02.752 --> 00:39:05.995
Essentially, the model
has two distributions.

00:39:05.995 --> 00:39:09.398
A layout model that chooses a layout for
sentence and

00:39:09.398 --> 00:39:13.267
also an execution model that
applies the network specified

00:39:13.267 --> 00:39:16.610
by a particular layout to
a world representation.

00:39:17.920 --> 00:39:20.210
We'll start from the Layout Model.

00:39:20.210 --> 00:39:24.960
So in order to obtain from the Layout
Model, there are three steps to take.

00:39:24.960 --> 00:39:29.770
First, we want to represent the input
sentence as a dependency tree.

00:39:29.770 --> 00:39:33.580
So the figure shows that dependency tree
for the query, what cities are in Georgia?

00:39:35.020 --> 00:39:38.990
The second step is to associate
fragments of the dependency parse

00:39:38.990 --> 00:39:41.020
with appropriate modules.

00:39:41.020 --> 00:39:47.140
So as we can see in this figure, the find
module is associated with city, the relate

00:39:47.140 --> 00:39:51.990
module is associated with in and the
lookup module is associated with Georgia.

00:39:53.250 --> 00:39:57.310
And the last step is to assemble
fragment into full layouts.

00:39:57.310 --> 00:40:01.380
It should be noted that each sentence
could have multiple layouts.

00:40:01.380 --> 00:40:05.900
So for our example,
one candidate layout is shown there.

00:40:05.900 --> 00:40:10.120
It's a tree structure with the and
module as a root module.

00:40:11.280 --> 00:40:16.530
And now, we will just talk about
how to score the kind of layouts.

00:40:16.530 --> 00:40:19.230
So, in order to score a kind of layout,

00:40:19.230 --> 00:40:23.250
we need to produce an LSTM
representation of the question.

00:40:23.250 --> 00:40:26.277
A feature based representation
of the query and

00:40:26.277 --> 00:40:30.299
pass both representations
through a multilayer perceptron.

00:40:30.299 --> 00:40:34.485
And then, the update to the layout
scoring model at each time step

00:40:34.485 --> 00:40:38.897
is simply the gradient of
the log-probability of the chosen layout,

00:40:38.897 --> 00:40:42.430
scaled by the accuracy of
that layout's predictions.

00:40:44.220 --> 00:40:47.470
And now,
just talk about the execution model.

00:40:47.470 --> 00:40:50.187
So, given the layout as shown in figure b,

00:40:50.187 --> 00:40:55.420
we could basically assemble

00:40:55.420 --> 00:41:01.270
the corresponding modules into a full
neural network, as showing in figure c.

00:41:01.270 --> 00:41:05.149
And we'll just apply it to a knowledge
source as shown in figure d.

00:41:06.170 --> 00:41:09.875
And basically, we should note that
immediate results flow between

00:41:09.875 --> 00:41:12.870
modules until your answer
is produced at the root.

00:41:12.870 --> 00:41:15.740
So in this case,
Atlanta is produced as the root

00:41:15.740 --> 00:41:18.958
which is also the answer to our query,
what cities are in Georgia?

00:41:18.958 --> 00:41:23.535
So essentially,
modules are just like small neural

00:41:23.535 --> 00:41:28.443
components that take inputs or
intermediate results.

00:41:28.443 --> 00:41:33.490
The slide now is showing a lookup
module,whose expression is

00:41:33.490 --> 00:41:34.900
also coloured in red.

00:41:34.900 --> 00:41:39.405
And the lookup module basically
just produces an attention focused

00:41:39.405 --> 00:41:41.467
entirely at the index f(i).

00:41:41.467 --> 00:41:45.403
Where you can just think of
the relationship f between words and

00:41:45.403 --> 00:41:50.245
positions in the input map as some sort
of string matches on database fields.

00:41:51.355 --> 00:41:54.495
And, we also have a relate module.

00:41:54.495 --> 00:41:58.925
So, the relate module is
a softmax that directs focus from

00:41:58.925 --> 00:42:00.905
one region of the input to another.

00:42:02.300 --> 00:42:05.320
The find module is sort of
similar to the relate module.

00:42:05.320 --> 00:42:09.790
It's also a softmax but it computes
a distribution or an indices by

00:42:09.790 --> 00:42:14.960
concatenating the parameter argument with
each position of the input feature map.

00:42:14.960 --> 00:42:18.060
And passing the concatenated to the
vectors through a multilayer perceptron.

00:42:19.210 --> 00:42:22.470
So, the last module is the and Module.

00:42:22.470 --> 00:42:27.471
Typically the and Module is at
the root of our dependency part tree.

00:42:27.471 --> 00:42:32.202
And for the and Module is sort of
similar to a set intersection but

00:42:32.202 --> 00:42:34.740
it's used for attentions.

00:42:34.740 --> 00:42:38.040
And is actually probabilities
are multiplied together.

00:42:38.040 --> 00:42:38.890
For the and Module.

00:42:40.470 --> 00:42:45.890
In order to train an execution model,
we need to maximize the sum of log

00:42:45.890 --> 00:42:51.190
probability of answer labels given a world
representation over a particular layout.

00:42:53.110 --> 00:42:57.827
And so our model actually achieves
State of the art performance,

00:42:57.827 --> 00:43:01.614
on both images and
also structure knowledge basis.

00:43:01.614 --> 00:43:06.628
So, we'll start from looking at how it
performs on a visual question answering

00:43:06.628 --> 00:43:07.620
dataset.

00:43:07.620 --> 00:43:11.818
So, the model is actually able to
figure out what's in the sheep's ear

00:43:11.818 --> 00:43:16.515
is actually a tag, it's gonna small but
it's actually a tag it's also able to

00:43:16.515 --> 00:43:19.746
figure out the color of
the robe the woman is wearing.

00:43:19.746 --> 00:43:24.470
And for the third image,the model
is not able to figure out that

00:43:24.470 --> 00:43:28.570
a man is dragging a boat, but
it says the man is dragging

00:43:28.570 --> 00:43:32.806
a board,which is fairly close,
so it's pretty amazing.

00:43:32.806 --> 00:43:38.040
And in terms of the numbers, so we can
see the model out of all the approaches

00:43:38.040 --> 00:43:43.000
it has the highest test set accuracy on
the available question answering dataset.

00:43:44.700 --> 00:43:48.440
And the model can also do
like general knowledge-based

00:43:48.440 --> 00:43:50.280
type of question answering.

00:43:50.280 --> 00:43:55.980
So in this case, the model is actually
able to figure out what national parks and

00:43:55.980 --> 00:44:00.490
in Florida, and
also whether Key Largo is an island.

00:44:00.490 --> 00:44:05.440
So, also in terms of numbers,
the model does really well.

00:44:05.440 --> 00:44:09.320
It actually beat every other
approach by at least 3%.

00:44:09.320 --> 00:44:11.320
So, this is really amazing model.

00:44:11.320 --> 00:44:15.900
And I will definitely recommend you to
take a look at the paper, and that's it.

00:44:15.900 --> 00:44:16.465
Thanks.

00:44:16.465 --> 00:44:24.410
[APPLAUSE]
&gt;&gt; Okay, thank Zhedi.

00:44:24.410 --> 00:44:29.270
Okay, so then for my remaining
time on the stage this quarter,

00:44:29.270 --> 00:44:34.219
I wanted to just sort of say a bit
more about a couple of things that

00:44:34.219 --> 00:44:38.161
I think of vaguely come up
at some point or another,

00:44:38.161 --> 00:44:41.857
but haven't really very
prominently come up.

00:44:41.857 --> 00:44:47.670
I mean, the first one is just
a very brief cameo appearance.

00:44:47.670 --> 00:44:52.690
But, I just so wanted to say a fraction
more about pointer copying models,

00:44:52.690 --> 00:44:55.060
just so that that idea's in your head.

00:44:55.060 --> 00:44:59.956
So, Richard sort of talked about a version
of these a long time ago, when he

00:44:59.956 --> 00:45:05.119
sort of told a bit about his group's
work on Pointer Sentinel mixture models.

00:45:05.119 --> 00:45:09.175
And I just want to sort of repeat
the idea of this as sort of one of

00:45:09.175 --> 00:45:14.173
the other ideas that people have been
using a bit in recent neural networks.

00:45:14.173 --> 00:45:19.890
So, one of the central papers is this
one on pointing the unknown words.

00:45:19.890 --> 00:45:24.370
And this diagram is a bit
confusing to read because for

00:45:24.370 --> 00:45:27.060
some reason they sort of did it backwards.

00:45:27.060 --> 00:45:31.830
So the source is on the right side,
and the target is on the left side.

00:45:31.830 --> 00:45:35.418
But, the idea here is so
that for the source sequence,

00:45:35.418 --> 00:45:40.020
right, that we're assuming that
we've run a bidirectional LSTM, or

00:45:40.020 --> 00:45:43.064
something like that,
over the source words.

00:45:43.064 --> 00:45:47.630
And then, we're in the kind of
usual kind of generational LSTM,

00:45:47.630 --> 00:45:51.980
where what we've done is sort of,
we've got some hidden state.

00:45:51.980 --> 00:45:58.000
And then, we go to be sorta starting
to generate the next word based on it.

00:45:59.190 --> 00:46:02.700
And then how is that gonna be done?

00:46:02.700 --> 00:46:06.910
So, we've had already the ideas
that you can just sort of

00:46:06.910 --> 00:46:11.150
generate from the hidden state,
based on a softmax.

00:46:11.150 --> 00:46:16.100
And we've had the idea that you could kind
of use an attention model to sort of look

00:46:16.100 --> 00:46:21.790
back at the source and coding, and
sort of use that as input to your softmax.

00:46:21.790 --> 00:46:27.070
And those are both good ideas, but in both
those cases at the end of the day you're

00:46:27.070 --> 00:46:33.710
so doing a softmax over your vocabulary,
based on some hidden state over here.

00:46:33.710 --> 00:46:38.970
And the suggestion that's come up is that,
at least in some applications,

00:46:38.970 --> 00:46:42.300
and those include machine translation.

00:46:42.300 --> 00:46:45.370
But also things like text summarization.

00:46:45.370 --> 00:46:49.840
Another thing that might be a good
idea is if you could just decide

00:46:49.840 --> 00:46:54.820
that what you wanna do is copy some
word that came from the source sequence.

00:46:54.820 --> 00:46:58.380
So, rather than having to have a hidden
state from which you can generate it from

00:46:58.380 --> 00:47:02.270
your softmax, you could just say
a good idea in this position

00:47:02.270 --> 00:47:05.240
would just be to copy word
17 from the source sequence.

00:47:05.240 --> 00:47:08.920
And that might be appropriate if
it's something like a name that you

00:47:08.920 --> 00:47:13.810
might have a rare name in the input or
some kind of lone word or

00:47:13.810 --> 00:47:18.310
anything like that, that just makes
sense to copy towards the output.

00:47:18.310 --> 00:47:24.230
And so, from the hidden state, you're
sort of having a binary logistic model,

00:47:24.230 --> 00:47:28.040
which is saying to what
extent do you want to

00:47:28.040 --> 00:47:33.640
generate from the vocabulary softmax,
versus do you want to point and copy?

00:47:33.640 --> 00:47:38.420
And so this choice here, you could
make it a hard choice, which would

00:47:38.420 --> 00:47:42.120
have the same kind of complications for
making it differentiable.

00:47:42.120 --> 00:47:45.600
But commonly, the way it's been done
is just being made as a soft choice.

00:47:45.600 --> 00:47:48.649
So with probability p,
you're generating for

00:47:48.649 --> 00:47:51.704
the vocabulary softmax of probability 1-p.

00:47:51.704 --> 00:47:57.620
You're kind of gonna look in to do another
attention distribution over the source and

00:47:57.620 --> 00:48:01.550
then you're gonna sort of just be copying
a word where you're placing attention.

00:48:01.550 --> 00:48:05.372
And so, that's been kind of
quite an effective mechanism

00:48:05.372 --> 00:48:08.180
to sort of just be able
to just copy words for

00:48:08.180 --> 00:48:12.710
the input to the output which has sort
of helped for several applications.

00:48:12.710 --> 00:48:17.765
So in their paper, where they
are using it for machine translation,

00:48:17.765 --> 00:48:22.472
having this kind of pointer model
was giving them about three and

00:48:22.472 --> 00:48:26.850
a half BLEU points of performance,
which is a ton, right?

00:48:26.850 --> 00:48:29.240
A lot of the time in MT,
if you can get a BLEU point,

00:48:29.240 --> 00:48:30.510
you think you're doing really well.

00:48:30.510 --> 00:48:33.780
So, that was sort of
a very big improvement.

00:48:33.780 --> 00:48:38.525
Another place where it's very effective
is in summarization models, cuz a lot of

00:48:38.525 --> 00:48:42.856
the time in the summarization models you
are wanting to sort of copy names of

00:48:42.856 --> 00:48:47.491
people, and places and things like that
straight from the input to the output.

00:48:47.491 --> 00:48:51.990
But just to give a cautionary
note on the other side.

00:48:51.990 --> 00:48:56.660
Interestingly, in the sort of, the Google
Neural Machine Translation paper that came

00:48:56.660 --> 00:49:01.900
out recently going along with
their sort of recent release of

00:49:01.900 --> 00:49:06.530
their big Neural Machine Translation
Models on the live servers,

00:49:06.530 --> 00:49:11.750
that they sort of state that they
weren't having much success from that.

00:49:11.750 --> 00:49:16.660
So they say in principle, you can train
a copy model that this approach is both

00:49:16.660 --> 00:49:21.000
unreliable at scale, the attention
mechanism is unstable when the network

00:49:21.000 --> 00:49:25.610
is deep, and copying may not always
be the best strategy for rare words.

00:49:25.610 --> 00:49:28.150
So, they don't seem to found
successful for that method.

00:49:28.150 --> 00:49:32.630
So I guess like all things you can try and
see if it works for you.

00:49:33.870 --> 00:49:38.880
Okay, so then the final thing I
wanted to talk about was a bit about

00:49:38.880 --> 00:49:43.070
work that goes below the word level.

00:49:43.070 --> 00:49:47.450
So, right from the beginning of this
course where we started off with

00:49:47.450 --> 00:49:49.540
is that we had words and

00:49:49.540 --> 00:49:53.840
we were gonna want to build
distributed representations for words.

00:49:53.840 --> 00:49:59.100
And that then led into Word2Vec and
all the stuff we talked about there.

00:49:59.100 --> 00:50:03.370
And I think it's fair
to say that sort of for

00:50:03.370 --> 00:50:08.200
around the sort of period of 2011,
12, 13, 14, that's what everybody

00:50:09.250 --> 00:50:13.070
was doing, and that's where we
still start for this course.

00:50:13.070 --> 00:50:16.920
But in the last couple of years,
there's now started to be a lot of work

00:50:16.920 --> 00:50:19.580
where people are going
below the word level.

00:50:19.580 --> 00:50:22.240
And so, I just wanted to
show a bit more about that.

00:50:22.240 --> 00:50:23.220
Before doing it,

00:50:23.220 --> 00:50:29.000
just sort of a couple of slides of
sort of context of how languages work.

00:50:29.000 --> 00:50:32.860
So most of the time in NLP,
with deep learning,

00:50:32.860 --> 00:50:36.420
we're sort of working with
language in its written form.

00:50:36.420 --> 00:50:38.930
It's easily processed, found data.

00:50:38.930 --> 00:50:42.130
And so, something that you're at least
sort of vaguely have in your head,

00:50:42.130 --> 00:50:46.160
that human writing systems aren't
all the same kind of system.

00:50:46.160 --> 00:50:49.720
So, that they sort of,
various to their nature.

00:50:49.720 --> 00:50:54.340
So, that there are many writing
systems that are alphabetic, and

00:50:54.340 --> 00:50:55.790
basically phonemic.

00:50:55.790 --> 00:50:59.430
Which is sorta if you see what
the letter is how to pronounce that.

00:50:59.430 --> 00:51:06.520
So, sorta something like Italian is
also a fairly phonemic writing system.

00:51:06.520 --> 00:51:09.540
This example here is from
an Australian language, Wambaya.

00:51:09.540 --> 00:51:12.100
But sort of it's jiyawu ngabulu,

00:51:12.100 --> 00:51:16.080
that it's sort of the sounds
are just read off the letters.

00:51:16.080 --> 00:51:19.620
So, that contrasts with something like
English which is an alphabetic writing

00:51:19.620 --> 00:51:24.840
system that as anyone who's spent
time learning English knows, that

00:51:24.840 --> 00:51:30.270
the correspondence between letters and
sounds is much more convoluted in English.

00:51:30.270 --> 00:51:33.950
So when you have a word like
thorough It's sort of not really

00:51:33.950 --> 00:51:38.110
spelled out how it sounds, that sort of
this complex historical stuff going on.

00:51:38.110 --> 00:51:38.720
But then,

00:51:38.720 --> 00:51:42.570
there are other language systems that
sort of represent slightly bigger units.

00:51:42.570 --> 00:51:47.560
So some languages represent syllabic units
or moraic units, so you've kind of got

00:51:47.560 --> 00:51:52.560
a syllable Like yawn or gargle, something
that's being represented by one letter.

00:51:52.560 --> 00:51:55.320
And this is Inuktitut from Canada.

00:51:55.320 --> 00:51:59.930
And then, there are sort of
the syllabic kind of languages.

00:51:59.930 --> 00:52:05.660
There sort of two kinds there are ones
where you're having a sort of individual

00:52:05.660 --> 00:52:10.800
letter is being used still on the basis
of it's sound like an alphabetic form.

00:52:10.800 --> 00:52:13.300
And then,
you have ideographic languages for

00:52:13.300 --> 00:52:16.860
which by far the dominant
sample is Chinese, where

00:52:16.860 --> 00:52:21.370
you're having the individual character
representing a semantic components.

00:52:21.370 --> 00:52:25.396
So that also has a pronunciation, but
there will be lots of characters,

00:52:25.396 --> 00:52:30.083
which have the same pronunciation, but
have different meanings attached to them.

00:52:30.083 --> 00:52:33.760
And then, you get writing systems
that are sorta a mixture of these.

00:52:33.760 --> 00:52:38.170
So something like modern Japanese, you
both have characters like these two and

00:52:38.170 --> 00:52:41.050
that one, which are, well, moraic,

00:52:41.050 --> 00:52:45.530
that they're sorta being kind
of either just a vowel or

00:52:45.530 --> 00:52:50.320
just a syllabic consonant or
a vowel-consonant-vowel form.

00:52:50.320 --> 00:52:54.770
But then, you also get the kind of
idiographic characters that Japanese

00:52:54.770 --> 00:52:55.770
borrows from Chinese.

00:52:55.770 --> 00:52:59.680
So, you get these sort of different
forms of writing systems.

00:52:59.680 --> 00:53:06.100
And then there's this question of, well do
your writing systems differentiate words?

00:53:06.100 --> 00:53:12.810
So, some writing systems differentiate
words explicitly and some don't.

00:53:12.810 --> 00:53:17.130
So, again, Chinese is a famous
example of a language that does not

00:53:17.130 --> 00:53:22.870
have any word boundaries marked in the
text, you just get a string of characters.

00:53:22.870 --> 00:53:29.050
Interestingly something that
most people are less aware of,

00:53:29.050 --> 00:53:34.170
is that if you actually go back to Ancient
Greek as it was written by Ancient Greeks.

00:53:34.170 --> 00:53:39.109
Ancient Greek was a language that was
written with no word segmentation.

00:53:39.109 --> 00:53:41.990
That the letters were
just a continuous stream.

00:53:41.990 --> 00:53:44.974
So it's sort of putting
spaces between words and

00:53:44.974 --> 00:53:49.706
Ancient Greek was actually something
that was first done by medieval Monks to

00:53:49.706 --> 00:53:52.348
make ancient Greek easier to understand.

00:53:52.348 --> 00:53:55.207
So, it was also a no word
segmentation language.

00:54:01.263 --> 00:54:03.880
So the question, was Latin segmented?

00:54:03.880 --> 00:54:07.320
So, it actually sort of varied.

00:54:07.320 --> 00:54:12.723
You can certainly find unsegmented
Latin but it started, they didn't use

00:54:12.723 --> 00:54:18.828
spacers but it started to become common to
sort of chisel a little dot between words.

00:54:18.828 --> 00:54:22.538
And so then, there was
a representation of word segmentation.

00:54:22.538 --> 00:54:23.038
Yeah.

00:54:26.087 --> 00:54:28.911
Okay, and
then when you do have word segmentation,

00:54:28.911 --> 00:54:32.940
there's still sort of some variety
as to how much you segment things.

00:54:32.940 --> 00:54:37.030
And so, there are kind of a couple of
big parameters of variation of that.

00:54:37.030 --> 00:54:40.080
So, a lot of languages sort of have little

00:54:40.080 --> 00:54:42.460
words that have sort
of functional meaning.

00:54:42.460 --> 00:54:47.540
And languages vary, how much they
separate them, or keep them together.

00:54:47.540 --> 00:54:52.650
So, a language like French has
these sort of little clitics for

00:54:52.650 --> 00:54:55.118
the first sort of pronouns.

00:54:55.118 --> 00:55:01.170
Je vous ai, but they sort of the vous for
you is represented with a space, even

00:55:01.170 --> 00:55:06.370
though it sort of joins on phonologically,
whereas other languages, such as Arabic,

00:55:06.370 --> 00:55:12.160
will sort of take the similar kinds
of clitic pronouns, like we and it.

00:55:12.160 --> 00:55:16.430
And sort of just glum them all together
and produce a slightly larger word

00:55:16.430 --> 00:55:20.920
even though it's sort of different content
words are space separated in Arabic.

00:55:20.920 --> 00:55:24.690
And the other languages vary
is when you have compounds.

00:55:24.690 --> 00:55:28.890
So, all languages pretty much have
lots of compounds like life insurance,

00:55:28.890 --> 00:55:30.440
company employee.

00:55:30.440 --> 00:55:34.715
But in English, we handily still
have the spaces between the words

00:55:34.715 --> 00:55:39.216
in Life Insurance Company employees,
whereas, once you go to German,

00:55:39.216 --> 00:55:43.510
that's then being written as one
big unsegmented word like that.

00:55:45.526 --> 00:55:51.930
Okay, so that's the sorta context for
then going below the word level.

00:55:51.930 --> 00:55:55.130
And so, there are lots of reasons
to want to go below the level,

00:55:55.130 --> 00:55:57.670
so if you'd like to handle a very large,

00:55:57.670 --> 00:56:02.560
open vocabulary It's kind of unappealing
if you need a word vector for every word.

00:56:02.560 --> 00:56:07.410
And that's especially true in a lot
of languages where they have a lot of

00:56:07.410 --> 00:56:13.100
different word forms that represent
different derivational morphologies,

00:56:13.100 --> 00:56:17.520
that's sort of relationships where
you sort of have causatives or

00:56:17.520 --> 00:56:18.790
possessives and other things.

00:56:18.790 --> 00:56:22.160
Things that joined onto words,
an inflectional relationship.

00:56:22.160 --> 00:56:25.662
So you have different forms,
a person, number, and agreement and

00:56:25.662 --> 00:56:27.088
things like that.

00:56:27.088 --> 00:56:32.250
So here's a very long
check word to the worst

00:56:32.250 --> 00:56:36.990
farmable one, which has a lot
of morphology join together.

00:56:36.990 --> 00:56:38.800
So if you have languages like that,

00:56:38.800 --> 00:56:41.870
it's sort of unappealing to have
word vectors for every word.

00:56:41.870 --> 00:56:46.370
Another place that you find things
happening everywhere over modern social

00:56:46.370 --> 00:56:52.410
meeting, is people use creative spellings
to express a little bit more emotion.

00:56:52.410 --> 00:56:56.580
And so then you have words like good,
with a lot of o's in it.

00:56:56.580 --> 00:57:01.202
Which probably isn't in
the vocabulary of your system.

00:57:01.202 --> 00:57:05.404
And then when you want to do
other tasks like translation,

00:57:05.404 --> 00:57:09.004
you often would like to go
below the word level, so

00:57:09.004 --> 00:57:13.977
if you'd like to Christopher into Czech,
you might want it to know it

00:57:13.977 --> 00:57:19.685
sort of translates into something
that's sort of related, Krystof.

00:57:19.685 --> 00:57:23.105
And that sort of makes sense if
you're at the character level, but

00:57:23.105 --> 00:57:25.485
not if you just have sort
of these individual words.

00:57:25.485 --> 00:57:28.585
And so
there's a question of how you can start to

00:57:28.585 --> 00:57:30.215
deal with some of these phenomena.

00:57:31.590 --> 00:57:36.810
So in traditional linguistics, the
smallest semantic units were morphemes.

00:57:36.810 --> 00:57:40.650
So big words would be divided up
into their individual morphemes, so

00:57:40.650 --> 00:57:46.100
if you had a word like unfortunately,
so if it has a root of fortune and

00:57:46.100 --> 00:57:50.020
then you add on the derivational
ending ate to get fortunate.

00:57:50.020 --> 00:57:54.510
You add on another derivational ending
to get, unfortunate and then you add on

00:57:54.510 --> 00:58:00.330
a final derivational ending, ly, to turn
into an adverb and you get, unfortunately.

00:58:00.330 --> 00:58:06.040
So this kind of use of morphology has been
very little studied in deep learning,

00:58:06.040 --> 00:58:10.880
though actually Mitchell and me had
a paper with Tom Luong a few years back.

00:58:10.880 --> 00:58:14.830
Or we try to use the same kind
of tree structured models to

00:58:14.830 --> 00:58:19.299
build up representations of
morphologically complex words.

00:58:19.299 --> 00:58:24.180
Most of the time people haven't done that,
but have done simple things.

00:58:24.180 --> 00:58:28.300
So a common alternative is to
work with character n-grams.

00:58:28.300 --> 00:58:33.180
That's something that actually
has a very long history, so

00:58:33.180 --> 00:58:37.000
back in the earlier age of
neural networks, Rumelhart and

00:58:37.000 --> 00:58:41.940
McClelland proposed a representation
that they humorously called

00:58:41.940 --> 00:58:47.260
wickelphones, and what wickelphones were,
were sort of triples of letters.

00:58:47.260 --> 00:58:51.950
And so they proposed this model in
some of their language models for

00:58:51.950 --> 00:58:57.060
representing inflections, and learning
inflectional forms of verbs in English,

00:58:57.060 --> 00:59:01.940
which was a model that at the time
linguists reacted to very negatively.

00:59:01.940 --> 00:59:05.120
But it's an idea that's kind of lived on,
so

00:59:05.120 --> 00:59:07.445
there's much more recent
work from Microsoft.

00:59:07.445 --> 00:59:12.175
Where essentially they're sort of
using the same kind of letter triples.

00:59:12.175 --> 00:59:15.375
The picture from the bottom is
actually from Rumelhart and

00:59:15.375 --> 00:59:18.675
McClelland's work, and so
when you start off with a word,

00:59:18.675 --> 00:59:23.780
they represent it internally
as a set of letter triples.

00:59:23.780 --> 00:59:29.910
So effectively the intermediate layer
encoder is sort of turning any word

00:59:30.920 --> 00:59:35.740
into just the set of letter triples
that are contained inside it and

00:59:35.740 --> 00:59:39.920
so that gives kind of a flat
representation without needing a sequence.

00:59:39.920 --> 00:59:43.310
The nimbleness captures most of
the structure of the word and

00:59:43.310 --> 00:59:45.850
it sort of works from there and
can then generate another word.

00:59:45.850 --> 00:59:49.030
And this is sort of doing it explicitly

00:59:49.030 --> 00:59:53.500
with character trigrams which
are then given vector encoding.

00:59:53.500 --> 00:59:58.270
But in some sense the idea is sort of
related to when we looked at convolutional

00:59:58.270 --> 01:00:03.530
layers briefly because they were also kind
of combining together multiple letters.

01:00:03.530 --> 01:00:07.910
But it was sort of doing it, after it
being turned into a continuous space.

01:00:07.910 --> 01:00:12.350
Rather than just separately learning
continuous vector representation for

01:00:12.350 --> 01:00:14.250
each letter trigram.

01:00:14.250 --> 01:00:19.629
And it seems to have been shown that
using these kind of character n-gram

01:00:19.629 --> 01:00:25.362
ideas whether like these wickelphones or
using convolutions can in practice

01:00:25.362 --> 01:00:31.480
give you a lot of the gains of morphemes
with perhaps less suffering, okay.

01:00:31.480 --> 01:00:36.410
And so what people have found
is that you can generate

01:00:36.410 --> 01:00:40.630
word embeddings by building them
up from character embeddings.

01:00:40.630 --> 01:00:44.790
And so, if you're able to do that,
you can then generate an embedding for

01:00:44.790 --> 01:00:49.590
any new word you see when someone sort of
has some weird sequence of letters and

01:00:49.590 --> 01:00:51.340
your social media text.

01:00:51.340 --> 01:00:57.415
You can just go letter by letter and
generate a word representation for it.

01:00:57.415 --> 01:01:02.590
And in a way that should work better,
because it has some of the gains

01:01:04.070 --> 01:01:06.735
that in general people argue for
deep learning.

01:01:06.735 --> 01:01:10.360
That since you can then kind
of have words with similar

01:01:10.360 --> 01:01:14.070
spellings should have similar embeddings,
so to the extent that they're so

01:01:14.070 --> 01:01:18.470
different morphological forms or
related words by derivation.

01:01:18.470 --> 01:01:21.240
You should be able to
capture that commonality

01:01:21.240 --> 01:01:23.210
into your character embeddings.

01:01:23.210 --> 01:01:26.030
And if you're using these kind
of character level models

01:01:26.030 --> 01:01:29.880
then you kind of don't have any
problems ever with unknown words

01:01:29.880 --> 01:01:32.310
because you can just represent them all.

01:01:32.310 --> 01:01:36.670
And so using character level
models in the last couple of years

01:01:36.670 --> 01:01:40.990
has just proven to work super,
super successfully.

01:01:40.990 --> 01:01:45.920
If I make an admission now, I mean, when
the idea first came up of using character

01:01:45.920 --> 01:01:51.630
level models, I was really pretty
skeptical as to whether it would work.

01:01:51.630 --> 01:01:55.310
So from the traditional
linguistic perspective but

01:01:55.310 --> 01:01:59.080
the idea had always been, well,
yeah, we have sort of morphemes.

01:01:59.080 --> 01:02:03.440
We have those units like fortune,
and fortunate, and unfortunate,

01:02:03.440 --> 01:02:08.050
that those morphemes like un,
fortune, and ate, they have meaning.

01:02:08.050 --> 01:02:11.720
But if you just have letters like a "u".

01:02:11.720 --> 01:02:15.900
Or a "f" they don't seem
to have any meaning.

01:02:15.900 --> 01:02:20.760
So, it sort of seemed a little
bit difficult to imagine

01:02:20.760 --> 01:02:23.840
that you can learn a vector
representation for f and

01:02:23.840 --> 01:02:27.250
a vector representation for u and
a vector representation for n.

01:02:27.250 --> 01:02:30.220
And then you can start
composing them together, and

01:02:30.220 --> 01:02:34.030
getting useful semantic
representations for words.

01:02:34.030 --> 01:02:37.170
But what people have found is actually,

01:02:37.170 --> 01:02:42.860
some of these modern models like these
LSTM models have sufficiently powerful

01:02:42.860 --> 01:02:47.760
composition functions that actually you
can learn very good word representations.

01:02:47.760 --> 01:02:50.430
By building them up letter by letter.

01:02:50.430 --> 01:02:53.110
So, here's a kind of
a clean version of this.

01:02:53.110 --> 01:02:59.330
And so this was some work that was
done by a bunch of people at CMU.

01:02:59.330 --> 01:03:01.040
Chris Dyer and colleagues.

01:03:01.040 --> 01:03:05.180
And so, in some sense you're
doing the obvious thing.

01:03:05.180 --> 01:03:09.710
So for word,
what you're doing is you're running

01:03:09.710 --> 01:03:14.010
a bidirectional LSTM over
the characters of the words.

01:03:14.010 --> 01:03:17.470
And so you're learning
character representations and

01:03:17.470 --> 01:03:22.560
then sort of hidden representations
using the LSTM above those characters.

01:03:22.560 --> 01:03:27.920
Then you append the two representations
at the end of those sequences and

01:03:27.920 --> 01:03:31.730
just say that's your word
representation for unfortunately.

01:03:31.730 --> 01:03:36.320
And then, to sort of train this whole
model you're then embedding it and another

01:03:36.320 --> 01:03:41.700
LSTM which is then going to give you
your sequence over words for some task.

01:03:41.700 --> 01:03:46.070
So we then have a word level LSTM
which is doing something like

01:03:46.070 --> 01:03:49.480
predicting the sequence of
words on the bank was closed.

01:03:49.480 --> 01:03:50.800
So you sort of have these.

01:03:50.800 --> 01:03:56.080
Doubly recurrent models that are nested
hierarchically inside each other's,

01:03:56.080 --> 01:03:58.950
and so they tested out this model for
two tasks.

01:03:58.950 --> 01:04:01.734
One was just the language
modeling task and

01:04:01.734 --> 01:04:07.003
one was the part of speech tagging task,
and it just worked super successfully.

01:04:07.003 --> 01:04:12.989
So in particular, they were able to show
better results for part of speech tagging

01:04:12.989 --> 01:04:18.950
than people had shown with word level
neural part of speech tagging models.

01:04:18.950 --> 01:04:23.307
And so that makes sense, if you're sort
of say well, this is because we can share

01:04:23.307 --> 01:04:27.620
the similarities between words in the
character level model that makes sense.

01:04:27.620 --> 01:04:32.066
And it worked though initially as I say,
I was kind of surprised.

01:04:32.066 --> 01:04:33.952
Cuz it actually just sort surprised me,

01:04:33.952 --> 01:04:37.730
you can learn effective enough character
level embeddings to be able to do this.

01:04:39.620 --> 01:04:44.840
I think that's a nice clean version
of this model that works very nicely.

01:04:44.840 --> 01:04:48.880
I mean effectively different people
have put character level models

01:04:48.880 --> 01:04:50.920
in in all sorts of ways.

01:04:50.920 --> 01:04:54.590
So there were slightly earlier
work that was calculating

01:04:54.590 --> 01:04:59.410
convolutions over characters
to generate word embeddings.

01:04:59.410 --> 01:05:03.291
I think this paper
the character-aware neural

01:05:03.291 --> 01:05:07.990
language models came up
earlier as a spotlight paper.

01:05:07.990 --> 01:05:12.551
And so this is a more recent and
very complex model where they're first

01:05:12.551 --> 01:05:16.969
of all doing convolutions and
then they've got a highway network.

01:05:16.969 --> 01:05:21.029
And then they've got an LSTM network,
cuz it's sort of very complex, but again,

01:05:21.029 --> 01:05:22.596
it's a character level model.

01:05:22.596 --> 01:05:26.807
You can also do simple things,
so just going right back to

01:05:26.807 --> 01:05:31.722
the word2vec beginnings that you
can sort of start off with a model

01:05:31.722 --> 01:05:37.330
that has exactly the same objective and
loss function as word2vec.

01:05:37.330 --> 01:05:41.980
And just say well I'm going to at
the top level train my word2vec model.

01:05:41.980 --> 01:05:44.690
But rather than storing a vector for

01:05:44.690 --> 01:05:49.830
each word and updating that,
I'm going to kind of like the CMU work.

01:05:49.830 --> 01:05:56.467
Say, I'm generating the representation for
each word using a character level LSTM.

01:05:56.467 --> 01:06:01.911
And then I'm feeding that into my skip
gram negative sampling algorithm and

01:06:01.911 --> 01:06:04.220
that works very nicely as well.

01:06:05.380 --> 01:06:09.806
So lots of stuff of that sort, and
so yeah, so these days kind of,

01:06:09.806 --> 01:06:14.560
there's just a lot of action and
use of these character level models.

01:06:14.560 --> 01:06:18.714
And sort of many people are thinking
it's sort of less necessary to do word

01:06:18.714 --> 01:06:19.520
level stuff.

01:06:21.270 --> 01:06:26.161
And so for my final bits, I just sort of
thought I'd show you again then back to

01:06:26.161 --> 01:06:30.527
a bit of neural machine translation
of a couple of the ways that people

01:06:30.527 --> 01:06:35.296
are incorporating these character
level models or sub word level models.

01:06:35.296 --> 01:06:41.058
So there are sort of two trends really,
one weight is to sort of build neural

01:06:41.058 --> 01:06:48.040
machine translation models which have
sub-word units, but the same architecture.

01:06:48.040 --> 01:06:51.690
And the other way is to have sort
of architectures explicitly put in

01:06:51.690 --> 01:06:52.910
characters.

01:06:52.910 --> 01:06:56.400
And so I just wanna show you
one example of both of those.

01:06:58.310 --> 01:07:01.430
So one of idea that's
been quite prominent,

01:07:01.430 --> 01:07:05.630
which sort of gets back more to
having something like morphology,

01:07:05.630 --> 01:07:09.530
is this notion that's referred
to as byte pair encoding.

01:07:09.530 --> 01:07:13.360
And the name byte pair encoding
is kind of a misnomer, but

01:07:13.360 --> 01:07:16.510
it sort of comes from
the inspiration of this algorithm.

01:07:16.510 --> 01:07:20.940
So byte pair encoding is
a compression algorithm that has been

01:07:20.940 --> 01:07:24.240
developed quite separately,
just as a way to compress stuff.

01:07:24.240 --> 01:07:28.830
And the idea of byte pair encoding is
that you're learning a code book for

01:07:28.830 --> 01:07:33.730
compression by allocating
codes to common sequences.

01:07:33.730 --> 01:07:40.969
So you look for common pairs of bytes and
you allocate a code book place to them.

01:07:40.969 --> 01:07:45.213
And so someone had the idea,
maybe we could run this algorithm, but

01:07:45.213 --> 01:07:48.880
do it with character
ngrams rather than byte.

01:07:48.880 --> 01:07:52.843
And so this is how it works, so you
stop with the vocabulary of characters.

01:07:52.843 --> 01:07:57.150
And then you replace the most frequent
character ngram with the new ngram.

01:07:57.150 --> 01:08:02.108
So if our dictionary is like this we have
the words low, lower, newest, wildest.

01:08:02.108 --> 01:08:03.912
And they occur that often,

01:08:03.912 --> 01:08:09.570
we can then say well we start with a basic
character in the vocab, that's them.

01:08:09.570 --> 01:08:13.679
And so now, we're gonna look for
the commonest character bigram, and

01:08:13.679 --> 01:08:16.571
their allocated as a new
thing in our vocabulary.

01:08:16.571 --> 01:08:20.159
So here,
the commonest character bigram is es,

01:08:20.159 --> 01:08:24.454
that occurs nine times, so
we add that to our vocabulary.

01:08:24.454 --> 01:08:28.592
And then we look again and
say est that also occurs nine times,

01:08:28.592 --> 01:08:31.410
let's add that to our vocabulary.

01:08:31.410 --> 01:08:33.971
Then we ask what's still commonest?

01:08:33.971 --> 01:08:38.891
That's l, o seven times,
add that to our vocabulary, and so

01:08:38.891 --> 01:08:42.028
you keep on doing this up to some limit.

01:08:42.028 --> 01:08:49.315
So you sort of say, okay, the size
vocabulary I wanna learn is 30,000 words.

01:08:49.315 --> 01:08:54.208
And so some of the things that's in your
vocabulary will actually end up as words.

01:08:54.208 --> 01:08:57.972
Cuz you will have sort of vocabulary
items like the and in and of,

01:08:57.972 --> 01:08:59.850
if you're doing English.

01:08:59.850 --> 01:09:03.895
But the other things that you're
getting are just letters and

01:09:03.895 --> 01:09:08.262
word pieces that are kind of things
that are pieces of morphology.

01:09:08.262 --> 01:09:11.467
So it's kind of an empirical
way to learn a vocab.

01:09:11.467 --> 01:09:16.179
And again you have problem with unknown
words, cuz at the end of the day you

01:09:16.179 --> 01:09:20.750
have these sort of individual letter
that are part of your vocabulary.

01:09:20.750 --> 01:09:24.191
And so you can always do
things just with the letters.

01:09:24.191 --> 01:09:29.056
And so that kind of automatically
decide of vocab which is sort of

01:09:29.056 --> 01:09:33.394
no longer word base in
the conventional linguistic way.

01:09:33.394 --> 01:09:36.577
When you wanna translate
the piece of text,

01:09:36.577 --> 01:09:40.801
you just use that vocab and
you greedily chop from the left.

01:09:40.801 --> 01:09:44.862
You chop off pieces that
you can find in your vocab,

01:09:44.862 --> 01:09:47.798
preferring the longest ones first.

01:09:47.798 --> 01:09:51.728
So it's a very simple way
to maintain a small vocab.

01:09:51.728 --> 01:09:54.881
But it was actually employed very
successfully by these people

01:09:54.881 --> 01:09:56.674
at the university of Edinborough.

01:09:56.674 --> 01:10:02.746
And so at the 2016 workshop on machine
translation a number of the language

01:10:02.746 --> 01:10:08.185
pairs were won by the Edinborough
team using this byte pair encoding.

01:10:08.185 --> 01:10:11.575
And actually, it turns out that again,
if we say, gee,

01:10:11.575 --> 01:10:15.532
what is Google doing in their
neural machine translation system,

01:10:15.532 --> 01:10:20.284
that they're actually essentially using
a variant of byte pairing encoding?

01:10:20.284 --> 01:10:24.804
So they've got a slightly different
criterion of when to join letter sequences

01:10:24.804 --> 01:10:28.725
together that's more probabilistic
rather than just count based.

01:10:28.725 --> 01:10:33.910
And it's essentially the same kind
of byte pairing encoding idea.

01:10:33.910 --> 01:10:38.597
Okay, and so then the final bit that
I wanted to show you is some work

01:10:38.597 --> 01:10:41.292
that Thang Luong and me did last year.

01:10:41.292 --> 01:10:46.229
Which was trying to get the benefits
of a character level system

01:10:46.229 --> 01:10:50.995
while also still allowing user
translate at the word level.

01:10:50.995 --> 01:10:55.588
And so the hope of this was to
gain the best of both worlds by

01:10:55.588 --> 01:11:00.187
having the performance of
a character level system while

01:11:00.187 --> 01:11:03.943
having the efficiency
of a word level system.

01:11:03.943 --> 01:11:08.499
So something I haven't mentioned so far,
is that even though you can get very good

01:11:08.499 --> 01:11:12.208
performance results by working
directly at the character level.

01:11:12.208 --> 01:11:16.820
A problem of it is it tends to
make things much, much slower.

01:11:16.820 --> 01:11:20.611
So if you sort of think back to
that example of saying when I said,

01:11:20.611 --> 01:11:22.795
you can train a word2vec style model.

01:11:22.795 --> 01:11:27.261
But instead of having word vectors
you'd have a little LSTM where you

01:11:27.261 --> 01:11:30.580
build a word representation
from characters.

01:11:30.580 --> 01:11:31.737
You can do that, but

01:11:31.737 --> 01:11:35.631
the difference is we have to go
doing standard word2vec training.

01:11:35.631 --> 01:11:40.076
You're just looking up a word's
representation then computing with that.

01:11:40.076 --> 01:11:44.647
If you have to run a whole LSTM to
calculate the words representation,

01:11:44.647 --> 01:11:48.000
that's a much more
computationally expensive thing.

01:11:48.000 --> 01:11:52.800
So purely character-based models
tend to take well over an order of

01:11:52.800 --> 01:11:57.798
magnitude longer to train and
well over And a similar kind of slow

01:11:57.798 --> 01:12:02.100
down at runtime as well, because you sort
of are going character by character.

01:12:04.420 --> 01:12:05.150
Yeah.

01:12:26.732 --> 01:12:30.554
So the question is how can you
evaluate character based models,

01:12:30.554 --> 01:12:33.760
can you sort of do the equivalent
of word similarity?

01:12:33.760 --> 01:12:37.726
I think there's nothing
that you can really

01:12:37.726 --> 01:12:41.890
do that directly does
it on the characters.

01:12:41.890 --> 01:12:44.928
But I mean,
you don't have to go all the way to say,

01:12:44.928 --> 01:12:49.336
let's run them in neural machine
translation system and see if it helps.

01:12:49.336 --> 01:12:51.430
I mean, the thing that people have done.

01:12:51.430 --> 01:12:56.061
And again, this has proven that
character based encoding works pretty

01:12:56.061 --> 01:12:59.934
successfully is to do word
similarity based evaluations.

01:12:59.934 --> 01:13:01.447
So you can say, okay,

01:13:01.447 --> 01:13:06.312
let's build up representations of
words using the character, LSTM and

01:13:06.312 --> 01:13:11.654
then see how good the result is as a word
similarity measure using the kind of word

01:13:11.654 --> 01:13:17.343
similarity metrics that we talked about
when we're doing things like word2vec.

01:13:17.343 --> 01:13:22.100
And I think the results are that it
doesn't work quite as well as the best

01:13:22.100 --> 01:13:26.303
results that people have gotten
from word-level models, but

01:13:26.303 --> 01:13:28.840
it actually works pretty close.

01:13:28.840 --> 01:13:32.533
And so that's again, where it sort of
seems to having done this successfully and

01:13:32.533 --> 01:13:34.885
gives you the advantage to
having this open vocab.

01:13:34.885 --> 01:13:41.877
So, the question is how
well do the character

01:13:41.877 --> 01:13:47.400
vectors generalize across tasks.

01:13:47.400 --> 01:13:49.836
Well, I think they generalize,

01:13:49.836 --> 01:13:55.500
as well as the kind of word vectors that
we train by something like word2vec.

01:13:55.500 --> 01:13:59.265
I mean, certainly people have used them
across tasks by doing different things

01:13:59.265 --> 01:14:01.962
like language modeling versus
part of speech tagging and

01:14:01.962 --> 01:14:03.600
that's worked completely fine.

01:14:03.600 --> 01:14:10.000
So, this hybrid model.

01:14:10.000 --> 01:14:13.300
Again, it was a kind of a hierarchical
character and word based model.

01:14:13.300 --> 01:14:18.157
So, the heart of it in the middle
is that there's a word-level

01:14:18.157 --> 01:14:21.100
neural machine translation system.

01:14:21.100 --> 01:14:25.800
And so, it was working over
a fixed moderate size vocabulary.

01:14:25.800 --> 01:14:28.900
And so,
that's going to have unknowns in it.

01:14:28.900 --> 01:14:32.773
So when you can sort of generate
an unknown vocabulary item and

01:14:32.773 --> 01:14:37.018
you can feed that through in
the generation time and encoding time,

01:14:37.018 --> 01:14:41.858
you're gonna have words for which there
are no word vectors which here we're

01:14:41.858 --> 01:14:46.280
having it be for cute though,
maybe cute would be in your vocabulary.

01:14:46.280 --> 01:14:49.970
So if you had a word not
in your word vocabulary,

01:14:49.970 --> 01:14:53.661
then what you're doing is then saying,
okay,

01:14:53.661 --> 01:14:57.930
well, we can do that by
having character level LSTMs.

01:14:57.930 --> 01:14:59.781
And so, there are two cases.

01:14:59.781 --> 01:15:03.582
So if a word is not in your input
vocabulary, you can just as

01:15:03.582 --> 01:15:09.171
a pre-processing step run a character LSTM
that generates a word representation for

01:15:09.171 --> 01:15:13.519
it and then you'll just say that's
it's word representation and

01:15:13.519 --> 01:15:16.470
you can run it through the encoder.

01:15:16.470 --> 01:15:19.490
The situation was slightly
different on the decoder side.

01:15:19.490 --> 01:15:21.131
So on the decoder side,

01:15:21.131 --> 01:15:26.886
what would happen is the word-level LSTM
could chose to generate an unknown word.

01:15:26.886 --> 01:15:32.262
And if it generated an unknown word,
you then sort of took the hidden state

01:15:32.262 --> 01:15:37.379
representation and used that as
a starting point of another character

01:15:37.379 --> 01:15:42.432
level LSTM which could then generate
a word character by character.

01:15:42.432 --> 01:15:44.337
And something that we didn't do for

01:15:44.337 --> 01:15:48.590
efficiency is you might think you'd want
to take the resulting meaning here and

01:15:48.590 --> 01:15:52.779
feed that back into the next timestamp and
that might have been a good idea, but

01:15:52.779 --> 01:15:56.101
we were interested in trying to
make this as fast as possible.

01:15:56.101 --> 01:15:57.964
So actually in this model,

01:15:57.964 --> 01:16:01.933
that representation was only
used to generate the word and

01:16:01.933 --> 01:16:07.120
it was still this sort of unk that was
being fed back into the next timestamp.

01:16:07.120 --> 01:16:12.142
So, standard kind of word-level
beam search and then you're doing

01:16:12.142 --> 01:16:17.004
a char-level beam search for
when you're spilling that one out.

01:16:17.004 --> 01:16:22.630
And so let's again, sort of showed
the strength of character level models.

01:16:22.630 --> 01:16:25.810
So, this shows some results
from English to Czech.

01:16:25.810 --> 01:16:28.314
And if you're wanting to
do character level models,

01:16:28.314 --> 01:16:30.371
Czechs are really good
language to look at,

01:16:30.371 --> 01:16:33.780
because of the fact that has a lot of
morphology and complex words forms.

01:16:33.780 --> 01:16:39.456
So, here are some results from machine
translation systems on English-Czech for

01:16:39.456 --> 01:16:43.110
the 2015 workshop on machine translation.

01:16:43.110 --> 01:16:49.896
So the system that won the competition
in 2015, it got 18.8 BLEU.

01:16:49.896 --> 01:16:55.358
There had been an entry that did
sort of word-level neural machine

01:16:55.358 --> 01:17:00.540
translation in WMT 2015,
which didn't do quite as well.

01:17:01.540 --> 01:17:06.070
That's maybe not a condemnation of
word-level neural machine translation,

01:17:06.070 --> 01:17:10.598
cuz it turns out the winning system had
been trained on 30 times as much data and

01:17:10.598 --> 01:17:12.570
was an ensemble of three systems.

01:17:12.570 --> 01:17:17.828
So, maybe the word level NMT
was pretty good by comparison.

01:17:17.828 --> 01:17:21.254
But nicely by putting in
this sort of hybrid word and

01:17:21.254 --> 01:17:26.512
character level system that, that was
able to sort of boost the performance of

01:17:26.512 --> 01:17:32.009
the NMT system trained on the same amount
of data as this one by 2.5 BLEU points,

01:17:32.009 --> 01:17:35.932
putting it well above
the performance of the best system.

01:17:35.932 --> 01:17:40.568
So, this then is just showing you one
example of the kind of places you can win

01:17:40.568 --> 01:17:41.226
in Czech.

01:17:41.226 --> 01:17:45.490
So the source sentence is her 11-year
old daughter, Shani Bart said,

01:17:45.490 --> 01:17:47.440
it felt a little bit weird.

01:17:47.440 --> 01:17:48.625
And so in Czech,

01:17:48.625 --> 01:17:53.980
11-year-old is turning into this
one morphologically complex word.

01:17:53.980 --> 01:17:58.968
And so if you're sort of running with
a medium size vocabulary in a neural MT

01:17:58.968 --> 01:18:03.020
system, well,
you're just gonna unks full of this stuff.

01:18:03.020 --> 01:18:04.410
And so, that's not very good.

01:18:04.410 --> 01:18:08.138
Well, one solution to that would be say,
hey, maybe we can copy and

01:18:08.138 --> 01:18:09.746
sometimes that's useful.

01:18:09.746 --> 01:18:13.126
Cuz if you can copy Shani,
that's kinda good,

01:18:13.126 --> 01:18:17.011
cuz then you just get the right
word copied across, but

01:18:17.011 --> 01:18:22.790
doing copying isn't a good strategy
if you're copying across 11-year old.

01:18:22.790 --> 01:18:27.394
Because that sort of fails badly,
generating what you want.

01:18:27.394 --> 01:18:31.699
But nicely, if you're running
this hybrid word character model,

01:18:31.699 --> 01:18:36.985
you could actually get this word correct,
because it could actually just generate

01:18:36.985 --> 01:18:41.935
it character by character as an unknown
word and give you the correct answer.

01:18:41.935 --> 01:18:48.695
Tada, that is my central result for
the day.

01:18:48.695 --> 01:18:51.774
And so remember, get help on projects.

01:18:51.774 --> 01:18:56.240
Hope you have good luck finishing,
last lecture which is on Thursday.

