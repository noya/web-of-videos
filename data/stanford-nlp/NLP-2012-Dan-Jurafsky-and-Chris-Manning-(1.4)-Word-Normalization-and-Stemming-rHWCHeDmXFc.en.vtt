WEBVTT
Kind: captions
Language: en

00:00:01.390 --> 00:00:06.030
Once we segmented out words or tokenized
them we need to normalize them and

00:00:06.030 --> 00:00:06.760
stem them.

00:00:07.810 --> 00:00:12.400
So, normalizing means different things for
information retrieval, for example.

00:00:12.400 --> 00:00:16.710
We require that the index text, and the
query terms have to have the same form so

00:00:16.710 --> 00:00:20.330
we want to match U.S.A to USA.

00:00:20.330 --> 00:00:24.230
If somebody asks a question of
a query with one of them and

00:00:24.230 --> 00:00:27.040
the answer has the other,
we want them to match.

00:00:27.040 --> 00:00:30.920
So it's like implicitly defining some
kind of of equivalent class of terms.

00:00:32.450 --> 00:00:35.370
We might do this by always
deleting periods, for example.

00:00:35.370 --> 00:00:40.305
We might take,
have a rule that takes U.f.a.tusa.

00:00:40.305 --> 00:00:43.970
An alternative is some kind
of asymmetric expansion.

00:00:43.970 --> 00:00:48.320
So for example, let's say that
we're doing information retrieval.

00:00:48.320 --> 00:00:51.720
If I enter the term window,
I might want to search for window or

00:00:51.720 --> 00:00:56.300
windows or any morphological
variant of the word window.

00:00:56.300 --> 00:01:02.550
But if I enter capital W windows, I might
only want to search for Windows because

00:01:02.550 --> 00:01:06.070
person is presently looking for the
product and not the part of your house.

00:01:07.720 --> 00:01:11.290
And this is a potentially more powerful
algorithm, but less efficient and

00:01:11.290 --> 00:01:12.340
much more complicated.

00:01:12.340 --> 00:01:17.560
So in general, we use symmetric and
relatively simple expansions.

00:01:17.560 --> 00:01:22.990
So for example in information retrieval,
we generally remove reduce all letters to

00:01:22.990 --> 00:01:28.160
lower case, since users tend to use lower
case, and with some small exceptions.

00:01:28.160 --> 00:01:30.820
So, for example, if we see upper
case in the middle of a sentence,

00:01:30.820 --> 00:01:34.050
like General Motors,
we might want to keep the case.

00:01:34.050 --> 00:01:38.740
And this matters for
distinguishing the verb fed from

00:01:38.740 --> 00:01:44.000
the Federal Reserve Bank with
the capital F Or a group

00:01:44.000 --> 00:01:49.470
like S-A-I-L that stands for Artificial
Intelligence Lab from the verb sail.

00:01:49.470 --> 00:01:53.070
And it turn out that for sentiment
analysis or machine translation or

00:01:53.070 --> 00:01:55.890
information extraction case
is in fact very helpful.

00:01:55.890 --> 00:01:58.780
So, is a big difference between US and us.

00:02:00.360 --> 00:02:04.140
We also often want to do lemmatizations,
so we're reducing our inflections or

00:02:04.140 --> 00:02:07.530
our variant forms to their base form.

00:02:07.530 --> 00:02:11.220
Words like am or are or
is will get lemmatized into be.

00:02:11.220 --> 00:02:15.910
Car, cars, car apostrophe s, and
so on get lemmatized to car.

00:02:15.910 --> 00:02:19.900
So a phrase like the boy's
cars are different colors

00:02:19.900 --> 00:02:23.818
should get lemmatized to
the boy car be different color.

00:02:23.818 --> 00:02:27.770
So in general, the task of lemmatization
is finding the correct dictionary

00:02:27.770 --> 00:02:32.240
head word form for
a word form that you are given.

00:02:32.240 --> 00:02:33.490
And of course, this is very important for

00:02:33.490 --> 00:02:38.100
all sorts of applications particularly
machine translations where, for

00:02:38.100 --> 00:02:42.510
example, if you have a Spanish verb like
quiero, I want or quieres, you want,

00:02:42.510 --> 00:02:47.280
it's very important to know that this is
the same lemma as querer the verb to want.

00:02:48.520 --> 00:02:53.840
So this general topic of looking at
parts of words leads us to morphology.

00:02:53.840 --> 00:02:56.140
Morphology is the study of morpheme and

00:02:56.140 --> 00:02:59.570
morpheme is the small unit
that makes up a word.

00:02:59.570 --> 00:03:03.340
We usually distinguish two
kinds of morphemes stems

00:03:03.340 --> 00:03:07.654
that's the core meaning-bearing
units in a word.

00:03:07.654 --> 00:03:12.080
And Affixes, affixes the bits and

00:03:12.080 --> 00:03:17.410
pieces that adhere to the stems
often with grammatical functions so

00:03:17.410 --> 00:03:24.490
on this particular slide in fact
stem is the stem and s is the affix.

00:03:24.490 --> 00:03:27.770
The word, affix,
just to confuse us, is a stem.

00:03:27.770 --> 00:03:30.450
And there, es is the affix,
so there's an affix.

00:03:30.450 --> 00:03:33.902
There's an s, and
there's an s, and meaningful,

00:03:33.902 --> 00:03:36.338
there's another affix, and so on.

00:03:39.338 --> 00:03:43.988
So stemming is the task of taking
off these affixes to reduce

00:03:43.988 --> 00:03:45.668
terms to their stem.

00:03:45.668 --> 00:03:50.179
And it's historically derived from
information retrieval although it's used

00:03:50.179 --> 00:03:51.910
in all sorts of applications.

00:03:53.720 --> 00:03:56.900
We use the word stemming when we
specifically mean a kind of a crude

00:03:56.900 --> 00:03:59.810
chopping off of affixes and
this is, of course,

00:03:59.810 --> 00:04:03.090
a language dependent kind of process.

00:04:03.090 --> 00:04:07.630
So the English word automate,
automates, automatic,

00:04:07.630 --> 00:04:12.320
automaton we like them all to
refer to the same stem, automat.

00:04:12.320 --> 00:04:16.620
So stemming is like a simplified
version of limitization we pick up

00:04:16.620 --> 00:04:20.170
a prefix of the word,
use that to represent the word and

00:04:20.170 --> 00:04:25.750
we chop off all the suffixes that
are relatively leading to that stem.

00:04:27.540 --> 00:04:28.980
So, here's an example of the text.

00:04:30.050 --> 00:04:32.200
For example, compressed and
compression are both accept for

00:04:32.200 --> 00:04:34.800
an equivalent to compress that's the text.

00:04:34.800 --> 00:04:39.260
And if we stemmed that text
here's the resulting output.

00:04:39.260 --> 00:04:44.970
So you can see that and
that we've lost the e on example and

00:04:44.970 --> 00:04:48.400
compressed and compressing have
both turned into compress.

00:04:48.400 --> 00:04:52.936
And here we used are, we could have use
be as our representation of are, but

00:04:52.936 --> 00:04:55.964
for this particular example
we used are and so on.

00:04:58.224 --> 00:05:01.856
The simplest algorithm,
the most commonly used one for

00:05:01.856 --> 00:05:05.340
simple stemming of English
is the Porter algorithm.

00:05:06.710 --> 00:05:12.110
And the Porter's algorithm is a series
of an iterated series of simple rules,

00:05:12.110 --> 00:05:14.570
simple replace rules.

00:05:14.570 --> 00:05:20.000
So, for example, one set of rules step one

00:05:20.000 --> 00:05:26.350
a takes string like s's and
replaces with them with ss.

00:05:26.350 --> 00:05:30.420
So in a word like caresses it
chops off the es of caresses.

00:05:30.420 --> 00:05:36.100
Or rule that takes ies to i, chops off
the ies of ponies and leaves poni.

00:05:36.100 --> 00:05:40.780
We're going to use pony with an i as a a
representation in Porter stemming of pony.

00:05:42.130 --> 00:05:47.440
And the rules operate in order so that
at this point if there's any ss's left,

00:05:47.440 --> 00:05:49.810
they stay as an s.

00:05:49.810 --> 00:05:52.380
But any other ss get
deleted at this point.

00:05:52.380 --> 00:05:56.598
So the s of cats is deleted,
while the ss of caress is kept.

00:05:58.898 --> 00:06:05.060
Similarly, in step 1b,
we remove all of the ings and the eds.

00:06:05.060 --> 00:06:09.140
So we want to cross out the ing of
walk and the ed of plastered but,

00:06:09.140 --> 00:06:12.680
we specify the role very carefully
that in a Porter's stem where

00:06:12.680 --> 00:06:17.110
only words with a vowel
get their ings removed.

00:06:17.110 --> 00:06:20.970
And that's because a word like sing, only
words where's there's an additional vowel

00:06:20.970 --> 00:06:24.570
before the ing, so a word like sing,
which has no extra vowel.

00:06:24.570 --> 00:06:26.409
Sing only has one vowel, the vowel in ing.

00:06:27.550 --> 00:06:33.520
Stays as sing but
walking which has a vowel and

00:06:33.520 --> 00:06:39.150
in addition a vowel before the ing
is allowed to delete the suffix.

00:06:39.150 --> 00:06:42.700
So if a word has a vowel,
followed by ing, the ing is deleted.

00:06:42.700 --> 00:06:48.812
And there's lots of other such rules so
ational turns into ate so

00:06:48.812 --> 00:06:53.562
we can cross off the O of relational and
the tion and

00:06:53.562 --> 00:06:59.693
end up with relate and izer to Is and
so we cross off the R and so on.

00:06:59.693 --> 00:07:04.661
And the rules get even more complicated so
as you get to very long stems you

00:07:04.661 --> 00:07:08.258
going to remove the al of revival and
the able and so on.

00:07:13.118 --> 00:07:18.332
Let's look again at this rule that
strips -ing and practice using

00:07:18.332 --> 00:07:24.730
the Unix tools that we say in the last
section to look at morphology in a corpus.

00:07:24.730 --> 00:07:27.730
So remember, why are we stripping
the -ings only if there's a vowel

00:07:27.730 --> 00:07:29.230
preceding the -ing?

00:07:29.230 --> 00:07:30.820
Here was the rule.

00:07:30.820 --> 00:07:35.730
Remember we said that in a word like
walking, we have a vowel before the ing,

00:07:35.730 --> 00:07:37.800
and so it's okay to remove the ing.

00:07:37.800 --> 00:07:40.210
In a word like sing,
there is no vowel before.

00:07:40.210 --> 00:07:41.740
There's no letters at all before that s.

00:07:41.740 --> 00:07:43.960
There's no previous vowels.

00:07:43.960 --> 00:07:45.800
And so sing, the rule doesn't apply.

00:07:47.890 --> 00:07:55.120
And let's do a little search four
words ending in 'ing' in Shakespeare.

00:07:55.120 --> 00:07:58.220
So we are going to first
take all of Shakespeare, and

00:07:58.220 --> 00:08:00.640
turn all the non-alphabetic characters.

00:08:09.470 --> 00:08:14.400
Oops, turn

00:08:14.400 --> 00:08:18.930
all on the characters into new lines so
we're going to get one word per line

00:08:18.930 --> 00:08:23.390
then we're going to translate all
of the upper case to lower case so

00:08:23.390 --> 00:08:26.410
we're dealing with, we're combining all
the upper case and lower case words.

00:08:26.410 --> 00:08:31.610
And now let's grep out,
grep is a program for

00:08:31.610 --> 00:08:35.270
finding any line that contains
a regular expression in the file.

00:08:35.270 --> 00:08:37.108
Very useful UNIX program.

00:08:37.108 --> 00:08:41.348
So we're going to look for
the regular expression ing$.

00:08:41.348 --> 00:08:47.568
And so we'll find all words ending
in ing and let's sort them.

00:08:47.568 --> 00:08:52.028
And then we'll just take one
copy of each and count them, and

00:08:52.028 --> 00:08:54.570
then sort them by the counts.

00:08:54.570 --> 00:08:59.318
Let's see what words we find
ending in ing in Shakespeare.

00:08:59.318 --> 00:09:04.118
And what we see is lots of
these words are not words

00:09:04.118 --> 00:09:08.917
that in fact we would like
to remove the ing from so

00:09:08.917 --> 00:09:13.600
we have words like king and
nothing and thing and

00:09:13.600 --> 00:09:18.868
ring and something and sing and
anything and spring so

00:09:18.868 --> 00:09:23.902
this is a lot of words lot
of frequent words in fact,

00:09:23.902 --> 00:09:28.268
that it would be a bad
idea to remove the ing.

00:09:28.268 --> 00:09:31.430
If we remove the ing from king,
we'd get K, and so on.

00:09:31.430 --> 00:09:34.030
Remove the ing from spring, we'd get spr.

00:09:34.030 --> 00:09:38.550
So, let's modify our rule that we did.

00:09:38.550 --> 00:09:42.870
Instead of saying, grepping for all words
ending in ing, let's just go back and

00:09:42.870 --> 00:09:45.380
change that to grep for
all words with a vowel.

00:09:45.380 --> 00:09:47.420
We'll just make it be a, e, i, o, u.

00:09:48.420 --> 00:09:50.840
Similarly our vowel which
is the old vowel letters.

00:09:52.060 --> 00:09:54.570
And we need some way to
say there's a vowel and

00:09:54.570 --> 00:09:57.690
then anything can happen in
between followed by the ing.

00:09:57.690 --> 00:09:59.710
Well, what or how do we say anything?

00:09:59.710 --> 00:10:03.770
Dot means any character,
star meaning zero and more of those.

00:10:03.770 --> 00:10:08.400
And now, let's look at what words
we get back from that pattern.

00:10:09.630 --> 00:10:12.830
And now, since we specified that
the word has to start with a vowel,

00:10:12.830 --> 00:10:15.890
we've done a much better
job of finding two-syllable

00:10:15.890 --> 00:10:18.180
words where the ing in fact
is supposed to be stripped.

00:10:18.180 --> 00:10:23.000
So there's still some problematic words,
like nothing and something.

00:10:23.000 --> 00:10:25.790
We don't want to get noth and someth.

00:10:25.790 --> 00:10:27.500
And anything.

00:10:27.500 --> 00:10:30.420
But otherwise, and maybe not cunning.

00:10:30.420 --> 00:10:34.900
But otherwise, we done a pretty good job
of making the rule a little bit better.

00:10:34.900 --> 00:10:39.650
So there's a little explanation of
how the porter stammer works and

00:10:39.650 --> 00:10:43.669
then how we can actually use our Unix
tools to do the corpus linguistics

00:10:45.280 --> 00:10:47.180
to help write rules of this kind.

00:10:49.230 --> 00:10:51.720
So that's a simple example of morphology.

00:10:51.720 --> 00:10:56.680
It terms out that in some languages,
much more complex morphology is necessary.

00:10:56.680 --> 00:10:59.100
And Turkish is the famous example of this.

00:10:59.100 --> 00:11:02.340
So here's a word in Turkish which
I wont be able to pronounce for

00:11:02.340 --> 00:11:07.290
you which means behaving as if you are
among those whom we could not civilize.

00:11:07.290 --> 00:11:11.030
So I assume it's the kind of thing
your mother says to you when you've

00:11:11.030 --> 00:11:15.190
been particularly naughty and
in Turkish this is one word so

00:11:15.190 --> 00:11:18.830
it's a very long word with a lot of stems.

00:11:18.830 --> 00:11:23.740
We have the civilized stem and
a affix meaning calm, and

00:11:23.740 --> 00:11:27.960
an affix meaning cause an affix
meaning notable, and so on.

00:11:27.960 --> 00:11:32.960
And so in languages like Turkish, and as
we saw earlier for the very long nouns in

00:11:32.960 --> 00:11:37.520
German, we're going to have to do a richer
and more complex morpheme segmentation.

00:11:38.890 --> 00:11:43.610
So, as we've seen word tokenization, and
now we've seen that words will have to be

00:11:43.610 --> 00:11:46.310
normalized and
stemmed to map them to a normal form.

