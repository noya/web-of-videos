WEBVTT
Kind: captions
Language: en

00:00:01.060 --> 00:00:05.130
Okay now let's look at how these
features are used inside a classifier.

00:00:05.130 --> 00:00:08.690
And in particular I want to introduce
the notion of linear classifiers.

00:00:10.110 --> 00:00:14.200
Our feature based maxent classifiers and
linear classifiers.

00:00:14.200 --> 00:00:19.310
And so what that means is that at
the end of the day what they're going to

00:00:19.310 --> 00:00:24.690
do is going to have a set of features and
then, it's going to calculate a linear

00:00:24.690 --> 00:00:30.620
function of those features, which will
end up as a score for a particular class.

00:00:30.620 --> 00:00:33.640
And the way that we do that is that for
each feature,

00:00:33.640 --> 00:00:36.990
fi, we assign it a weight, lambda i.

00:00:38.100 --> 00:00:40.980
And so what we're going to
do is then consider for

00:00:40.980 --> 00:00:46.120
an observed datum every particular
class that we can give it,

00:00:46.120 --> 00:00:51.340
then we're going to work out what
features match against that datum and

00:00:51.340 --> 00:00:55.800
therefore what the weight of the vote for

00:00:55.800 --> 00:00:59.017
class in terms of these lambda i weights.

00:01:00.680 --> 00:01:03.070
So let's look through a concrete example.

00:01:03.070 --> 00:01:07.320
So this is, again, the example for where
we're going to be wanting to determine

00:01:07.320 --> 00:01:14.180
the class for this word, where our choice
is here, a person, location, or drug.

00:01:14.180 --> 00:01:19.250
And so what we're going to do
is consider which of the classes

00:01:19.250 --> 00:01:23.490
gets the most votes, in terms of
the three features we defined before.

00:01:23.490 --> 00:01:27.720
So the vote for
a class is just going to be the sums

00:01:27.720 --> 00:01:32.100
of the weights that are assigned
to each of the features.

00:01:32.100 --> 00:01:34.760
So remember the three
features from before.

00:01:34.760 --> 00:01:39.940
The first feature looked whether
the preceding word was in,

00:01:39.940 --> 00:01:46.710
where the word is capitalized, and
then whether the class is location.

00:01:46.710 --> 00:01:51.130
So it's not going to match here,
but it's going to match over here.

00:01:53.050 --> 00:01:57.190
And so that's a feature that
would tend to pick out locations.

00:01:57.190 --> 00:02:01.768
So we'll assume that it has
a positive weight of 1.8.

00:02:01.768 --> 00:02:07.592
The second feature was then also
a feature matched on locations and

00:02:07.592 --> 00:02:11.860
it looked for
an accent in latin character here.

00:02:14.030 --> 00:02:20.000
It will match against this datum but
not against the other two datums again.

00:02:20.000 --> 00:02:25.680
And so in general I'm assuming that that
feature, at least for American English,

00:02:25.680 --> 00:02:29.280
will have a negative weight because
you're more likely to see accents and

00:02:29.280 --> 00:02:35.110
things like person names so
it might have a weight of -0.6.

00:02:35.110 --> 00:02:39.680
Okay, then the third
feature that we had defined

00:02:39.680 --> 00:02:44.900
was that the class was drug and
the word ends in c.

00:02:44.900 --> 00:02:50.380
So of these three datums, the one
which we'll match is this one here.

00:02:50.380 --> 00:02:52.890
And it's not true in this case, but

00:02:52.890 --> 00:02:56.930
in general I think that's likely to be
true because quite a few drug names

00:02:56.930 --> 00:03:01.610
like Zantac end in a c,
where not that many regular words do.

00:03:01.610 --> 00:03:07.110
So maybe we'll give that a weak positive
vote and say the that feature has 0.3.

00:03:07.110 --> 00:03:10.410
And so then, what we're going to be
doing is that we're going to be choosing

00:03:10.410 --> 00:03:14.070
the class that has
the highest total votes.

00:03:14.070 --> 00:03:17.090
And so,
there are three class choices here.

00:03:17.090 --> 00:03:21.980
So, this class choice of person
actually matched no features.

00:03:21.980 --> 00:03:24.700
So, its total vote is zero,

00:03:24.700 --> 00:03:29.000
which is kind of neither a positive nor
a negative preference.

00:03:29.000 --> 00:03:33.056
This class choice of location
in aggregate had a vote for

00:03:33.056 --> 00:03:39.290
it of 1.2 and this class choice of drug
in aggregate had a vote for it 0.3.

00:03:39.290 --> 00:03:43.340
And so
what we're going to choose is the class

00:03:43.340 --> 00:03:47.980
which maximizes this voting
quantity is going to be location.

00:03:52.131 --> 00:03:58.070
There are many ways to choose the weights
for the features in our classifier.

00:03:58.070 --> 00:04:01.810
So for example, perceptron algorithms,
what they do is they look for

00:04:01.810 --> 00:04:04.120
a currently misclassified example.

00:04:04.120 --> 00:04:06.150
And then they nudge
the weights in a direction that

00:04:06.150 --> 00:04:08.820
will correct the classification
of that one example.

00:04:10.180 --> 00:04:14.530
Another popular form of discriminative
classifiers support vector machines,

00:04:14.530 --> 00:04:17.890
which we're not going to
cover in these classes, but

00:04:17.890 --> 00:04:22.110
what they're trying to do is create a lot
of distance between examples of different

00:04:22.110 --> 00:04:25.810
classes by adjusting the featured
weights to achieve that direction.

00:04:25.810 --> 00:04:28.531
So, they're called max-margin classifiers.

00:04:28.531 --> 00:04:33.460
But, the classifiers that we're going to
look at here are maxent classifiers,

00:04:33.460 --> 00:04:38.760
which are in a family which is referred to
as exponential or log-linear classifiers.

00:04:38.760 --> 00:04:41.320
In general, this family has lots and
lots of names.

00:04:41.320 --> 00:04:45.740
So there are things in this class also
referred to as logistic classifiers and

00:04:45.740 --> 00:04:50.510
Gibbs models which are very
closely related to maxent models.

00:04:50.510 --> 00:04:56.510
And so the idea from this class is that
what we're going to do is we're going

00:04:56.510 --> 00:05:01.070
to make a probabilistic model out of the
linear combination that we already saw.

00:05:01.070 --> 00:05:05.000
The sum over the dot product,
so it's a dot product,

00:05:05.000 --> 00:05:10.300
the lambda f dot product, which is
the sum of lambda i f i of c and d.

00:05:10.300 --> 00:05:11.680
So how do we do it?

00:05:11.680 --> 00:05:16.610
Well what we're going to imagine doing
is kind of in the simplest possible way.

00:05:16.610 --> 00:05:24.140
So here's the sum of lambda I, fi of,
which is a function of c and d.

00:05:24.140 --> 00:05:29.100
Now the problem with this linear
combination is that it might

00:05:29.100 --> 00:05:34.050
come out as either positive or negative,
and that's bad if you're a probability.

00:05:34.050 --> 00:05:38.670
And so, what we're going to want to do is
we're going to make that always positive.

00:05:38.670 --> 00:05:42.340
And the way we're going to do that
is taking an exponential of it, so

00:05:42.340 --> 00:05:44.940
we're going to take E to this power.

00:05:44.940 --> 00:05:48.390
So this is something that will
always make the vote positive.

00:05:53.141 --> 00:05:57.710
And this form I'm showing here,
so it's It's E,

00:05:57.710 --> 00:06:01.350
that's the 2.818 etcetera.

00:06:01.350 --> 00:06:06.170
And then your raising
the feature dot product to that.

00:06:06.170 --> 00:06:09.360
So you can also see it's
written in as e to the lambda

00:06:09.360 --> 00:06:12.720
dot f where these are our features.

00:06:14.150 --> 00:06:18.670
Okay, so now we've got something
that's a positive quantity but

00:06:18.670 --> 00:06:22.180
it's not necessarily a probability.

00:06:22.180 --> 00:06:25.560
So we're going to make it a probability
in the simplest way possible to,

00:06:25.560 --> 00:06:28.090
which is we're going to normalise it.

00:06:28.090 --> 00:06:33.021
So this is some kind of score for
one class and so

00:06:33.021 --> 00:06:37.446
then what we can do is
work out the score for

00:06:37.446 --> 00:06:41.508
every class and divide through by it.

00:07:02.780 --> 00:07:08.831
Okay and then we're going to say that
this quantity here is the probability

00:07:08.831 --> 00:07:14.187
that we assign to a class,
probability of a particular class for

00:07:14.187 --> 00:07:17.758
a datum given a certain set of parameters,

00:07:17.758 --> 00:07:21.749
where this is our vector
of all the parameters.

00:07:23.070 --> 00:07:28.370
And so we can work out a concrete example
so that for our previous example where we

00:07:28.370 --> 00:07:34.526
had the probability of
choosing location given

00:07:34.526 --> 00:07:39.470
in Quebec.

00:07:39.470 --> 00:07:42.730
Well what we're going to work out is for

00:07:42.730 --> 00:07:46.810
a numerator we've got
the features that match.

00:07:46.810 --> 00:07:52.000
So we're working out

00:07:52.000 --> 00:07:57.611
e to the 1.8 plus minus 0.6.

00:07:58.730 --> 00:08:02.740
And then we're going to divide that
through by the total votes for

00:08:02.740 --> 00:08:03.840
all of the classes.

00:08:03.840 --> 00:08:06.710
So, one class had no features matching.

00:08:07.710 --> 00:08:11.360
The second class had
the single vote of 0.3 and

00:08:11.360 --> 00:08:15.590
then here's the one for location

00:08:17.000 --> 00:08:22.060
which is e to the 1.8 plus -0.6.

00:08:22.060 --> 00:08:25.620
And you can work that out
on your calculator and

00:08:25.620 --> 00:08:29.500
it comes out to be equal to about 0.59.

00:08:29.500 --> 00:08:33.670
And so these kind of terms there
you can also simplify them.

00:08:33.670 --> 00:08:36.910
So adding exponents is just
the same as multiplying.

00:08:36.910 --> 00:08:42.939
So even also think of this as e to
the 1.8 times e to the minus 0.6.

00:08:42.939 --> 00:08:47.450
And so
that's the sense in which each of these

00:08:47.450 --> 00:08:52.830
features gives some extra multiplicative
term that changes the probability.

00:08:52.830 --> 00:08:56.850
That when you add a feature you're
adding an extra multiplicative term that

00:08:56.850 --> 00:08:58.290
changes the probability.

00:08:59.960 --> 00:09:04.120
So if you worked through all of
the examples of the different classes

00:09:04.120 --> 00:09:05.710
are just skipping through the math,

00:09:06.820 --> 00:09:09.950
that what you get is that these
are the different probabilities.

00:09:09.950 --> 00:09:12.700
And so note that we're giving
a probability of each amount three

00:09:12.700 --> 00:09:16.790
class choices and
together that they do add up to one.

00:09:16.790 --> 00:09:21.380
And that's precisely by the normalization
that we've defined here.

00:09:21.380 --> 00:09:27.230
So these weights lambda are now the
parameters of a probability distribution.

00:09:27.230 --> 00:09:30.940
And we're combining them
by this function here.

00:09:30.940 --> 00:09:34.910
This function is also referred
to as the soft max function.

00:09:34.910 --> 00:09:38.940
And the reason for that is because, when
you're using this exponent function to

00:09:38.940 --> 00:09:41.440
make things positive, things get bigger.

00:09:41.440 --> 00:09:46.315
So, if you take something like e to
the three, that's approximately 25 or

00:09:46.315 --> 00:09:48.090
something.

00:09:48.090 --> 00:09:52.560
And so the effect of that is
the features with the strongest votes

00:09:52.560 --> 00:09:56.140
dominate more strongly in this form here.

00:09:56.140 --> 00:09:58.772
And so
it's sort of like a soft maximum, so

00:09:58.772 --> 00:10:04.060
it'll give you a value that's most
strongly determined by the biggest votes.

00:10:05.070 --> 00:10:07.740
In maxent or exponential models,

00:10:07.740 --> 00:10:13.080
what we're wanting to do is say,
given this model form,

00:10:13.080 --> 00:10:17.190
what we are going to do is try and
choose parameters, lambda i,

00:10:17.190 --> 00:10:22.470
that maximize the conditional likelihood
of the data according to this model.

00:10:22.470 --> 00:10:27.430
So we've defined probabilities over
data and classes, and we're going to

00:10:27.430 --> 00:10:32.710
maximize those choice of classes according
to that probability distribution.

00:10:36.500 --> 00:10:40.752
So a feature of these models is that
we construct not only a classification

00:10:40.752 --> 00:10:45.220
function, but also probability
distribution over classifications.

00:10:45.220 --> 00:10:48.810
So there are many good ways
of discriminating classes.

00:10:48.810 --> 00:10:53.690
So SVMs, boosting, even perceptrons,
many machine learning algorithms will

00:10:53.690 --> 00:10:57.240
give you a way to distinguish
between classes, but

00:10:57.240 --> 00:11:02.270
these are the ones that are naturally and
inherently thought of as distributions

00:11:02.270 --> 00:11:06.460
of the classes though people have thought
of ways to extend them in this direction.

00:11:08.910 --> 00:11:13.770
You may have seen logistic regression or
multi-class logistic regression in

00:11:13.770 --> 00:11:19.165
a statistics or machine learning model,
so that maybe divides people.

00:11:19.165 --> 00:11:23.475
If you haven't seen these before, don't
worry about it, because the presentation

00:11:23.475 --> 00:11:27.595
and derivations that we show here
are completely self-contained.

00:11:27.595 --> 00:11:31.240
But on the other hand,
if you have seen these models before.

00:11:31.240 --> 00:11:34.130
That something you might want
to do is just think about

00:11:34.130 --> 00:11:37.200
how what was presented here
is a little bit different.

00:11:37.200 --> 00:11:39.490
And it's effectively
different in two ways.

00:11:39.490 --> 00:11:41.860
One is the choice of parameterization.

00:11:41.860 --> 00:11:46.530
So the parameterization is slightly
different than the parameterization that's

00:11:46.530 --> 00:11:51.670
normally shown from multiclass logistic
regression models in statistics.

00:11:51.670 --> 00:11:55.510
And there are kind of pluses and
minuses to the presentation here.

00:11:55.510 --> 00:11:59.390
The minus is it's slightly statistically
inelegant because the model is over

00:11:59.390 --> 00:12:00.720
parameterized.

00:12:00.720 --> 00:12:04.780
But the plus if that this style of
parameterization is actually very

00:12:04.780 --> 00:12:09.700
advantageous to NLP models, which have
the property that they're always a very

00:12:09.700 --> 00:12:14.650
large number of features most of which
are inactive for any particular datum.

00:12:16.080 --> 00:12:20.340
But secondly, the thing to observe is
that the way things are formulated here

00:12:20.340 --> 00:12:22.570
is in terms of feature functions.

00:12:22.570 --> 00:12:28.410
Feature functions of both
the observed data and of the class.

00:12:28.410 --> 00:12:33.650
This both shows how you can put
text into the model most simply.

00:12:33.650 --> 00:12:36.560
But it's actually also
more general formulation

00:12:36.560 --> 00:12:40.200
than you normally see from all
the class logistic regression models.

00:12:40.200 --> 00:12:44.070
We actually won't use the more
general formulation in the material

00:12:44.070 --> 00:12:45.140
that is presented here.

00:12:45.140 --> 00:12:48.545
But it has actually been shown to be
useful once you go onto more complex

00:12:48.545 --> 00:12:51.560
natural language processing
structured prediction tasks.

00:12:55.307 --> 00:12:59.028
Here's a quiz question for
you guys to work through.

00:12:59.028 --> 00:13:02.878
So we're still in the situation that
we're wanting to choose some class for

00:13:02.878 --> 00:13:04.814
the final word, which is now Goeric.

00:13:04.814 --> 00:13:09.532
And our choice of classes are the same
classes as before person, location, or

00:13:09.532 --> 00:13:14.464
drug and we have exactly the same three
features that we've been using throughout

00:13:14.464 --> 00:13:15.840
these examples.

00:13:15.840 --> 00:13:19.600
And so what we'd like you to do
is just concretely work through

00:13:19.600 --> 00:13:24.010
what the probabilities that are assigned
to each of the three classes this time.

00:13:27.040 --> 00:13:31.240
Okay, now I hope you understand
the math of a maxent model and

00:13:31.240 --> 00:13:34.620
the sense in which it's one of
the family of linear classifiers.

