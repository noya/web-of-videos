WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:03.330
Up until now, I've referred to
the models that we're using as

00:00:03.330 --> 00:00:08.550
Maximum Entropy Models, but
actually what we find is these likelihood

00:00:08.550 --> 00:00:13.616
maximizing models which I define in
a certain exponential model form.

00:00:13.616 --> 00:00:16.970
In this part of I want to show why
these models are also referred

00:00:16.970 --> 00:00:21.150
to as Maximum Entropy Models and
what the motivating intuition is for

00:00:21.150 --> 00:00:22.860
the maximum entropy idea.

00:00:22.860 --> 00:00:27.730
The motivating intuition for Maximum
Entropy Models is that, in general,

00:00:27.730 --> 00:00:30.700
there are tons of probability
distributions out there.

00:00:30.700 --> 00:00:33.690
Most of which are very spiked and
specific, and

00:00:33.690 --> 00:00:37.480
would tend to overfit on
particular data write ons.

00:00:37.480 --> 00:00:40.390
What we want to do is to
find the distribution

00:00:40.390 --> 00:00:44.740
that says uniform as possible
except in places where we know

00:00:44.740 --> 00:00:49.590
there's some reason to believe that the
probability distribution isn't uniform.

00:00:49.590 --> 00:00:52.390
Uniformity can be thought
of as high entropy.

00:00:52.390 --> 00:00:56.840
We searched for distributions which
have the properties we desire but

00:00:56.840 --> 00:00:59.010
which also have high entropy.

00:00:59.010 --> 00:01:03.180
So this is kind of embodying
a statement of Thomas Jefferson's.

00:01:03.180 --> 00:01:07.540
Ignorance is preferable to error, and he
is less remote from the truth who believes

00:01:07.540 --> 00:01:10.730
nothing than he who
believes what is wrong.

00:01:10.730 --> 00:01:13.650
So what we're wanting to do is not

00:01:13.650 --> 00:01:17.440
have any beliefs in our model that
we haven't particularly stated.

00:01:17.440 --> 00:01:20.780
To the extent that we haven't stated
any constraints in our model.

00:01:20.780 --> 00:01:23.039
We want the probabilities to
be as uniform as possible.

00:01:24.370 --> 00:01:28.970
So entropy is a quantity that measures
the uncertainty of a distribution.

00:01:28.970 --> 00:01:33.990
So if we have an event x which has
some probability, we can work out

00:01:33.990 --> 00:01:39.790
the surprise of that event by taking
the log of the inverse of the probability.

00:01:39.790 --> 00:01:45.930
So if you see an event which you think has
a very small probability of happening,

00:01:45.930 --> 00:01:47.890
your surprise is then great.

00:01:47.890 --> 00:01:52.210
If you thought it had zero probability
of happening, your surprise is infinite.

00:01:52.210 --> 00:01:53.000
On the other hand,

00:01:53.000 --> 00:01:58.450
if the event had a very high probability
of happening, your surprise is small.

00:01:58.450 --> 00:02:03.890
In particular, if you think it had
probability 1, then your surprise is 0.

00:02:03.890 --> 00:02:09.740
We can then workout the entropy of
the distribution by taking the expectation

00:02:09.740 --> 00:02:14.220
over the surprise which can be
written out in this form here.

00:02:14.220 --> 00:02:16.230
And this gives us the equation for
entropy.

00:02:18.770 --> 00:02:22.000
So here's an example of
that with the simple

00:02:22.000 --> 00:02:24.930
case of flipping up
possibly weighted coin.

00:02:24.930 --> 00:02:30.260
So, if the coin always comes down heads or
always comes down tails,

00:02:30.260 --> 00:02:34.180
then there's no entropy in
the distribution, and the entropy is

00:02:34.180 --> 00:02:39.160
maximized, and the coin is equally
likely to come down heads or tails.

00:02:40.540 --> 00:02:45.220
Let me go through some concrete examples
that show what happens when we try and

00:02:45.220 --> 00:02:49.600
maximize the entropy of a distribution
either minimize the commitment.

00:02:51.310 --> 00:02:55.010
So, we're starting off with no beliefs and

00:02:55.010 --> 00:02:59.370
then in some ways we're going to want
a probability distribution that resembles

00:02:59.370 --> 00:03:05.030
some reference distribution which for us
we can be taking straight from these data.

00:03:05.030 --> 00:03:09.426
So what we're going to say in our model
is it we want to maximize entropy subject

00:03:09.426 --> 00:03:11.740
to feature-based constraints.

00:03:11.740 --> 00:03:16.110
And precisely what our feature
based constraints are is to say,

00:03:16.110 --> 00:03:21.020
that the expectations of the values,
of features in the model, should be

00:03:21.020 --> 00:03:27.120
the same as the empirical expectations of
those features over our observed data.

00:03:27.120 --> 00:03:31.850
So, every time that we
add features that puts

00:03:31.850 --> 00:03:36.910
constraints into the model, and,
therefore, it lowers the maximum entropy.

00:03:36.910 --> 00:03:41.590
But, on the other hand, it raises
the likelihood of the observed data.

00:03:41.590 --> 00:03:44.640
It takes the distribution
further from uniform, but

00:03:44.640 --> 00:03:47.150
it brings the distribution
closer to the data.

00:03:48.670 --> 00:03:51.880
So, here's a very simple example of that.

00:03:51.880 --> 00:03:56.260
So here was our unconstrained
entropy distribution,

00:03:56.260 --> 00:03:58.560
which has it's maximum right here.

00:03:58.560 --> 00:04:03.590
If we put in the constraint that said
the probability of a hit was .3.

00:04:03.590 --> 00:04:07.750
Then we constrained the distribution
to just a single point.

00:04:07.750 --> 00:04:12.670
And so
at this point the constrained distribution

00:04:12.670 --> 00:04:17.150
has a lower maximum entropy
than we had before.

00:04:17.150 --> 00:04:22.620
That's example in one dimensions so
simple it's hard to see much.

00:04:22.620 --> 00:04:27.096
So let's do a slightly more
complex example in two dimensions.

00:04:27.096 --> 00:04:30.792
So here,

00:04:30.792 --> 00:04:35.760
we have two probabilities.

00:04:35.760 --> 00:04:39.358
The probability of heads and
the probability of tails.

00:04:39.358 --> 00:04:44.113
So, let's just assume those
are two numbers between 0 and

00:04:44.113 --> 00:04:48.391
1 and we haven't even modeled
the fact that heads and

00:04:48.391 --> 00:04:52.570
tails are in complimentary distribution.

00:04:52.570 --> 00:04:57.070
Well then, if we model this
entropy surface that we find that

00:04:57.070 --> 00:05:02.160
the maximum entropy comes
about when the probability of

00:05:04.570 --> 00:05:08.580
p of H and p of T is around there and

00:05:08.580 --> 00:05:12.464
why is that well that's because for

00:05:14.285 --> 00:05:18.705
components of the entropy
distribution minus x log x.

00:05:18.705 --> 00:05:23.652
They have their optimum at the value
of 1 on e which is about .38, .4 or

00:05:23.652 --> 00:05:25.957
something like that.

00:05:25.957 --> 00:05:31.057
So that's not what we normally see when
we see the entropy picture for a coin.

00:05:31.057 --> 00:05:34.657
And that's because normally we're
immediately put in this constraint,

00:05:34.657 --> 00:05:38.507
say that heads and
tails are in complimentary distributions.

00:05:38.507 --> 00:05:41.547
The sum of their probabilities
have to add up to one.

00:05:41.547 --> 00:05:43.207
And so then, once we do that,

00:05:43.207 --> 00:05:48.980
we've constrained the space by saying we
have to be at somewhere along this line.

00:05:48.980 --> 00:05:52.900
And then we are in the situation
that interferes maximize

00:05:52.900 --> 00:05:58.330
by having the probability of head equaling
the probability of tails equaling a half.

00:05:58.330 --> 00:06:01.870
But again we can sort of stick
in one more constraint and

00:06:01.870 --> 00:06:05.310
say the probability of heads as .3 and
then again,

00:06:05.310 --> 00:06:09.600
we've constrained the distribution
down to a single feasible point.

00:06:09.600 --> 00:06:13.110
And the point to notice is that
with each of these constraints,

00:06:13.110 --> 00:06:15.990
the maximum entropy of
the model has gone lower.

00:06:15.990 --> 00:06:20.780
So here are the true
maximum of the function.

00:06:20.780 --> 00:06:23.990
Now, we're sort of away from the true
maximum of the function, but

00:06:23.990 --> 00:06:27.110
still have a reasonably
high entropy point and

00:06:27.110 --> 00:06:31.260
here, we've wandered even further
from the maximum of the function.

00:06:31.260 --> 00:06:34.270
So, now maximum entropy is going down, but

00:06:34.270 --> 00:06:37.820
we're modeling facts about the situation
in the world that we want to model.

00:06:40.540 --> 00:06:42.080
Let's look at a concrete example,

00:06:42.080 --> 00:06:45.300
it's a little bit closer to
a natural language problem now.

00:06:45.300 --> 00:06:48.130
Let's suppose we have this event space.

00:06:48.130 --> 00:06:52.640
So we have parts of speech,
where here we have six parts of speech.

00:06:52.640 --> 00:06:59.850
Nouns, plural nouns, proper nouns, proper
pronouns, and two third parts of speech.

00:06:59.850 --> 00:07:01.777
And this was our empirical data.

00:07:01.777 --> 00:07:05.592
So that we saw 36 different events and

00:07:05.592 --> 00:07:10.217
of those the most common
things we saw are proper

00:07:10.217 --> 00:07:14.957
nouns which is actually
two thirds of the data and

00:07:14.957 --> 00:07:20.060
then we saw some regular nouns and
we saw a few verbs.

00:07:20.060 --> 00:07:26.520
Right, so if we have a probability to each
of this events and we just say maximize.

00:07:26.520 --> 00:07:32.648
And theirs sort of P1, P2, P3 and
we say maximize the probabilities again,

00:07:32.648 --> 00:07:37.316
the Maxent entropy distribution
is by setting each of them to

00:07:37.316 --> 00:07:40.811
the value 1 on e but
that's not what we want.

00:07:40.811 --> 00:07:45.015
We want to say that this is a set
of categorical probabilities and so

00:07:45.015 --> 00:07:48.769
then the maximum entropy
distribution is to say that each of

00:07:48.769 --> 00:07:51.720
those probabilities is one sixth.

00:07:51.720 --> 00:07:56.690
Uniformity is the maximum entropy
distribution in categorical distributions.

00:07:58.720 --> 00:08:01.888
But that's too uniform given
what happened in the data,

00:08:01.888 --> 00:08:04.930
that nouns are much more
common than verbs in our data.

00:08:04.930 --> 00:08:10.797
And so we're going to add a feature that
has value 1 if they take as a noun and

00:08:10.797 --> 00:08:12.019
0 otherwise.

00:08:12.019 --> 00:08:17.490
Well, the expected value of that
feature is 32 over 36, 8/9ths.

00:08:17.490 --> 00:08:21.820
That's just using the data
from the previous slide.

00:08:21.820 --> 00:08:26.820
So if we add that constraint and then say,
what's the Maxent entropy distribution.

00:08:26.820 --> 00:08:31.640
It's going to set the probabilities so
that this constraint is satisfied.

00:08:31.640 --> 00:08:34.766
The sum of these
probabilities equals 32/36.

00:08:34.766 --> 00:08:39.474
But within that category probability
mass is still going to be distributed

00:08:39.474 --> 00:08:44.235
uniformly because that's
the Maxent entropy distribution.

00:08:44.235 --> 00:08:46.775
Similarly, within the verb classes,

00:08:46.775 --> 00:08:50.142
you're going to get the remaining
probability mass distributed uniformly.

00:08:50.142 --> 00:08:53.832
At that point we might notice that proper

00:08:53.832 --> 00:08:58.472
nouns are much more frequent than common
nouns so we can add a second constraint

00:08:58.472 --> 00:09:03.072
that is a feature that has value
one when the tag is a proper noun.

00:09:03.072 --> 00:09:08.011
And the expectation of that feature
is 2/3 of the data of proper nouns,

00:09:08.011 --> 00:09:09.711
as we noted before.

00:09:09.711 --> 00:09:11.971
If we put that into the model,

00:09:11.971 --> 00:09:17.091
we then get the expectation of
that feature observed that 2/3 of

00:09:17.091 --> 00:09:22.950
the probability mask goes to proper nouns,
which is again distributed uniformly.

00:09:22.950 --> 00:09:26.736
We still have the feature that 32/36 of

00:09:26.736 --> 00:09:30.839
the probability mass goes
to some kind of noun.

00:09:30.839 --> 00:09:34.930
So the remainder is distributed
evenly among the other nouns and

00:09:34.930 --> 00:09:36.149
this is as before.

00:09:36.149 --> 00:09:38.549
Now of course we can keep
on refining our model.

00:09:38.549 --> 00:09:43.158
We could for
example add a feature to say that singular

00:09:43.158 --> 00:09:47.159
nouns comprised a certain
amount of the data.

00:09:47.159 --> 00:09:50.845
So singular nouns comprised
18/36 of the data and

00:09:50.845 --> 00:09:56.213
we can add that constraint in, and
eventually if we added enough constraints,

00:09:56.213 --> 00:10:01.920
we'd force the distribution to be exactly
the same as the empirical distribution.

00:10:03.210 --> 00:10:07.780
It's very easy to see that Maxent
entropy models are convex models.

00:10:07.780 --> 00:10:09.620
So what's the idea of a convex function?

00:10:09.620 --> 00:10:13.888
The idea of a convex function
here f is that if you take

00:10:13.888 --> 00:10:18.641
the function value at a weighted
mean of any set of points,

00:10:18.641 --> 00:10:23.685
then that function value is greater
than what you do if you take

00:10:23.685 --> 00:10:29.117
what function value at those points and
then weight those function

00:10:29.117 --> 00:10:34.355
values that you've found that
we're kind of above it up here and

00:10:34.355 --> 00:10:38.138
so that's distinguished
from something like

00:10:38.138 --> 00:10:43.109
a non convex function where you
can have these local minima.

00:10:43.109 --> 00:10:48.826
And so then the function value at this
average point is beneath what the function

00:10:48.826 --> 00:10:53.880
value is if you just take the average
of the values at the 2 red points.

00:10:55.470 --> 00:10:59.580
Convexity guarantees that a function
has a single global maximum.

00:10:59.580 --> 00:11:03.300
Because any higher points of
the function are greedily reachable.

00:11:03.300 --> 00:11:06.540
In the maximum entropy formulation,

00:11:06.540 --> 00:11:10.140
it's easy to see that we
have a convex function.

00:11:10.140 --> 00:11:15.370
So, we can start off by showing that
the entropy function is convex.

00:11:15.370 --> 00:11:21.890
And so, we have the minus x
log x is a convex function,

00:11:21.890 --> 00:11:26.020
and so therefore we take a sum and
we drew it over here.

00:11:26.020 --> 00:11:27.750
That's a convex function.

00:11:27.750 --> 00:11:32.540
If we then take sum of convex functions,
that's always convex.

00:11:32.540 --> 00:11:36.329
Then after that what we're doing is
adding constraints to the function.

00:11:36.329 --> 00:11:41.109
And so the feasible region of
a constrained entropy function is

00:11:41.109 --> 00:11:45.360
a linear subspace of it which
then also has to be convex.

00:11:45.360 --> 00:11:50.523
So when we put a linear subspace
constraint right here, throughout

00:11:50.523 --> 00:11:56.150
entropy surface that we've still got
a convex function coming out of it.

00:11:56.150 --> 00:12:00.521
And so, I'm not going to show it here but
the same is true for our original maximum

00:12:00.521 --> 00:12:03.630
likelihood presentation that
we get a convex function.

00:12:05.529 --> 00:12:10.502
Okay, well I hope you now understand where
the name, Maxent Models, comes from,

00:12:10.502 --> 00:12:14.000
and what the key idea of
the maximum entropy principle is-

